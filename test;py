import requests
import csv
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def checkResponse(response):
    return response.status_code == 200;

def getParsedHTML(url):
    response = requests.get(url)
    response.encoding = response.apparent_encoding
    # Check if the request was successful (status code 200)
    if not checkResponse(response):
        print("Failed to retrieve webpage" + url)
        return None
    # Parse the HTML content of the page using BeautifulSoup
    return BeautifulSoup(response.text, 'html.parser')

def printSoupToFile(soup):
    with open("out.txt", "a") as file:
        print(soup.prettify(), file = file)
        
def scrapeOpenAIBlogLinks():
    base = 'https://openai.com/blog?page='
    pageNum = 7
    links_all = []
    for page in range(1, pageNum):
        url = base + str(page)
        soup = getParsedHTML(url)
        if soup == None: continue
        # Extract all the blog post links on the page
        postLinks = soup.find_all('a', type = "blog-details")
        for link in postLinks:
            link = [urljoin(url, link.get('href')), None]
            links_all.append(link)
    return links_all

def scrapeOpenAIBlogArticle(url):
    soup = getParsedHTML(url)
    if soup == None: return None
    title = soup.find_all('title')[0].text
    textClasses = soup.find_all(class_ = "ui-richtext")
    text = ""
    for textClass in textClasses:
        text += textClass.get_text().replace('\xa0', ' ') + "\n\n"
    return [title, text]

def scrapeOpenAIBlogArticles():
    OpenAIBlogLinks = scrapeOpenAIBlogLinks()
    result = []
    for [blogUrl, articleUrl] in OpenAIBlogLinks:
        result.append(scrapeOpenAIBlogArticle(blogUrl))
    return result
        
def scrapeOpenAIResearchLinks():
    base = 'https://openai.com/research?page='
    pageNum = 9
    links_all = []
    for page in range(1,pageNum):
        url = base + str(page)
        soup = getParsedHTML(url)
        if soup == None: continue
        printSoupToFile(soup)
        # Extract all the blog post links on the page
        postClasses = soup.find_all(class_ = "cols-container relative")
        for postClass in postClasses:
            posts = postClass.find_all(class_ = "ui-link group f-ui-1 inline-block relative ui-link--inherit relative")
            articleLink, postLink = None, None
            for item in posts:
                postLink = urljoin(url, item.get('href'))
            articles = postClass.find_all(class_ = "ui-link group f-ui-1 inline-block relative ui-link--inherit ml-auto shrink-0 relative self-start")
            for item in articles:
                articleLink = item.get('href')
            links_all.append([postLink,articleLink])
    return links_all

def scrapeOpenAIResearchArticle(url):
    soup = getParsedHTML(url)
    if soup == None: return None
    title = soup.find_all('title')[0].text
    textClasses = soup.find_all(class_ = "ui-richtext")
    text = ""
    for textClass in textClasses:
        text += textClass.get_text().replace('\xa0', ' ') + "\n\n"
    return [title, text]

def scrapeOpenAIResearchArticles():
    OpenAIResearchLinks = scrapeOpenAIResearchLinks()
    result = []
    for [researchUrl, articleUrl] in OpenAIResearchLinks:
        result.append(scrapeOpenAIResearchArticle(researchUrl))
    return result

def print_list_to_csv(csvName, source, links, content):
    with open(csvName, 'a', newline='') as file:
        writer = csv.writer(file)
        header = ["source", "postLink", "researchLink", "title", "text"]
        writer.writerow(header)
        rows =  [[source] + row1 + row2 for row1, row2 in zip(links, content)]
        writer.writerows(rows)

if __name__ == "__main__":
    csvName = "result.csv"
    print_list_to_csv(csvName, "OpenAIBlog", scrapeOpenAIBlogLinks(), scrapeOpenAIBlogArticles())
    print_list_to_csv(csvName, "OpenAIResearch", scrapeOpenAIResearchLinks(), scrapeOpenAIResearchArticles())
    
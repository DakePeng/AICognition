8
1
0
2

r
p
A
3
2

]

G
L
.
s
c
[

2
v
0
2
7
3
0
.
4
0
8
1
:
v
i
X
r
a

GottaLearnFast:ANewBenchmarkforGeneralizationinRLAlexNichol,VickiPfau,ChristopherHesse,OlegKlimov,JohnSchulmanOpenAI{alex,vickipfau,csh,oleg,joschu}@openai.comAbstractInthisreport,wepresentanewreinforcementlearning(RL)benchmarkbasedontheSonictheHedgehogTMvideogamefranchise.Thisbenchmarkisintendedtomea-suretheperformanceoftransferlearningandfew-shotlearningalgorithmsintheRLdomain.Wealsopresentandevaluatesomebaselinealgorithmsonthenewbenchmark.1MotivationInthepastfewyears,ithasbecomeclearthatdeepreinforcementlearningcansolvediﬃcult,high-dimensionalproblemswhengivenagoodrewardfunctionandunlimitedtimetointeractwiththeenvironment.However,whilethiskindoflearningisakeyaspectofintelligence,itisnottheonlyone.Ideally,intelligentagentswouldalsobeabletogeneralizebetweentasks,usingpriorexperiencetopickupnewskillsmorequickly.Inthisreport,weintroduceanewbenchmarkthatwedesignedtomakeiteasierforresearcherstodevelopandtestRLalgorithmswiththiskindofcapability.MostpopularRLbenchmarkssuchastheALE[1]arenotidealfortestinggeneralizationbetweensimilartasks.Asaresult,RLresearchtendsto“trainonthetestset”,boastinganalgorithm’sﬁnalperformanceonthesameenvironment(s)itwastrainedon.Fortheﬁeldtoadvancetowardsalgorithmswithbettergeneralizationproperties,weneedRLbench-markswithpropersplitsbetween“train”and“test”environments,similartosupervisedlearningdatasets.Ourbenchmarkhassuchasplit,makingitidealformeasuringcross-taskgeneralization.Oneinterestingapplicationofcross-taskgeneralizationisfew-shotlearning.Recently,supervisedfew-shotlearningalgorithmshaveimprovedbyleapsandbounds[2]–[4].Thisprogresshashingedontheavailabilityofgoodmeta-learningdatasetssuchasOmniglot[5]andMini-ImageNet[6].Thus,ifwewantbetterfew-shotRLalgorithms,itmakessensetoconstructasimilarkindofdatasetforRL.Ourbenchmarkisdesignedtobeameta-learningdataset,consistingofmanysimilartaskssampledfromasingletaskdistribution.Thus,itisasuitabletestbedforfew-shotRLalgorithms.Beyondfew-shotlearning,therearemanyotherapplicationsofcross-taskgeneralizationthatrequiretherightkindofbenchmark.Forexample,youmightwantanRLalgorithmtolearnhowtoexploreinnewenvironments.Ourbenchmarkposesafairlychallengingexplorationproblem,andthetrain/testsplitpresentsauniqueopportunitytolearnhowtoexploreonsomelevelsandtransferthisabilitytootherlevels.1 
 
 
 
 
 
2RelatedWorkOurGymRetroproject,asdetailedinSection3.1,isrelatedtoboththeRetroLearningEn-vironment(RLE)[7]andtheArcadeLearningEnvironment(ALE)[1].Unliketheseprojects,however,GymRetroaimstobeﬂexibleandeasytoextend,makingitstraightforwardtocreateahugenumberofRLenvironments.Ourbenchmarkisrelatedtoothermeta-learningdatasetslikeOmniglot[5]andMini-ImageNet[6].Inparticular,ourbenchmarkisintendedtoservethesamepurposeforRLasdatasetslikeOmniglotserveforsupervisedlearning.OurbaselinesinSection4exploretheabilityofRLalgorithmstotransferbetweenvideogameenvironments.Severalpriorworkshavereportedpositivetransferresultsinthevideogamesetting:•Parisottoetal.[8]observedthatpre-trainingoncertainAtarigamescouldincreaseanetwork’slearningspeedonotherAtarigames.•Rusuetal.[9]proposedanewarchitecturefortransferlearningcalledprogressivenetworks,andshowedthatitcouldboostlearningspeedacrossavarietyofpreviouslyunseenAtarigames.•Pathaketal.[10]foundthatanexploratoryagenttrainedononelevelofSuperMarioBros.couldbeusedtoboostperformanceontwootherlevels.•Fernandoetal.[11]foundthattheirPathNetalgorithmincreasedlearningspeedonaveragewhentransferringfromoneAtarigametoanother.•Higginsetal.[12]usedanunsupervisedvisionobjectivetoproducerobustfeaturesforapolicy,andfoundthatthispolicywasabletotransfertopreviouslyunseenvisiontasksinDeepMindLab[13]andMuJoCo[14].InpreviousliteratureontransferlearninginRL,therearetwocommonevaluationtech-niques:evaluationonsynthetictasks,andevaluationontheALE.Theformerevaluationtechniqueisratheradhocandmakesithardtocomparediﬀerentalgorithms,whilethelattertypicallyrevealsfairlysmallgainsinsamplecomplexity.OneproblemwiththeALEinparticularisthatallthegamesarequitediﬀerent,meaningthatitmaynotbepossibletogetlargeimprovementsfromtransferlearning.Ideally,furtherresearchintransferlearningwouldbeabletoleverageastandardizedbenchmarkthatisdiﬃcultliketheALEbutrichwithsimilarenvironmentslikewell-craftedsynthetictasks.Wedesignedourproposedbenchmarktosatisfybothcriteria.3TheSonicBenchmarkThissectiondescribestheSonicbenchmarkindetail.Eachsubsectionfocusesonadiﬀerentaspectofthebenchmark,rangingfromtechnicaldetailstohigh-leveldesignfeatures.23.1GymRetroUnderlyingtheSonicbenchmarkisGymRetro,aprojectaimedatcreatingRLenvironmentsfromvariousemulatedvideogames.AtthecoreofGymRetroisthegym-retroPythonpackage,whichexposesemulatedgamesasGym[15]environments.LikeRLE[7],gym-retrousesthelibretroAPI1tointerfacewithgameemulators,makingitveryeasytoaddnewemulatorstogym-retro.Thegym-retropackageincludesadatasetofgames.EachgameinthedatasetconsistsofaROM,oneormoresavestates,oneormorescenarios,andadataﬁle.Herearehigh-leveldescriptionsofeachofthesecomponents:•ROM–thedataandcodethatmakeupagame;loadedbyanemulatortoplaythatgame.•Savestate–asnapshotoftheconsole’sstateatsomepointinthegame.Forexample,asavestatecouldbecreatedforthebeginningofeachlevel.•Dataﬁle–aﬁledescribingwherevariouspiecesofinformationarestoredinconsolememory.Forexample,adataﬁlemightindicatewherethescoreislocated.•Scenario–adescriptionofdoneconditionsandrewardfunctions.Ascenarioﬁlecanreferenceﬁeldsfromthedataﬁle.3.2TheSonicVideoGameFigure1:ScreenshotsfromSonic3&Knuckles.Left:asituationwheretheplayercanbeshotintotheairbyutilizinganobjectwithlever-likedynamics(MushroomHillZone,Act2).Middle:adoorthatopenswhentheplayerjumpsonabutton(HydrocityZone,Act1).Right:aswingthattheplayermustjumpfromatexactlytherighttimetoreachahighplatform(MushroomHillZone,Act2).Inthisbenchmark,weusethreesimilargames:SonicTheHedgehogTM,SonicTheHedgehogTM2,andSonic3&Knuckles.Allofthesegameshaveverysimilarrulesandcon-trols,althoughtherearesubtlediﬀerencesbetweenthem(e.g.Sonic3&Knucklesincludessomeextracontrolsandcharacters).Weusemultiplegamestogetasmanyenvironmentsforourdatasetaspossible.1https://www.libretro.com/index.php/api3EachSonicgameisdividedupintozones,andeachzoneisfurtherdividedupintoacts.Whiletherulesandoverarchingobjectiveremainthesamethroughouttheentiregame,eachzonehasauniquesetoftexturesandobjects.Diﬀerentactswithinazonetendtosharethesetexturesandobjects,butdiﬀerinspatiallayout.Wewillrefertoa(ROM,zone,act)tupleasa“level”.TheSonicgamesprovidearichsetofchallengesfortheplayer.Forexample,somezonesincludeplatformsthattheplayermustjumponinordertoopendoors.Otherzonesrequiretheplayertoﬁrstjumponalevertosendaprojectileintotheair,thenwaitfortheprojectiletofallbackonthelevertosendtheplayeroversomesortofobstacle.OnezoneevenhasaswingthattheplayermustjumpoﬀofataprecisetimeinordertolaunchSonicuptoahigherplatform.ExamplesofthesechallengesarepresentedinFigure1.3.3GamesandLevelsOurbenchmarkconsistsofatotalof58savestatestakenfromthreediﬀerentgames,whereeachofthesesavestateshastheplayeratthebeginningofadiﬀerentlevel.Anumberofactsfromtheoriginalgameswerenotusedbecausetheycontainedonlybossﬁghtsorbecausetheywerenotcompatiblewithourrewardfunction.Wesplitthetestsetbyrandomlychoosingzoneswithmorethanoneactandthenrandomlychoosinganactfromeachselectedzone.Inthissetup,thetestsetcontainsmostlyobjectsandtexturespresentinthetrainingset,butwithdiﬀerentlayouts.Thetestlevelsarelistedinthefollowingtable:ROMZoneActSonicTheHedgehogSpringYardZone1SonicTheHedgehogGreenHillZone2SonicTheHedgehogStarLightZone3SonicTheHedgehogScrapBrainZone1SonicTheHedgehog2MetropolisZone3SonicTheHedgehog2HillTopZone2SonicTheHedgehog2CasinoNightZone2Sonic3&KnucklesLavaReefZone1Sonic3&KnucklesFlyingBatteryZone2Sonic3&KnucklesHydrocityZone1Sonic3&KnucklesAngelIslandZone23.4FrameSkipThestep()methodonrawgym-retroenvironmentsprogressesthegamebyroughly160thofasecond.However,followingcommonpracticeforALEenvironments,werequiretheuseofaframeskip[16]of4.Thus,fromhereonout,wewillusetimestepsasthemainunitofmeasuringin-gametime.Withaframeskipof4,atimesteprepresentsroughly115thofasecond.WebelievethatthisismorethanenoughtemporalresolutiontoplaySonicwell.Moreover,sincedeterministicenvironmentsareoftensusceptibletotrivialscriptedso-lutions[17],werequiretheuseofastochastic“stickyframeskip”.Stickyframeskipadds4asmallamountofrandomnesstotheactionstakenbytheagent;itdoesnotdirectlyalterobservationsorrewards.Likestandardframeskip,stickyframeskipappliesnactionsover4nframes.However,foreachaction,wedelayitbyoneframewithprobability0.25,applyingthepreviousactionforthatframeinstead.Thefollowingdiagramshowsanexampleofanactionsequencewithstickyframeskip:3.5EpisodeBoundariesExperienceinthegameisdividedupintoepisodes,whichroughlycorrespondtolives.Attheendofeachepisode,theenvironmentisresettoitsoriginalsavestate.Episodescanendonthreeconditions:•Theplayercompletesalevelsuccessfully.Inthisbenchmark,completingalevelcorre-spondstopassingacertainhorizontaloﬀsetwithinthelevel.•Theplayerlosesalife.•4500timestepshaveelapsedinthecurrentepisode.Thisamountstoroughly5minutesofin-gametime.Theenvironmentshouldonlyberesetifoneoftheaforementioneddoneconditionsismet.AgentsshouldnotusespecialAPIstotelltheenvironmenttostartanewepisodeearly.Notethatourbenchmarkomitsthebossﬁghtsthatoftentakeplaceattheendofalevel.Forlevelswithbossﬁghts,ourdoneconditionisdeﬁnedasahorizontaloﬀsetthattheagentmustreachbeforethebossﬁght.Althoughbossﬁghtscouldbeaninterestingproblemtosolve,theyarefairlydiﬀerentfromtherestofthegame.Thus,wechosenottoincludethemsothatwecouldfocusmoreonexploration,navigation,andspeed.3.6ObservationsAgym-retroenvironmentproducesanobservationatthebeginningofeverytimestep.Thisobservationisalwaysa24-bitRGBimage,butthedimensionsvarybygame.ForSonic,thescreenimagesare320pixelswideand224pixelstall.3.7ActionsAteverytimestep,anagentproducesanactionrepresentingacombinationofbuttonsonthegameconsole.Actionsareencodedasbinaryvectors,where1means“pressed”and0means“notpressed”.ForSegaGenesisgames,theactionspacecontainsthefollowingbuttons:B,A,MODE,START,UP,DOWN,LEFT,RIGHT,C,Y,X,Z.5AsmallsubsetofallpossiblebuttoncombinationsmakessenseinSonic.Infact,thereareonlyeightessentialbuttoncombinations:{{},{LEFT},{RIGHT},{LEFT,DOWN},{RIGHT,DOWN},{DOWN},{DOWN,B},{B}}TheUPbuttonisalsousefulonoccasion,butforthemostpartitcanbeignored.3.8RewardsDuringanepisode,agentsarerewardedsuchthatthecumulativerewardatanypointintimeisproportionaltothehorizontaloﬀsetfromtheplayer’sinitialposition.Thus,goingrightalwaysyieldsapositivereward,whilegoingleftalwaysyieldsanegativereward.Thisrewardfunctionisconsistentwithourdonecondition,whichisbasedonthehorizontaloﬀsetinthelevel.Therewardconsistsoftwocomponents:ahorizontaloﬀset,andacompletionbonus.Thehorizontaloﬀsetrewardisnormalizedperlevelsothatanagent’stotalrewardwillbe9000ifitreachesthepredeﬁnedhorizontaloﬀsetthatmarkstheendofthelevel.Thisway,itiseasytocomparescoresacrosslevelsofvaryinglength.Thecompletionbonusis1000forreachingtheendofthelevelinstantly,anddropslinearlytozeroat4500timesteps.Thisway,agentsareencouragedtoﬁnishlevelsasfastaspossible2.Sincetherewardfunctionisdense,RLalgorithmslikePPO[18]andDQN[16]caneasilymakeprogressonnewlevels.However,theimmediaterewardscanbedeceptive;itisoftennecessarytogobackwardsforprolongedamountsoftime(Figure2).InourRLbaselines,weuserewardpreprocessingsothatouragentsarenotpunishedforgoingbackwards.Note,however,thatthepreprocessedrewardstillgivesnoinformationaboutwhenorhowanagentshouldgobackwards.3.9EvaluationIngeneral,allbenchmarksmustprovidesomekindofperformancemetric.ForSonic,thismetrictakestheformofa“meanscore”asmeasuredacrossallthelevelsinthetestset.HerearethegeneralstepsforevaluatinganalgorithmonSonic:1.Attrainingtime,usethetrainingsetasmuchoraslittleasyoulike.2.Attesttime,playeachtestlevelfor1milliontimesteps.Playeachtestlevelsepa-rately;donotallowinformationtoﬂowbetweentestlevels.Multiplecopiesofeachenvironmentmaybeused(asisdoneinalgorithmslikeA3C[19]).3.Foreach1milliontimestepevaluation,averagethetotalrewardperepisodeacrossallepisodes.Thisgivesaper-levelmeanscore.4.Averagethemeanscoresforallthetestlevels,givinganaggregatemetricofperfor-mance.2Inpractice,RLagentsmaynotbeabletoleverageabonusattheendofanepisodeduetoadiscountfactor.6Figure2:AtraceofasuccessfulpaththroughtheﬁrstpartofLabyrinthZone,Act2inSonicTheHedgehogTM.Intheinitialgreensegment,theagentismovingrightwards,gettingpositivereward.Intheredsegment,theagentmustmovetotheleft,gettingnegativereward.Duringtheorangesegment,theagentisonceagainmovingright,butitscumulativerewardisstillnotashighasitwasaftertheinitialgreensegment.Intheﬁnalgreensegment,theagentisﬁnallyimprovingitscumulativerewardpasttheinitialgreensegment.Foranaverageplayer,ittakes20to30secondstogetthroughtheredandorangesegments.Themostimportantaspectofthisprocedureisthetimesteplimitforeachtestlevel.Intheinﬁnite-timestepregime,thereisnostrongreasontobelievethatmeta-learningortransferlearningisnecessary.However,inthelimited-timestepregime,transferlearningmaybenecessarytoachievegoodperformancequickly.WeaimforthisversionoftheSonicbenchmarktobeeasierthanzero-shotlearningbutharderthan∞-shotlearning.1milliontimestepswaschosenasthetimesteplimitbecausemodernRLalgorithmscanmakesomeprogressinthisamountoftime.4BaselinesInthissection,wepresentseveralbaselinelearningalgorithmsanddiscusstheirperformanceonthebenchmark.Ourbaselinesincludehumanplayers,severalmethodsthatdonotmakeuseofthetrainingset,andasimpletransferlearningapproachconsistingofjointtrainingfollowedbyﬁnetuning.Table1givestheaggregatescoresforeachofthebaselines,andFigure3comparesthebaselines’aggregatelearningcurves.4.1HumansForthehumanbaseline,wehadfourtestsubjectsplayeachtestlevelforonehour.Beforeseeingthetestlevels,eachsubjecthadtwohourstopracticeonthetraininglevels.Table7inAppendixCshowsaveragehumanscoresoverthecourseofanhour.7Table1:Aggregatetestscoresforeachofthebaselinealgorithms.AlgorithmScoreRainbow2748.6±102.2JERK1904.0±21.9PPO1488.8±42.8PPO(joint)3127.9±116.9Rainbow(joint)2969.2±170.2Human7438.2±624.20.00.20.40.60.81.0Timesteps (millions)1000200030004000500060007000Human AverageRainbowJERKPPOPPO (joint)Rainbow (joint)Figure3:Themeanlearningcurvesforallthebaselinesacrossallthetestlevels.Everycurveisanaverageoverthreeruns.They-axisrepresentsinstantaneousscore,notaverageovertraining.4.2RainbowDeepQ-learning(DQN)[16]isapopularclassofalgorithmsforreinforcementlearninginhigh-dimensionalenvironmentslikevideogames.WeuseaspeciﬁcvariantofDQN,namelyRainbow[20],whichperformsparticularlywellontheALE.Weretainthearchitectureandmostofthehyper-parametersfrom[20],withafewsmallchanges.First,wesetVmax=200toaccountforSonic’srewardscale.Second,weuseareplaybuﬀersizeof0.5Minsteadof1Mtolowerthealgorithm’smemoryconsumption.Third,wedonotusehyper-parameterschedules;rather,wesimplyusetheinitialvaluesoftheschedulesfrom[20].SinceDQNtendstoworkbestwithasmall,discreteactionspace,weuseanactionspacecontainingsevenactions:{{LEFT},{RIGHT},{LEFT,DOWN},{RIGHT,DOWN}{DOWN},{DOWN,B},{B}}Weuseanenvironmentwrapperthatrewardstheagentbasedondeltasinthemaximum8x-position.Thisway,theagentisrewardedforgettingfurtherthanithasbeenbefore(inthecurrentepisode),butitisnotpunishedforbacktrackinginthelevel.Thisrewardpreprocessinggivesasizableperformanceboost.Table2inAppendixCshowsRainbow’sscoresforeachtestlevel.4.3JERK:AScriptedApproachInthissection,wepresentasimplealgorithmthatachieveshighrewardsonthebenchmarkwithoutusinganydeeplearning.Thisalgorithmcompletelyignoresobservationsandinsteadlookssolelyatrewards.WecallthisalgorithmJustEnoughRetainedKnowledge(JERK).WenotethatJERKislooselyrelatedtoTheBrute[21],asimplealgorithmthatﬁndsgoodtrajectoriesindeterministicAtarienvironmentswithoutleveraginganydeeplearning.Algorithm1inAppendixAdescribesJERKindetail.Themainideaistoexploreusingasimplealgorithm,thentoreplaythebestactionsequencesmoreandmorefrequentlyastrainingprogresses.Sincetheenvironmentisstochastic,itisneverclearwhichactionsequenceisthebesttoreplay.Thus,eachactionsequencehasarunningmeanofitsrewards.Table3inAppendixCshowsJERK’sscoresforeachtestlevel.WenotethatJERKactuallyperformsbetterthanregularPPO,whichislikelyduetoJERK’sperfectmemoryanditstailoredexplorationstrategy.4.4PPOProximalPolicyOptimization(PPO)[18]isapolicygradientalgorithmwhichperformswellontheALE.Forthisbaseline,werunPPOindividuallyoneachofthetestlevels.ForPPOweusethesameactionandobservationspacesasforRainbow,aswellasthesamerewardpreprocessing.Forourexperiments,wescaledtherewardsbyasmallconstantfactorinordertobringtheadvantagestoasuitablerangeforneuralnetworks.ThisissimilartohowwesetVmaxforRainbow.TheCNNarchitectureisthesameastheoneusedin[18]forAtari.Weusethefollowinghyper-parametersforPPO:Hyper-parameterValueWorkers1Horizon8192Epochs4Minibatchsize8192Discount(γ)0.99GAEparameter(λ)0.95Clippingparameter((cid:15))0.2Entropycoeﬀ.0.001Rewardscale0.005Table4inAppendixCshowsPPO’sscoresforeachtestlevel.94.5JointPPOWhileSection4.4evaluatesPPOwithnometa-learning,thissectionexplorestheabilityofPPOtotransferfromthetraininglevelstothetestlevels.Todothis,weuseasimplejointtrainingalgorithm3,whereinwetrainapolicyonallthetraininglevelsandthenuseitasaninitializationonthetestlevels.Duringmeta-training,wetrainasinglepolicytoplayeverylevelinthetrainingset.Speciﬁcally,werun188parallelworkers,eachofwhichisassignedalevelfromthetrainingset.Ateverygradientstep,alltheworkersaveragetheirgradientstogether,ensuringthatthepolicyistrainedevenlyacrosstheentiretrainingset.Thistrainingprocessrequireshundredsofmillionsoftimestepstoconverge(seeFigure4),sincethepolicyisbeingforcedtolearnalotmorethanasinglelevel.Besidesthediﬀerenttrainingsetup,weusethesamehyper-parametersasforregularPPO.Oncethejointpolicyhasbeentrainedonallthetraininglevels,weﬁne-tuneitoneachtestlevelunderthestandardevaluationrules.Inessence,thetrainingsetprovidesaninitializationthatispluggedinwhenevaluatingonthetestset.Asidefromtheinitialization,nothingischangedfromtheevaluationprocedureusedforSection4.4.Figure4showsthat,afterroughly50milliontimestepsofjointtraining,furtherim-provementonthetrainingsetstopsleadingtobetterperformanceonthetestset.Thiscanbethoughtofasthepointwherethemodelstartstooverﬁt.Theﬁgurealsoshowsthatzero-shotperformancedoesnotincreasemuchaftertheﬁrstfewmilliontimestepsofjointtraining.0100200300400Timesteps ×106010002000300040005000ScoreZero-shot, trainFine-tuning, average, testFine-tuning, final, testZero-shot, testFigure4:IntermediateperformanceduringtheprocessofjointtrainingaPPOmodel.Thex-axiscorrespondstotimestepsintothejointtrainingprocess.Thezero-shotcurvesweredenselysampledduringtraining,whiletheﬁne-tuningcurvesweresampledperiodically.Table5inAppendixCshowsJointPPO’sscoresforeachtestlevel.Table9inAppendixDshowsJointPPO’sﬁnalscoresforeachtraininglevel.TheresultingtestperformanceissuperiortothatofRainbow,andisroughly100%betterthanthatofregularPPO.Thus,itisclearthatsomekindofusefulinformationisbeingtransferredfromthetraininglevelstothetestlevels.3WealsotriedaversionofReptile[22],butfoundthatityieldedworseresults.104.6JointRainbowSinceRainbowoutperformsPPOwithnojointtraining,itisnaturaltoaskifJointRainbowanalogouslyoutperformsJointPPO.Surprisingly,ourexperimentsindicatethatthisisnotthecase.TotrainasingleRainbowmodelontheentiretrainingset,weuseamulti-machinetrain-ingsetupwith32GPUs.EachGPUcorrespondstoasingleworker,whereeachworkerhasitsownreplaybuﬀerandeightenvironments.Theenvironmentsareall“jointenviron-ments”,meaningthattheysampleanewtraininglevelatthebeginningofeveryepisode.EachworkerrunsthealgorithmdescribedinAlgorithm2inAppendixA.Besidestheunusualbatchsizeanddistributedworkersetup,allthehyper-parametersarekeptthesameasfortheregularRainbowexperiment.Table6inAppendixCshowstheperformanceofﬁne-tuningoneverytestlevel.Table8inAppendixDshowstheperformanceofthejointlytrainedmodeloneverytraininglevel.5DiscussionWehavepresentedanewreinforcementlearningbenchmarkandusedittoevaluateseveralbaselinealgorithms.Ourresultsleavealotofroomforimprovement,especiallysinceourbesttransferlearningresultsarenotmuchbetterthanourbestresultslearningfromscratch.Also,ourresultsarenowhereclosetothemaximumachievablescore(which,bydesign,issomewherebetween9000and10000).Nowthatthebenchmarkandbaselineresultshavebeenlaidout,therearemanydirec-tionstotakefurtherresearch.Herearesomequestionsthatfutureresearchmightseektoanswer:•Howmuchcanexplorationobjectiveshelptrainingperformanceonthebenchmark?•Cantransferlearningbeimprovedusingdataaugmentation?•Isitpossibletoimproveperformanceonthetestsetusingagoodfeaturerepresentationlearnedonthetrainingset(likeinHigginsetal.[12])?•Candiﬀerentarchitectures(e.g.Transformers[23]andResNets[24])beusedtoimprovetrainingand/ortestperformance?WhilewebelievetheSonicbenchmarkisastepintherightdirection,itmaynotbesuﬃcientforexploringmeta-learning,transferlearning,andgeneralizationinRL.Herearesomepossibleproblemswiththisbenchmark,whichwillonlybeprovenordisprovenoncemoreworkhasbeendone:•ItmaybepossibletosolveaSoniclevelinmanyfewerthan1Mtimestepswithoutanytransferlearning.•Sonic-speciﬁchacksmayoutperformgeneralmeta-learningapproaches.•ExplorationstrategiesthatworkwellinSonicmaynotgeneralizebeyondSonic.11•MasteringaSoniclevelinvolvessomedegreeofmemorization.Algorithmswhicharegoodatfew-shotmemorizationmaynotbegoodatothertasks.References[1]M.G.Bellemare,Y.Naddaf,J.Veness,andM.Bowling,“TheArcadeLearningEnvironment:Anevaluationplatformforgeneralagents,”JournalofArtiﬁcialIntelligenceResearch,vol.47,pp.253–279,Jun.2013.[2]A.Santoro,S.Bartunov,M.Botvinick,D.Wierstra,andT.Lillicrap,“Meta-learningwithmemory-augmentedneuralnetworks,”inInternationalconferenceonmachinelearning,2016,pp.1842–1850.[3]C.Finn,P.Abbeel,andS.Levine,“Model-agnosticmeta-learningforfastadaptationofdeepnetworks,”2017.eprint:arXiv:1703.03400.[4]N.Mishra,M.Rohaninejad,X.Chen,andP.Abbeel,“Meta-learningwithtemporalconvo-lutions,”2017.eprint:arXiv:1707.03141.[5]B.M.Lake,R.Salakhutdinov,J.Gross,andJ.B.Tenenbaum,“Oneshotlearningofsimplevisualconcepts,”inConferenceoftheCognitiveScienceSociety(CogSci),2011.[6]O.Vinyals,C.Blundell,T.Lillicrap,K.Kavukcuoglu,andD.Wierstra,“Matchingnetworksforoneshotlearning,”inAdvancesinNeuralInformationProcessingSystems29,D.D.Lee,M.Sugiyama,U.V.Luxburg,I.Guyon,andR.Garnett,Eds.,CurranAssociates,Inc.,2016,pp.3630–3638.[7]N.Bhonker,S.Rozenberg,andI.Hubara,“PlayingSNESintheRetroLearningEnviron-ment,”2016.eprint:arXiv:1611.02205.[8]E.Parisotto,J.L.Ba,andR.Salakhutdinov,“Actor-Mimic:Deepmultitaskandtransferreinforcementlearning,”2015.eprint:arXiv:1511.06342.[9]A.A.Rusu,N.C.Rabinowitz,G.Desjardins,H.Soyer,J.Kirkpatrick,K.Kavukcuoglu,R.Pascanu,andR.Hadsell,“Progressiveneuralnetworks,”2016.eprint:arXiv:1606.04671.[10]D.Pathak,P.Agrawal,A.A.Efros,andT.Darrell,“Curiosity-drivenexplorationbyself-supervisedprediction,”2017.eprint:arXiv:1705.05363.[11]C.Fernando,D.Banarse,C.Blundell,Y.Zwols,D.Ha,A.A.Rusu,A.Pritzel,andD.Wierstra,“PathNet:Evolutionchannelsgradientdescentinsuperneuralnetworks,”2017.eprint:arXiv:1701.08734.[12]I.Higgins,A.Pal,A.A.Rusu,L.Matthey,C.P.Burgess,A.Pritzel,M.Botvinick,C.Blundell,andA.Lerchner,“DARLA:Improvingzero-shottransferinreinforcementlearning,”2017.eprint:arXiv:1707.08475.[13]C.Beattie,J.Z.Leibo,D.Teplyashin,T.Ward,M.Wainwright,H.K¨uttler,A.Lefrancq,S.Green,V.Vald´es,A.Sadik,J.Schrittwieser,K.Anderson,S.York,M.Cant,A.Cain,A.Bolton,S.Gaﬀney,H.King,D.Hassabis,S.Legg,andS.Petersen,“DeepMindLab,”2016.eprint:arXiv:1612.03801.[14]E.Todorov,T.Erez,andY.Tassa,“MuJoCo:Aphysicsengineformodel-basedcontrol.,”inIROS,IEEE,2012,pp.5026–5033.12[15]G.Brockman,V.Cheung,L.Pettersson,J.Schneider,J.Schulman,J.Tang,andW.Zaremba,“OpenAIGym,”2016.eprint:arXiv:1606.01540.[16]V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,etal.,“Human-levelcontrolthroughdeepreinforcementlearning,”Nature,vol.518,no.7540,pp.529–533,2015.[17]M.C.Machado,M.G.Bellemare,E.Talvitie,J.Veness,M.J.Hausknecht,andM.Bowling,“RevisitingtheArcadeLearningEnvironment:Evaluationprotocolsandopenproblemsforgeneralagents,”CoRR,vol.abs/1709.06009,2017.[18]J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Proximalpolicyopti-mizationalgorithms,”2017.eprint:arXiv:1707.06347.[19]V.Mnih,A.P.Badia,M.Mirza,A.Graves,T.P.Lillicrap,T.Harley,D.Silver,andK.Kavukcuoglu,“Asynchronousmethodsfordeepreinforcementlearning,”2016.eprint:arXiv:1602.01783.[20]M.Hessel,J.Modayil,H.vanHasselt,T.Schaul,G.Ostrovski,W.Dabney,D.Horgan,B.Piot,M.Azar,andD.Silver,“Rainbow:Combiningimprovementsindeepreinforcementlearning,”2017.eprint:arXiv:1710.02298.[21]M.G.Bellemare,Y.Naddaf,J.Veness,andM.Bowling,“Thearcadelearningenvironment:Anevaluationplatformforgeneralagents,”inProceedingsoftheTwenty-FourthInternationalJointConferenceonArtiﬁcialIntelligence,2015,pp.4148–4152.[22]A.NicholandJ.Schulman,“Onﬁrst-ordermeta-learningalgorithms,”2018.eprint:arXiv:1803.02999.[23]A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez, L.Kaiser,andI.Polosukhin,“Attentionisallyouneed,”inAdvancesinNeuralInformationProcessingSystems,2017,pp.6000–6010.[24]K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimagerecognition,”inPro-ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,2016,pp.770–778.13ADetailedAlgorithmDescriptionsAlgorithm1TheJERKalgorithm.Forourexperiments,wesetβ=0.25,Jn=4,Jp=0.1,Rn=100,Ln=70.Require:initialexploitationfraction,β.Require:consecutivetimestepsforholdingthejumpbutton,Jn.Require:probabilityoftriggeringasequenceofjumps,Jp.Require:consecutivetimestepstogoright,Rn.Require:consecutivetimestepstogoleft,Ln.Require:evaluationtimesteplimit,Tmax.S←{},T←0.repeatif|S|>0andRandomUniform(0,1)<β+TTmaxthenReplaythebesttrajectoryτ∈S.Padtheepisodewithno-opsasneeded.Updatethemeanrewardofτbasedonthenewepisodereward.AddtheelapsedtimestepstoT.elserepeatGorightforRntimesteps,jumpingforJntimestepsatatimewithJpprobability.ifcumulativerewarddidnotincreaseoverthepastRnstepsthenGoleftforLntimesteps,jumpingperiodically.endifAddtheelapsedtimestepstoT.untilepisodecompleteFindthetimesteptfromthepreviousepisodewiththehighestcumulativerewardr.insert(τ,r)intoS,whereτistheactionsequenceuptotimestept.endifuntilT≥Tmax14Algorithm2ThejointtrainingprocedureforeachworkerinJointRainbow.Forourexperiments,wesetN=256.R←emptyreplaybuﬀer.θ←initialweights.repeatforeachenvironmentdoT←nextstatetransition.addTtoR.endforB←sampleNtransitionsfromR.L←Loss(B)UpdatetheprioritiesinRaccordingtoL.G←∇θLGagg←AllReduce(G)(averagegradientbetweenworkers).θ←Adam(θ,Gagg)untilconvergenceBPlotsforMultipleSeedsInthissection,wepresentper-algorithmlearningcurvesonthetestset.Foreachalgorithm,werunthreediﬀerentrandomseeds.0.00.20.40.60.81.0Timesteps (millions)140016001800200022002400Figure5:TestlearningcurvesforJERK.0.00.20.40.60.81.0Timesteps (millions)10001200140016001800Figure6:TestlearningcurvesforPPO.150.00.20.40.60.81.0Timesteps (millions)1000150020002500300035004000Figure7:TestlearningcurvesforRainbow.0.00.20.40.60.81.0Timesteps (millions)150020002500300035004000Figure8:TestlearningcurvesforJointRainbow.0.00.20.40.60.81.0Timesteps (millions)150020002500300035004000Figure9:TestlearningcurvesforJointPPO.16CScoresonTestSetTable2:DetailedevaluationresultsforRainbow.StateScoreFinalScoreAngelIslandZoneAct23576.0±89.25070.1±433.1CasinoNightZoneAct26045.2±845.48607.9±1022.5FlyingBatteryZoneAct21657.5±10.12195.4±190.8GreenHillZoneAct26332.0±263.56817.2±392.8HillTopZoneAct22847.8±161.93432.7±252.9HydrocityZoneAct1886.4±31.4867.2±0.0LavaReefZoneAct12623.6±78.02908.5±106.1MetropolisZoneAct31178.1±229.32278.8±280.6ScrapBrainZoneAct1879.1±141.02050.0±1089.9SpringYardZoneAct11787.6±136.53861.0±782.2StarLightZoneAct32421.9±110.82680.3±366.2Aggregate2748.6±102.23706.3±192.7Table3:DetailedevaluationresultsforJERK.StateScoreFinalScoreAngelIslandZoneAct21305.2±13.31605.1±158.7CasinoNightZoneAct22231.0±556.82639.7±799.5FlyingBatteryZoneAct21384.9±13.01421.8±25.0GreenHillZoneAct23702.1±199.14862.2±178.7HillTopZoneAct21901.6±56.01840.4±326.8HydrocityZoneAct12613.0±149.63895.5±50.0LavaReefZoneAct1267.1±71.6200.3±71.9MetropolisZoneAct32623.7±209.23291.4±398.2ScrapBrainZoneAct11442.6±108.81756.3±314.2SpringYardZoneAct1838.9±186.1829.2±158.2StarLightZoneAct32633.5±23.43033.3±53.8Aggregate1904.0±21.92306.8±74.017Table4:DetailedevaluationresultsforPPO.StateScoreFinalScoreAngelIslandZoneAct21491.3±537.82298.3±1355.8CasinoNightZoneAct22517.8±1033.02343.6±1044.5FlyingBatteryZoneAct21105.8±177.31305.7±221.9GreenHillZoneAct22477.6±435.32655.7±373.4HillTopZoneAct22408.0±140.43173.1±549.7HydrocityZoneAct1622.8±288.6433.5±348.4LavaReefZoneAct1885.8±125.6683.9±206.3MetropolisZoneAct31007.6±145.11058.6±400.4ScrapBrainZoneAct11162.0±202.82190.8±667.5SpringYardZoneAct1564.2±195.6644.2±337.4StarLightZoneAct32134.4±313.42519.0±98.8Aggregate1488.8±42.81755.1±65.2Table5:DetailedevaluationresultsforJointPPO.StateScoreFinalScoreAngelIslandZoneAct23283.0±681.04375.3±1132.8CasinoNightZoneAct25410.2±635.66142.4±1098.7FlyingBatteryZoneAct21513.3±48.31748.0±15.1GreenHillZoneAct28769.3±308.88921.2±59.5HillTopZoneAct24289.9±334.24688.6±109.4HydrocityZoneAct11249.8±206.32821.7±154.1LavaReefZoneAct12409.0±253.53076.0±13.7MetropolisZoneAct31409.5±72.92004.3±110.4ScrapBrainZoneAct11634.6±287.02112.0±713.9SpringYardZoneAct12992.9±350.04663.4±799.5StarLightZoneAct31445.3±110.52636.7±103.3Aggregate3127.9±116.93926.3±78.118Table6:DetailedevaluationresultsforJointRainbow.StateScoreFinalScoreAngelIslandZoneAct23770.5±231.84615.1±1082.5CasinoNightZoneAct27877.7±556.08851.2±305.4FlyingBatteryZoneAct22110.2±114.42585.7±131.1GreenHillZoneAct26106.8±667.16793.5±643.6HillTopZoneAct22378.4±92.53531.3±4.9HydrocityZoneAct1865.0±1.3867.2±0.0LavaReefZoneAct12753.6±192.82959.7±134.1MetropolisZoneAct31340.6±224.01843.2±253.0ScrapBrainZoneAct1983.5±34.32075.0±568.3SpringYardZoneAct12661.0±293.64090.1±700.2StarLightZoneAct31813.7±94.52533.8±239.0Aggregate2969.2±170.23704.2±151.1Table7:Detailedevaluationresultsforhumans.StateScoreAngelIslandZoneAct28758.3±477.9CasinoNightZoneAct28662.3±1402.6FlyingBatteryZoneAct26021.6±1006.7GreenHillZoneAct28166.1±614.0HillTopZoneAct28600.9±772.1HydrocityZoneAct17146.0±1555.1LavaReefZoneAct16705.6±742.4MetropolisZoneAct36004.8±440.4ScrapBrainZoneAct16413.8±922.2SpringYardZoneAct16744.0±1172.0StarLightZoneAct38597.2±729.5Aggregate7438.2±624.219DScoresonTrainingSetTable8:FinalperformanceforthejointRainbowmodeloverthelast10episodesforeachenvi-ronment.Errormarginsarecomputedusingthestandarddeviationoverthreeruns.StateScoreStateScoreAngelIslandZoneAct14765.6±1326.2LaunchBaseZoneAct21850.1±124.3AquaticRuinZoneAct15382.3±1553.1LavaReefZoneAct2820.3±80.9AquaticRuinZoneAct24752.7±1815.0MarbleGardenZoneAct12733.2±232.1CarnivalNightZoneAct13554.8±379.6MarbleGardenZoneAct2180.7±150.2CarnivalNightZoneAct22613.7±46.4MarbleZoneAct14127.0±375.9CasinoNightZoneAct12165.7±75.9MarbleZoneAct21615.7±47.6ChemicalPlantZoneAct14483.5±954.6MarbleZoneAct31595.1±77.6ChemicalPlantZoneAct22840.4±216.4MetropolisZoneAct1388.9±184.2DeathEggZoneAct12334.3±61.0MetropolisZoneAct23048.6±1599.9DeathEggZoneAct23197.8±32.0MushroomHillZoneAct12076.0±1107.8EmeraldHillZoneAct19273.4±385.8MushroomHillZoneAct22869.1±1150.4EmeraldHillZoneAct29410.1±421.1MysticCaveZoneAct11606.8±776.9FlyingBatteryZoneAct1711.8±99.1MysticCaveZoneAct24359.4±547.5GreenHillZoneAct14164.7±311.2OilOceanZoneAct11998.8±10.0GreenHillZoneAct35481.3±1095.1OilOceanZoneAct23613.7±1244.9HiddenPalaceZone9308.9±119.1SandopolisZoneAct11475.3±205.1HillTopZoneAct1778.0±8.1SandopolisZoneAct2539.9±0.7HydrocityZoneAct2825.7±2.2ScrapBrainZoneAct2692.6±67.6IcecapZoneAct15507.0±167.5SpringYardZoneAct23162.3±38.7IcecapZoneAct23198.2±774.7SpringYardZoneAct32029.6±211.3LabyrinthZoneAct13005.3±197.8StarLightZoneAct14558.9±1094.1LabyrinthZoneAct21420.8±533.0StarLightZoneAct27105.5±404.2LabyrinthZoneAct31458.7±255.4WingFortressZone3004.6±7.1LaunchBaseZoneAct12044.5±601.7Aggregate3151.7±218.220Table9:FinalperformanceforthejointPPOmodeloverthelast10episodesforeachenvironment.Errormarginsarecomputedusingthestandarddeviationovertworuns.StateScoreStateScoreAngelIslandZoneAct19668.2±117.0LaunchBaseZoneAct21836.0±545.0AquaticRuinZoneAct19879.8±4.0LavaReefZoneAct22155.1±1595.2AquaticRuinZoneAct28676.0±1183.2MarbleGardenZoneAct13760.0±108.5CarnivalNightZoneAct14429.5±452.0MarbleGardenZoneAct21366.4±23.5CarnivalNightZoneAct22688.2±110.4MarbleZoneAct15007.8±172.5CasinoNightZoneAct19378.8±409.3MarbleZoneAct21620.6±30.9ChemicalPlantZoneAct19825.0±6.0MarbleZoneAct32054.4±60.8ChemicalPlantZoneAct22586.8±516.9MetropolisZoneAct11102.8±281.5DeathEggZoneAct13332.5±39.1MetropolisZoneAct26666.7±53.0DeathEggZoneAct23141.5±282.5MushroomHillZoneAct13210.2±2.7EmeraldHillZoneAct19870.7±0.3MushroomHillZoneAct26549.6±1802.9EmeraldHillZoneAct29901.6±18.9MysticCaveZoneAct16755.9±47.8FlyingBatteryZoneAct11642.4±512.9MysticCaveZoneAct26189.6±16.6GreenHillZoneAct17116.0±2783.5OilOceanZoneAct14938.8±13.3GreenHillZoneAct39878.5±5.1OilOceanZoneAct26964.9±1929.3HiddenPalaceZone9918.3±1.4SandopolisZoneAct12548.1±80.8HillTopZoneAct14074.2±370.1SandopolisZoneAct21087.5±21.5HydrocityZoneAct24756.8±3382.3ScrapBrainZoneAct21403.7±3.3IcecapZoneAct15389.9±35.6SpringYardZoneAct29306.8±489.1IcecapZoneAct26819.4±67.9SpringYardZoneAct32608.1±113.2LabyrinthZoneAct15041.4±194.6StarLightZoneAct16363.6±198.7LabyrinthZoneAct21337.9±61.9StarLightZoneAct28336.1±998.3LabyrinthZoneAct31918.7±33.5WingFortressZone3109.2±50.9LaunchBaseZoneAct12714.0±17.7Aggregate5083.6±91.821
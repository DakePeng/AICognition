7
1
0
2

l
u
J

3
1

]

G
L
.
s
c
[

2
v
0
7
0
4
0
.
3
0
7
1
:
v
i
X
r
a

PredictionandControlwithTemporalSegmentModelsNikhilMishra1PieterAbbeel12IgorMordatch2AbstractWeintroduceamethodforlearningthedynamicsofcomplexnonlinearsystemsbasedondeepgen-erativemodelsovertemporalsegmentsofstatesandactions.Unlikedynamicsmodelsthatoper-ateoverindividualdiscretetimesteps,welearnthedistributionoverfuturestatetrajectoriescon-ditionedonpaststate,pastaction,andplannedfutureactiontrajectories,aswellasalatentprioroveractiontrajectories.Ourapproachisbasedonconvolutionalautoregressivemodelsandvari-ationalautoencoders.Itmakesstableandaccu-ratepredictionsoverlonghorizonsforcomplex,stochasticsystems,effectivelyexpressinguncer-taintyandmodelingtheeffectsofcollisions,sen-sorynoise,andactiondelays.Thelearneddy-namicsmodelandactionpriorcanbeusedforend-to-end,fullydifferentiabletrajectoryopti-mizationandmodel-basedpolicyoptimization,whichweusetoevaluatetheperformanceandsample-efﬁciencyofourmethod.1.IntroductionTheproblemoflearningdynamics–whereanagentlearnsamodelofhowitsactionswillaffectitsstateandthatofitsenvironment–isakeyopenprobleminroboticsandreinforcementlearning.Anagentequippedwithadynam-icsmodelcanleveragemodel-predictivecontrolormodel-basedreinforcementlearning(RL)toperformawideva-rietyoftasks,whoseexactnatureneednotbeknowninadvance,andwithoutadditionalaccesstotheenvironment.Incontrastwithmodel-freeRL,whichseekstodirectlylearnapolicy(mappingfromstatestoactions)inordertoaccomplishaspeciﬁctask,learningdynamicshasthead-vantagethatdynamicsmodelscanbelearnedwithouttask-speciﬁcsupervision.Sincedynamicsmodelsaredecoupledfromanyparticulartask,theycanbereusedacrossdifferent1UniversityofCalifornia,Berkeley2OpenAI.Correspondenceto:NikhilMishra<nmishra@berkeley.edu>.Proceedingsofthe34thInternationalConferenceonMachineLearning,Sydney,Australia,PMLR70,2017.Copyright2017bytheauthor(s).tasksinthesameenvironment.Additionally,learningdif-ferentiabledynamicsmodels(suchasthosebasedonneuralnetworks)enablestheuseofend-to-endbackpropagation-basedmethodsforpolicyandtrajectoryoptimizationthataremuchmoreefﬁcientthanmodel-freemethods.Typicalapproachestodynamicslearningbuildaone-stepmodelofthedynamics,predictingthenextstateasafunc-tionofthecurrentstateandthecurrentaction.However,whenchainedsuccessivelyformanytimestepsintothefu-ture,thepredictionsfromaone-stepmodeltendtodivergefromthetruedynamics,eitherduetotheaccumulationofsmallerrorsordeviationfromtheregimerepresentedbythedatathemodelwastrainedon.Anylearneddynamicsmodelisonlyvalidunderthedistributionofstatesandac-tionsrepresentedbyitstrainingdata,andone-stepmodelsmakenoattempttodealwiththefactthattheycannotmakeaccuratepredictionsfaroutsidethisdistribution.Whenthetruedynamicsarestochastic,orthesensorymea-surementsnoisyorunreliable,theseproblemsareonlyex-acerbated.Moreover,thedynamicsmaybeinherentlydif-ﬁculttolearn:bifurcationssuchascollisionsinducesharpchangesinstatethatarehardtomodelwithcertaintywhenlookingatasingletimestep.Theremayalsobehystere-siseffectssuchasgearbacklashinrobots,orhigh-orderdynamicsinhydraulicrobotactuatorsandhumanmusclesthatrequirelookingatahistoryofpaststates.Wepresentanovelapproachtolearningdynamicsbasedonadeepgenerativemodelovertemporalsegments:wewishtomodelthedistributionoverpossiblefuturestatetrajec-toriesconditionedonplannedfutureactionsandahistoryofpaststatesandactions.Byconsideringanentireseg-mentoffuturestates,ourapproachcanmodelbothuncer-taintyandcomplexinteractions(likecollisions)holisticallyoverasegment,evenifitmakessmallerrorsatindividualtimesteps.Wealsomodelaprioroveractionsegmentsus-ingasimilargenerativemodel,whichcanbeusedtoensurethattheactiondistributionexploredduringplanningisthesameastheonethemodelwastrainedon.Weshowthatthatourmethodmakesbetterpredictionsoverlonghori-zonsthanone-stepmodels,isrobusttostochasticdynam-icsandmeasurements,andcanbeusedinavarietyofcon-trolsettingswhileonlyconsideringactionsfromtheregimewherethemodelisvalid. 
 
 
 
 
 
PredictionandControlwithTemporalSegmentModels2.RelatedWorkAnumberofoptionsareavailableforrepresentationoflearneddynamicsmodels,includinglinearfunctions(Mor-datchetal.,2016;Yip&Camarillo,2014),Gaussianpro-cesses(Boedeckeretal.,2014;Ko&Fox,2009;Deisen-roth&Rasmussen,2011),predictivestaterepresentations(PSRs)(Littmanetal.,2002;Rosencrantzetal.,2004),anddeepneuralnetworks(Punjani&Abbeel,2015;Fragki-adakietal.,2015;Agrawaletal.,2016).Linearfunctionsareefﬁcienttoevaluateandsolvecontrolsfor,buthavelimitedexpressivepower.Gaussianprocesses(Williams&Rasmussen,1996)provideuncertaintyestimates,butscal-ingthemtolargedatasetsremainsachallenge(Shenetal.;Lawrenceetal.,2003).PSRsandvariantsmakemulti-steppredictions,butsufferfromthesamescalabilitychallengesasGaussianprocesses.Ourmethodcombinestheexpres-sivenessandscalabilityofneuralnetworkswiththeabilitytoprovidesamplinganduncertaintyestimates,modelingentiresegmentstoimprovestabilityandrobustness.Analternativeistolearndynamicsmodelsinanonlinefashion,constantlyadaptingthemodelbasedonanincom-ingstreamofobservedstatesandactions(Fuetal.,2016;Mordatchetal.,2016;Yip&Camarillo,2014;Lenzetal.,2015).However,suchapproachesareslowtoadapttorapidly-changingdynamicsmodes(suchasthosearisingwhenmakingorbreakingcontact)andmaybeproblematicwhenappliedonrobotsperformingrapidmotions.Severalapproachesexisttoimprovethestabilityofmodelsthatmakesequentialpredictions.Abbeel&Ng(2004)andVenkatramanetal.(2015)consideralternativelossfunc-tionsthatimproverobustnessoverlongpredictionhori-zons.Bengioetal.(2015)andVenkatramanetal.(2016)alsousesimplecurriculaforasimilareffect.Whiletheyallconsidermulti-steppredictionlosses,theyonlydosointhecontextoftrainingmodelsthatareintrinsicallyone-step.Existingmethodsforvideoprediction(Finn&Levine,2016;Ohetal.,2015)lookatahistoryofpreviousstatesandactionstopredictthenextframe;wetakethisastepfurtherbymodelingadistributionoveranentiresegmentoffuturestatesthatisalsoconditionedonfutureactions.Inthiswork,wefocusondemonstratingthebeneﬁtsofaprob-abilisticsegment-basedapproach;thesemethodscouldeas-ilybeincorporatedwithourstolearndynamicsfromim-ages,butweleavethistofuturework.Watteretal.(2015)andJohnsonetal.(2016)usevaria-tionalautoencoderstolearnalow-dimensionallatent-spacerepresentationofimageobservations.Finnetal.(2016)takesasimilarapproach,butwithoutthevariationalas-pect.Theseworksutilizedautoencodersasameansofdi-mensionalityreduction(ratherthanfortemporalcoherencelikewedo)toenabletheuseofexistingcontrolalgorithmsbasedonlocally-linearone-stepdynamicsmodels.Temporally-extendedactionswereshowntobeeffectiveinreinforcementlearning,suchastheoptionsframework(Suttonetal.,1999b)orsequencingofsub-plans(Vezhn-evetsetal.,2016).Consideringentiretrajectoriesasop-posedtosingletimestepscanalsoleadtosimplercon-trolpolicies.Forexample,thereareeffectiveandsim-plemanually-designedcontrollaws(Raibert,1986),(Prattetal.,2006)thatformulateoptimalactionsasafunctionoftheentirefuturetrajectoryratherthanasinglefuturestate.3.Segment-BasedDynamicsModelSupposewehaveanon-lineardynamicalsystemwithstatesxtandactionsut.Theconventionalapproachtolearningdynamicsistolearnafunctionxt+1=f(xt,ut)usinganapproximatorsuchasaneuralnetwork(possiblyrecurrent).Weconsideramoregeneralformulationoftheproblem,whichisdepictedinFigure1:givensegments(oflengthH)ofpaststatesX−={xt−H,...,xt−1}andactionsU−={ut−H,...,ut−1},wewishtopredicttheentiresegmentoffuturestatesX+={xt,...,xt+H}thatwouldresultfromtakingactionsU+={ut,...,ut+H−1}.Treatingthesefourtemporalsegmentsasrandomvariables,thenwewishtolearntheconditionaldistributionP(X+|X−,U−,U+).WeintroducedependencyonpastactionsU−tosupportdynamicswithdelayedorﬁlteredactions.P(X+)U+UXtt+HtHFigure1.Anoverviewoftheprobabilisticmodelwewishtolearn.GivenobservedpaststatesandactionsX−,U−(blue),andplannedfutureactionsU+(green),wewishtosamplepossiblefuturestatetrajectoriesX+(red).Withthisinmind,weproposetheuseofadeepconditionalvariationalautoencoder(Kingma&Welling,2014):ourencoderwilllearnthedistribu-tionQ(x)(Z|X+,X−,U−,U+)overlatentcodesZ,andourdecoderwilllearntoreconstructX+fromX−,U−,U+andasamplefromZ,modelingthedistri-butionP(x)(X+|X−,U−,U+,Z).NotethattherandomvariableZisavectorthatdescribesanentiresegmentofstates,X+.Aftertrainingiscomplete,wecandiscardthePredictionandControlwithTemporalSegmentModelsencoder,andthedecoderwillallowustopredictthefuturestatetrajectoryˆX+usingX−,U−,U+,asdesired,sam-plinglatentcodesfromanenforcedpriorP(Z)=N(0,I).Empirically,weobservethathavingtheencodermodelQ(x)(Z|X+)insteadofQ(x)(Z|X+,X−,U−,U+)givesequivalentperformance,andsowetakethisapproachinallofourexperimentsforsimplicity.3.1.ModelArchitectureandTrainingIntheprevioussectionwediscussedaconditionalvari-ationalautoencoderwhosegenerativepathservesasastochasticdynamicsmodel.Herewewillexpandonsomeofthearchitecturaldetails.AdiagramoftheentiretrainingsetupisshowninFigure2.Formoredetailsofthearchi-tecturesusedinourexperiments,seeAppendixA.X+Q(x)Z2ZZU+UXP(x)Figure2.Thedynamicsmodelthatwelearn.TheencoderQ(x)parametrizesadiagonalGaussiandistributionZ∼N(µZ,σ2Z)overlatentcodesdescribingastatetrajectoryX+.Theautore-gressivedecoderP(x)takesinsegmentsofpaststatesX−,pastactionsU−,andfutureactionsU+,alongwithasamplefromZ,andusesdilatedcausalconvolutionstoreconstructX+.TheencodernetworkQ(x)explicitlyparametrizesaGaus-siandistributionoverlatentcodesZwithdiagonalcovari-ance.Itconsistsofastackof1D-convolutionallayers,whoseoutputisﬂattenedandprojectedintoasinglevec-torcontainingameanµZandvarianceσ2Z.Wethensam-plez∼N(µZ,σ2Z)inadifferentiablemannerusingthereparametrizationtrick(Kingma&Welling,2014).ThedecodernetworkP(x)seekstomodeladistributionoverasegmentofstatesP(X+)=P(xt,...,xt+H).Thecausalnatureofthissegment(aparticulartimestepisonlyaffectedbytheonesthatoccurbeforeit)suggeststhatanautoregressivemodelwithdilatedconvolutionsisappro-priate,similartoarchitecturespreviouslyusedformodel-ingaudio(vandenOordetal.,2016a)andimage(vandenOordetal.,2016b)data.Liketheseworks,weuselayerswiththefollowingactivationfunction:tanh(Wf,k∗s+VTf,kz)(cid:12)σ(Wg,k∗s+VTg,kz)(1)where∗denotesconvolution,(cid:12)denoteselementwisemul-tiplication,σ(·)isthesigmoidfunction,sistheinputtothelayer,zisalatentcodesampledfromtheoutputofthedecoder,andW,Varenetworkweightstobelearned.Wefoundthatresiduallayersandskipconnectionsbetweenlayersgiveslightlybetterperformancebutarenotessential.Wetrainthemodelparametersend-to-end,minimizingthel2-lossbetweenX+anditsreconstructionˆX+,alongwiththeKL-divergenceofthelatentcodeZ∼N(µZ,σ2Z)fromN(0,I)similarlytoKingma&Welling(2014).4.ControlwithSegment-BasedModelsOncewehavelearnedadynamicsmodel,wewanttoutilizeitinordertoaccomplishdifferenttasks,eachofwhichcanbeexpressedasrewardfunctionr(xt,ut).Trajectoryop-timizationandpolicyoptimizationaretwosettingswhereadynamicsmodelwouldcommonlybeused,andprovidemeaningfulwayswithwhichtoevaluateadynamicsmodel.4.1.TrajectoryOptimizationIntrajectoryoptimization,wewishtoﬁndasequenceofactionsthatcanbeappliedtoaccomplishaparticularin-stanceofatask.Speciﬁcally,givenarewardfunctionr,wewanttomaximizethesumofrewardsalongthetrajectorythatresultsfromapplyingtheactionsu1,...,uT,begin-ningfromaninitialstatex0.Thiscanbesummarizedbythefollowingoptimizationproblem:maxu1,...,uTE(cid:20)TXt=1r(xt,ut)(cid:21)withxt∼P(x)(xt|x0:t−1,u1:t)(2)wherer(xt,ut)istherewardreceivedattimet,andX={x1,...,xT}isthesequenceofstatesthatwouldresultfromtakingactionsU={u1,...,uT}frominitialstatex0,underdynamicsmodelP(x).Theexpectationistakenoverstatetrajectoriessampledfromthemodel.4.2.LatentActionPriorsIfweattempttosolvetheoptimizationproblemasposedin(2),thesolutionwilloftenattempttoapplyactionse-quencesoutsidethemanifoldwherethedynamicsmodelPredictionandControlwithTemporalSegmentModelsisvalid:theseactionscomefromaverydifferentdistribu-tionthantheactiondistributionofthetrainingdata.Thiscanbeproblematic:theoptimizationmayﬁndactionsthatachievehighrewardsunderthemodel(byexploitingitinaregimewhereitisinvalid)butthatdonotaccomplishthegoalwhentheyareexecutedintherealenvironment.Tomitigatethisproblem,weproposetheuseofanotherconditionalvariationalautoencoder,thisoneoversegmentsofactions.Inparticular,givensequencesofpastac-tionsU−={ut−H,...,ut−1},andfutureactionsU+={ut,...,ut+H},wewishtomodelthetheconditionaldis-tributionP(U+|U−).TheencoderlearnsQ(u)(Z|U+)andthedecoderlearnsP(u)(U+|Z,U−).WeconditiononU−tosupporttemporalcoherenceinthegeneratedactionse-quence.LikethedynamicsmodelintroducedinSection3.1,theencoderuses1D-convolutionallayers,andthede-coderisautoregressive,withdilatedcausalconvolutions.Thelatentspacethatthisautoencoderlearnsdescribesaprioroveractionsthatcanbeusedwhenplanningwithadynamicsmodel;hencewerefertothisautoencoderoveractionsequencesasalatentactionprior.Toincorporatealatentactionprior,wedivideanactionsequenceU={u1,...,uT}intosegmentsU1,...UKoflengthH(whereKisdeterminedsuchthatT=HK,andU0=0).Thenwecangenerateactionsequencesthataresimilartotheonesinourtrainingsetbysamplingdifferentlatentcodesz1,...,zKandusingthedecodertosamplefromP(u)(Uk|Uk−1,zk),∀k=1,...,K.Theoptimiza-tionproblemposedin(2)canthenbeexpressedas:maxz1,...,zKE(cid:20)TXt=1r(xt,ut)(cid:21)withxt∼P(x)(xt|x0:t−1,u1:t)ut∼P(u)(ut|u1:t−1,z1:K)(3)wheretheactionsu1,...,uTandstatesx1,...,xTaregeneratedbythelatentactionprioranddynamicsmodel(seeFigure3foranillustration).Sincethedynamicsmodelisdifferentiable,theaboveoptimizationproblemcanbesolvedend-to-endwithbackpropagation.Whileitisstillnonconvex,weareoptimizingoverfewervariables,andthepossibleactionsequencesthatareexploredwillbefromthesamedistributionasthemodel’strainingdata.Moreover,thegradientsoftherewardswithrespecttothelatentcodesarelikelytohavestrongersignalthanthosewithrespecttoasingleaction.WeusedAdam(Kingma&Ba,2015)withstepsize0.01toperformthisoptimizationandfoundthatitgenerallyconvergedinaround100iterations.4.3.PolicyOptimizationTrajectoryoptimizationenablesanagenttoaccomplishasingleinstanceofatask,butmoreoften,weareinterestedX0P(u)z(1)z(2)U0U2t+Ht+2Ht+3HtP(x)P(x)P(u)Figure3.Trajectoryoptimizationoverlatentcodes(blue).Theactionsequencesaregeneratedusingthelatentcodesandthela-tentactionpriorP(u),andthestatetrajectoryusingthegeneratedactionsandthedynamicsmodelP(x).inpolicyoptimization,wheretheagentlearnsapolicythatdictatesoptimalbehaviorinordertoaccomplishthetaskinthegeneralcase.Inparticular,apolicyisalearnedfunction(withparametersθ)thatdeﬁnesaconditionaldistributionoveractionsgivenstates,denotedπθ(u|x).Thevalueofapolicyisdeﬁnedastheexpectedsumofdiscountedrewardswhenactingunderthepolicy,andcanbeexpressedas:η(θ)=E(cid:20)∞Xt=1γt·r(xt,ut)(cid:21)(4)wheretheactionsaresampledfromπθ(ut|xt),andγisadiscountfactor.Thegoalofpolicyoptimizationistomax-imizethevalueofthepolicywithrespecttoitsparameters.Theclassofalgorithmsknownaspolicygradientmeth-ods(Suttonetal.,1999a;Peters&Schaal,2006)attempttosolvethisoptimizationproblemwithoutconsideringadynamicsmodel.Theyexecuteapolicyπθtogetsamplesx1,u1,r1,...,xT,uT,rTfromtheenvironment,andthenupdateθtogetanimprovedpolicy,relyingonlikelihoodratiomethods(Williams,1992)toestimatethegradient∂η∂θbecausetheycannotdirectlycomputethederivativesoftherewardsr(xt,ut)withrespecttotheactionsu1,...,ut.Model-basedpolicyoptimizationcanbemoreefﬁcientthantraditionalpolicy-gradientmethods,becausethegradient∂η∂θcanbecomputeddirectlybybackpropagationthroughadifferentiablemodel.However,itssuccesshingesonthePredictionandControlwithTemporalSegmentModelsaccuracyofthedynamicsmodel,astheoptimizationcanexploitﬂawsinthemodelinthesamewayasdiscussedinSection4.2.Heessetal.(2015)useamodel-basedap-proachwhereaone-stepdynamicsmodelislearnedjointlywithapolicyinanonlinemanner.Toevaluatetherobust-nessofourmodels,weexperimentwithlearningpoliciesofﬂine,wherethedynamicsmodelislearnedthroughun-supervisedexplorationoftheenvironment,andnoenviron-mentinteractionisallowedbeyondthisexploration.Insteadofaone-steppolicyoftheformπθ(ut|xt),wealsoexploredusingasegment-basedpolicyπθ(Z|X−,U−)thatgeneratesactionsusinglatentactionpriorP(u)asfollows:X−,U−={xt−H,...,xt−1},{ut−H,...,ut−1}sampleZ∼πθ(Z|X−,U−){ut,...,ut+H}=U+∼P(u)(U+|Z,U−)andthenactsaccordingtoactionut.Theresultingpolicywilllearntoaccomplishthetaskwhileonlyconsideringactionsforwhichthedynamicsmodelisvalid.Intermsoftheoptionsframework(Suttonetal.,1999b),wecanthinkofthispolicyasconsideringacontinuousspectrumofoptions,allofwhichareconsistentwithbothpastobservedstatesandactions,andthedatadistributionunderwhichthedynamicsmodelmakesgoodpredictions.5.ExperimentsOurexperimentsinvestigatethefollowingquestions:(i)Howwelldosegment-basedmodelspredictdynamics?(ii)Howdoespredictionaccuracytransfertocontrolappli-cations?Howdoesthisscalewiththedifﬁcultyofthetaskandstochasticityinthedynamics?(iii)Howisthisaffectedbytheuseoflatentactionpriors?(iv)Isthereanymeaningorstructureencodedbythelatentspacelearnedbythedynamicsmodel?5.1.EnvironmentsInorderforadynamicsmodeltobeversatileenoughforuseincontrolsettings,thetrainingdataneedstocontainavarietyofactionsthatexploreadiversesubsetofthestatespace.Efﬁcientexplorationstrategiesareanopenprob-leminreinforcementlearningandarenotthefocusofthiswork.Withthisinmind,webaseourexperimentsonasimulated2-DOFarmmovinginaplane(asimplementedintheReacherenvironmentinOpenAIGym),becauseper-formingrandomactionsinthisenvironmentresultsinsufﬁ-cientexploration.Weconsiderthefollowingenvironmentsthroughoutourexperiments(illustratedinFigure4):(i)Thebasic,unmodiﬁedReacherenvironment.(ii)Aversioncontaininganobstaclethatthearmcancol-lidewith:theobstaclecannotmove,butitspositionisran-domlychosenatthestartofeachepisode.(iii)Aversioninwhichthearmcanpushadampedcylin-dricalobjectaroundthearena.Figure4.Theenvironmentsweusedinourexperiments.Fromlefttoright:(i)theunmodiﬁedReacherenvironment,(ii)aver-sionwithanobstacle,(iii)aversionwithanobjecttopush.Thebluemarkerisusedtovisualizethegoalduringexperimentsin-volvingcontrolbuthasnoeffectonthedynamics.Tolearnadynamicsmodel,thetrainingdataconsistsofacollectionoftrajectoriesx1,u1,...,xT,uTfromtheenvi-ronmentwewishtomodel.Weused500trajectoriesinthebasicenvironment,and5000intheothertwo.Forallenvironments,thestaterepresentationconsistsofthejointangles,thejointvelocities,andtheend-effectorposition;whenobstaclesarepresent,theirpositionsandvelocitiesarealsoincluded.Theactionsarealwaysthetorquesforthearm’sjoints.Inallexperiments,thetrainingsetiscom-prisedoftrajectoriesoflengthT=100ofthearmexe-cutingsmoothrandomtorques,andweusedsegmentsoflengthH=10and8-dimensionallatentspaces.Whilethesegmentlengthanddimensionalityofthelatentspacecouldbevaried,wefoundthatthesevalueswererea-sonablechoicesfortheseenvironments.Asthesegmentlengthapproaches1,themodeldegeneratesintoaone-stepmodel,andforlongersegments,itsperformanceplateausbecausethestatestowardstheendofthesegmentbecomeindependentofthoseatthebeginning.Likewise,weob-servedthatthislatent-spacedimensionalitywasagoodtrade-offbetweenexpressivenessandinformationdensity.5.2.BaselinesWecompareourmethodagainstthefollowingbaselines:(i)Aone-stepmodel:alearnedfunctionxt+1=f(xt,ut),wherefisafully-connectedneuralnetwork.Itistrainedusingaone-step-predictionl2-lossontuples(xt,ut,xt+1).(ii)Aone-stepmodelthatisrolledoutseveraltimestepsattrainingtime.Themodelisstillalearnedfunctionxt+1=f(xt,ut),butitistrainedwithamulti-steppre-dictionloss,overahorizonoflength2H.Whilethisdoesnotincreasethemodel’sexpressivepower,weexpectittobemorerobusttotheaccumulationofsmallerrors(e.g.,Venkatramanetal.(2015);Abbeel&Ng(2004)).(iii)AnLSTMmodel,whichcanstoreinformationaboutthepastinahiddenstateht:xt+1,ht+1=f(xt,ut,ht),andistrainedwiththesamemulti-steppredictionloss(alsooverahorizonof2H).WeexpectthattheLSTMcanlearnfairlycomplexdynamics,butthehiddenstatedependenciescanmaketrajectoryandpolicyoptimizationmoredifﬁcult.PredictionandControlwithTemporalSegmentModels5.3.Results5.3.1.DYNAMICSPREDICTIONAfterlearningadynamicsmodel,weevaluateitonatestsetofheld-outtrajectoriesbycomputingtheaveragelog-likelihoodofthetestdataunderthemodel.Forourmethod,wedothisbyobtainingsamplesfromthemodel,ﬁttingaGaussiantothesamples,anddetermin-ingthelog-likelihoodofthetruetrajectoryundertheﬁttedGaussian.Sincethebaselinemethodsdonotexpressuncer-tainty,butaretrainedusingl2-loss,weinterprettheirpre-dictionsasthemeanofaGaussiandistributionwhosevari-anceisconstantacrossallstatedimensionsandtimesteps(sinceminimizingl2-lossisequivalenttomaximizingthislog-likelihood).Wethenﬁtthevalueofthevariancecon-stanttomaximizethelog-likelihoodonthetestset.Figure5comparesourmethodtothebaselinesineachenvironment.Thevaluesreportedarelog-likelihoodspertimestep,averagedoveratestsetof1000trajectories.OurmodelandtheLSTMarecompetitiveinthebasicenviron-ment(andbothsubstantiallybetterthantheone-stepmod-els),buttheLSTM’sperformancedegradesonmorechal-lengingenvironmentswithcollisions.BasicPushing ObjectWith ObstacleEnvironment32.1626.5517.9528.4416.4211.9618.4913.0211.3619.9313.9812.55OursLSTMOne StepOne Step, rolled outFigure5.Predictionqualityofourmethodcomparedtoseveralbaselinesinarangeofenvironments.Thereportedvaluesaretheaveragelog-likelihoodpertimesteponatestset(higherisbetter).Ourmethodsigniﬁcantlyoutperformsthebaselinemethods,eveninenvironmentswithcomplexdynamicssuchascollisions.5.3.2.CONTROL1Next,wecompareourmethodtothebaselinesontrajec-toryandpolicyoptimization.Ofinterestisboththeactualrewardachievedintheenvironment,andthedifferencebe-tweenthetruerewardandtheexpectedrewardunderthemodel.Ifacontrolalgorithmexploitsthemodeltopredictunrealisticbehavior,thenthelatterwillbelarge.Weconsidertwotasks:(i)ReachingTask:thearmmustmoveitsendeffectortoadesiredposition.Therewardfunctionisthenegativedis-tancebetweentheendeffectorandthetargetposition,mi-nusaquadraticpenaltyonapplyinglargetorques.1Videosofourexperimentalresultscanbeseenhere:https://sites.google.com/site/temporalsegmentmodels/.(ii)PushingTask:thearmmustpushacylindricalobjecttothedesiredposition.Likeinthereachingtask,therewardfunctionisthenegativedistancebetweentheobjectandthetarget,againminusapenaltyonlargetorques.Thetrajectory-optimizationresultsaresummarizedinFig-ure6.Foreachtaskanddynamicsmodel,wesampled100targetpositionsuniformlyatrandom,solvedtheoptimiza-tionproblemasdescribedin(2)or(3),andthenexecutedtheactionsequencesintheenvironmentinopenloop.Undereachmodel,theoptimizationﬁndsactionsthatachievesimilarmodel-predictedrewards,butthebaselinessufferfromlargediscrepanciesbetweenmodelpredictionandthetruedynamics.Qualitatively,wenoticethat,onthepushingtask,theoptimizationexploitstheLSTMandone-stepmodelstopredictunrealisticstatetrajectories,suchastheobjectmovingwithoutbeingtouchedorthearmpassingthroughtheobjectinsteadofcollidingwithit.Ourmodelconsistentlyperformsbetter,and,withalatentactionprior,thetrueexecutioncloselymatchesthemodel’sprediction.Whenitmakesinaccuratepredictions,itrespectsphysi-calinvariants,suchasobjectsstayingstillunlesstheyaretouched,ornotpenetratingeachotherwhentheycollide.ReachingPushing7.237.917.3812.158.1516.068.6919.657.8919.01Negative Reward in EnvironmentReachingPushing0.210.131.014.301.658.442.2711.711.5811.47Discrepancy between Model and EnvironmentOurs, with latent priorOurs, without latent priorLSTMOne StepOne Step, rolled outFigure6.Trajectoryoptimizationonthereachingandpushingtasks.Thetopplotreportsthenegativerewardfromopen-loopex-ecutionofthereturnedactionsequences(lowerisbetter,averagedover100trials),andthebottomshowsthedifferencebetweentruerewardandmodel-predictedreward.Ourmodel,withalatentac-tionprior,achievesboththebestin-environmentperformanceandthesmallestdiscrepancybetweenenvironmentandmodel.Figure7depictstheresultsfrompolicy-optimization(Sec-tion4.3)intheformoflearningcurvesforeachtaskanddynamicsmodel.SeeAppendixAformodelarchitecturesandhyperparameters.Forcomparison,wealsoplottheperformanceofatraditionalpolicygradientmethod.Al-thoughthismethodandourseventuallyachievesimilarper-formance,oursdoessomuchmoreefﬁciently,learningthepolicyofﬂinewithfewersamplesfromthemodelthanthetraditionalmethodneededfromtheenvironment.PredictionandControlwithTemporalSegmentModels05000100001500020000Episode #141210864Reaching Task05000100001500020000Episode #14121086Pushing TaskLSTMLikelihood Ratio Policy GradientOne StepOne Step, rolled outOurs, with priorOurs, without priorFigure7.Learningcurvesofpolicyoptimizationonthereachingandpushingtasks(topandbottom,respectively).Thequantitiesplottedarethetrueperformanceintheenvironment(100-episodeaveragereward).Notonlydoesourdynamicsmodel,withanactionprior,consistentlyperformthebest,itisconsiderablyfasterthanamodel-freepolicygradientmethod.5.3.3.SENSORYNOISEANDDELAYEDACTIONSToexploretheeffectsofstochasticdynamicsandde-layedactions,weconsidertwomoremodiﬁcationsoftheReacherenvironment,oneinwhichthereisconsiderableGaussiannoiseinthestateobservations(σ=0.25ondataintherange[−1,+1]),andoneinwhichactionsarede-layed:theydonottakeeffectforτ=5timestepsaftertheyareapplied.Thesechallengescommonlyariseinreal-worldroboticsapplications(Atkesonetal.,2016),andsoitisimportanttobeabletolearnausefuldynamicsmodelineithersetting.Forboththenoisy-stateanddelayed-actionenvironments,welearnadynamicsmodelwitheachmethod,andthenuseittolearnapolicyforthereachingtask.Figure8displaystheresultinglearningcurves.Ourdynamicsmodelperformsmuchbetterthanthebaselines,bothwithandwithoutanactionprior.Notably,usingtheLSTMmodelresultsinasubstantiallyworsepolicythanourseventhoughitspredictionaccuracyisonlyslightlylower.Becauseourmodeloperatesoversegments,itim-plicitlylearnstoﬁlternoisyobservations.Thisremovestheneedtoexplicitlyapplyandtuneaﬁlteringprocess,asistraditionallydone.05000100001500020000Episode #141210864Sensory Noise05000100001500020000Episode #141210864Delayed ActionsLSTMOne StepOne Step, rolled outOurs, with priorOurs, without priorFigure8.Ourdynamicsmodel,bothwithandwithoutalatentactionprior,cangracefullydealwithnoisystateobservationsanddelayedactions,asdepictedbytheselearningcurvesfromthereachingtask.Althoughtheaveragerewardsareslightlylowerthanintheabsenceofnoiseordelays,policiestrainedwiththebaselinemodelsgenerallyfailtoperformthetask.5.3.4.ANALYSISOFLATENTSPACEVariationalautoencodersareknownforlearninglossyla-tentcodesthatpreservehigh-levelsemanticsofthedata,leavingthedecodertodeterminethelow-leveldetails.Asaresult,wearecurioustoseewhetherourdynamicsmodellearnsalatentspacethatpossessessimilarproperties.Appliedtodynamicsdata,onemightexpectalatentcodetoprovideanoveralldescriptionofwhathappensinthestatetrajectoryX+itencodes.Alternatively,pertheargumentmadebyChenetal.(2016),itisalsoconceivablethatthedecoderwouldignorethelatentcodeentirely,becausethesegmentsX−,U−,U+providebetterinformationthanZaboutX+.However,weobservethatourmodeldoeslearnameaningfullatentspace:onethatencodesuncertaintyaboutthefuture.Aparticularlatentcodecorrespondstoaparticularfuturewithinthespaceofpossibleonesconsis-tentwiththegivenX−,U−,U+.Whenthedynamicsaresimpleanddeterministic(suchasintheoriginalReacherenvironment),themodeldoesex-presscertaintybyignoringthelatentcode.Withstochas-PredictionandControlwithTemporalSegmentModelsticity(suchasintheprevioussection),itprovidesaspreadofreasonablestatetrajectories.Interestingly,whenthedy-namicsaredeterministicbutcomplex,themodelalsousesthelatentcodestoexpressuncertainty.Thiscanoccurre-gardingtheorientationsandvelocitiesofobjectsimmedi-atelyfollowingacollision,asillustratedinFigure9.Figure9.Anepisodefromthepushingenvironment.Thearmisabouttoswingcounterclockwiseandpushthebrownobject;theredpathindicatestheobservedmotionoftheobject.Thebluepathsaresamplesfromourmodel,giventhesameactionsequenceandinitialstate.Itcorrectlypredictscollisionsbetweenarmandobject,andbetweenobjectandwall,butexpressessomeuncer-taintyinthedeﬂectionanglesandhowfartheobjecttravelsafterbouncingoffthewall.5.3.5.EFFECTOFLATENTACTIONPRIOROurearlierexperimentsdemonstratedthebeneﬁtsofala-tentactionprior:byonlyconsideringactionsforwhichthedynamicsmodelisvalid,thediscrepancybetweenthemodelandthetruedynamicsisminimized,resultinginhigherrewardsachievedintheactualenvironment.Inthissection,wequalitativelyexaminehowtheactionsreturnedbycontrolalgorithmsdifferasaconsequenceofthelatentactionprior.AnexampleisillustratedinFigure10.Inthetrainingdata,theactionsthattheagenttakesaresmooth,randomtorques,andweobservethatwhenweuseanactionprior,solutionsfromtrajectoryoptimizationlooksimilar.Wecontrastthiswiththesolutionsfromoptimizingdirectlyoveractions,whicharesharpanddiscontinuous,unlikeanythingthedynamicsmodelhasseenbefore.Thisletsusinferthatthebaselinesperformpoorlyonthepush-ingtask(asshowninFigure6)becauseoflargediscrepan-ciesbetweenthemodelpredictionandthetrueexecution.020406080100TimestepSample of Actions from Training Data0102030405060TimestepOptimize latent codes (with action prior)0102030405060TimestepOptimize actions directly (without prior)Figure10.Thequalitativeeffectsofusingalatentactionprior,asseenduringtrajectoryoptimization.Thetopplotshowsanexampleofactionsequencesfromthetrainingdata.Whenweoptimizeoverlatentcodes,theactionslooksimilar(bottomleft),butwhenwedirectlyoptimizeoveractions,theresultingsequencelooksunlikeanythingthemodelhasseenbefore.6.ConclusionandFutureWorkWepresentedanovelapproachtodynamicslearningbasedontemporalsegments,usingavariationalautoencodertolearnthedistributionoverfuturestatetrajectoriescondi-tionedonpaststates,pastactions,andplannedfutureac-tions.Wealsointroducedthelatentactionprior,avari-ationalautoencoderthatmodelsaprioroveractionseg-ments,andshowedhowitcanbeusedtoperformcon-trolusingactionsfromthesamedistributionasadynamicsmodel’strainingdata.Finally,throughexperimentsinvolv-ingtrajectoryoptimizationandmodel-basedpolicyopti-mization,weshowedthattheresultingmethodcanmodelcomplexphenomenasuchascollisions,isrobusttosen-sorynoiseandactiondelays,andlearnsameaningfullatentspacethatexpressesuncertaintyaboutthefuture.Themostprominentdirectionforfutureworkthatweplantoexplore,isthedatacollectionprocedure.Inourex-periments,correlatedrandomactionsresultedinsufﬁcientexplorationforthetasksweconsideredandallowedustodemonstratethebeneﬁtsofasegment-basedapproach.However,incorporatingamoresophisticatedexplorationstrategytogatherdata(inaniterativeprocedure,potentiallyusingthemodel’spredictionstoguideexploration)wouldallowustotackleamorediversesetofenvironments,bothsimulatedandreal-world.Theactionpriorandsegment-basedpolicycouldbeusedasastartingpointforhierarchi-calreinforcement-learningalgorithms.Leveragingexistingworkonfew-shotlearningcouldhelpﬁnetuneadynamicsmodelduringthepolicylearningprocess.Suchapproachescouldyieldsigniﬁcantadvancesinreinforcementlearning,improvingbothsampleefﬁciencyandknowledgetransferbetweenrelatedtasks.PredictionandControlwithTemporalSegmentModelsAcknowledgementsWorkdoneatBerkeleywassupportedinpartbyanONRPECASEaward.ReferencesAbbeel,PieterandNg,AndrewY.Learningﬁrst-ordermarkovmodelsforcontrol.InAdvancesinNeuralIn-formationProcessingSystems(NIPS),2004.Agrawal,Pulkit,Nair,Ashvin,Abbeel,Pieter,Malik,Ji-tendra,andLevine,Sergey.Learningtopokebypoking:Experientiallearningofintuitivephysics.InAdvancesinNeuralInformationProcessingSystems(NIPS),2016.Atkeson,ChristopherG,Babu,BPW,Banerjee,N,Beren-son,D,Bove,CP,Cui,X,DeDonato,M,Du,R,Feng,S,Franklin,P,etal.Whathappenedatthedarparoboticschallenge,andwhy.submittedtotheDRCFinalsSpecialIssueoftheJournalofFieldRobotics,2016.Bengio,Samy,Vinyals,Oriol,Jaitly,Navdeep,andShazeer,Noam.Scheduledsamplingforsequencepre-dictionwithrecurrentneuralnetworks.InAdvancesinNeuralInformationProcessingSystems(NIPS),2015.Boedecker,Joschka,Springenberg,JostTobias,Wlﬁng,Jan,andRiedmiller,Martin.Approximatereal-timeop-timalcontrolbasedonsparsegaussianprocessmodels.InAdaptiveDynamicProgrammingandReinforcementLearning(ADPRL),2014.Chen,Xi,Kingma,DiederikP,Salimans,Tim,Duan,Yan,Dhariwal,Prafulla,Schulman,John,Sutskever,Ilya,andAbbeel,Pieter.Variationallossyautoencoder.arXivpreprintarXiv:1611.02731,2016.Deisenroth,MarcPeterandRasmussen,CarlEdward.Pilco:Amodel-basedanddata-efﬁcientapproachtopol-icysearch.InInternationalConferenceonMachineLearning(ICML),2011.Finn,ChelseaandLevine,Sergey.Deepvisualfore-sightforplanningrobotmotion.arXivpreprintarXiv:1610.00696,2016.Finn,Chelsea,Tan,XinYu,Duan,Yan,Darrell,Trevor,Levine,Sergey,andAbbeel,Pieter.Deepspatialautoen-codersforvisuomotorlearning.InInternationalConfer-enceonRoboticsandAutomation(ICRA),2016.Fragkiadaki,Katerina,Agrawal,Pulkit,Levine,Sergey,andMalik,Jitendra.Learningvisualpredictivemodelsofphysicsforplayingbilliards.InInternationalConfer-enceonLearningRepresentations(ICLR),2015.Fu,Justin,Levine,Sergey,andAbbeel,Pieter.One-shotlearningofmanipulationskillswithonlinedynamicsadaptationandneuralnetworkpriors.InInternationalConferenceonIntelligentRobotsandSystems(IROS),2016.Heess,Nicolas,Wayne,Gregory,Silver,David,Lillicrap,Tim,Erez,Tom,andTassa,Yuval.Learningcon-tinuouscontrolpoliciesbystochasticvaluegradients.InAdvancesinNeuralInformationProcessingSystems(NIPS),2015.Johnson,Matthew,Duvenaud,DavidK,Wiltschko,Alex,Adams,RyanP,andDatta,SandeepR.Composinggraphicalmodelswithneuralnetworksforstructuredrepresentationsandfastinference.InAdvancesinNeu-ralInformationProcessingSystems(NIPS),2016.Kingma,DiederikandBa,Jimmy.Adam:Amethodforstochasticoptimization.InInternationalConferenceonLearningRepresentations(ICLR),2015.Kingma,DiederikPandWelling,Max.Auto-encodingvariationalbayes.InInternationalConferenceonLearn-ingRepresentations(ICLR),2014.Ko,JonathanandFox,Dieter.Gp-bayesﬁlters:Bayesianﬁlteringusinggaussianprocesspredictionandob-servationmodels.Auton.Robots,27(1):75–90,2009.URLhttp://dblp.uni-trier.de/db/journals/arobots/arobots27.html#KoF09.Lawrence,Neil,Seeger,Matthias,Herbrich,Ralf,etal.Fastsparsegaussianprocessmethods:Theinformativevectormachine.AdvancesinNeuralInformationPro-cessingSystems(NIPS),2003.Lenz,Ian,Knepper,Ross,andSaxena,Ashutosh.Deepmpc:Learningdeeplatentfeaturesformodelpre-dictivecontrol.InRobotics:ScienceandSystems(RSS),2015.Littman,MichaelL,Sutton,RichardS,Singh,Satinder,etal.Predictiverepresentationsofstate.AdvancesinNeuralInformationProcessingSystems(NIPS),2002.Mordatch,Igor,Mishra,Nikhil,Eppner,Clemens,andAbbeel,Pieter.Combiningmodel-basedpolicysearchwithonlinemodellearningforcontrolofphysicalhu-manoids.InInternationalConferenceonRoboticsandAutomation(ICRA),2016.Oh,Junhyuk,Guo,Xiaoxiao,Lee,Honglak,Lewis,RichardL,andSingh,Satinder.Action-conditionalvideopredictionusingdeepnetworksinatarigames.InAdvancesinNeuralInformationProcessingSystems(NIPS),2015.PredictionandControlwithTemporalSegmentModelsPeters,JanandSchaal,Stefan.Policygradientmethodsforrobotics.InInternationalConferenceonIntelligentRobotsandSystems(IROS),2006.Pratt,Jerry,Carff,John,Drakunov,Sergey,andGoswami,Ambarish.Capturepoint:Astepto-wardhumanoidpushrecovery.InProceedingsoftheSixthIEEE-RASInternationalConferenceonHumanoidRobots(Humanoids2006),pp.200–207.IEEE,2006.URLhttp://www.ambarish.com/paper/Pratt_Goswami_Humanoids2006.pdf.Punjani,AliandAbbeel,Pieter.Deeplearningheli-copterdynamicsmodels.InInternationalConferenceonRoboticsandAutomation(ICRA),2015.Raibert,M.H.LeggedRobotsthatBalance.ArtiﬁcialIntelligence.MITPress,1986.ISBN9780262181174.URLhttps://books.google.com/books?id=EXRiBnQ37RwC.Rosencrantz,Matthew,Gordon,Geoff,andThrun,Sebas-tian.Learninglowdimensionalpredictiverepresenta-tions.InInternationalConferenceonMachineLearning(ICML),2004.Shen,Yirong,Ng,Andrew,andSeeger,Matthias.Fastgaussianprocessregressionusingkd-trees.AdvancesinNeuralInformationProcessingSystems(NIPS).Sutton,RichardS,McAllester,DavidA,Singh,Satin-derP,Mansour,Yishay,etal.Policygradientmethodsforreinforcementlearningwithfunctionapproximation.InAdvancesinNeuralInformationProcessingSystems(NIPS),1999a.Sutton,RichardS.,Precup,Doina,andSingh,Satinder.Betweenmdpsandsemi-mdps:Aframeworkfortem-poralabstractioninreinforcementlearning.ArtiﬁcialIntelligence,112(1):181–211,1999b.ISSN0004-3702.doi:http://dx.doi.org/10.1016/S0004-3702(99)00052-1.URLhttp://www.sciencedirect.com/science/article/pii/S0004370299000521.vandenOord,A¨aron,Dieleman,Sander,Zen,Heiga,Si-monyan,Karen,Vinyals,Oriol,Graves,Alex,Kalch-brenner,Nal,Senior,AndrewW.,andKavukcuoglu,Ko-ray.Wavenet:Agenerativemodelforrawaudio.CoRR,abs/1609.03499,2016a.URLhttp://arxiv.org/abs/1609.03499.vandenOord,Aaron,Kalchbrenner,Nal,Espeholt,Lasse,Vinyals,Oriol,Graves,Alex,etal.Conditionalimagegenerationwithpixelcnndecoders.InAdvancesinNeu-ralInformationProcessingSystems(NIPS),2016b.Venkatraman,Arun,Hebert,Martial,andBagnell,JAn-drew.Improvingmulti-steppredictionoflearnedtimeseriesmodels.InAAAI,pp.3024–3030,2015.Venkatraman,Arun,Capobianco,Roberto,Pinto,Lerrel,Hebert,Martial,Nardi,Daniele,andBagnell,JAn-drew.Improvedlearningofdynamicsmodelsforcontrol.InInternationalSymposiumonExperimentalRobotics(ISER),2016.Vezhnevets,Alexander(Sasha),Mnih,Volodymyr,Aga-piou,John,Osindero,Simon,Graves,Alex,Vinyals,Oriol,andKavukcuoglu,Koray.Strategicattentivewriterforlearningmacro-actions.InAdvancesinNeuralInformationProcessingSystems(NIPS),2016.Watter,Manuel,Springenberg,Jost,Boedecker,Joschka,andRiedmiller,Martin.Embedtocontrol:Alocallylin-earlatentdynamicsmodelforcontrolfromrawimages.InAdvancesinNeuralInformationProcessingSystems(NIPS),2015.Williams,ChristopherKIandRasmussen,CarlEdward.Gaussianprocessesforregression.Advancesinneuralinformationprocessingsystems,pp.514–520,1996.Williams,RonaldJ.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.Machinelearning,8(3-4):229–256,1992.Yip,MichaelC.andCamarillo,DavidB.Model-LessFeedbackControlofContinuumManipulatorsinCon-strainedEnvironments.IEEETransactionsonRobotics,30(4):880–889,August2014.ISSN1552-3098,1941-0468.doi:10.1109/TRO.2014.2309194.00005.PredictionandControlwithTemporalSegmentModelsA.AppendixHerewegiveamoreprecisedescriptionofthearchitec-turesofthemodelsweintroducedinthepaper.BoththedynamicsmodelandthelatentactionpriorweretrainedusingAdamwiththedefaultparameters.A.1.DynamicsModelThefollowingﬁguredepictsthedetailedencoderandde-coderarchitecturesforourdynamicsmodels.Theencoderuses1D-convolutions(acrossthetemporaldimension)andtheReLUactivationfunction.Thedecoderisautoregres-sive,usingdilatedcausal1D-convolutionsandthegatedactivationfunctiondescribedinSection3.1.ThelayersizesindicatedbelowcorrespondtothemodelwetrainedforthebasicReacherenvironment.Fortheobsta-cleandpushingenvironments,weusedthesameencoderarchitecture.Thedecoderforthoseenvironmentshad64channelsinalllayers,andhadanadditional1×1convolu-tionwith128channelsbeforetheﬁnallayer.X+2 x 1 convstride 132 cEncoder, Q(x)2 x 1 convstride 216 catten, projectZ2ZZInputSampled OutputDecoder, P(x)U+UXZ32 c2 x 1 dilated convrate 132 c3 x 1 dilated convrate 232 c2 x 1 dilated convrate 41 x 1 convX+InputOutputDynamics ModelPredictionandControlwithTemporalSegmentModelsA.2.LatentActionPriorThearchitectureforthelatentactionpriorisquitesimilartothatofourdynamicsmodelsasdepictedonthepreviouspage.Weusedthesamearchitectureforallexperiments.2 x 1 convstride 112 cEncoder, Q(u)2 x 1 convstride 212 catten, projectZ2ZZInputSampled OutputDecoder, P(u)UZ32 c2 x 1 dilated convrate 132 c3 x1 dilated convrate 232 c2 x 1 dilated convrate 41 x 1 convInputOutputLatent Action PriorPredictionandControlwithTemporalSegmentModelsA.3.PolicyOptimizationTheone-steppolicieshadtwohiddenlayersofsize64,andusedtheReLUactivationfunction.Thefollowingﬁgureillustratesthearchitectureofthesegment-basedpolicy,asintroducedinSection4.3.Itusesthesamegatedactivationfunctionasthedecodersoftheothertwomodels.Thelatentcodethatthepolicyproducesisusedbytheactionpriortogenerateasequenceofactions,ofwhichtheﬁrstoneisexecuted.Latent Action Prior, P(u)2 x 1 convstride 116 cPolicy (Z | X, U) 3 x 1 convstride 216 catten, projectZ2Z  is constantZInputSampleP(u)U+Segment-Based PolicyUXutOutputForallpolicyoptimizationexperiments,weusedadiscountfactorγ=0.99,andcomputedthegradientofthepolicy’svaluewithrespecttoitsparameters(asdiscussedinSection4.3)usingbackpropagationthroughtimeover50timesteps.Weinterpretedeachpolicy’soutputasthemeanofadi-agonalGaussiandistribution,andsampledactionsusingaconstantstandarddeviationof0.01.PolicyupdateswerecomputedusingAdamover20-episodebatches,wherethestepsizewasinitializedto10−3anddecayedbyafactorof0.9per100iterationsuntilitreached10−4(allotherpa-rameterswereleftatthedefaults).
8
1
0
2

t
c
O
4
1

]

G
L
.
s
c
[

4
v
0
4
4
6
0
.
4
0
7
1
:
v
i
X
r
a

Equivalence Between Policy Gradients and Soft Q-Learning

John Schulman1, Xi Chen1,2, and Pieter Abbeel1,2

1OpenAI

2UC Berkeley, EECS Dept.

joschu, peter, pieter
{

@openai.com
}
Abstract

Two of the leading approaches for model-free reinforcement learning are policy gradient methods
and Q-learning methods. Q-learning methods can be eﬀective and sample-eﬃcient when they work,
however, it is not well-understood why they work, since empirically, the Q-values they estimate are very
inaccurate. A partial explanation may be that Q-learning methods are secretly implementing policy
gradient updates: we show that there is a precise equivalence between Q-learning and policy gradient
methods in the setting of entropy-regularized reinforcement learning, that “soft” (entropy-regularized)
Q-learning is exactly equivalent to a policy gradient method. We also point out a connection between
Q-learning methods and natural policy gradient methods.

Experimentally, we explore the entropy-regularized versions of Q-learning and policy gradients, and
we ﬁnd them to perform as well as (or slightly better than) the standard variants on the Atari benchmark.
We also show that the equivalence holds in practical settings by constructing a Q-learning method that
closely matches the learning dynamics of A3C without using a target network or (cid:15)-greedy exploration
schedule.

1 Introduction

|

Policy gradient methods (PG) and Q-learning (QL) methods perform updates that are qualitatively similar.
In both cases, if the return following an action at is high, then that action is reinforced: in policy gradient
st) is increased; whereas in Q-learning methods, the Q-value Q(st, at) is
methods, the probability π(at
increased. The connection becomes closer when we add entropy regularization to these algorithms. With
an entropy cost added to the returns, the optimal policy has the form π(a
exp(Q(s, a)); hence policy
gradient methods solve for the optimal Q-function, up to an additive constant (Ziebart [2010]). O’Donoghue
et al. [2016] also discuss the connection between the ﬁxed points and updates of PG and QL methods, though
the discussion of ﬁxed points is restricted to the tabular setting, and the discussion comparing updates is
informal and shows an approximate equivalence. Going beyond past work, this paper shows that under
appropriate conditions, the gradient of the loss function used in n-step Q-learning is equal to the gradient
of the loss used in an n-step policy gradient method, including a squared-error term on the value function.
Altogether, the update matches what is typically done in “actor-critic” policy gradient methods such as
A3C, which explains why Mnih et al. [2016] obtained qualitatively similar results from policy gradients and
n-step Q-learning.

s)

∝

|

Section 2 uses the bandit setting to provide the reader with a simpliﬁed version of our main calculation.
(The main calculation applies to the MDP setting.) Section 3 discusses the entropy-regularized formulation
of RL, which is not original to this work, but is included for the reader’s convenience. Section 4 shows that
the soft Q-learning loss gradient can be interpreted as a policy gradient term plus a baseline-error-gradient
term, corresponding to policy gradient instantiations such as A3C [Mnih et al., 2016]. Section 5 draws
a connection between QL methods that use batch updates or replay-buﬀers, and natural policy gradient
methods.

Some previous work on entropy regularized reinforcement learning (e.g., O’Donoghue et al. [2016],
Nachum et al. [2017]) uses entropy bonuses, whereas we use a penalty on Kullback-Leibler (KL) diver-
gence, which is a bit more general. However, in the text, we often refer to “entropy” terms; this refers to
“relative entropy”, i.e., the KL divergence.

2 Bandit Setting

Let’s consider a bandit problem with a discrete or continuous action space: at each timestep the agent
a), where P is unknown to the agent. Let
chooses an action a, and the reward r is sampled according to P (r

|

1

 
 
 
 
 
 
¯r(a) = E [r
timestep reward of the policy π is Ea∼π [r] = (cid:80)
η(π), an entropy-regularized version of this objective:

a], and let π denote a policy, where π(a) is the probability of action a. Then, the expected per-
a π(a)¯r(a) or (cid:82)da π(a)¯r(a). Let’s suppose we are maximizing

|

η(π) = Ea∼π,r [r]

τ DKL [π

π]

(cid:107)

−

(1)

where π is some “reference” policy, τ is a “temperature” parameter, and DKL is the Kullback-Leibler
divergence. Note that the temperature τ can be eliminated by rescaling the rewards. However, we will
leave it so that our calculations are checkable through dimensional analysis, and to make the temperature-
dependence more explicit.

First, let us calculate the policy π that maximizes η. We claim that η(π) is maximized by πB

¯r , deﬁned as

¯r (a) = π(a) exp(¯r(a)/τ )/ Ea(cid:48)∼π [exp(¯r(a(cid:48))/τ )]
πB
(cid:125)
(cid:123)(cid:122)
normalizing constant

(cid:124)

.

To derive this, consider the KL divergence between π and πB
¯r :
¯r (a)(cid:3)

(cid:13)
(cid:13) πB
¯r

DKL

(cid:2)π

(cid:3) = Ea∼π
(cid:2)log π(a)
= Ea∼π [log π(a)
= DKL [π

π]

(cid:107)

−

−

log πB
log π(a)

¯r(a)/τ + log Ea∼π [exp(¯r(a)/τ )]]

−
Ea∼π [¯r(a)/τ ] + log Ea∼π [exp(¯r(a)/τ )]

−

Rearranging and multiplying by τ ,

Ea∼π [¯r(a)]

τ DKL [π

−

(cid:107)

π] = τ log Ea∼π [exp(¯r(a)/τ )]

τ DKL

(cid:2)π (cid:13)

(cid:13) πB
¯r

(cid:3)

−

(2)

(3)

(4)

(5)

(6)

Clearly the left-hand side is maximized (with respect to π) when the KL term on the right-hand side is
minimized (as the other term does not depend on π), and DKL

(cid:3) is minimized at π = πB
¯r .

(cid:2)π (cid:13)

(cid:13) πB
¯r

The preceding calculation gives us the optimal policy when ¯r is known, but in the entropy-regularized
bandit problem, it is initially unknown, and the agent learns about it by sampling. There are two approaches
for solving the entropy-regularized bandit problem:

1. A direct, policy-based approach, where we incrementally update the agent’s policy π based on stochastic

gradient ascent on η.

2. An indirect, value-based approach, where we learn an action-value function qθ that estimates and

approximates ¯r, and we deﬁne π based on our current estimate of qθ.

For the policy-based approach, we can obtain unbiased estimates the gradient of η. For a parameterized
policy πθ, the gradient is given by

θη(πθ) = Ea∼πθ,r [

∇

∇

θ log πθ(a)r

τ

∇

−

θDKL [πθ

π]] .

(cid:107)

We can obtain an unbiased gradient estimate using a single sample (a, r).

In the indirect, value-based approach approach, it is natural to use a squared-error loss:

Taking the gradient of this loss, with respect to the parameters of qθ, we get

Lπ(θ) := 1
2

Ea∼π,r

(cid:104)
(qθ(a)

r)2(cid:105)

−

(7)

(8)

θLπ(θ) = Ea∼π,r [

θqθ(a)(qθ(a)

r)]

(9)

∇
Soon, we will calculate the relationship between this loss gradient and the policy gradient from Equation (7).
In the indirect, value-based approach, a natural choice for policy π is the one that would be optimal if

∇

−

qθ = ¯r. Let’s denote this policy, called the Boltzmann policy, by πB

qθ , where

qθ (a) = π(a) exp(qθ(a)/τ )/Ea(cid:48)∼π [exp(qθ(a(cid:48))/τ )] .
πB

(10)

2

It will be convenient to introduce a bit of notation for the normalizing factor; namely, we deﬁne the scalar

vθ = τ log Ea∼π [exp(qθ(a))/τ ]

Then the Boltzmann policy can be written as

πB
qθ (a) = π(a) exp((qθ(a)

vθ)/τ ).

−

Note that the term τ log Ea∼π [exp(¯r(a)/τ )], appeared earlier in Equation (6)). Repeating the calculation
from Equation (2) through Equation (6), but with qθ instead of ¯r,
(cid:2)πB
qθ

(cid:13)
(cid:13) π(cid:3) .

[qθ(a)]

τ DKL

(13)

vθ = Ea∼πB
qθ

Hence, vθ is an estimate of η(πB

−
qθ ), plugging in qθ for ¯r.

Now we shall show the connection between the gradient of the squared-error loss (Equation (9)) and the
policy gradient (Equation (7)). Rearranging Equation (12), we can write qθ in terms of vθ and the Boltzmann
policy πB
qθ :

Let’s substitute this expression for qθ into the squared-error loss gradient (Equation (9)).

qθ(a) = vθ + τ log

(cid:19)

(cid:18) πB
qθ
π(a)

(a)

θLπ(qθ) = Ea∼π,r [
(cid:20)

(cid:18)

∇
= Ea∼π,r

∇
vθ + τ log

θqθ(a)(qθ(a)
(cid:18) πB
qθ
π(a)

(a)

r)]

−
(cid:19)(cid:19)(cid:18)

vθ + τ log

(cid:19)

(cid:18) πB
qθ
π(a)

(a)

(cid:19)(cid:21)

−

r

(cid:18)

= Ea∼π,r

(cid:18)

θ log πB

qθ (a)

vθ + τ log

(cid:19)

(cid:18) πB
qθ
π(a)

(a)

(cid:19)

r

+

−

∇

θvθ

vθ + τ log

(cid:19)

(cid:18) πB
qθ
π(a)

(a)

(cid:19)(cid:21)

r

−

θ

∇

(cid:20)
τ

∇

Note that we have not yet decided on a sampling distribution π. Henceforth, we’ll assume actions were
sampled by π = πB

qθ . Also, note the derivative of the KL-divergence:

θDKL

(cid:2)πB
qθ

(cid:13)
(cid:13) π(cid:3) =

∇

(cid:90)

θ

∇
(cid:90)

da πB

qθ (a) log
(cid:18)

θπB

qθ (a)

log

(cid:19)

(a)

(cid:18) πB
qθ
π(a)
(cid:18) πB
qθ
π(a)

(a)

=

da

∇

(cid:19)

(cid:19)

+ πB

qθ (a)

1
(a)

πB
qθ

(cid:90)

moving gradient inside and using identity

da

(cid:90)

=

da πB

θ log πB

qθ (a)
(cid:20)

∇

(cid:19)

(cid:18) πB
qθ
π(a)

(a)

qθ (a) log
(cid:18) πB
qθ
π(a)

(a)

(cid:19)(cid:21)

= Ea∼πB
qθ

∇

θ log πB

qθ (a) log

Continuing from Equation (17) but setting π = πB
qθ ,

θπB

qθ (a)=0

∇

θLπ(qθ)(cid:12)

(cid:12)π=πB
qθ

∇

= Ea∼πB
qθ

,r

(cid:2)τ

∇

θ log πB

qθ (a)(vθ
(cid:2)vθ
(cid:2)πB
qθ

,r

r) + τ 2
θDKL
∇
−
(cid:2)πB
(cid:0)vθ + τ DKL
qθ
(cid:13)
(cid:13) π(cid:3)(cid:3)
θEa∼π,r
(cid:125)

∇
(cid:124)

+

(cid:13)
(cid:13) π(cid:3)(cid:3)
r(cid:1)(cid:3)

(cid:2)πB
qθ
(cid:13)
(cid:13) π(cid:3)
−
(cid:2) 1
2 (vθ
(r
(cid:123)(cid:122)
value error gradient

−

−

+
∇
(cid:2)r

θEa∼πB
qθ
τ DKL
(cid:123)(cid:122)
policy gradient

−

,r

τ DKL [π

π]))2(cid:3)
(cid:125)

(cid:107)

(cid:12)
(cid:12)π=πB
qθ

θEa∼πB
qθ

=

τ

−

∇
(cid:124)

Hence, the gradient of the squared error for our action-value function can be broken into two parts: the ﬁrst
part is the policy gradient of the Boltzmann policy corresponding to qθ, the second part arises from a squared
(cid:13)
(cid:13) π(cid:3).
error objective, where we are ﬁtting vθ to the entropy-augmented expected reward ¯r(a)
Soon we will derive an equivalent interpretation of Q-function regression in the MDP setting, where we
are approximating the state-value function Qπ,γ. However, we ﬁrst need to introduce an entropy-regularized
version of the reinforcement learning problem.

(cid:2)πB
qθ

τ DKL

−

3

(11)

(12)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

3 Entropy-Regularized Reinforcement Learning

We shall consider an entropy-regularized version of the reinforcement learning problem, following various
prior work (Ziebart [2010], Fox et al. [2015], Haarnoja et al. [2017], Nachum et al. [2017]). Speciﬁcally, let
us deﬁne the entropy-augmented return to be (cid:80)∞
t=0 γt(rt
[0, 1] is the
discount factor, τ is a scalar temperature coeﬃcient, and KLt is the Kullback-Leibler divergence between the
current policy π and a reference policy π at timestep t: KLt = DKL [π(
st)]. We will sometimes
s)]. To emulate the eﬀect of a standard
use the notation KL(s) = DKL [π
entropy bonus (up to a constant), one can deﬁne π to be the uniform distribution. The subsequent sections
will generalize some of the concepts from reinforcement learning to the setting where we are maximizing the
entropy-augmented discounted return.

τ KLt) where rt is the reward, γ

π] (s) = DKL [π(

st)

π(

π(

s)

−

· |

· |

· |

· |

∈

(cid:107)

(cid:107)

(cid:107)

3.1 Value Functions

We are obliged to alter our deﬁnitions of value functions to include the new KL penalty terms. We shall
deﬁne the state-value function as the expected return:

Vπ(s) = E

(cid:34) ∞
(cid:88)

t=0

γt(rt

−

τ KLt)

and we shall deﬁne the Q-function as

(cid:34)

Qπ(s, a) = E

r0 +

∞
(cid:88)

t=1

γt(rt

−

τ KLt)

(cid:35)

s0 = s

(cid:35)

s0 = s, a0 = a

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(24)

(25)

Note that this Q-function does not include the ﬁrst KL penalty term, which does not depend on the action
a0. This deﬁnition makes some later expressions simpler, and it leads to the following relationship between
Qπ and Vπ:

which follows from matching terms in the sums in Equations (24) and (25).

Vπ(s) = Ea∼π [Qπ(s, a)]

τ KL(s),

−

(26)

3.2 Boltzmann Policy

Q](s) = arg maxa Q(s, a). With
In standard reinforcement learning, the “greedy policy” for Q is deﬁned as [
entropy regularization, we need to alter our notion of a greedy policy, as the optimal policy is stochastic.
Since Qπ omits the ﬁrst entropy term, it is natural to deﬁne the following stochastic policy, which is called
the Boltzmann policy, and is analogous to the greedy policy:

G

πB
Q(

· |

s) = arg max
{

Ea∼π [Q(s, a)]

τ DKL [π

π] (s)
}

(cid:107)

−

π
= π(a

s) exp(Q(s, a)/τ )/ Ea(cid:48)∼π [exp(Q(s, a(cid:48))/τ )]
(cid:125)

(cid:123)(cid:122)
normalizing constant

(cid:124)

.

|

where the second equation is analogous to Equation (2) from the bandit setting.

Also analogously to the bandit setting, it is natural to deﬁne VQ (a function of Q) as

VQ(s) = τ log Ea(cid:48)∼π [exp(Q(s, a(cid:48))/τ )]

so that

Under this deﬁnition, it also holds that

πB
Q(a

|

s) = π(a

|

s) exp((Q(s, a)

VQ(s))/τ )

−

VQ(s) = E

a∼πB

Q(s) [Q(s, a)]

τ DKL

(cid:2)πB

Q

(cid:13)
(cid:13) π(cid:3) (s)

−

4

(27)

(28)

(29)

(30)

(31)

in analogy with Equation (13). Hence, VQ(s) can be interpreted as an estimate of the expected entropy-
augmented return, under the Boltzmann policy πB
Q.

Another way to interpret the Boltzmann policy is as the exponentiated advantage function. Deﬁning the

advantage function as AQ(s, a) = Q(s, a)

VQ(s), Equation (30) implies that

−

3.3 Fixed-Policy Backup Operators

πB
Q(a | s)
π(a | s) = exp(AQ(s, a)/τ ).

T

The
π operators (for Q and V ) in standard reinforcement learning correspond to computing the expected
return with a one-step lookahead: they take the expectation over one step of dynamics, and then fall back on
the value function at the next timestep. We can easily generalize these operators to the entropy-regularized
setting. We deﬁne

πV ](s) = Ea∼π,(r,s(cid:48))∼P (r,s(cid:48) | s,a) [r

[
T
πQ](s, a) = E(r,s(cid:48))∼P (r,s(cid:48) | s,a) [r + γ(Ea(cid:48)∼π [Q(s(cid:48), a(cid:48))]

τ KL(s) + γV (s(cid:48))]

−

τ KL(s(cid:48)))] .

−

[
T

(32)

(33)

Repeatedly applying the

π operator (

T

n
π V =

T

π(

π(. . .
T
(cid:123)(cid:122)
n times

T
(cid:124)

T

π
(cid:125)

(V )))) corresponds to computing the expected

return with a multi-step lookahead. That is, repeatedly expanding the deﬁnition of

π, we obtain

T

π V ](s) = E
n

(cid:34)n−1
(cid:88)

t=0

γt(rt

−

n
π Q](s, a)

−

τ KL(s) = E

[
T

[
T

τ KLt) + γnV (sn)

(cid:35)

s0 = s

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)n−1
(cid:88)

t=0

γt(rt

−

τ KLt) + γn(Q(sn, an)

τ KLn)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(34)

(cid:35)

s0 = s, a0 = a

.

(35)

As a sanity check, note that in both equations, the left-hand side and right-hand side correspond to estimates
of the total discounted return (cid:80)∞

τ KLt).

t=0 γt(rt

The right-hand side of these backup formulas can be rewritten using “Bellman error” terms δt. To rewrite

−

the state-value (V ) backup, deﬁne

Then we have

δt = (rt

−

τ KLt) + γV (st+1)

V (st)

−

π V ](s) = E
n

[
T

(cid:34)n−1
(cid:88)

t=0

γtδt + γnV (sn)

(cid:35)

s0 = s

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(36)

(37)

3.4 Boltzmann Backups

We can deﬁne another set of backup operators corresponding to the Boltzmann policy, π(a
|
We deﬁne the following Boltzmann backup operator:

s)

π(a
|

∝

s) exp(Q(s, a)/τ ).

Q](s, a) = E(r,s(cid:48))∼P (r,s(cid:48) | s,a)

[

T


r + γ Ea(cid:48)∼GQ [Q(s, a)]


(cid:124)



τ DKL [
−
(cid:123)(cid:122)
(∗)

G

Q

π] (s(cid:48))


(cid:125)

(cid:107)

= E(r,s(cid:48))∼P (r,s(cid:48) | s,a)



r + γ τ log Ea(cid:48)∼π [exp(Q(s(cid:48), a(cid:48))/τ )]



(cid:125)

(cid:124)

(cid:123)(cid:122)
(∗∗)

(38)

(39)

where the simpliﬁcation from (
) to (
∗
setting (Equations (11) and (13)).

∗∗

) follows from the same calculation that we performed in the bandit

The n-step operator

policy. Starting with the equation for

n
π for Q-functions also simpliﬁes in the case that we are executing the Boltzmann
Q, and then using Equation (31)

n
π Q (Equation (35)) and setting π = πB

T

T

5

to rewrite the expected Q-function terms in terms of VQ, we obtain

)nQ](s, a)

[(
TπB

Q

−

τ KL(s) = E

= E

(cid:34)n−1
(cid:88)

t=0
(cid:34)n−1
(cid:88)

t=0

γt(rt

γt(rt

−

−

τ KLt) + γn(Q(sn, an)

τ KLt) + γnVQ(sn)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

τ KLn)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

s0 = s, a0 = a

(40)

(cid:35)

s0 = s, a0 = a

.

(41)

From now on, let’s denote this n-step backup operator by

Q, because

depends on Q.)

T

TπB

Q

One can similarly deﬁne the TD(λ) version of this backup operator

TπB,n. (Note TπB

Q,n (cid:54)

=

T

nQ, even though

Q,1Q =

TπB

[
TπB

Q,λQ] = (1

−

λ)(1 + λ

TπB

Q

+ (λ

TπB

Q

)2 + . . . )

TπB

Q

One can straightforwardly verify by comparing terms that it satisﬁes

Q.

(cid:35)

Q,λQ](s, a) = Q(s, a) + E

[
TπB

(cid:34) ∞
(cid:88)

(γλ)tδt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t=0
τ KLt) + γVQ(st+1)

s0 = s, a0 = a

,

VQ(st).

−

where

δt = (rt

−

3.5 Soft Q-Learning

(42)

(43)

The Boltzmann backup operators deﬁned in the preceding section can be used to deﬁne practical variants
of Q-learning that can be used with nonlinear function approximation. These methods, which optimize the
entropy-augmented return, will be called soft Q-learning. Following Mnih et al. [2015], modern implemen-
tations of Q-learning, and n-step Q-learning (see Mnih et al. [2016]) update the Q-function incrementally
to compute the backup against a ﬁxed target Q-function, which we’ll call Q. In the interval between each
target network update, the algorithm is approximately performing the backup operation Q
Q (1-step)
or Q
Q,nQ (n-step). To perform this approximate minimization, the algorithms minimize the least
← TπB
squares loss

← T

L(Q) = Et,st,at

(cid:2) 1
2 (Q(st, at)

yt = rt + γVQ(st+1)

yt)2(cid:3) , where

−

1-step Q-learning

yt = τ KLt +

n−1
(cid:88)

d=0

γd(rt+d

−

τ KLt+d) + γnVQ(st+n)

n-step Q-learning

= τ KLt +VQ(st) +

n−1
(cid:88)

γdδt+d

where

δt = (rt

d=0
τ KLt) + γVQ(st+1)

−

VQ(st)

−

(44)

(45)

(46)

(47)

Q](st, at), regardless of what
In one-step Q-learning (Equation (45)), yt is an unbiased estimator of [
behavior policy was used to collect the data. In n-step Q-learning (Equation (46)), for n > 1, yt is only an
unbiased estimator of [

Q,nQ](st, at) if actions at, at+1, . . . , at+d−1 are sampled using πB
Q.

T

TπB

3.6 Policy Gradients

Entropy regularization is often used in policy gradient algorithms, with gradient estimators of the form

Et,st,at





∇

θ log πθ(at

st)

|

(cid:88)

t(cid:48)≥t



rt(cid:48)

τ

∇

−

θDKL [πθ

(cid:107)

π] (st)



(48)

(Williams [1992], Mnih et al. [2016]).

6

However, these are not proper estimators of the entropy-augmented return (cid:80)

τ KLt), since they
don’t account for how actions aﬀect entropy at future timesteps. Intuitively, one can think of the KL terms
as a cost for “mental eﬀort”. Equation (48) only accounts for the instantaneous eﬀect of actions on mental
eﬀort, not delayed eﬀects.

t(rt

−

To compute proper gradient estimators, we need to include the entropy terms in the return. We will
deﬁne the discounted policy gradient in the following two equivalent ways—ﬁrst, in terms of the empirical
return; second, in terms of the value functions Vπ and Qπ:

gγ(πθ) = E

= E

(cid:34) ∞
(cid:88)
t=0 ∇
(cid:34) ∞
(cid:88)
t=0 ∇

θ log πθ(at

θ log πθ(at

|

|

(cid:32)

st)

r0 +

∞
(cid:88)

d=1

γd(rt+d

τ KLt+d)

τ

∇

−

−

θDKL [πθ

(cid:107)
(cid:35)

(cid:33)(cid:35)

π] (st)

(49)

st)(Qπ(st, at)

Vπ(st))

τ

∇

−

−

θDKL [πθ

(cid:107)

π] (st)

(50)

In the special case of a ﬁnite-horizon problem—i.e.,
rt = KLt = 0 for all t
T —the undiscounted (γ = 1)
return is ﬁnite, and it is meaningful to compute its
gradient. In this case, g1(πθ) equals the undiscounted
policy gradient:

≥

g1(π) =

(cid:34)T −1
(cid:88)

(rt

t=0

θE

∇

(cid:35)

τ KLt)

−

(51)

This result is obtained directly by considering the
stochastic computation graph for the loss (Schul-
[2015a]), shown in the ﬁgure on the
man et al.
The edges from θ to the KL loss terms
right.
lead to the
π] (st) terms in the gradi-
ent; the edges to the stochastic actions at lead to the
τ KLt+d) terms in the

∇
st) (cid:80)T −1

θ log πθ(at

θDKL [πθ

t=d (rt+d

(cid:107)

∇
gradient.

|

−

Since g1(πθ) computes the gradient of the entropy-regularized return, one interpretation of gγ(πθ) is
that it is an approximation of the undiscounted policy gradient g1(πθ), but that it allows for lower-variance
gradient estimators by ignoring some long-term dependencies. A diﬀerent interpretation of gγ(π) is that it
gives a gradient ﬂow such that π∗ = πB

Q∗ is the (possibly unique) ﬁxed point.

As in the standard MDP setting, one can deﬁne approximations to gγ that use a value function to truncate
the returns for variance reduction. These approximations can take the form of n-step methods (Mnih et al.
[2016]) or TD(λ)-like methods (Schulman et al. [2015b]), though we will focus on n-step returns here. Based
on the deﬁnition of gγ above, the natural choice of variance-reduced estimator is

Et,st,at

(cid:34)

∇

θ log πθ(at

(cid:35)

γdδt+d

n−1
(cid:88)

d=0

st)

|

(52)

where δt was deﬁned in Equation (36).

The state-value function V we use in the above formulas should approximate the entropy augmented
n
π V , by

τ KLt). We can ﬁt V iteratively by approximating the n-step backup V

return (cid:80)∞
minimizing a squared-error loss

t=0 γt(rt

−

← T

L(V ) = Et,st

(cid:2) 1
2 (V (st)
n−1
(cid:88)

yt)2(cid:3) ,

−

where

yt =

γdrt+d + γdV (st+d) = V (st) +

d=0

7

n−1
(cid:88)

d=0

γdδt+d.

(53)

(54)

s0s1...sT−1a0a1...aT−1r0r1...rT−1k0k1...kT−1θ4 Soft Q-learning Gradient Equals Policy Gradient

This section shows that the gradient of the squared-error loss from soft Q-learning (Section 3.5) equals the
policy gradient (in the family of policy gradients described in Section 3.6) plus the gradient of a squared-error
term for ﬁtting the value function. We will not make any assumption about the parameterization of the
Q-function, but we deﬁne Vθ and πθ as the following functions of the parameterized Q-function Qθ:

Vθ(s) := τ log Ea [exp(Qθ(s, a)/τ )]

πθ(a

|

s) := π(a

|

s) exp((Qθ(s, a)

Vθ(s))/τ )

−

(55)

(56)

Here, πθ is the Boltzmann policy for Qθ, and Vθ is the normalizing factor we described above. From these
deﬁnitions, it follows that the Q-function can be written as

Qθ(s, a) = Vθ(s) + τ log πθ(a | s)
π(a | s)

(57)

We will substitute this expression into the squared-error loss function. First, for convenience, let us deﬁne
∆t = (cid:80)n−1

d=0 γdδt+d.

Now, let’s consider the gradient of the n-step soft Q-learning objective:

(58)

(59)

(60)

(61)

(62)

θEt,st,at∼π

∇

(cid:104) 1
2 (cid:107)

Qθ(st, at)

2(cid:105) (cid:12)
(cid:107)

(cid:12)π=πθ

yt

−

swap gradient and expectation, treating state-action distribution as ﬁxed:

= Et,st,at∼π [

∇

θQθ(st, at)(Qθ(st, at)

yt)] (cid:12)

(cid:12)π=πθ

−

replace Qθ using Equation (57), and replace Q-value backup yt by Equation (46):

= Et,st,at∼π

(cid:104)

∇

θQθ(st, at)(τ log π(at | st)

π(at | st) + Vθ(st)

(Vθ(st) + τ DKL [πθ

−

π] (st) + ∆t))

(cid:107)

(cid:105)(cid:12)
(cid:12)π=πθ

cancel out Vθ(st):

= Et,st,at∼π

(cid:104)

∇

θQθ(st, at)(τ log π(at | st)

π(at | st) −

τ DKL [πθ

π] (st)

(cid:107)

−

∆t)

(cid:105)(cid:12)
(cid:12)π=πθ

replace the other Qθ by Equation (57):

= Et,st,at∼π

(cid:104)
(τ

∇

θ log πθ(at

expand out terms:

= Et,st,at∼π

(cid:104)
τ 2

∇

θ log πθ(at

|

|

τ

∇

−

θ log πθ(at

|

st)∆t + τ
(cid:124)

∇

st) +

∇

θVθ(st))

(τ log π(at | st)

π(at | st) −

·

τ DKL [πθ

π] (st)

(cid:107)

−

∆t)

(cid:105)(cid:12)
(cid:12)π=πθ

st) log πθ(at | st)

π(at | st) −

τ 2

θ log πθ(at

|

∇
(cid:124)

st)DKL [πθ

(cid:123)(cid:122)
(∗)

π] (st)
(cid:125)

(cid:107)

θVθ(st) log πθ(at | st)

π(at | st) −

τ
∇
(cid:123)(cid:122)
(∗∗)

θVθ(st)DKL [πθ

(cid:107)

π] (st)
(cid:125)

−∇

θVθ(st)∆t

(cid:105)(cid:12)
(cid:12)π=πθ

(63)

(

(

) vanishes because Ea∼πθ(· | st) [
∗
) vanishes because Ea∼πθ(· | st)
(cid:104)
τ 2

π] (st) + 0

θDKL [πθ

∗∗
= Et,st,at∼π

θ log πθ(at
∇
(cid:105)
(cid:104) πθ(at | st)
π(at | st)

st)

const] = 0

|
·
= DKL [πθ

(cid:107)
st)∆t + 0

τ

θ log πθ(at

∇

|

−

π] (st)

θVθ(st)∆t

− ∇

(cid:105)(cid:12)
(cid:12)π=πθ

∇
rearrange terms:

(cid:107)

= Et,st,at∼π

(cid:104)

τ

−
(cid:124)

∇

θ log πθ(at

st)∆t + τ 2

∇
(cid:123)(cid:122)
policy grad

|

θDKL [πθ

+

π]](st)
(cid:125)

(cid:107)

θ

1
2

(cid:13)
(cid:13)
(cid:13)Vθ(st)
(cid:123)(cid:122)
value function grad

ˆVt

−

(cid:13)
2
(cid:13)
(cid:13)
(cid:125)

∇
(cid:124)

(cid:105)(cid:12)
(cid:12)π=πθ

(64)

(65)

Note that the equivalent policy gradient method multiplies the policy gradient by a factor of τ , relative
to the value function error. Eﬀectively, the value function error has a coeﬃcient of τ −1, which is larger
than what is typically used in practice (Mnih et al. [2016]). We will analyze this choice of coeﬃcient in the
experiments.

8

5 Soft Q-learning and Natural Policy Gradients

The previous section gave a ﬁrst-order view on the equivalence between policy gradients and soft Q-learning;
this section gives a second-order, coordinate-free view. As previous work has pointed out, the natural
gradient is the solution to a regression problem; here we will explore the relation between that problem and
the nonlinear regression in soft Q-learning.

|

∇

(cid:2)(

s))T (

θ log πθ(a

θ log πθ(a

The natural gradient is deﬁned as F −1g, where F is the average Fisher information matrix, F =
Es,a∼π
s)∆],
where ∆ is an estimate of the advantage function. As pointed out by Kakade [2002], the natural gradient
step can be computed as the solution to a least squares problem. Given timesteps t = 1, 2, . . . , T , deﬁne
st). Deﬁne Ψ as the matrix whose tth row is ψt, let ∆ denote the vector whose tth ele-
ψt =
ment is the advantage estimate ∆t, and let (cid:15) denote a scalar stepsize parameter. Consider the least squares
problem

s))(cid:3), and g is the policy gradient estimate g

θ log πθ(at

θ log πθ(a

E [

∇

∇

∇

∝

|

|

|

The least-squares solution is w = (cid:15)(ΨT Ψ)−1ΨT ∆. Note that E (cid:2)ΨT Ψ(cid:3) is the Fisher information matrix F ,
and E (cid:2)ΨT ∆(cid:3) is the policy gradient g, so w is the estimated natural gradient.

min
w

1
2 (cid:107)

Ψw

2
(cid:15)∆
(cid:107)

−

(66)

Now let us interpret the least-squares problem in Equation (66). Ψw is the vector whose tth row is
w. According to the deﬁnition of the gradient, if we perform a parameter update with

θ log πθ(a

s)

θold = (cid:15)w, the change in log πθ(a

|

·

∇
θ

−

s) is as follows, to ﬁrst order in (cid:15):

|

log πθ(a

s)

|

−

log πθold (a

s)

|

θ log πθ(a

≈ ∇

s)

|

·

(cid:15)w = (cid:15)ψ

w

·

Thus, we can interpret the least squares problem (Equation (66)) as solving

min
θ

T
(cid:88)

t=1

1
2 (log πθ(at

st)

|

−

log πθold (at

st)

|

−

(cid:15)∆t)2

(67)

(68)

That is, we are adjusting each log-probility log πθold (at

st) by the advantage function ∆t, scaled by (cid:15).

|
In entropy-regularized reinforcement learning, we have an additional term for the gradient of the KL-

divergence:

g

E [
∇
(cid:104)

∝
= E

∇

θ log πθ(at

θ log πθ(at

st)∆t
(cid:16)

st)

−
∆t

|

|

τ

θ KL[πθ, π](st)]
∇
(cid:16) πθ(at | st)
(cid:104)
(cid:17)
log
τ
π(at | st)

−

(cid:105)(cid:17)(cid:105)

KL[πθ, π](st)

−

(69)

(70)

where the second line used the formula for the KL-divergence (Equation (21)) and the identity that
Eat∼πθ [
st)
least squares problem (to compute F −1g) is

const] = 0 (where the KL term is the constant.) In this case, the corresponding

θ log πθ(at

∇

|

·

min
θ

T
(cid:88)

t=1

(cid:16)

1
2

log πθ(at

st)

|

−

log πθold (at

st)

(cid:15)

−

|

(cid:16)

∆t

−

(cid:104)
log

τ

(cid:16) πθ(at | st)
π(at | st)

(cid:17)

−

KL[πθold , π](st)

(cid:105)(cid:17)(cid:17)2

.

(71)

Now let’s consider Q-learning. Let’s assume that the value function is unchanged by optimization, so
Vθ = Vθold. (Otherwise, the equivalence will not hold, since the value function will try to explain the measured
advantage ∆, shrinking the advantage update.)

1
2 (Qθ(st, at)

−

(cid:16)(cid:16)

yt)2 = 1
2

Vθ(st, at) + τ log
(cid:16) πθ(at | st)
π(at | st)

(cid:17)

τ log

−

(cid:16)

= 1
2

(cid:16) πθ(at | st)
π(at | st)

(cid:17)(cid:17)

−

(∆t + τ KL[πθold , π](st))

(cid:17)2

(Vθold (st) + τ KL[πθold , π](st) + ∆t)

(cid:17)2

(72)

(73)

Evidently, we are regressing log πθ(at
equivalent to the natural policy gradient loss that we obtained above.

st) towards log πθold (at

|

|

st) + ∆t/τ + KL[πθold , π](st). This loss is not

9

We can recover the natural policy gradient by instead solving a damped version of the Q-function regres-
(cid:15))Qθold (st, at) + (cid:15) ˆQt, i.e., we are interpolating between the old value and the

t = (1

sion problem. Deﬁne ˆQ(cid:15)
backed-up value.
ˆQ(cid:15)

t = (1

−

(cid:15))Qθold (st, at) + (cid:15) ˆQt = Qθold (st, at) + (cid:15)( ˆQt

Qθold (st, at) = (Vθ(st) + τ KL[πθold , π](st) + ∆t)

Vθold (st) + τ log

−

ˆQt

−
= ∆t + τ

(cid:104)
KL[πθold , π](st)

ˆQ(cid:15)

−
t = Qθ(st, at)
(cid:16) πθ(at | st)
π(at | st)

(cid:17)

log
(cid:16)

−
(cid:110)

Qθ(st, at)

−

= Vθ(st) + log

= log πθ(at

st)

|

−

−
log πθold (at

(cid:16) πθold (at | st)
π(at | st)

(cid:17)(cid:105)

(cid:17)(cid:17)

(cid:16)

ˆQt
(cid:16) πθold (at | st)
π(at | st)

−

Qθold(st, at) + (cid:15)

Qθold (st, at)

Vθold (st) + log
(cid:16)

st)

(cid:15)

∆t

|

−

−

(cid:104)
log

τ

(cid:17)

(cid:16)

+ (cid:15)
(cid:16) πθold (at | st)
π(at | st)

∆ + τ
(cid:17)

−

−

Qθold (st, at))
(cid:16)

−

(cid:16) πθold (at | st)
π(at | st)

(cid:17)(cid:17)

(cid:104)
KL[πθold , π](st)

(cid:16) πθold (at | st)
π(at | st)

log

−

KL[πθold , π](st)

(cid:105)(cid:17)

(74)

(75)

(76)

(77)
(cid:17)(cid:105)(cid:17)(cid:111)

(78)

which exactly matches the expression in the least squares problem in Equation (71), corresponding to entropy-
regularized natural policy gradient. Hence, the “damped” Q-learning update corresponds to a natural
gradient step.

6 Experiments

To complement our theoretical analyses, we designed experiments to study the following questions:

1. Though one-step entropy bonuses are used in PG methods for neural network policies (Williams [1992],
Mnih et al. [2016]), how do the entropy-regularized RL versions of policy gradients and Q-learning
described in Section 3 perform on challenging RL benchmark problems? How does the “proper”
entropy-regularized policy gradient method (with entropy in the returns) compare to the naive one
(with one-step entropy bonus)? (Section 6.1)

2. How do the entropy-regularized versions of Q-learning (with logsumexp) compare to the standard DQN

of Mnih et al. [2015]? (Section 6.2)

3. The equivalence between PG and soft Q-learning is established in expectation, however, the actual
gradient estimators are slightly diﬀerent due to sampling. Furthermore, soft Q-learning is equivalent
to PG with a particular penalty coeﬃcient on the value function error. Does the equivalence hold
under practical conditions? (Section 6.3)

6.1 A2C on Atari: Naive vs Proper Entropy Bonuses

Here we investigated whether there is an empirical eﬀect of including entropy terms when computing returns,
as described in Section 3. In this section, we compare the naive and proper policy gradient estimators:

naive / 1-step:

∇

log πθ(at

st)

proper:

log πθ(at

|

∇

st)

(cid:32)n−1
(cid:88)

d=0

(cid:33)

γdrt+d

V (st)

−

τ

∇

−

θDKL [πθ

π] (st)

(cid:107)
(cid:33)

(79)

γd(rt+d

τ DKL [πθ

π] (st+d))

V (st)

−

τ

∇

−

θDKL [πθ

(cid:107)

(cid:107)

−

π] (st)

(80)

|
(cid:32)n−1
(cid:88)

d=0

In the experiments on Atari, we take π to be the uniform distribution, which gives a standard entropy bonus
up to a constant.

We start with a well-tuned (synchronous, deterministic) version of A3C (Mnih et al. [2016]), henceforth
called A2C (advantage actor critic), to optimize the entropy-regularized return. We use the parameter
τ = 0.01 and train for 320 million frames. We did not tune any hyperparameters for the “proper” algorithm—
we used the same hyperparameters that had been tuned for the “naive” algorithm.

As shown in Figure 1, the “proper” version yields performance that is the same or possibly greater than
the “naive” version. Hence, besides being attractive theoretically, the entropy-regularized formulation could
lead to practical performance gains.

10

Figure 1: Atari performance with diﬀerent RL objectives. EntRL is A2C modiﬁed to optimize for return
augmented with entropy (instead of KL penalty). Solid lines are average evaluation return over 3 random
seeds and shaded area is one standard deviation.

6.2 DQN on Atari: Standard vs Soft

Here we investigated whether soft Q-learning (which optimizes the entropy-augmented return) performs
diﬀerently from standard “hard” Q-learning on Atari. We made a one-line change to a DQN implementation:

yt = rt + γ max

a(cid:48)

Q(st+1, a(cid:48))
(cid:88)

exp(Q(st+1, a(cid:48))/τ )

Standard

log

−

|A|

“Soft”: KL penalty

exp(Q(st+1, a(cid:48))/τ )

“Soft”: Entropy bonus

yt = rt + γ log

yt = rt + γ log

a(cid:48)
(cid:88)

a(cid:48)

(81)

(82)

(83)

The diﬀerence between the entropy bonus and KL penalty (against uniform) is simply a constant, however,
this constant made a big diﬀerence in the experiments, since a positive constant added to the reward
encourages longer episodes. Note that we use the same epsilon-greedy exploration in all conditions; the
only diﬀerence is the backup equation used for computing yt and deﬁning the loss function.

The results of two runs on each game are shown in Figure 2. The entropy-bonus version with τ = 0.1
seems to perform a bit better than standard DQN, however, the KL-bonus version performs worse, so the
beneﬁt may be due to the eﬀect of adding a small constant to the reward. We have also shown the results for
5-step Q-learning, where the algorithm is otherwise the same. The performance is better on Pong and Q-bert
but worse on other games—this is the same pattern of performance found with n-step policy gradients. (E.g.,
see the A2C results in the preceding section.)

6.3 Entropy Regularized PG vs Online Q-Learning on Atari

Next we investigate if the equivalence between soft Q-learning and PG is relevant in practice—we showed
above that the gradients are the same in expectation, but their variance might be diﬀerent, causing diﬀerent

11

0320MFrames2505007501000125015001750SpaceInvadersA2C (1-step)A2C (proper)0320MFrames0100200300400BreakoutA2C (1-step)A2C (proper)0320MFrames10002000300040005000BeamRiderA2C (1-step)A2C (proper)0320MFrames201001020PongA2C (1-step)A2C (proper)0320MFrames0200040006000800010000120001400016000QbertA2C (1-step)A2C (proper)0320MFrames25050075010001250150017502000SeaquestA2C (1-step)A2C (proper)Figure 2: Diﬀerent variants of soft Q-learning and standard Q-learning, applied to Atari games. Note that
4 frames = 1 timestep.

learning dynamics. For these experiments, we modiﬁed the gradient update rule used in A2C while making
no changes to any algorithmic component, i.e. parallel rollouts, updating parameters every 5 steps, etc. The
Q-function was represented as: Qθ(s, a) = Vθ(s) + τ log πθ(a
s), which can be seen as a form of dueling
architecture with τ log πθ(a
s) being the “advantage stream” (Wang et al. [2015]). Vθ, πθ are parametrized
as the same neural network as A2C, where convolutional layers and the ﬁrst fully connected layer are shared.
πθ(a

s) is used as behavior policy.

|

|

A2C can be seen as optimizing a combination of a policy surrogate loss and a value function loss, weighted

|

by hyperparameter c:

log πθ(at

Lpolicy =

−
(cid:13)
Lvalue = 1
(cid:13)
(cid:13)Vθ(st)
2

st)∆t + τ DKL [πθ
(cid:13)
2
(cid:13)
(cid:13)
La2c = Lpolicy + cLvalue

|
ˆVt

−

π]](st)

(cid:107)

(84)

(85)

(86)

In normal A2C, we have found c = 0.5 to be a robust setting that works across multiple environments. On
the other hand, our theory suggests that if we use this Q-function parametrization, soft Q-learning has the
same expected gradient as entropy-regularized A2C with a speciﬁc weighting c = 1
τ . Hence, for the usual
entropy bonus coeﬃcient setting τ = 0.01, soft Q-learning is implicitly weighting value function loss a lot
more than usual A2C setup (c = 100 versus c = 0.5). We have found that such emphasis on value function
(c = 100) results in unstable learning for both soft Q-learning and entropy-regularized A2C. Therefore, to
make Q-learning exactly match known good hyperparameters used in A2C, we scale gradients that go into
advantage stream by 1

γ and scale gradients that go into value function stream by c = 0.5.
With the same default A2C hyperparameters, learning curves of PG and QL are almost identical in most
games (Figure 3), which indicates that the learning dynamics of both update rules are essentially the same
even when the gradients are approximated with a small number of samples. Notably, the Q-learning method
here demonstrates stable learning without the use of target network or (cid:15) schedule.

12

040MFrames02000400060008000100001200014000BeamRiderNoFrameskip-v3standard, n=5soft (ent), =0.1soft (KL), =0.1soft (ent), =0.01soft (KL), =0.01standard040MFrames0100200300400500600BreakoutNoFrameskip-v3standard, n=5soft (ent), =0.1soft (KL), =0.1soft (ent), =0.01soft (KL), =0.01standard040MFrames0500100015002000EnduroNoFrameskip-v3standard, n=5soft (ent), =0.1soft (KL), =0.1soft (ent), =0.01soft (KL), =0.01standard040MFrames201001020PongNoFrameskip-v3standard, n=5soft (ent), =0.1soft (KL), =0.1soft (ent), =0.01soft (KL), =0.01standard040MFrames02000400060008000100001200014000QbertNoFrameskip-v3standard, n=5soft (ent), =0.1soft (KL), =0.1soft (ent), =0.01soft (KL), =0.01standard040MFrames0200040006000800010000SeaquestNoFrameskip-v3standard, n=5soft (ent), =0.1soft (KL), =0.1soft (ent), =0.01soft (KL), =0.01standardFigure 3: Atari performance with policy gradient vs Q-learning update rules. Solid lines are average evalu-
ation return over 3 random seeds and shaded area is one standard deviation.

7 Related Work

Three recent papers have drawn the connection between policy-based methods and value-based methods,
which becomes close with entropy regularization.

•

O’Donoghue et al. [2016] begin with a similar motivation as the current paper: that a possible expla-
nation for Q-learning and SARSA is that their updates are similar to policy gradient updates. They
decompose the Q-function into a policy part and a value part, inspired by dueling Q-networks (Wang
et al. [2015]):

Q(s, a) = V (s) + τ (log π(a

s) + τ S[π(

· |

|

s)])

(87)

This form is chosen so that the term multiplying τ has expectation zero under π, which is a property
that the true advantage function satisﬁes: Eπ [Aπ] = 0. Note that our work omits that S term, because
it is most natural to deﬁne the Q-function to not include the ﬁrst entropy term. The authors show
that taking the gradient of the Bellman error of the above Q-function leads to a result similar to the
policy gradient. They then propose an algorithm called PGQ that mixes together the updates from
diﬀerent prior algorithms.

•

Nachum et al. [2017] also discuss the entropy-regularized reinforcement learning setting, and develop
an oﬀ-policy method that applies in this setting. Their argument (modiﬁed to use our notation and
KL penalty instead of entropy bonus) is as follows. The advantage function Aπ(s, a) = Qπ(s, a)
Vπ(s)
lets us deﬁne a multi-step consistency equation, which holds even if the actions were sampled from a
diﬀerent (suboptimal) policy. In the setting of deterministic dynamics, Qπ(st, at) = rt + γVπ(st+1),
hence

−

n−1
(cid:88)

t=0

γtAπ(st, at) =

n−1
(cid:88)

t=0

γt(rt + γVπ(st+1)

Vπ(st)) =

−

n−1
(cid:88)

t=0

γtrt + γnVπ(sn)

Vπ(s0)

(88)

−

13

0320MFrames2505007501000125015001750SpaceInvadersPGQL0320MFrames0100200300400500BreakoutPGQL0320MFrames010002000300040005000BeamRiderPGQL0320MFrames201001020PongPGQL0320MFrames025005000750010000125001500017500QbertPGQL0320MFrames2505007501000125015001750SeaquestPGQLIf π is the optimal policy (for the discounted, entropy-augmented return), then it is the Boltzmann
policy for Qπ, thus

This expression for the advantage can be substituted into Equation (88), giving the consistency equation

τ (log π(a

s)

|

−

log π(a

|

s)) = AQπ (s, a)

(89)

n−1
(cid:88)

t=0

γtτ (log π(st, at)

log π(st, at)) =

−

n−1
(cid:88)

t=0

γtrt + γnVπ(sn)

Vπ(s0),

−

(90)

which holds when π is optimal. The authors deﬁne a squared error objective formed from by taking
LHS - RHS in Equation (90), and jointly minimize it with respect to the parameters of π and V . The
resulting algorithm is a kind of Bellman residual minimization—it optimizes with respect to the future
target values, rather than treating them as ﬁxed Scherrer [2010].

•

Haarnoja et al. [2017] work in the same setting of soft Q-learning as the current paper, and they are
concerned with tasks with high-dimensional action spaces, where we would like to learn stochastic
policies that are multi-modal, and we would like to use Q-functions for which there is no closed-form
s) exp(Q(s, a)/τ ). Hence, they use
way of sampling from the Boltzmann distribution π(a
a method called Stein Variational Gradient Descent to derive a procedure that jointly updates the Q-
function and a policy π, which approximately samples from the Boltzmann distribution—this resembles
variational inference, where one makes use of an approximate posterior distribution.

π(a

s)

∝

|

|

8 Conclusion

We study the connection between two of the leading families of RL algorithms used with deep neural net-
In a framework of entropy-regularized RL we show that soft Q-learning is equivalent to a policy
works.
gradient method (with value function ﬁtting) in terms of expected gradients (ﬁrst-order view).
In addi-
tion, we also analyze how a damped Q-learning method can be interpreted as implementing natural policy
gradient (second-order view). Empirically, we show that the entropy regularized formulation considered in
our theoretical analysis works in practice on the Atari RL benchmark, and that the equivalence holds in a
practically relevant regime.

9 Acknowledgements

We would like to thank Matthieu Geist for pointing out an error in the ﬁrst version of this manuscript, Chao
Gao for pointing out several errors in the second version, and colleagues at OpenAI for insightful discussions.

References

Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates.

arXiv preprint arXiv:1512.08562, 2015.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep

energy-based policies. arXiv preprint arXiv:1702.08165, 2017.

Sham Kakade. A natural policy gradient. Advances in neural information processing systems, 2:1531–1538,

2002.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. arXiv
preprint arXiv:1602.01783, 2016.

14

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and

policy based reinforcement learning. arXiv preprint arXiv:1702.08892, 2017.

Brendan O’Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Pgq: Combining policy

gradient and q-learning. arXiv preprint arXiv:1611.01626, 2016.

Bruno Scherrer. Should one compute the temporal diﬀerence ﬁx point or minimize the bellman residual?

the uniﬁed oblique projection view. arXiv preprint arXiv:1011.4362, 2010.

John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic
computation graphs. In Advances in Neural Information Processing Systems, pages 3528–3536, 2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional

continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.

Ziyu Wang, Nando de Freitas, and Marc Lanctot. Dueling network architectures for deep reinforcement

learning. arXiv preprint arXiv:1511.06581, 2015.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.

Machine learning, 8(3-4):229–256, 1992.

Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy.

2010.

15


7
1
0
2

c
e
D
5

]
I

A
.
s
c
[

3
v
7
1
7
4
0
.
1
1
6
1
:
v
i
X
r
a

#Exploration:AStudyofCount-BasedExplorationforDeepReinforcementLearningHaoranTang1∗,ReinHouthooft34∗,DavisFoote2,AdamStooke2,XiChen2†,YanDuan2†,JohnSchulman4,FilipDeTurck3,PieterAbbeel2†1UCBerkeley,DepartmentofMathematics2UCBerkeley,DepartmentofElectricalEngineeringandComputerSciences3GhentUniversity–imec,DepartmentofInformationTechnology4OpenAIAbstractCount-basedexplorationalgorithmsareknowntoperformnear-optimallywhenusedinconjunctionwithtabularreinforcementlearning(RL)methodsforsolvingsmalldiscreteMarkovdecisionprocesses(MDPs).Itisgenerallythoughtthatcount-basedmethodscannotbeappliedinhigh-dimensionalstatespaces,sincemoststateswillonlyoccuronce.RecentdeepRLexplorationstrategiesareabletodealwithhigh-dimensionalcontinuousstatespacesthroughcomplexheuristics,oftenrelyingonoptimisminthefaceofuncertaintyorintrinsicmotivation.Inthiswork,wedescribeasurprisingﬁnding:asimplegeneralizationoftheclassiccount-basedapproachcanreachnearstate-of-the-artperformanceonvarioushigh-dimensionaland/orcontinuousdeepRLbenchmarks.Statesaremappedtohashcodes,whichallowstocounttheiroccurrenceswithahashtable.Thesecountsarethenusedtocomputearewardbonusaccordingtotheclassiccount-basedexplorationtheory.Weﬁndthatsimplehashfunctionscanachievesurprisinglygoodresultsonmanychallengingtasks.Furthermore,weshowthatadomain-dependentlearnedhashcodemayfurtherimprovetheseresults.Detailedanalysisrevealsimportantaspectsofagoodhashfunction:1)havingappropriategranularityand2)encodinginformationrelevanttosolvingtheMDP.Thisexplorationstrategyachievesnearstate-of-the-artperformanceonbothcontinuouscontroltasksandAtari2600games,henceprovidingasimpleyetpowerfulbaselineforsolvingMDPsthatrequireconsiderableexploration.1IntroductionReinforcementlearning(RL)studiesanagentactinginaninitiallyunknownenvironment,learningthroughtrialanderrortomaximizerewards.Itisimpossiblefortheagenttoactnear-optimallyuntilithassufﬁcientlyexploredtheenvironmentandidentiﬁedalloftheopportunitiesforhighreward,inallscenarios.AcorechallengeinRLishowtobalanceexploration—activelyseekingoutnovelstatesandactionsthatmightyieldhighrewardsandleadtolong-termgains;andexploitation—maximizingshort-termrewardsusingtheagent’scurrentknowledge.WhilethereareexplorationtechniquesforﬁniteMDPsthatenjoytheoreticalguarantees,therearenofullysatisfyingtechniquesforhigh-dimensionalstatespaces;therefore,developingmoregeneralandrobustexplorationtechniquesisanactiveareaofresearch.∗Theseauthorscontributedequally.Correspondenceto:HaoranTang<hrtang@math.berkeley.edu>,ReinHouthooft<rein.houthooft@openai.com>†WorkdoneatOpenAI31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. 
 
 
 
 
 
Mostoftherecentstate-of-the-artRLresultshavebeenobtainedusingsimpleexplorationstrategiessuchasuniformsampling[21]andi.i.d./correlatedGaussiannoise[19,30].Althoughtheseheuristicsaresufﬁcientintaskswithwell-shapedrewards,thesamplecomplexitycangrowexponentially(withstatespacesize)intaskswithsparserewards[25].RecentlydevelopedexplorationstrategiesfordeepRLhaveledtosigniﬁcantlyimprovedperformanceonenvironmentswithsparserewards.Boot-strappedDQN[24]ledtofasterlearninginarangeofAtari2600gamesbytraininganensembleofQ-functions.Intrinsicmotivationmethodsusingpseudo-countsachievestate-of-the-artperformanceonMontezuma’sRevenge,anextremelychallengingAtari2600game[4].VariationalInformationMaximizingExploration(VIME,[13])encouragestheagenttoexplorebyacquiringinformationaboutenvironmentdynamics,andperformswellonvariousroboticlocomotionproblemswithsparserewards.However,wehavenotseenaverysimpleandfastmethodthatcanworkacrossdifferentdomains.Someoftheclassic,theoretically-justiﬁedexplorationmethodsarebasedoncountingstate-actionvisitations,andturningthiscountintoabonusreward.Inthebanditsetting,thewell-knownUCBalgorithmof[18]choosestheactionatattimetthatmaximizesˆr(at)+q2logtn(at)whereˆr(at)istheestimatedreward,andn(at)isthenumberoftimesactionatwaspreviouslychosen.IntheMDPsetting,someofthealgorithmshavesimilarstructure,forexample,ModelBasedIntervalEstimation–ExplorationBonus(MBIE-EB)of[34]countsstate-actionpairswithatablen(s,a)andaddingabonusrewardoftheformβ√n(s,a)toencourageexploringlessvisitedpairs.[16]showthattheinverse-square-rootdependenceisoptimal.MBIEandrelatedalgorithmsassumethattheaugmentedMDPissolvedanalyticallyateachtimestep,whichisonlypracticalforsmallﬁnitestatespaces.Thispaperpresentsasimpleapproachforexploration,whichextendsclassiccounting-basedmethodstohigh-dimensional,continuousstatespaces.Wediscretizethestatespacewithahashfunctionandapplyabonusbasedonthestate-visitationcount.Thehashfunctioncanbechosentoappropriatelybalancegeneralizationacrossstates,anddistinguishingbetweenstates.Weselectproblemsfromrllab[8]andAtari2600[3]featuringsparserewards,anddemonstratenearstate-of-the-artperformanceonseveralgamesknowntobehardfornaïveexplorationstrategies.Themainstrengthofthepresentedapproachisthatitisfast,ﬂexibleandcomplementarytomostexistingRLalgorithms.Insummary,thispaperproposesageneralizationofclassiccount-basedexplorationtohigh-dimensionalspacesthroughhashing(Section2);demonstratesitseffectivenessonchallengingdeepRLbenchmarkproblemsandanalyzeskeycomponentsofwell-designedhashfunctions(Section4).2Methodology2.1NotationThispaperassumesaﬁnite-horizondiscountedMarkovdecisionprocess(MDP),deﬁnedby(S,A,P,r,ρ0,γ,T),inwhichSisthestatespace,Atheactionspace,Patransitionprobabil-itydistribution,r:S×A→Rarewardfunction,ρ0aninitialstatedistribution,γ∈(0,1]adiscountfactor,andTthehorizon.ThegoalofRListomaximizethetotalexpecteddiscountedrewardEπ,PhPTt=0γtr(st,at)ioverapolicyπ,whichoutputsadistributionoveractionsgivenastate.2.2Count-BasedExplorationviaStaticHashingOurapproachdiscretizesthestatespacewithahashfunctionφ:S→Z.Anexplorationbonusr+:S→Risaddedtotherewardfunction,deﬁnedasr+(s)=βpn(φ(s)),(1)whereβ∈R≥0isthebonuscoefﬁcient.Initiallythecountsn(·)aresettozeroforthewholerangeofφ.Foreverystatestencounteredattimestept,n(φ(st))isincreasedbyone.Theagentistrainedwithrewards(r+r+),whileperformanceisevaluatedasthesumofrewardswithoutbonuses.2Algorithm1:Count-basedexplorationthroughstatichashing,usingSimHash1Deﬁnestatepreprocessorg:S→RD2(IncaseofSimHash)InitializeA∈Rk×Dwithentriesdrawni.i.d.fromthestandardGaussiandistributionN(0,1)3Initializeahashtablewithvaluesn(·)≡04foreachiterationjdo5Collectasetofstate-actionsamples{(sm,am)}Mm=0withpolicyπ6ComputehashcodesthroughanyLSHmethod,e.g.,forSimHash,φ(sm)=sgn(Ag(sm))7Updatethehashtablecounts∀m:0≤m≤Masn(φ(sm))←n(φ(sm))+18Updatethepolicyπusingrewards(cid:26)r(sm,am)+β√n(φ(sm))(cid:27)Mm=0withanyRLalgorithmNotethatourapproachisadeparturefromcount-basedexplorationmethodssuchasMBIE-EBsinceweuseastate-spacecountn(s)ratherthanastate-actioncountn(s,a).State-actioncountsn(s,a)areinvestigatedintheSupplementaryMaterial,butnosigniﬁcantperformancegainsoverstatecountingcouldbewitnessed.Apossiblereasonisthatthepolicyitselfissufﬁcientlyrandomtotrymostactionsatanovelstate.Clearlytheperformanceofthismethodwillstronglydependonthechoiceofhashfunctionφ.Oneimportantchoicewecanmakeregardsthegranularityofthediscretization:wewouldlikefor“distant”statestobebecountedseparatelywhile“similar”statesaremerged.Ifdesired,wecanincorporatepriorknowledgeintothechoiceofφ,iftherewouldbeasetofsalientstatefeatureswhichareknowntoberelevant.AshortdiscussiononthismatterisgivenintheSupplementaryMaterial.Algorithm1summarizesourmethod.Themainideaistouselocality-sensitivehashing(LSH)toconvertcontinuous,high-dimensionaldatatodiscretehashcodes.LSHisapopularclassofhashfunctionsforqueryingnearestneighborsbasedoncertainsimilaritymetrics[2].AcomputationallyefﬁcienttypeofLSHisSimHash[6],whichmeasuressimilaritybyangulardistance.SimHashretrievesabinarycodeofstates∈Sasφ(s)=sgn(Ag(s))∈{−1,1}k,(2)whereg:S→RDisanoptionalpreprocessingfunctionandAisak×Dmatrixwithi.i.d.entriesdrawnfromastandardGaussiandistributionN(0,1).Thevalueforkcontrolsthegranularity:highervaluesleadtofewercollisionsandarethusmorelikelytodistinguishstates.2.3Count-BasedExplorationviaLearnedHashingWhentheMDPstateshaveacomplexstructure,asisthecasewithimageobservations,measuringtheirsimilaritydirectlyinpixelspacefailstoprovidethesemanticsimilaritymeasureonewoulddesire.Previousworkincomputervision[7,20,36]introducemanuallydesignedfeaturerepresentationsofimagesthataresuitableforsemantictasksincludingdetectionandclassiﬁcation.Morerecentmethodslearncomplexfeaturesdirectlyfromdatabytrainingconvolutionalneuralnetworks[12,17,31].Consideringtheseresults,itmaybedifﬁcultforamethodsuchasSimHashtoclusterstatesappropriatelyusingonlyrawpixels.Therefore,ratherthanusingSimHash,weproposetouseanautoencoder(AE)tolearnmeaningfulhashcodesinoneofitshiddenlayersasamoreadvancedLSHmethod.ThisAEtakesasinputstatessandcontainsonespecialdenselayercomprisedofDsigmoidfunctions.Byroundingthesigmoidactivationsb(s)ofthislayertotheirclosestbinarynumberbb(s)e∈{0,1}D,anystatescanbebinarized.ThisisillustratedinFigure1foraconvolutionalAE.Aproblemwiththisarchitectureisthatdissimilarinputssi,sjcanmaptoidenticalhashcodesbb(si)e=bb(sj)e,buttheAEstillreconstructsthemperfectly.Forexample,ifb(si)andb(sj)havevalues0.6and0.7ataparticulardimension,thedifferencecanbeexploitedbydeconvolutionallayersinordertoreconstructsiandsjperfectly,althoughthatdimensionroundstothesamebinaryvalue.Onecanimaginereplacingthebottlenecklayerb(s)withthehashcodesbb(s)e,butthengradientscannotbeback-propagatedthroughtheroundingfunction.AsolutionisproposedbyGregoretal.[10]andSalakhutdinov&Hinton[28]istoinjectuniformnoiseU(−a,a)intothesigmoid36×66×66×66×66×66×6b·ecodedownsamplesoftmaxlinear64×52×521×52×5296×24×2496×10×1096×5×52400b(·)512102496×5×596×11×1196×24×241×52×52Figure1:Theautoencoder(AE)architectureforALE;thesolidblockrepresentsthedensesigmoidalbinarycodelayer,afterwhichnoiseU(−a,a)isinjected.Algorithm2:Count-basedexplorationusinglearnedhashcodes1Deﬁnestatepreprocessorg:S→{0,1}Dasthebinarycoderesultingfromtheautoencoder(AE)2InitializeA∈Rk×Dwithentriesdrawni.i.d.fromthestandardGaussiandistributionN(0,1)3Initializeahashtablewithvaluesn(·)≡04foreachiterationjdo5Collectasetofstate-actionsamples{(sm,am)}Mm=0withpolicyπ6Addthestatesamples{sm}Mm=0toaFIFOreplaypoolR7ifjmodjupdate=0then8UpdatetheAElossfunctioninEq.(3)usingsamplesdrawnfromthereplaypool{sn}Nn=1∼R,forexampleusingstochasticgradientdescent9Computeg(sm)=bb(sm)e,theD-dimroundedhashcodeforsmlearnedbytheAE10Projectg(sm)toalowerdimensionkviaSimHashasφ(sm)=sgn(Ag(sm))11Updatethehashtablecounts∀m:0≤m≤Masn(φ(sm))←n(φ(sm))+112Updatethepolicyπusingrewards(cid:26)r(sm,am)+β√n(φ(sm))(cid:27)Mm=0withanyRLalgorithmactivations.Bychoosinguniformnoisewitha>14,theAEisonlycapableof(always)reconstructingdistinctstateinputssi6=sj,ifithaslearnedtospreadthesigmoidoutputssufﬁcientlyfarapart,|b(si)−b(sj)|>(cid:15),inordertocounteracttheinjectednoise.Assuch,thelossfunctionoverasetofcollectedstates{si}Ni=1isdeﬁnedasL(cid:0){sn}Nn=1(cid:1)=−1NNXn=1hlogp(sn)−λKPDi=1minn(1−bi(sn))2,bi(sn)2oi,(3)withp(sn)theAEoutput.Thisobjectivefunctionconsistsofanegativelog-likelihoodtermandatermthatpressuresthebinarycodelayertotakeonbinaryvalues,scaledbyλ∈R≥0.Thereasoningbehindthislattertermisthatitmighthappenthatforparticularstates,acertainsigmoidunitisneverused.Therefore,itsvaluemightﬂuctuatearound12,causingthecorrespondingbitinbinarycodebb(s)etoﬂipovertheagentlifetime.Addingthissecondlosstermensuresthatanunusedbittakesonanarbitrarybinaryvalue.ForAtari2600imageinputs,sincethepixelintensitiesarediscretevaluesintherange[0,255],wemakeuseofapixel-wisesoftmaxoutputlayer[37]thatsharesweightsbetweenallpixels.ThearchitecturaldetailsaredescribedintheSupplementaryMaterialandaredepictedinFigure1.Becausethecodedimensionoftenneedstobelargeinordertocorrectlyreconstructtheinput,weapplyadownsamplingproceduretotheresultingbinarycodebb(s)e,whichcanbedonethroughrandomprojectiontoalower-dimensionalspaceviaSimHashasinEq.(2).Ontheonehand,itisimportantthatthemappingfromstatetocodeneedstoremainrelativelyconsistentovertime,whichisnontrivialastheAEisconstantlyupdatedaccordingtothelatestdata(Algorithm2line8).Asolutionistodownsamplethebinarycodetoaverylowdimension,orbyslowingdownthetrainingprocess.Ontheotherhand,thecodehastoremainrelativelyunique4forstatesthatarebothdistinctandclosetogetherontheimagemanifold.ThisistackledbothbythesecondterminEq.(3)andbythesaturatingbehaviorofthesigmoidunits.StatesalreadywellrepresentedbytheAEtendtosaturatethesigmoidactivations,causingtheresultinglossgradientstobeclosetozero,makingthecodelesspronetochange.3RelatedWorkClassiccount-basedmethodssuchasMBIE[33],MBIE-EBand[16]solveanapproximateBellmanequationasaninnerloopbeforetheagenttakesanaction[34].Assuch,bonusrewardsarepropagatedimmediatelythroughoutthestate-actionspace.Incontrast,contemporarydeepRLalgorithmspropagatethebonussignalbasedonrolloutscollectedfrominteractingwithenvironments,withvalue-based[21]orpolicygradient-based[22,30]methods,atlimitedspeed.Inaddition,ourproposedmethodisintendedtoworkwithcontemporarydeepRLalgorithms,itdiffersfromclassicalcount-basedmethodinthatourmethodreliesonvisitingunseenstatesﬁrst,beforethebonusrewardcanbeassigned,makinguninformedexplorationstrategiesstillanecessityatthebeginning.Fillingthegapsbetweenourmethodandclassictheoriesisanimportantdirectionoffutureresearch.Arelatedlineofclassicalexplorationmethodsisbasedontheideaofoptimisminthefaceofuncertainty[5]butnotrestrictedtousingcountingtoimplement“optimism”,e.g.,R-Max[5],UCRL[14],andE3[15].Thesemethods,similartoMBIEandMBIE-EB,havetheoreticalguaranteesintabularsettings.BayesianRLmethods[9,11,16,35],whichkeeptrackofadistributionoverMDPs,areanalternativetooptimism-basedmethods.Extensionstocontinuousstatespacehavebeenproposedby[27]and[25].Anothertypeofexplorationiscuriosity-basedexploration.Thesemethodstrytocapturetheagent’ssurpriseabouttransitiondynamics.Astheagenttriestooptimizeforsurprise,itnaturallydiscoversnovelstates.Wereferthereaderto[29]and[26]foranextensivereviewoncuriosityandintrinsicrewards.SeveralexplorationstrategiesfordeepRLhavebeenproposedtohandlehigh-dimensionalstatespacerecently.[13]proposeVIME,inwhichinformationgainismeasuredinBayesianneuralnetworksmodelingtheMDPdynamics,whichisusedanexplorationbonus.[32]proposetousethepredictionerrorofalearneddynamicsmodelasanexplorationbonus.Thompsonsamplingthroughbootstrappingisproposedby[24],usingbootstrappedQ-functions.Themostrelatedexplorationstrategyisproposedby[4],inwhichanexplorationbonusisaddedinverselyproportionaltothesquarerootofapseudo-countquantity.Astatepseudo-countisderivedfromitslog-probabilityimprovementaccordingtoadensitymodeloverthestatespace,whichinthelimitconvergestotheempiricalcount.Ourmethodissimilartopseudo-countapproachinthesensethatbothmethodsareperformingapproximatecountingtohavethenecessarygeneralizationoverunseenstates.Thedifferenceisthatadensitymodelhastobedesignedandlearnedtoachievegoodgeneralizationforpseudo-countwhereasinourcasegeneralizationisobtainedbyawiderangeofsimplehashfunctions(notnecessarilySimHash).Anotherinterestingconnectionisthatourmethodalsoimpliesadensitymodelρ(s)=n(φ(s))Noverallvisitedstates,whereNisthetotalnumberofstatesvisited.Anothermethodsimilartohashingisproposedby[1],whichclustersstatesandcountsclustercentersinsteadofthetruestates,butthismethodhasyettobetestedonstandardexplorationbenchmarkproblems.4ExperimentsExperimentsweredesignedtoinvestigateandanswerthefollowingresearchquestions:1.Cancount-basedexplorationthroughhashingimproveperformancesigniﬁcantlyacrossdifferentdomains?HowdoestheproposedmethodcomparetothecurrentstateoftheartinexplorationfordeepRL?2.Whatistheimpactoflearnedorstaticstatepreprocessingontheoverallperformancewhenimageobservationsareused?5Toanswerquestion1,weruntheproposedmethodondeepRLbenchmarks(rllabandALE)thatfeaturesparserewards,andcompareittootherstate-of-the-artalgorithms.Question2isansweredbytryingoutdifferentimagepreprocessorsonAtari2600games.TrustRegionPolicyOptimization(TRPO,[30])ischosenastheRLalgorithmforallexperiments,becauseitcanhandlebothdiscreteandcontinuousactionspaces,canconvenientlyensurestableimprovementinthepolicyperformance,andisrelativelyinsensitivetohyperparameterchanges.ThehyperparameterssettingsarereportedintheSupplementaryMaterial.4.1ContinuousControlTherllabbenchmark[8]consistsofvariouscontroltaskstotestdeepRLalgorithms.Weselectedseveralvariantsofthebasicandlocomotiontasksthatusesparserewards,asshowninFigure2,andadopttheexperimentalsetupasdeﬁnedin[13]—adescriptioncanbefoundintheSupplementaryMaterial.Thesetasksareallhighlydifﬁculttosolvewithnaïveexplorationstrategies,suchasaddingGaussiannoisetotheactions.Figure2:Illustrationsoftherllabtasksusedinthecontinuouscontrolexperiments,namelyMoun-tainCar,CartPoleSwingup,SimmerGather,andHalfCheetah;takenfrom[8].(a)MountainCar(b)CartPoleSwingup(c)SwimmerGather(d)HalfCheetahFigure3:Meanaveragereturnofdifferentalgorithmsonrllabtaskswithsparserewards.Thesolidlinerepresentsthemeanaveragereturn,whiletheshadedarearepresentsonestandarddeviation,over5seedsforthebaselineandSimHash(thebaselinecurveshappentooverlapwiththeaxis).Figure3showstheresultsofTRPO(baseline),TRPO-SimHash,andVIME[13]ontheclassictasksMountainCarandCartPoleSwingup,thelocomotiontaskHalfCheetah,andthehierarchicaltaskSwimmerGather.Usingcount-basedexplorationwithhashingiscapableofreachingthegoalinallenvironments(whichcorrespondstoanonzeroreturn),whilebaselineTRPOwithGaussiancontrolnoisefailscompletely.AlthoughTRPO-SimHashpicksupthesparserewardonHalfCheetah,itdoesnotperformaswellasVIME.Incontrast,theperformanceofSimHashiscomparablewithVIMEonMountainCar,whileitoutperformsVIMEonSwimmerGather.4.2ArcadeLearningEnvironmentTheArcadeLearningEnvironment(ALE,[3]),whichconsistsofAtari2600videogames,isanimportantbenchmarkfordeepRLduetoitshigh-dimensionalstatespaceandwidevarietyofgames.Inordertodemonstratetheeffectivenessoftheproposedexplorationstrategy,sixgamesareselectedfeaturinglonghorizonswhilerequiringsigniﬁcantexploration:Freeway,Frostbite,Gravitar,Montezuma’sRevenge,Solaris,andVenture.Theagentistrainedfor500iterationsinallexperiments,witheachiterationconsistingof0.1Msteps(theTRPObatchsize,correspondsto0.4Mframes).Policiesandvaluefunctionsareneuralnetworkswithidenticalarchitecturesto[22].Althoughthepolicyandbaselinetakeintoaccountthepreviousfourframes,thecountingalgorithmonlylooksatthelatestframe.6Table1:Atari2600:averagetotalrewardaftertrainingfor50Mtimesteps.Boldfacenumbersindicatebestresults.Italicnumbersarethebestamongourmethods.FreewayFrostbiteGravitarMontezumaSolarisVentureTRPO(baseline)16.5286948602758121TRPO-pixel-SimHash31.6468346802897263TRPO-BASS-SimHash28.431506042381201616TRPO-AE-SimHash33.55214482754467445Double-DQN33.316834120306898.0Duelingnetwork0.0467258802251497Gorila11.760510544N/A1245DQNPop-Art33.43469483045441172A3C+27.350724614221750pseudo-count29.21450–3439–369BASSTocomparewiththeautoencoder-basedlearnedhashcode,weproposeusingBasicAb-stractionoftheScreenShots(BASS,alsocalledBasic;see[3])asastaticpreprocessingfunctiong.BASSisahand-designedfeaturetransformationforimagesinAtari2600games.BASSbuildsonthefollowingobservationsspeciﬁctoAtari:1)thegamescreenhasalowresolution,2)mostobjectsarelargeandmonochrome,and3)winningdependsmostlyonknowingobjectlocationsandmotions.WedesignedanadaptedversionofBASS3,thatdividestheRGBscreenintosquarecells,computestheaverageintensityofeachcolorchannelinsideacell,andassignstheresultingvaluestobinsthatuniformlypartitiontheintensityrange[0,255].Mathematically,letCbethecellsize(widthandheight),Bthenumberofbins,(i,j)celllocation,(x,y)pixellocation,andzthechannel,thenfeature(i,j,z)=jB255C2P(x,y)∈cell(i,j)I(x,y,z)k.(4)Afterwards,theresultinginteger-valuedfeaturetensorisconvertedtoanintegerhashcode(φ(st)inLine6ofAlgorithm1).ABASSfeaturecanberegardedasaminiaturethatefﬁcientlyencodesobjectlocations,butremainsinvarianttonegligibleobjectmotions.Itiseasytoimplementandintroduceslittlecomputationoverhead.However,itisdesignedforgenericAtarigameimagesandmaynotcapturethestructureofeachspeciﬁcgameverywell.WecompareourresultstodoubleDQN[39],duelingnetwork[40],A3C+[4],doubleDQNwithpseudo-counts[4],Gorila[23],andDQNPop-Art[38]onthe“nullop”metric4.WeshowtrainingcurvesinFigure4andsummarizeallresultsinTable1.Surprisingly,TRPO-pixel-SimHashalreadyoutperformsthebaselinebyalargemarginandbeatsthepreviousbestresultonFrostbite.TRPO-BASS-SimHashachievessigniﬁcantimprovementoverTRPO-pixel-SimHashonMontezuma’sRevengeandVenture,whereitcapturesobjectlocationsbetterthanothermethods.5TRPO-AE-SimHashachievesnearstate-of-the-artperformanceonFreeway,FrostbiteandSolaris.AsobservedinTable1,preprocessingimageswithBASSorusingalearnedhashcodethroughtheAEleadstomuchbetterperformanceonGravitar,Montezuma’sRevengeandVenture.Therefore,astaticoradaptivepreprocessingstepcanbeimportantforagoodhashfunction.Inconclusion,ourcount-basedexplorationmethodisabletoachieveremarkableperformancegainsevenwithsimplehashfunctionslikeSimHashontherawpixelspace.Ifcoupledwithdomain-dependentstatepreprocessingtechniques,itcansometimesachievefarbetterresults.Areasonwhyourproposedmethoddoesnotachievestate-of-the-artperformanceonallgamesisthatTRPOdoesnotreuseoff-policyexperience,incontrasttoDQN-basedalgorithms[4,23,38]),andis3TheoriginalBASSexploitsthefactthatatmost128colorscanappearonthescreen.Ouradaptedversiondoesnotmakethisassumption.4Theagenttakesnoactionforarandomnumber(within30)offramesatthebeginningofeachepisode.5WeprovidevideosofexamplegameplayandvisualizationsofthedifferencebewteenPixel-SimHashandBASS-SimHashathttps://www.youtube.com/playlist?list=PLAd-UMX6FkBQdLNWtY8nH1-pzYJA_1T5570100200300400500−505101520253035(a)Freeway01002003004005000200040006000800010000(b)Frostbite01002003004005001002003004005006007008009001000TRPO-AE-SimHashTRPOTRPO-BASS-SimHashTRPO-pixel-SimHash(c)Gravitar01002003004005000100200300400500(d)Montezuma’sRevenge0100200300400500−100001000200030004000500060007000(e)Solaris0100200300400500−200020040060080010001200(f)VentureFigure4:Atari2600games:thesolidlineisthemeanaverageundiscountedreturnperiteration,whiletheshadedareasrepresenttheonestandarddeviation,over5seedsforthebaseline,TRPO-pixel-SimHash,andTRPO-BASS-SimHash,whileover3seedsforTRPO-AE-SimHash.hencelessefﬁcientinharnessingextremelysparserewards.Thisexplanationiscorroboratedbytheexperimentsdonein[4],inwhichA3C+(anon-policyalgorithm)scoresmuchlowerthanDQN(anoff-policyalgorithm),whileusingtheexactsameexplorationbonus.5ConclusionsThispaperdemonstratesthatageneralizationofclassicalcountingtechniquesthroughhashingisabletoprovideanappropriatesignalforexploration,evenincontinuousand/orhigh-dimensionalMDPsusingfunctionapproximators,resultinginnearstate-of-the-artperformanceacrossbenchmarks.ItprovidesasimpleyetpowerfulbaselineforsolvingMDPsthatrequireinformedexploration.AcknowledgmentsWewouldliketothankourcolleaguesatBerkeleyandOpenAIforinsightfuldiscussions.ThisresearchwasfundedinpartbyONRthroughaPECASEaward.YanDuanwasalsosupportedbyaBerkeleyAIResearchlabFellowshipandaHuaweiFellowship.XiChenwasalsosupportedbyaBerkeleyAIResearchlabFellowship.WegratefullyacknowledgethesupportoftheNSFthroughgrantIIS-1619362andoftheARCthroughaLaureateFellowship(FL110100281)andthroughtheARCCentreofExcellenceforMathematicalandStatisticalFrontiers.AdamStookegratefullyacknowledgesfundingfromaFannieandJohnHertzFoundationfellowship.ReinHouthooftwassupportedbyaPh.D.FellowshipoftheResearchFoundation-Flanders(FWO).References[1]Abel,David,Agarwal,Alekh,Diaz,Fernando,Krishnamurthy,Akshay,andSchapire,RobertE.Exploratorygradientboostingforreinforcementlearningincomplexdomains.arXivpreprintarXiv:1603.04119,2016.[2]Andoni,AlexandrandIndyk,Piotr.Near-optimalhashingalgorithmsforapproximatenear-estneighborinhighdimensions.InProceedingsofthe47thAnnualIEEESymposiumonFoundationsofComputerScience(FOCS),pp.459–468,2006.[3]Bellemare,MarcG,Naddaf,Yavar,Veness,Joel,andBowling,Michael.Thearcadelearningenvironment:Anevaluationplatformforgeneralagents.JournalofArtiﬁcialIntelligenceResearch,47:253–279,062013.8[4]Bellemare,MarcG,Srinivasan,Sriram,Ostrovski,Georg,Schaul,Tom,Saxton,David,andMunos,Remi.Unifyingcount-basedexplorationandintrinsicmotivation.InAdvancesinNeuralInformationProcessingSystems29(NIPS),pp.1471–1479,2016.[5]Brafman,RonenIandTennenholtz,Moshe.R-max-ageneralpolynomialtimealgorithmfornear-optimalreinforcementlearning.JournalofMachineLearningResearch,3:213–231,2002.[6]Charikar,MosesS.Similarityestimationtechniquesfromroundingalgorithms.InProceedingsofthe34thAnnualACMSymposiumonTheoryofComputing(STOC),pp.380–388,2002.[7]Dalal,NavneetandTriggs,Bill.Histogramsoforientedgradientsforhumandetection.InProceedingsoftheIEEEInternationalConferenceonComputerVisionandPatternRecognition(CVPR),pp.886–893,2005.[8]Duan,Yan,Chen,Xi,Houthooft,Rein,Schulman,John,andAbbeel,Pieter.Benchmarkingdeepreinforcementlearningforcontinouscontrol.InProceedingsofthe33rdInternationalConferenceonMachineLearning(ICML),pp.1329–1338,2016.[9]Ghavamzadeh,Mohammad,Mannor,Shie,Pineau,Joelle,andTamar,Aviv.Bayesianrein-forcementlearning:Asurvey.FoundationsandTrendsinMachineLearning,8(5-6):359–483,2015.[10]Gregor,Karol,Besse,Frederic,JimenezRezende,Danilo,Danihelka,Ivo,andWierstra,Daan.Towardsconceptualcompression.InAdvancesinNeuralInformationProcessingSystems29(NIPS),pp.3549–3557.2016.[11]Guez,Arthur,Heess,Nicolas,Silver,David,andDayan,Peter.Bayes-adaptivesimulation-basedsearchwithvaluefunctionapproximation.InAdvancesinNeuralInformationProcessingSystems(AdvancesinNeuralInformationProcessingSystems(NIPS)),pp.451–459,2014.[12]He,Kaiming,Zhang,Xiangyu,Ren,Shaoqing,andSun,Jian.Deepresiduallearningforimagerecognition.2015.[13]Houthooft,Rein,Chen,Xi,Duan,Yan,Schulman,John,DeTurck,Filip,andAbbeel,Pieter.VIME:Variationalinformationmaximizingexploration.InAdvancesinNeuralInformationProcessingSystems29(NIPS),pp.1109–1117,2016.[14]Jaksch,Thomas,Ortner,Ronald,andAuer,Peter.Near-optimalregretboundsforreinforcementlearning.JournalofMachineLearningResearch,11:1563–1600,2010.[15]Kearns,MichaelandSingh,Satinder.Near-optimalreinforcementlearninginpolynomialtime.MachineLearning,49(2-3):209–232,2002.[16]Kolter,JZicoandNg,AndrewY.Near-bayesianexplorationinpolynomialtime.InProceedingsofthe26thInternationalConferenceonMachineLearning(ICML),pp.513–520,2009.[17]Krizhevsky,Alex,Sutskever,Ilya,andHinton,GeoffreyE.ImageNetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinNeuralInformationProcessingSystems25(NIPS),pp.1097–1105,2012.[18]Lai,TzeLeungandRobbins,Herbert.Asymptoticallyefﬁcientadaptiveallocationrules.AdvancesinAppliedMathematics,6(1):4–22,1985.[19]Lillicrap,TimothyP,Hunt,JonathanJ,Pritzel,Alexander,Heess,Nicolas,Erez,Tom,Tassa,Yuval,Silver,David,andWierstra,Daan.Continuouscontrolwithdeepreinforcementlearning.arXivpreprintarXiv:1509.02971,2015.[20]Lowe,DavidG.Objectrecognitionfromlocalscale-invariantfeatures.InProceedingsofthe7thIEEEInternationalConferenceonComputerVision(ICCV),pp.1150–1157,1999.[21]Mnih,Volodymyr,Kavukcuoglu,Koray,Silver,David,Rusu,AndreiA,Veness,Joel,Bellemare,MarcG,Graves,Alex,Riedmiller,Martin,Fidjeland,AndreasK,Ostrovski,Georg,etal.Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533,2015.9[22]Mnih,Volodymyr,Badia,AdriaPuigdomenech,Mirza,Mehdi,Graves,Alex,Lillicrap,Timo-thyP,Harley,Tim,Silver,David,andKavukcuoglu,Koray.Asynchronousmethodsfordeepreinforcementlearning.arXivpreprintarXiv:1602.01783,2016.[23]Nair,Arun,Srinivasan,Praveen,Blackwell,Sam,Alcicek,Cagdas,Fearon,Rory,DeMaria,Alessandro,Panneershelvam,Vedavyas,Suleyman,Mustafa,Beattie,Charles,Petersen,Stig,etal.Massivelyparallelmethodsfordeepreinforcementlearning.arXivpreprintarXiv:1507.04296,2015.[24]Osband,Ian,Blundell,Charles,Pritzel,Alexander,andVanRoy,Benjamin.DeepexplorationviabootstrappedDQN.InAdvancesinNeuralInformationProcessingSystems29(NIPS),pp.4026–4034,2016.[25]Osband,Ian,VanRoy,Benjamin,andWen,Zheng.Generalizationandexplorationviarandom-izedvaluefunctions.InProceedingsofthe33rdInternationalConferenceonMachineLearning(ICML),pp.2377–2386,2016.[26]Oudeyer,Pierre-YvesandKaplan,Frederic.Whatisintrinsicmotivation?Atypologyofcomputationalapproaches.FrontiersinNeurorobotics,1:6,2007.[27]Pazis,JasonandParr,Ronald.PACoptimalexplorationincontinuousspaceMarkovdecisionprocesses.InProceedingsofthe27thAAAIConferenceonArtiﬁcialIntelligence(AAAI),2013.[28]Salakhutdinov,RuslanandHinton,Geoffrey.Semantichashing.InternationalJournalofApproximateReasoning,50(7):969–978,2009.[29]Schmidhuber,Jürgen.Formaltheoryofcreativity,fun,andintrinsicmotivation(1990–2010).IEEETransactionsonAutonomousMentalDevelopment,2(3):230–247,2010.[30]Schulman,John,Levine,Sergey,Moritz,Philipp,Jordan,MichaelI,andAbbeel,Pieter.Trustregionpolicyoptimization.InProceedingsofthe32ndInternationalConferenceonMachineLearning(ICML),2015.[31]Simonyan,KarenandZisserman,Andrew.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.arXivpreprintarXiv:1409.1556,2014.[32]Stadie,BradlyC,Levine,Sergey,andAbbeel,Pieter.Incentivizingexplorationinreinforcementlearningwithdeeppredictivemodels.arXivpreprintarXiv:1507.00814,2015.[33]Strehl,AlexanderLandLittman,MichaelL.Atheoreticalanalysisofmodel-basedintervalestimation.InProceedingsofthe21stInternationalConferenceonMachineLearning(ICML),pp.856–863,2005.[34]Strehl,AlexanderLandLittman,MichaelL.Ananalysisofmodel-basedintervalestimationforMarkovdecisionprocesses.JournalofComputerandSystemSciences,74(8):1309–1331,2008.[35]Sun,Yi,Gomez,Faustino,andSchmidhuber,Jürgen.Planningtobesurprised:OptimalBayesianexplorationindynamicenvironments.InProceedingsofthe4thInternationalConfer-enceonArtiﬁcialGeneralIntelligence(AGI),pp.41–51.2011.[36]Tola,Engin,Lepetit,Vincent,andFua,Pascal.DAISY:Anefﬁcientdensedescriptorappliedtowide-baselinestereo.IEEETransactionsonPatternAnalysisandMachineIntelligence,32(5):815–830,2010.[37]vandenOord,Aaron,Kalchbrenner,Nal,andKavukcuoglu,Koray.Pixelrecurrentneuralnetworks.InProceedingsofthe33rdInternationalConferenceonMachineLearning(ICML),pp.1747–1756,2016.[38]vanHasselt,Hado,Guez,Arthur,Hessel,Matteo,andSilver,David.Learningfunctionsacrossmanyordersofmagnitudes.arXivpreprintarXiv:1602.07714,2016.[39]vanHasselt,Hado,Guez,Arthur,andSilver,David.DeepreinforcementlearningwithdoubleQ-learning.InProceedingsofthe30thAAAIConferenceonArtiﬁcialIntelligence(AAAI),2016.[40]Wang,Ziyu,deFreitas,Nando,andLanctot,Marc.Duelingnetworkarchitecturesfordeepreinforcementlearning.InProceedingsofthe33rdInternationalConferenceonMachineLearning(ICML),pp.1995–2003,2016.10SupplementaryMaterialAnonymousAuthor(s)AfﬁliationAddressemail1HyperparameterSettings1Throughoutallexperiments,weuseAdam[8]foroptimizingthebaselinefunctionandtheautoen-2coder.HyperparametersforrllabexperimentsaresummarizedinTable1.Herethepolicytakes3astatesasinput,andoutputsaGaussiandistributionN(µ(s),σ2),whereµ(s)istheoutputofa4multi-layerperceptron(MLP)withtanhnonlinearity,andσ>0isastate-independentparameter.5Table1:TRPOhyperparametersforrllabexperimentsExperimentMountainCarCartPoleSwingUpHalfCheetahSwimmerGathererTRPObatchsize5k5k5k50kTRPOstepsize0.01Discountfactorγ0.99Policyhiddenunits(32,32)(32,)(32,32)(64,32)BaselinefunctionlinearlinearlinearMLP:32unitsExplorationbonusβ=0.01SimHashdimensionk=32HyperparametersforAtari2600experimentsaresummarizedinTable2and3.Bydefault,all6convolutionallayersarefollowedbyReLUnonlinearity.7Table2:TRPOhyperparametersforAtariexperimentswithimageinputExperimentTRPO-pixel-SimHashTRPO-BASS-SimHashTRPO-AE-SimHashTRPObatchsize100kTRPOstepsize0.01Discountfactor0.995#randomseeds553Inputpreprocessinggrayscale;downsampledto52×52;eachpixelrescaledto[−1,1]4previousframesareconcatenatedtoformtheinputstatePolicystructure16convﬁltersofsize8×8,stride432convﬁltersofsize4×4,stride2fully-connectlayerwith256unitslineartransformandsoftmaxtooutputactionprobabilities(usebatchnormalization[7]ateverylayer)Baselinestructure(sameaspolicy,exceptthatthelastlayerisasinglescalar)Explorationbonusβ=0.01Hashingparametersk=256cellsizeC=20b(s)size:256bitsB=20binsdownsampledto64bitsTheautoencoderarchitecturewasshowninFigure1ofSection2.3.Speciﬁcally,uniformnoise8U(−a,a)witha=0.3isaddedtothesigmoidactivations.ThelossfunctionEq.(3)(inthemain9Submittedto31stConferenceonNeuralInformationProcessingSystems(NIPS2017).Donotdistribute.Table3:TRPOhyperparametersforAtariexperimentswithRAMinputExperimentTRPO-RAM-SimHashTRPObatchsize100kTRPOstepsize0.01Discountfactor0.995#randomseeds10Inputpreprocessingvectoroflength128intherange[0,255];downsampledto[−1,1]PolicystructureMLP:(32,32,number_of_actions),tanhBaselinestructureMLP:(32,32,1),tanhExplorationbonusβ=0.01SimHashdimensionk=256text),usingλ=10,isupdatedeveryjupdate=3iterations.Thearchitecturelooksasfollows:an10inputlayerofsize52×52,representingtheimageluminanceisfollowedby3consecutive6×611convolutionallayerswithstride2and96ﬁltersfeedintoafullyconnectedlayerofsize1024,which12connectstothebinarycodelayer.Thisbinarycodelayerfeedsintoafully-connectedlayerof102413units,connectingtoafully-connectedlayerof2400units.Thislayerfeedsinto3consecutive6×614transposedconvolutionallayersofwhichtheﬁnaloneconnectstoapixel-wisesoftmaxlayerwith6415bins,representingthepixelintensities.Moreover,labelsmoothingisappliedtothedifferentsoftmax16bins,inwhichthelog-probabilityofeachofthebinsisincreasedby0.003,beforenormalizing.The17softmaxweightsaresharedamongeachpixel.18Inaddition,weapplycountingBloomﬁlters[5]tomaintainasmallhashtable.Detailscanbefound19inAppendix4.202DescriptionoftheAdaptedrllabTasks21Thissectiondescribesthecontinuouscontrolenvironmentsusedintheexperiments.Thetasksare22implementedasdescribedin[4],followingthesparserewardadaptationof[6].Thetaskshavethe23followingstateandactiondimensions:CartPoleSwingup,S⊆R4,A⊆R;MountainCarS⊆R3,24A⊆R1;HalfCheetah,S⊆R20,A⊆R6;SwimmerGather,S⊆R33,A⊆R2.Forthesparse25rewardexperiments,thetaskshavebeenmodiﬁedasfollows.InCartPoleSwingup,theagentreceives26arewardof+1whencos(β)>0.8,withβthepoleangle.InMountainCar,theagentreceives27arewardof+1whenthegoalstateisreached,namelyescapingthevalleyfromtherightside.28Therefore,theagenthastoﬁgureouthowtoswingupthepoleintheabsenceofanyinitialexternal29rewards.InHalfCheetah,theagentreceivesarewardof+1whenxbody>5.Assuch,ithastoﬁgure30outhowtomoveforwardwithoutanyinitialexternalreward.ThetimehorizonissettoT=500for31alltasks.323AnalysisofLearnedBinaryRepresentation33Figure1showsthedownsampledcodeslearnedbytheautoencoderforseveralAtari2600games34(Frostbite,Freeway,andMontezuma’sRevenge).Eachrowdepicts50consecutiveframes(from0to3549,goingfromlefttoright,toptobottom).Thepicturesintherightcolumndepictthebinarycodes36thatcorrespondwitheachoftheseframes(oneframeperrow).Figure2showsthereconstructions37ofseveralsubsequentimagesaccordingtotheautoencoder.Somebinariesstayconsistentacross38frames,andsomeappeartorespondtospeciﬁcobjectsorevents.Althoughtheprecisemeaningof39eachbinarynumberisnotimmediatelyobvious,theﬁguresuggeststhatthelearnedhashcodeisa40reasonableabstractionofthegamestate.414CountingBloomFilter/Count-MinSketch42Weexperimentedwithdirectlybuildingahashingdictionarywithkeysφ(s)andvaluesthestate43counts,butobservedanunnecessaryincreaseincomputationtime.Ourimplementationconvertsthe44integerhashcodesintobinarynumbersandthenintothe“bytes”typeinPython.Thehashtableisa45dictionaryusingthosebytesaskeys.462Figure1:Frostbite,Freeway,andMontezuma’sRevenge:subsequentframes(left)andcorrespondingcode(right);theframesareorderedfromleft(startingwithframenumber0)toright,toptobottom;theverticalaxisintherightimagescorrespondtotheframenumber.3However,analternativetechniquecalledCount-MinSketch[3],withadatastructureidentical47tocountingBloomﬁlters[5],cancountwithaﬁxedintegerarrayandthusreducecomputation48time.Speciﬁcally,letp1,...,plbedistinctlargeprimenumbersanddeﬁneφj(s)=φ(s)modpj.49Thecountofstatesisreturnedasmin1≤j≤lnj(cid:0)φj(s)(cid:1).Toincreasethecountofs,weincrement50nj(cid:0)φj(s)(cid:1)by1forallj.Intuitively,themethodreplacesφbyweakerhashfunctions,whileitreduces51theprobabilityofover-countingbyreportingcountsagreedbyallsuchweakerhashfunctions.The52ﬁnalhashcodeisrepresentedas(cid:0)φ1(s),...,φl(s)(cid:1).53Throughoutallexperimentsabove,theprimenumbersforthecountingBloomﬁlterare999931,54999953,999959,999961,999979,and999983,whichweabbreviateas“6M”.Inaddition,we55experimentedwith6otherprimenumbers,eachapproximately15M,whichweabbreviateas“90M”.56AswecanseeinFigure3,countingstateswithadictionaryorwithBloomﬁltersleadtosimilar57performance,butthecomputationtimeoflatterislower.Moreover,thereislittledifferencebetween58directcountingandusingaverylargertableforBloomﬁlters,astheaveragebonusrewardsare59almostthesame,indicatingthesamedegreeofexploration-exploitationtrade-off.Ontheotherhand,60Bloomﬁltersrequireaﬁxedtablesize,whichmaynotbeknownbeforehand.61TheoryofBloomFiltersBloomﬁlters[2]arepopularfordeterminingwhetheradatasamples062belongstoadatasetD.Supposewehavelfunctionsφjthatindependentlyassigneachdatasample63toanintegerbetween1andpuniformlyatrandom.Initially1,2,...,paremarkedas0.Thenevery64s∈Dis“inserted”throughmarkingφj(s)as1forallj.Anewsamples0isreportedasamember65ofDonlyifφj(s)aremarkedas1forallj.Abloomﬁlterhaszerofalsenegativerate(anys∈Dis66reportedamember),whilethefalsepositiverate(probabilityofreportinganonmemberasamember)67decaysexponentiallyinl.68ThoughBloomﬁlterssupportdatainsertion,itdoesnotallowdatadeletion.CountingBloomﬁlters69[5]maintainacountern(·)foreachnumberbetween1andp.Inserting/deletingscorresponds70toincrementing/decrementingn(cid:0)φj(s)(cid:1)by1forallj.Similarly,sisconsideredamemberif71∀j:n(cid:0)φj(s)(cid:1)=0.72Count-Minsketchisdesignedtosupportmemory-efﬁcientcountingwithoutintroducingtoomany73over-counts.Itmaintainsaseparatecountnjforeachhashfunctionφjdeﬁnedasφj(s)=φ(s)74modpj,wherepjisalargeprimenumber.Forsimplicity,wemayassumethatpj≈p∀jandφj75assignsstoanyof1,...,pwithuniformprobability.76Wenowderivetheprobabilityofover-counting.Letsbeaﬁxeddatasample(notnecessarily77insertedyet)andsupposeadatasetDofNsamplesareinserted.Weassumethatpl(cid:29)N.Let78n:=min1≤j≤lnj(cid:0)φj(s)(cid:1)bethecountreturnedbytheBloomﬁlter.Weareinterestedincomputing79Prob(n>0|s/∈D).Duetoassumptionsaboutφj,weknownj(φ(s))∼Binomial(cid:16)N,1p(cid:17).80Therefore,81Prob(n>0|s/∈D)=Prob(n>0,s/∈D)Prob(s6∈D)=Prob(n>0)−Prob(s∈D)Prob(s/∈D)≈Prob(n>0)Prob(s/∈D)=Qlj=1Prob(nj(φj(s))>0)(1−1/pl)N=(1−(1−1/p)N)l(1−1/pl)N≈(1−e−N/p)le−N/pl≈(1−e−N/p)l.(1)Inparticular,theprobabilityofover-countingdecaysexponentiallyinl.Wereferthereadersto[3]82forotherpropertiesoftheCount-Minsketch.8345RobustnessAnalysis845.1Granularity85Whileourproposedmethodisabletoachieveremarkableresultswithoutrequiringmuchtuning,86thegranularityofthehashfunctionshouldbechosenwisely.Granularityplaysacriticalrolein87count-basedexploration,wherethehashfunctionshouldclusterstateswithoutunder-generalizing88orover-generalizing.Table4summarizesgranularityparametersforourhashfunctions.InTable589wesummarizetheperformanceofTRPO-pixel-SimHashunderdifferentgranularities.Wechoose90FrostbiteandVentureonwhichTRPO-pixel-SimHashoutperformsthebaseline,andchooseasreward91bonuscoefﬁcientβ=0.01×256ktokeepaveragebonusrewardsatapproximatelythesamescale.92k=16onlycorrespondsto65536distincthashcodes,whichisinsufﬁcienttodistinguishbetween93semanticallydistinctstatesandhenceleadstoworseperformance.Weobservedthatk=512tends94tocapturetrivialimagedetailsinFrostbite,leadingtheagenttobelievethateverystateisnewand95equallyworthexploring.Similarresultsareobservedwhiletuningthegranularityparametersfor96TRPO-BASS-SimHashandTRPO-AE-SimHash.97Table4:GranularityparametersofvarioushashfunctionsSimHashk:sizeofthebinarycodeBASSC:cellsizeB:numberofbinsforeachcolorchannelAEk:downstreamSimHashparameterλ:binarizationparameterSmartHashs:gridsizeagent(x,y)coordinatesTable5:Averagescoreat50MtimestepsachievedbyTRPO-pixel-SimHashk1664128256512Frostbite33264029393246831117Venture0218142263306ThebestgranularitydependsonboththehashfunctionandtheMDP.Whileadjustinggranularity98parameter,weobservedthatitisimportanttolowerthebonuscoefﬁcientasgranularityisincreased.99Thisisbecauseahighergranularityislikelytocauselowerstatecounts,leadingtohigherbonus100rewardsthatmayoverwhelmthetruerewards.101ApartfromtheexperimentalresultsshowninTable1inthemaintextandTable5,additional102experimentshavebeenperformedtostudyseveralpropertiesofouralgorithm.1035.2Hyperparametersensitivity104Tostudytheperformancesensitivitytohyperparameterchanges,wefocusonevaluatingTRPO-105RAM-SimHashontheAtari2600gameFrostbite,wherethemethodhasaclearadvantageoverthe106baseline.Becausetheﬁnalscorescanvarybetweendifferentrandomseeds,weevaluatedeachsetof107hyperparameterswith30seeds.Toreducecomputationtimeandcost,RAMstatesareusedinstead108ofimageobservations.109TheresultsaresummarizedinTable6.Herein,kreferstothelengthofthebinarycodeforhashing110whileβisthemultiplicativecoefﬁcientfortherewardbonus,asdeﬁnedinSection2.2ofthemain111text.Thistabledemonstratesthatmosthyperparametersettingsoutperformthebaseline(β=0)112signiﬁcantly.Moreover,theﬁnalscoresshowaclearpatterninresponsetochanginghyperparameters.113Smallβ-valuesleadtoinsufﬁcientexploration,whilelargeβ-valuescausethebonusrewardsto114overwhelmthetruerewards.Withaﬁxedk,thescoresareroughlyconcaveinβ,peakingataround1150.2.Highergranularitykleadstobetterperformance.Therefore,itcanbeconcludedthatthe116proposedexplorationmethodisrobusttohyperparameterchangesincomparisontothebaseline,and117thatthebestparametersettingscanbeobtainedfromarelativelycoarse-grainedgridsearch.1185Table6:TRPO-RAM-SimHashperformancerobustnesstohyperparameterchangesonFrostbiteβk00.010.050.10.20.40.81.6–397–––––––64–87924642243248915871107441128–147542482801323936211543395256–258344974437784935162260374Table7:Averagescoreat50MtimestepsachievedbyTRPO-SmartHashonMontezuma’sRevenge(RAMobservations)s1510204060score259825003533302525001921Table8:InterpretationofparticularRAMentriesinMontezuma’sRevengeIDGroupMeaning3roomroomnumber42agentxcoordinate43agentycoordinate52agentorientation(left/right)27beamson/off83beamsbeamcountdown(on:0,off:36→0)0countercountsfrom0to255andrepeats55counterdeathscenecountdown67objectsDoors,skull,andkeyin1stroom47skullxcoordinate(1stand2ndroom)5.3ACaseStudyofMontezuma’sRevenge119Montezuma’sRevengeiswidelyknownforitsextremelysparserewardsanddifﬁcultexploration120[1].Whileourmethoddoesnotoutperform[1]onthisgame,weinvestigatethereasonsbehindthis121throughvariousexperiments.Theexperimentprocessbelowagaindemonstratestheimportanceofa122hashfunctionhavingthecorrectgranularityandencodingrelevantinformationforsolvingtheMDP.123OurﬁrstattemptistousegameRAMstatesinsteadofimageobservationsasinputstothepolicy,124whichleadstoagamescoreof2500withTRPO-BASS-SimHash.Oursecondattemptistomanually125designahashfunctionthatincorporatesdomainknowledge,calledSmartHash,whichusesan126integer-valuedvectorconsistingoftheagent’s(x,y)location,roomnumberandotherusefulRAM127informationasthehashcode.ThebestSmartHashagentisabletoobtainascoreof3500.Still128theperformanceisnotoptimal.Weobservethataslightchangeintheagent’scoordinatesdoes129notalwaysresultinasemanticallydistinctstate,andthusthehashcodemayremainunchanged.130Thereforewechoosegridsizesandreplacethexcoordinatebyb(x−xmin)/sc(similarlyfory).131Thebonuscoefﬁcientischosenasβ=0.01√stomaintainthescalerelativetothetruereward1(see132Table7).Finally,thebestagentisabletoobtain6600totalrewardsaftertrainingfor1000iterations133(1000Mtimesteps),withagridsizes=10.1341Thebonusscalingischosenbyassumingallstatesarevisiteduniformlyandtheaveragebonusrewardshouldremainthesameforanygridsize.6Table9:Performancecomparisonbetweenstatecounting(leftoftheslash)andstate-actioncounting(rightoftheslash)usingTRPO-RAM-SimHashonFrostbiteβk0.010.050.10.20.40.81.664879/9762464/14912243/39542489/55231587/59851107/2052441/7421281475/8084248/43022801/48023239/72913621/42431543/1941395/3622562583/15844497/54024437/54317849/48723516/31752260/1238374/96Table8liststhesemanticinterpretationofcertainRAMentriesinMontezuma’sRevenge.SmartHash,135asdescribedinSection5.3,makesuseofRAMindices3,42,43,27,and67.“Beamwalls”are136deadlybarriersthatoccurperiodicallyinsomerooms.137Duringourpursuit,wehadanotherinterestingdiscoverythattheidealhashfunctionshouldnot138simplyclusterstatesbytheirvisualsimilarity,butinsteadbytheirrelevancetosolvingtheMDP.We139experimentedwithincludingenemylocationsintheﬁrsttworoomsintoSmartHash(s=10),and140observedthataveragescoredroppedto1672(atiteration1000).Thoughitisimportantfortheagent141tododgeenemies,theagentalsoerroneously“enjoys”watchingenemymotionsatdistance(since142newstatesareconstantlyobserved)and“forgets”thathismainobjectiveistoenterotherrooms.An143alternativehashfunctionkeepsthesameentry“enemylocations”,butinsteadonlyputsrandomly144sampledvaluesinit,whichsurprisinglyachievesbetterperformance(3112).However,byignoring145enemylocationsaltogether,theagentachievesamuchhigherscore(5661)(seeFigure4).Inretrospect,146weexaminethehashcodesgeneratedbyBASS-SimHashandﬁndthatcodesclearlydistinguish147betweenvisuallydifferentstates(includingvariousenemylocations),butfailstoemphasizethatthe148agentneedstoexploredifferentrooms.Againthisexampleshowcasestheimportanceofencoding149relevantinformationindesigninghashfunctions.1505.4Stateandstate-actioncounting151ContinuingtheresultsinTable6,theperformanceofstate-actioncountingisstudiedusingthesame152experimentalsetup,summarizedinTable9.Inparticular,abonusrewardr+(s,a)=β√n(s,a)instead153ofr+(s)=β√n(s)isassigned.Theseresultsshowthattherelativeperformanceofstatecounting154comparedtostate-actioncountingdependshighlyontheselectedhyperparametersettings.However,155wenoticethatthebestperformanceisachievedusingstatecountingwithk=256andβ=0.2.156References157[1]Bellemare,MarcG,Srinivasan,Sriram,Ostrovski,Georg,Schaul,Tom,Saxton,David,and158Munos,Remi.Unifyingcount-basedexplorationandintrinsicmotivation.InAdvancesinNeural159InformationProcessingSystems29(NIPS),pp.1471–1479,2016.160[2]Bloom,BurtonH.Space/timetrade-offsinhashcodingwithallowableerrors.Communications161oftheACM,13(7):422–426,1970.162[3]Cormode,GrahamandMuthukrishnan,S.Animproveddatastreamsummary:thecount-min163sketchanditsapplications.JournalofAlgorithms,55(1):58–75,2005.164[4]Duan,Yan,Chen,Xi,Houthooft,Rein,Schulman,John,andAbbeel,Pieter.Benchmarking165deepreinforcementlearningforcontinouscontrol.InProceedingsofthe33rdInternational166ConferenceonMachineLearning(ICML),pp.1329–1338,2016.167[5]Fan,Li,Cao,Pei,Almeida,Jussara,andBroder,AndreiZ.Summarycache:Ascalablewide-area168webcachesharingprotocol.IEEE/ACMTransactionsonNetworking,8(3):281–293,2000.169[6]Houthooft,Rein,Chen,Xi,Duan,Yan,Schulman,John,DeTurck,Filip,andAbbeel,Pieter.170VIME:Variationalinformationmaximizingexploration.InAdvancesinNeuralInformation171ProcessingSystems29(NIPS),pp.1109–1117,2016.1727[7]Ioffe,SergeyandSzegedy,Christian.Batchnormalization:Acceleratingdeepnetworktraining173byreducinginternalcovariateshift.InProceedingsofthe32ndInternationalConferenceon174MachineLearning(ICML),pp.448–456,2015.175[8]Kingma,DiederikandBa,Jimmy.Adam:Amethodforstochasticoptimization.InProceedings176oftheInternationalConferenceonLearningRepresentations(ICLR),2015.1778Figure2:Freeway:subsequentframesandcorrespondingcode(top);theframesareorderedfromleft(startingwithframenumber0)toright,toptobottom;theverticalaxisintherightimagescorrespondtotheframenumber.Withineachimage,theleftpictureistheinputframe,themiddlepicturethereconstruction,andtherightpicture,thereconstructionerror.0100200300400500−1000010002000300040005000600070008000direct countBloom 6MBloom 90M(a)Meanaverageundiscountedre-turn01002003004005000.0000.0020.0040.0060.0080.0100.012direct countBloom 6MBloom 90M(b)AveragebonusrewardFigure3:StatisticsofTRPO-pixel-SimHash(k=256)onFrostbite.Solidlinesarethemean,whiletheshadedareasrepresenttheonestandarddeviation.Resultsarederivedfrom10randomseeds.Directcountingwithadictionaryuses2.7timesmorecomputationsthancountingBloomﬁlters(6Mor90M).902004006008001000−1000010002000300040005000600070008000exact enemy locationsignore enemiesrandom enemy locationsFigure4:SmartHashresultsonMontezuma’sRevenge(RAMobservations):thesolidlineisthemeanaverageundiscountedreturnperiteration,whiletheshadedareasrepresenttheonestandarddeviation,over5seeds.10
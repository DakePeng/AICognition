Transfer of Adversarial Robustness Between Perturbation Types

Daniel Kang * 1 2 Yi Sun * 1 3 Tom Brown 1 Dan Hendrycks 4 Jacob Steinhardt 1

9
1
0
2

y
a
M
3

]

G
L
.
s
c
[

1
v
4
3
0
1
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

∞

We study the transfer of adversarial robustness of
deep neural networks between different perturba-
tion types. While most work on adversarial exam-
ples has focused on L
and L2-bounded pertur-
bations, these do not capture all types of perturba-
tions available to an adversary. The present work
evaluates 32 attacks of 5 different types against
models adversarially trained on a 100-class sub-
set of ImageNet. Our empirical results suggest
that evaluating on a wide range of perturbation
sizes is necessary to understand whether adver-
sarial robustness transfers between perturbation
types. We further demonstrate that robustness
against one perturbation type may not always im-
ply and may sometimes hurt robustness against
other perturbation types. In light of these results,
we recommend evaluation of adversarial defenses
take place on a diverse range of perturbation types
and sizes.

1. Introduction

Deep networks have shown remarkable accuracy on bench-
mark tasks (He et al., 2016), but can also be fooled by
imperceptible changes to inputs, known as adversarial ex-
amples (Goodfellow et al., 2014). In response, researchers
have studied the robustness of models, or how well mod-
els generalize in the presence of (potentially adversarial)
bounded perturbations to inputs.

How can we tell if a model is robust? Evaluating model ro-
bustness is challenging because, while evaluating accuracy
only requires a ﬁxed distribution, evaluating the robustness
of a model requires that the model have good performance
in the presence of many, potentially hard to anticipate and
model, perturbations. In the context of image classiﬁca-

*Equal contribution, alphabetical 1OpenAI, San Francisco, CA,
USA 2Department of Computer Science, Stanford University, Palo
Alto, CA, USA 3Department of Mathematics, Columbia Univer-
sity, New York, NY, USA 4Department of Electrical Engineering
and Computer Science, UC Berkeley, Berkeley, CA, USA. Corre-
spondence to: Daniel Kang <ddkang@stanford.edu>.

Copyright 2019 by the author(s).

∞

-
tion, considerable work has focused on robustness to “L
∞
bounded” perturbations (perturbations with bounded per-
pixel magnitude) (Goodfellow et al., 2014; Madry et al.,
2017; Xie et al., 2018). However, models hardened against
-bounded perturbations are still vulnerable to even small,
L
perceptually minor departures from this family, such as
small rotations and translations (Engstrom et al., 2017).
Meanwhile, researchers continue to develop creative attacks
that are difﬁcult to even mathematically specify, such as
fake eyeglasses, adversarial stickers, and 3D-printed objects
(Sharif et al., 2018; Brown et al., 2017; Athalye et al., 2017).

The perspective of this paper is that any single, simple-to-
deﬁne type of perturbation is likely insufﬁcient to capture
what a deployed model will be subject to in the real world.
To address this, we investigate robustness of models with
respect to a broad range of perturbation types. We start with
the following question:

When and how much does robustness to one type
of perturbation transfer to other perturbations?

We study this question using adversarial training, a strong
technique for adversarial defense applicable to any ﬁxed at-
tack (Goodfellow et al., 2014; Madry et al., 2017). We eval-
uate 32 attacks of 5 different types–L
(Goodfellow et al.,
2014), L2 (Carlini & Wagner, 2017), L1 (Chen et al., 2018),
elastic deformations (Xiao et al., 2018), and JPEG (Shin &
Song, 2017)–against adversarially trained ResNet-50 mod-
els on a 100-class subset of full-resolution ImageNet.

∞

Our results provide empirical evidence that models robust
under one perturbation type are not necessarily robust under
other natural perturbation types. We show that:

1. Evaluating on a carefully chosen range of perturbation
sizes is important for measuring robustness transfer.

2. Adversarial training against the elastic deformation
attack demonstrates that adversarial robustness against
one perturbation type can transfer poorly to and at
times hurt robustness to other perturbation types.

3. Adversarial training against the L2 attack may be better

than training against the widely used L

∞

attack.

While any given set of perturbation types may not encom-
pass all potential perturbations that can occur in practice, our
results demonstrate that robustness can fail to transfer even

 
 
 
 
 
 
Transfer of Adversarial Robustness Between Perturbation Types

across a small but diverse set of perturbation types. Prior
work in this area (Sharma & Chen, 2017; Jordan et al., 2019;
Tram`er & Boneh, 2019) has studied transfer using single
values of ε for each attack on lower resolution datasets; we
believe our larger-scale study provides a more comprehen-
sive and interpretable view on transfer between these attacks.
We therefore suggest considering performance against sev-
eral different perturbation types and sizes as a ﬁrst step for
rigorous evaluation of adversarial defenses.

2. Adversarial attacks

We consider ﬁve types of adversarial attacks under the fol-
224 → R100 be a
lowing framework. Let f : R3
model mapping images to logits1, and let (cid:96)(f (x), y) denote
the cross-entropy loss. For an input x with true label y and
a target class y(cid:48) (cid:54)= y, the attacks attempt to ﬁnd x(cid:48) such that

224

×

×

1. the attacked image x(cid:48) is a perturbation of x, constrained

in a sense which differs for each attack, and

2. the loss (cid:96)(f (x(cid:48)), y(cid:48)) is minimized (targeted attack).

We consider the targeted setting and the following attacks,
described in more detail below:

∞

(Goodfellow et al., 2014)

• L
• L2 (Szegedy et al., 2013; Carlini & Wagner, 2017)
• L1 (Chen et al., 2018)
• JPEG

• Elastic deformation (Xiao et al., 2018)

∞

The L
and L2 attacks are standard in the adversarial ex-
amples literature (Athalye et al., 2018; Papernot et al., 2016;
Madry et al., 2017; Carlini & Wagner, 2017) and we chose
the remaining attacks for diversity in perturbation type. We
now describe each attack, with sample images in Figure 1
and Appendix A. We clamp output pixel values to [0, 255].

For Lp attacks with p ∈ {1, 2, ∞}, the constraint allows an
image x ∈ R3
224, viewed as a vector of RGB pixel
×
values, to be modiﬁed to an attacked image x(cid:48) = x + δ with

224

×

(cid:107)x(cid:48) − x(cid:107)p ≤ ε,

×

∞

224

where (cid:107) · (cid:107)p denotes the Lp-norm on R3
224. For the
×
and L2 attacks, we optimize using randomly-initialized
L
projected gradient descent (PGD), which optimizes the per-
turbation δ by gradient descent and projection to the L
∞
and L2 balls (Madry et al., 2017). For the L1 attack, we use
the randomly-initialized Frank-Wolfe algorithm (Frank &
Wolfe, 1956), detailed in Appendix C. We believe that our
Frank-Wolfe algorithm is more principled than the optimiza-
tion used in existing L1 attacks such as EAD.

1For all experiments, the input is a 224 × 224 image, and the

output is one of 100 classes.

clean

L

∞

L2

L1

JPEG

elastic

Figure 1. Sample attacked images with label “black swan” for ε at
the top end of our range.

As discussed in Shin & Song (2017) as a defense, JPEG
compression applies a lossy linear transformation based on
the discrete cosine transform (denoted by JPEG) to image
space, followed by quantization. The JPEG attack, which
we believe is new to this work, imposes on the attacked
image x(cid:48) an L

-constraint in this transformed space:

∞

(cid:107)JPEG(x) − JPEG(x(cid:48))(cid:107)

≤ ε.

∞

We optimize z = JPEG(x(cid:48)) with randomly initialized PGD
and apply a right inverse of JPEG to obtain the attacked
image.

The elastic deformation attack allows perturbations

x(cid:48) = Flow(x, V ),

where V : {1, . . . , 224}2 → R2 is a vector ﬁeld on pixel
space, and Flow sets the value of pixel (i, j) to the (bilinearly
interpolated) value at (i, j) + V (i, j). We constrain V to be
the convolution of a vector ﬁeld W with a 25 × 25 Gaussian
kernel with standard deviation 3, and enforce that

(cid:107)W (i, j)(cid:107)

≤ ε

∞

for i, j ∈ {1, . . . , 224}.

We optimize the value of W with randomly initialized PGD.
Note that our attack differs in details from Xiao et al. (2018),
but is similar in spirit.

3. Experiments

We measure transfer of adversarial robustness by evaluating
our attacks against adversarially trained models. For each
attack, we adversarially train models against the attack for
a range of perturbation sizes ε. We then evaluate each
adversarially trained model against each attack, giving the
2-dimensional accuracy grid of attacks evaluated against
adversarially trained models shown in Figure 2 (analyzed in
detail in Section 3.2).

Transfer of Adversarial Robustness Between Perturbation Types

Figure 2. Evaluation accuracies of adversarial attacks (columns) against adversarially trained models (rows).

3.1. Experimental setup

momentum was 0.9, and weight decay was 5 × 10−

6.

Dataset and model. We use the 100-class subset of
ImageNet-1K (Deng et al., 2009) containing classes whose
WordNet ID is a multiple of 10. We use the ResNet-50 (He
et al., 2016) architecture with standard 224×224 resolution
as implemented in torchvision. We believe this full
resolution is necessary for the elastic and JPEG attacks.

Training hyperparameters. We trained on machines with
8 Nvidia V100 GPUs using standard data augmentation
practices (He et al., 2016). Following best practices for
multi-GPU training (Goyal et al., 2017), we used synchro-
nized SGD for 90 epochs with a batch size of 32×8 and a
learning rate schedule in which the learning rate is “warmed
up” for 5 epochs and decayed at epochs 30, 60, and 80 by a
factor of 10. Our initial learning rate after warm-up was 0.1,

Adversarial training. We harden models against attacks
using adversarial training (Madry et al., 2017). To train
against attack A, for each mini-batch of training images, we
select target classes for each image uniformly at random
from the 99 incorrect classes. We generate adversarial im-
ages by applying the targeted attack A to the current model
with ε chosen uniformly at random between 0 and εmax. Fi-
nally, we update the model with a step of synchronized SGD
using these adversarial images alone.

We list attack parameters used for training in Table 1. For
√steps , motivated by
the PGD attack, we chose step size
√
steps is
the fact that taking step size proportional to 1/
optimal for non-smooth convex functions (Nemirovski &
Yudin, 1978; 1983). Note that the greater number of PGD

ε

NoattackL∞ε=1L∞ε=2L∞ε=4L∞ε=8L∞ε=16L∞ε=32L2ε=150L2ε=300L2ε=600L2ε=1200L2ε=2400L2ε=4800L1ε=9562.5L1ε=19125L1ε=38250L1ε=76500L1ε=153000L1ε=306000L1ε=612000JPEGε=0.03125JPEGε=0.0625JPEGε=0.125JPEGε=0.25JPEGε=0.5JPEGε=1Elasticε=0.25Elasticε=0.5Elasticε=1Elasticε=2Elasticε=4Elasticε=8Elasticε=16Attack(evaluation)NormaltrainingL∞ε=1L∞ε=2L∞ε=4L∞ε=8L∞ε=16L∞ε=32L2ε=150L2ε=300L2ε=600L2ε=1200L2ε=2400L2ε=4800L1ε=9562.5L1ε=19125L1ε=38250L1ε=76500L1ε=153000L1ε=306000L1ε=612000JPEGε=0.03125JPEGε=0.0625JPEGε=0.125JPEGε=0.25JPEGε=0.5JPEGε=1Elasticε=0.25Elasticε=0.5Elasticε=1Elasticε=2Elasticε=4Elasticε=8Elasticε=16Attack(adversarialtraining)86282111160131111704515311123211117849711118583681411184804941179694312211837015111837539411185847948311848270201180735524411848048211837751911183828172231183827849417974613681182806611118178621621179797875586179787660141757059391431797766232178766632311707070696432270696443716453371741169674791169676345122154535250391115349359114028135111493815211525044298218680503111847732211807042911180483111837128211185827220111848164911827660244118372221118376455111848380555118483774121828072491211838160611837857111118080797127218080786715179797565376180797437217977652321176767573557176767572473757574695725376757461131757367395116868676660251686867665819676767656144156867676337367666348123185723121118368211118378622431172282111826824211185785061118376423118481734581179516111837334311184816519111838061121183827763242181702621182764651118381733931182807131218282807348818175518118176561111179787557111179787352817979797665293787767313179766320111767572612521767572592027676767572581875747155121757265313117070685933417069675629370717070696644706967613126867623962185732721118158911176582551118480544118266182111857946411183712021177643261118583741911826921211185826716111837844411797045132118484815721827333211183827741311828065141180735624311838382761818174414111807977641811797873424178746341102180807978632797554911176767469343176757357111757267502141767676757115757357152118665172111784871117759286111651821118476413111847332311181621611178654012311754041118480631411185764461118168252117667441741179561211184827749411827752111118071375117565462151178591721182818072262180745212211776833411675534153117244911179797877681117669439111725821211543720721157193111767574757342574623051116642101114224931113981111737271706945140.00.20.40.60.81.0AdversarialaccuracyTransfer of Adversarial Robustness Between Perturbation Types

attack
L
∞
L2
L1
JPEG
Elastic

optimization algorithm ε or εmax values
{2i | 0 ≤ i ≤ 5}
PGD
{150 · 2i | 0 ≤ i ≤ 5}
PGD
{9562.5 · 2i | 0 ≤ i ≤ 6}
Frank-Wolfe
{0.03125 · 2i | 0 ≤ i ≤ 5}
PGD
{0.25 · 2i | 0 ≤ i ≤ 6}
PGD

step size

ε
√steps
ε
√steps
N/A
ε
√steps
ε
√steps

steps (adversarial training)
10
10
10
10
30

steps (eval)
50
50
50
50
100

Table 1. Attack parameters for adversarial training and evaluation

steps for elastic deformation is due to the greater difﬁculty
of its optimization problem, which we are not conﬁdent is
fully solved even with this greater number of steps.

Attack hyperparameters. We evaluate our adversarially
trained models on the (subsetted) ImageNet-1K validation
set against targeted attacks with target chosen uniformly
at random from among the 99 incorrect classes. We list
attack parameters for evaluation in Table 1. As suggested in
(Carlini et al., 2019), we use more steps for evaluation than
for adversarial training to ensure PGD converges.

3.2. Results and analysis

Using the results of our adversarial training and evaluation
experiments in Figure 2, we draw the following conclusions.

Choosing ε well is important. Because attack strength in-
creases with the allowed perturbation magnitude ε, compar-
ing robustness between different perturbation types requires
a careful choice of ε for both attacks. First, we observe that
a range of ε yielding comparable attack strengths should be
used for all attacks to avoid drawing misleading conclusions.
We suggest the following principles for choosing this range,
which we followed for the parameters in Table 1:

1. Models adversarially trained against the minimum
value of ε should have validation accuracy comparable
to that of a model trained on unattacked data.

2. Attacks with the maximum value of ε should substan-
tially reduce validation accuracy in adversarial training
or perturb the images enough to confuse humans.

To illustrate this point, we provide in Appendix B a subset
of Figure 2 with ε ranges that differ in strength between
attacks; the (deliberately) biased ranges of ε chosen in this
subset cause the L1 and elastic attacks to be perceived as
stronger than our full results reveal.

Second, even if two attacks are evaluated on ranges of ε of
comparable strength, the speciﬁc values of ε chosen within
those ranges may be important. In our experiments, we
scaled ε geometrically for all attacks, but when interpreting
our results, attack strength may not scale in the same way
with ε for different attacks. As a result, we only draw
conclusions which are invariant to the precise scaling of

attack strength with ε. We illustrate this type of analysis
with the following two examples.

Robustness against elastic transfers poorly to the other
attacks.
In Figure 2, the accuracies of models adversari-
ally trained against elastic are higher against elastic than the
other attacks, meaning that for these values of ε, robustness
against elastic does not imply robustness against other at-
tacks. On the other hand, training against elastic with ε ≥ 4
generally increases accuracy against elastic with ε ≥ 4, but
decreases accuracy against all other attacks.

Together, these imply that the lack of transfer we observe in
Figure 2 is not an artifact of the speciﬁc values of ε we chose,
but rather a broader effect at the level of perturbation types.
In addition, this example shows that increasing robustness to
larger perturbation sizes of a given type can hurt robustness
to other perturbation types. This effect is only visible by
considering an appropriate range of ε and cannot be detected
from a single value of ε alone.

∞

∞

. Com-
L2 adversarial training is weakly better than L
paring rows of Figure 2 corresponding to training against
L2 with ε ∈ {300, 600, 1200, 2400, 4800} with rows corre-
with ε ∈ {1, 2, 4, 8, 16},
sponding to training against L
we see that training against L2 yields slightly lower accu-
racies against L
attacks and higher accuracies against all
other attacks. Because this effect extends to all ε for which
is helpful, it does not depend on the
training against L
attack strength and ε. In fact, against
relation between L
the stronger half of our attacks, training against L2 with
ε = 4800 gives comparable or better accuracy to training
with adaptive choice of ε. This provides some
against L
.
evidence that L2 is more effective to train against than L

∞

∞

∞

∞

∞

4. Conclusion

This work presents an empirical study of when and how
much robustness transfers between different adversarial per-
turbation types. Our results on adversarial training and
evaluation of 32 different attacks on a 100-class subset of
ImageNet-1K highlight the importance of considering a di-
verse range of perturbation sizes and types for assessing
transfer between types, and we recommend this as a guide-
line for evaluating adversarial robustness.

Transfer of Adversarial Robustness Between Perturbation Types

Acknowledgements

D. K. was supported by NSF Grant DGE-1656518. Y. S. was
supported by a Junior Fellow award from the Simons Foun-
dation and NSF Grant DMS-1701654. D. K., Y. S., and
J. S. were supported by a grant from the Open Philanthropy
Project.

References

Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K.
CoRR,
Synthesizing robust adversarial examples.
abs/1707.07397, 2017. URL http://arxiv.org/
abs/1707.07397.

Athalye, A., Carlini, N., and Wagner, D. Obfuscated
gradients give a false sense of security: Circumvent-
ing defenses to adversarial examples. arXiv preprint
arXiv:1802.00420, 2018.

Brown, T. B., Man´e, D., Roy, A., Abadi, M., and Gilmer, J.
Adversarial patch. CoRR, abs/1712.09665, 2017. URL
http://arxiv.org/abs/1712.09665.

Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In 2017 IEEE Symposium on
Security and Privacy (SP), pp. 39–57. IEEE, 2017.

Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber,
J., Tsipras, D., Goodfellow, I. J., Madry, A., and Ku-
rakin, A. On evaluating adversarial robustness. CoRR,
abs/1902.06705, 2019. URL http://arxiv.org/
abs/1902.06705.

Chen, P.-Y., Sharma, Y., Zhang, H., Yi, J., and Hsieh, C.-
J. EAD: Elastic-net attacks to deep neural networks via
adversarial examples. In Thirty-second AAAI conference
on artiﬁcial intelligence, 2018.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE conference on computer vision and pattern
recognition, pp. 248–255. IEEE, 2009.

Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., and
Madry, A. A rotation and a translation sufﬁce: Fool-
ing CNNs with simple transformations. arXiv preprint
arXiv:1712.02779, 2017.

Goyal, P., Doll´ar, P., Girshick, R., Noordhuis, P.,
Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He,
K. Accurate, large minibatch SGD: Training Imagenet in
1 hour. arXiv preprint arXiv:1706.02677, 2017.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings
in deep residual networks. In European conference on
computer vision, pp. 630–645. Springer, 2016.

Jordan, M., Manoj, N., Goel, S., and Dimakis, A. G. Quan-
tifying Perceptual Distortion of Adversarial Examples.
arXiv e-prints, art. arXiv:1902.08265, Feb 2019.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to
adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.

Nemirovski, A. and Yudin, D. On Cezari’s convergence
of the steepest descent method for approximating saddle
point of convex-concave functions. In Soviet Math. Dokl,
volume 19, pp. 258–269, 1978.

Nemirovski, A. and Yudin, D. Problem Complexity and
Method Efﬁciency in Optimization. Intersci. Ser. Discrete
Math. Wiley, New York, 1983.

Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
A. Distillation as a defense to adversarial perturbations
against deep neural networks. In 2016 IEEE Symposium
on Security and Privacy (SP), pp. 582–597. IEEE, 2016.

Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M. K. Ad-
versarial generative nets: Neural network attacks on state-
of-the-art face recognition. CoRR, abs/1801.00349, 2018.
URL http://arxiv.org/abs/1801.00349.

Sharma, Y. and Chen, P.-Y. Attacking the Madry Defense
Model with L1-based Adversarial Examples. arXiv e-
prints, art. arXiv:1710.10733, Oct 2017.

Shin, R. and Song, D. JPEG-resistant adversarial images.
In NIPS 2017 Workshop on Machine Learning and Com-
puter Security, 2017.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
D., Goodfellow, I., and Fergus, R. Intriguing properties of
neural networks. arXiv preprint arXiv:1312.6199, 2013.

Tram`er, F. and Boneh, D. Adversarial Training and Ro-
bustness for Multiple Perturbations. arXiv e-prints, art.
arXiv:1904.13000, Apr 2019.

Frank, M. and Wolfe, P. An algorithm for quadratic pro-
gramming. Naval research logistics quarterly, 3(1-2):
95–110, 1956.

Xiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song,
D. Spatially transformed adversarial examples. arXiv
preprint arXiv:1801.02612, 2018.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

Xie, C., Wu, Y., van der Maaten, L., Yuille, A., and He, K.
Feature denoising for improving adversarial robustness.
arXiv preprint arXiv:1812.03411, 2018.

Transfer of Adversarial Robustness Between Perturbation Types

A. Sample attacked images

In this appendix, we give more comprehensive sample out-
puts for our adversarial attacks. Figures 3 and 4 show sam-
ple attacked images for attacks with relatively large and
small ε in our range, respectively. Figure 5 shows exam-
ples of how attacked images can be inﬂuenced by different
types of adversarial training for defense models. In all cases,
the images were generated by running the speciﬁed attack
against an adversarially trained model with parameters spec-
iﬁed in Table 1 for both evaluation and adversarial training.

B. Evaluation on a truncated ε range

In this appendix, we show in Figure 6 a subset of Figure 2
with a truncated range of ε. In particular, we omitted small
values of ε for L1, elastic, and JPEG and large values of ε
for L
and L2. The resulting accuracy grid gives several
misleading impressions, including:

∞

1. The L1 attack is stronger than L

∞

, L2, and JPEG.

2. Training against the other attacks gives almost no ro-

bustness against the elastic attack.

The full range of results in Figure 2 shows that these two
purported effects are artifacts of the incorrectly truncated
range of ε used in Figure 6. In particular:

1. The additional smaller ε columns for the L1 attack in
Figure 2 demonstrate its perceived strength in Figure 6
is an artifact of incorrectly omitting these values.

2. The additional smaller ε columns for the elastic attack
in Figure 2 reveal that training against the other at-
tacks is effective in defending against weak versions
of the elastic attack, contrary to the impression given
by Figure 6.

C. L1 Attack

We chose to use the Frank-Wolfe algorithm for optimizing
the L1 attack, as Projected Gradient Descent would require
projecting onto a truncated L1 ball, which is a complicated
operation. In contrast, Frank-Wolfe only requires optimiz-
ing linear functions g(cid:62)x over a truncated L1 ball; this can
be done by sorting coordinates by the magnitude of g and
moving the top k coordinates to the boundary of their range
(with k chosen by binary search). This is detailed in Algo-
rithm 1.

Transfer of Adversarial Robustness Between Perturbation Types

clean

L
∞
ε = 32

L2
ε = 4800

L1
ε = 306000

JPEG
ε = 1

elastic
ε = 8

black
swan

chain
mail

espresso
maker

manhole
cover

water
tower

orange

volcano

Figure 3. Strong attacks applied to sample images

Transfer of Adversarial Robustness Between Perturbation Types

clean

L
∞
ε = 4

L2
ε = 600

L1
ε = 38250

JPEG
ε = 0.125

elastic
ε = 1

black
swan

chain
mail

espresso
maker

manhole
cover

water
tower

orange

volcano

Figure 4. Weak attacks applied to sample images

Transfer of Adversarial Robustness Between Perturbation Types

L2
ε = 2400
L1
ε = 153000

L2
ε = 2400

elastic ε = 4

L1
ε = 153000
L2
ε = 2400

L1
ε = 153000

elastic ε = 4

elastic ε = 4

elastic ε = 4

L2
ε = 2400

L1
ε = 153000

Figure 5. Transfer across attack types

attack

clean

adversarial
training

black
swan

chain mail

espresso
maker

manhole
cover

water
tower

orange

volcano

Transfer of Adversarial Robustness Between Perturbation Types

Figure 6. Evaluation accuracies of adversarial attacks (columns) against adversarially trained models (rows) for a truncated ε range.

NoattackL∞ε=1L∞ε=2L∞ε=4L∞ε=8L∞ε=16L2ε=150L2ε=300L2ε=600L2ε=1200L2ε=2400L1ε=38250L1ε=76500L1ε=153000L1ε=306000L1ε=612000JPEGε=0.0625JPEGε=0.125JPEGε=0.25JPEGε=0.5JPEGε=1Elasticε=1Elasticε=2Elasticε=4Elasticε=8Elasticε=16Attack(evaluation)NormaltrainingL∞ε=1L∞ε=2L∞ε=4L∞ε=8L∞ε=16L2ε=150L2ε=300L2ε=600L2ε=1200L2ε=2400L1ε=38250L1ε=76500L1ε=153000L1ε=306000L1ε=612000JPEGε=0.0625JPEGε=0.125JPEGε=0.25JPEGε=0.5JPEGε=1Elasticε=1Elasticε=2Elasticε=4Elasticε=8Elasticε=16Attack(adversarialtraining)8628211160131111531112111171111858368141184804941431221170151113941118584794831848270201552441180482115191118382817223183827849461368118066111162162117979787558679787660145939143177662321663231170707069643270696443737174116747911634512218680503118477322142911148311128211185827220118481649160244117222111455111848380555184837741272491211816061157111118080797127280807867157565376179743721652321176767573557767675724774695725375746113167395118481651911838061121776324217026211465111838173393182807131280734881755181156111117978755711179787352879766529377673131632011176757261252767572592076757258187471551216531311707068593347069675629707069664469676131262396218579464118371202132611183741911212111858267161183784441451321184815721332111838277413182806514156243118382761814141118079776418179787342463411021807978632549111767674693437675735711675021417676757115571521185764461181682521441741156121117749411827752111180713751462151159172118072262180745212217768334134153114491117877681117669439117258212120721119311174757342574623051166421011931118111171706945140.00.20.40.60.81.0AdversarialaccuracyTransfer of Adversarial Robustness Between Perturbation Types

(cid:46) Random initialization

(cid:46) Obtain gradient

−

else

bi ← 1 − xi

for i = 1, . . . , d do
if gi > 0 then

end for
Sk ← {s1, . . . , sk}.

g ← ∇f (x(t
for k = 1, . . . , d do

sk ← index of the coordinate of g by with kth largest norm

Algorithm 1 Pseudocode for the Frank-Wolfe algorithm for the L1 attack.
1: Input: function f , initial input x ∈ [0, 1]d, L1 radius ρ, number of steps T .
2: Output: approximate maximizer ¯x of f over the truncated L1 ball B1(ρ; x) ∩ [0, 1]d centered at x.
3:
4: x(0) ← RandomInit(x)
5: for t = 1, . . . , T do
1))
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31: end for
32: ¯x ← x(T )

end for
Mk ← (cid:80)
Sk
∈
k∗ ← max{k | Mk ≤ ρ}
for i = 1, . . . , d do
if i ∈ Sk∗ then

ˆxi ← xi + bi
else if i = sk∗+1 then

ˆxi ← xi + (ρ − Mk∗ ) sign(gi)

end for
x(t) ← (1 − 1

bi ← −xi

ˆxi ← xi

1) + 1

t )x(t

end if

end if

else

|bi|

t ˆx

−

i

(cid:46) Compute move to boundary of [0, 1] for each coordinate.

(cid:46) Compute L1-perturbation of moving k largest coordinates.
(cid:46) Choose largest k satisfying L1 constraint.
(cid:46) Compute ˆx maximizing g(cid:62)x over the L1 ball.

(cid:46) Average ˆx with previous iterates


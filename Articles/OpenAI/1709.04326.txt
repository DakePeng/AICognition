LearningwithOpponent-LearningAwarenessJakobFoerster†,‡UniversityofOxfordRichardY.Chen†OpenAIMaruanAl-Shedivat‡CarnegieMellonUniversityShimonWhitesonUniversityofOxfordPieterAbbeel‡UCBerkeleyIgorMordatchOpenAIABSTRACTMulti-agentsettingsarequicklygatheringimportanceinmachinelearning.Thisincludesaplethoraofrecentworkondeepmulti-agentreinforcementlearning,butalsocanbeextendedtohierar-chicalreinforcementlearning,generativeadversarialnetworksanddecentralisedoptimization.Inallthesesettingsthepresenceofmul-tiplelearningagentsrendersthetrainingproblemnon-stationaryandoftenleadstounstabletrainingorundesiredfinalresults.WepresentLearningwithOpponent-LearningAwareness(LOLA),amethodinwhicheachagentshapestheanticipatedlearningoftheotheragentsintheenvironment.TheLOLAlearningrulein-cludesanadditionaltermthataccountsfortheimpactofoneagent’spolicyontheanticipatedparameterupdateoftheotheragents.Pre-liminaryresultsshowthattheencounteroftwoLOLAagentsleadstotheemergenceoftit-for-tatandthereforecooperationintheiteratedprisoners’dilemma(IPD),whileindependentlearningdoesnot.Inthisdomain,LOLAalsoreceiveshigherpayoutscomparedtoanaivelearner,andisrobustagainstexploitationbyhigherordergradient-basedmethods.Appliedtoinfinitelyrepeatedmatchingpennies,LOLAagentsconvergetotheNashequilibrium.InaroundrobintournamentweshowthatLOLAagentscansuccessfullyshapethelearningofarangeofmulti-agentlearningalgorithmsfromliterature,resultinginthehighestaveragereturnsontheIPD.WealsoshowthattheLOLAupdaterulecanbeefficientlycalculatedusinganextensionofthelikelihoodratiopolicygradientestimator,makingthemethodsuitableformodel-freereinforcementlearning.Thismethodthusscalestolargeparameterandinputspacesandnonlinearfunctionapproximators.WealsoapplyLOLAtoagridworldtaskwithanembeddedsocialdilemmausingdeeprecurrentpoliciesandopponentmodelling.Again,byexplicitlyconsideringthelearningoftheotheragent,LOLAagentslearntocooperateoutofself-interest.Ourcodeisavailableatgithub.com/alshedivat/lola.KEYWORDSmulti-agentlearning;deepreinforcementlearning;gametheoryACMReferenceFormat:JakobFoerster†,‡,RichardY.Chen†,MaruanAl-Shedivat‡,ShimonWhite-son,PieterAbbeel‡,andIgorMordatch.2018.LearningwithOpponent-LearningAwareness.InProc.ofthe17thInternationalConferenceonAu-tonomousAgentsandMultiagentSystems(AAMAS2018),Stockholm,Sweden,July10–15,2018,IFAAMAS,9pages.†equalcontribution,‡WorkdoneatOpenAI,correspondence:jakob.foerster@cs.ox.ac.uk.Proc.ofthe17thInternationalConferenceonAutonomousAgentsandMultiagentSystems(AAMAS2018),M.Dastani,G.Sukthankar,E.André,S.Koenig(eds.),July10–15,2018,Stockholm,Sweden.©2018InternationalFoundationforAutonomousAgentsandMultiagentSystems(www.ifaamas.org).Allrightsreserved.1INTRODUCTIONDuetotheadventofdeepreinforcementlearning(RL)methodsthatallowthestudyofmanyagentsinrichenvironments,multi-agentRLhasflourishedinrecentyears.However,mostofthisrecentworkconsidersfullycooperativesettings[13,14,34]andemergentcommunicationinparticular[11,12,22,32,40].Consideringfutureapplicationsofmulti-agentRL,suchasself-drivingcars,itisobviousthatmanyofthesewillbeonlypartiallycooperativeandcontainelementsofcompetitionandconflict.Thehumanabilitytomaintaincooperationinavarietyofcom-plexsocialsettingshasbeenvitalforthesuccessofhumansocieties.Emergentreciprocityhasbeenobservedeveninstronglyadversar-ialsettingssuchaswars[1],makingitaquintessentialandrobustfeatureofhumanlife.Inthefuture,artificiallearningagentsarelikelytotakeanactivepartinhumansociety,interactingbothwithotherlearningagentsandhumansincomplexpartiallycompetitivesettings.Failingtodeveloplearningalgorithmsthatleadtoemergentreciprocityintheseartificialagentswouldleadtodisastrousoutcomes.Howreciprocitycanemergeamongagroupoflearning,self-interested,rewardmaximizingRLagentsisthusaquestionbothoftheoreticalinterestandofpracticalimportance.Gametheoryhasalonghistoryofstudyingthelearningoutcomesingamesthatcontaincooperativeandcompetitiveelements.Inparticular,thetensionbetweencooperationanddefectioniscommonlystudiedintheiteratedprisoners’dilemma.Inthisgame,selfishinterestscanleadtoanoutcomethatisoverallworseforallparticipants,whilecooperationmaximizessocialwelfare,onemeasureofwhichisthesumofrewardsforallagents.Interestingly,inthesimplesettingofaninfinitelyrepeatedpris-oners’dilemmawithdiscounting,randomlyinitialisedRLagentspursuingindependentgradientdescentontheexactvaluefunc-tionlearntodefectwithhighprobability.Thisshowsthatcurrentstate-of-the-artlearningmethodsindeepmulti-agentRLcanleadtoagentsthatfailtocooperatereliablyeveninsimplesocialset-tings.Onewell-knownshortcomingisthattheyfailtoconsiderthelearningprocessoftheotheragentsandsimplytreattheotheragentasastaticpartoftheenvironment[19].Asasteptowardsreasoningoverthelearningbehaviourofotheragentsinsocialsettings,weproposeLearningwithOpponent-Learn-ingAwareness(LOLA).TheLOLAlearningruleincludesanadditionaltermthataccountsfortheimpactofoneagent’spolicyonthelearningstepoftheotheragents.Forconvenienceweusetheword‘opponents’todescribetheotheragents,eventhoughthemethodisnotlimitedtozero-sumgamesandcanbeappliedinthegeneral-sumsetting.Weshowthatthisadditionalterm,whenarXiv:1709.04326v4  [cs.AI]  19 Sep 2018appliedbybothagents,leadstoemergentreciprocityandcoop-erationintheiteratedprisoners’dilemma(IPD).ExperimentallywealsoshowthatintheIPD,eachagentisincentivisedtoswitchfromnaivelearningtoLOLA,whiletherearenoadditionalgainsinattemptingtoexploitLOLAwithhigherordergradientterms.Thissuggeststhatwithinthespaceoflocal,gradient-basedlearn-ingrulesbothagentsusingLOLAisastableequilibrium.ThisisfurthersupportedbythegoodperformanceoftheLOLAagentinaround-robintournament,whereitsuccessfullymanagestoshapethelearningofanumberofmulti-agentlearningalgorithmsfromliterature.ThisleadstotheoverallhighestaveragereturnontheIPDandgoodperformanceonIteratedMatchingPennies(IMP).WealsopresentaversionofLOLAadoptedtothedeepRLsettingusinglikelihoodratiopolicygradients,makingLOLAscalabletosettingswithhighdimensionalinputandparameterspaces.WeevaluatethepolicygradientversionofLOLAontheIPDanditeratedmatchingpennies(IMP),asimplifiedversionofrock-paper-scissors.WeshowthatLOLAleadstocooperationwithhighsocialwelfare,whileindependentpolicygradients,astandardmulti-agentRLapproach,doesnot.Thepolicygradientfindingisconsistentwithpriorwork,e.g.,SandholmandCrites[39].WealsoextendLOLAtosettingswheretheopponentpolicyisunknownandneedstobeinferredfromobservationsoftheopponent’sbehaviour.Finally,weapplyLOLAwithandwithoutopponentmodellingtoagrid-worldtaskwithanembeddedunderlyingsocialdilemma.Thistaskhastemporallyextendedactionsandthereforerequireshighdimensionalrecurrentpoliciesforagentstolearntorecipro-cate.Again,cooperationemergesinthistaskwhenusingLOLA,evenwhentheopponent’spolicyisunknownandneedstobeestimated.2RELATEDWORKThestudyofgeneral-sumgameshasalonghistoryingametheoryandevolution.Manypapersaddresstheiteratedprisoners’dilemma(IPD)inparticular,includingtheseminalworkonthetopicbyAx-elrod[1].Thisworkpopularisedtit-for-tat(TFT),astrategyinwhichanagentcooperatesonthefirstmoveandthencopiestheopponent’smostrecentmove,asaneffectiveandsimplestrategy.Anumberofmethodsinmulti-agentRLaimtoachieveconver-genceinself-playandrationalityinsequential,generalsumgames.SeminalworkincludesthefamilyofWoLFalgorithms[3],whichusesdifferentlearningratesdependingonwhetheranagentiswinningorlosing,joint-action-learners(JAL),andAWESOME[9].UnlikeLOLA,thesealgorithmstypicallyhavewellunderstoodconvergencebehaviourgivenanappropriatesetofconstraints.However,noneofthesealgorithmhavetheabilitytoshapethelearningbehaviouroftheopponentsinordertoobtainhigherpay-outsatconvergence.AWESOMEaimstolearntheequilibriaoftheone-shotgame,asubsetoftheequilibriaoftheiteratedgame.DetailedstudieshaveanalysedthedynamicsofJALsingeneralsumsettings:ThisincludesworkbyUtherandVeloso[43]inzero-sumsettingsandbyClausandBoutilier[8]incooperativesettings.SandholmandCrites[39]studythedynamicsofindependentQ-learningintheIPDunderarangeofdifferentexplorationschedulesandfunctionapproximators.Wunderetal.[45]andZinkevichetal.[47]explicitlystudytheconvergencedynamicsandequilibriaoflearninginiteratedgames.UnlikeLOLA,thesepapersdonotproposenovellearningrules.Littman[26]proposeamethodthatassumeseachopponentei-thertobeafriend,i.e.,fullycooperative,orfoe,i.e.,fullyadversarial.Instead,LOLAconsidersgeneralsumgames.Bycomparingasetofmodelswithdifferenthistorylengths,ChakrabortyandStone[7]proposeamethodtolearnabestre-sponsetomemoryboundedagentswithfixedpolicies.Incontrast,LOLAassumeslearningagents,whicheffectivelycorrespondtounboundedmemorypolicies.BrafmanandTennenholtz[4]introducethesolutionconceptofanefficientlearningequilibrium(ELE),inwhichneithersideisencouragedtodeviatefromthelearningrule.ThealgorithmtheyproposeappliestosettingswhereallNashequilibriacanbecomputedandenumerated;LOLAdoesnotrequireeitheroftheseassumptions.Bycontrast,mostworkindeepmulti-agentRLfocusesonfullycooperativeorzero-sumsettings,inwhichlearningprogressiseasiertoevaluate,[13,14,34]andemergentcommunicationinpar-ticular[11,12,22,32,40].Asanexception,Leiboetal.[24]analysetheoutcomesofindependentlearningingeneralsumsettingsusingfeedforwardneuralnetworksaspolicies.Loweetal.[27]proposeacentralisedactor-criticarchitectureforefficienttraininginthesegeneralsumenvironments.However,noneofthesemethodsexplic-itlyreasonsaboutthelearningbehaviourofotheragents.Lanctotetal.[21]generalisetheideasofgame-theoreticbest-response-stylealgorithms,suchasNFSP[17].IncontrasttoLOLA,thesebest-responsealgorithmsassumeagivensetofopponentpolicies,ratherthanattemptingtoshapethelearningoftheotheragents.TheproblemsettingandapproachofLererandPeysakhovich[25]isclosesttoours.Theydirectlygeneralisetit-for-tattocomplexenvironmentsusingdeepRL.Theauthorsexplicitlytrainafullycooperativeandadefectingpolicyforbothagentsandthencon-structatit-for-tatpolicythatswitchesbetweenthesetwoinordertoencouragetheopponenttocooperate.Similarinspirittothiswork,MunozdeCoteandLittman[33]proposeaNashequilibriumalgorithmforrepeatedstochasticgamesthatattemptstofindtheegalitarianequilibriumbyswitchingbetweencompetitiveandco-operativestrategies.AsimilarideaunderliesM-Qubed,[10],whichbalancesbest-response,cautiousandoptimisticlearningbiases.Reciprocityandcooperationarenotemergentpropertiesofthelearningrulesinthesealgorithmsbutratherdirectlycodedintothealgorithmviaheuristics,limitingtheirgenerality.Ourworkalsorelatestoopponentmodelling,suchasfictitiousplay[5]andaction-sequenceprediction[29,36].MealingandShapiro[30]alsoproposeamethodthatfindsapolicybasedonpredictingthefutureactionofamemoryboundedopponent.Furthermore,Hernandez-LealandKaisers[18]directlymodelthedistributionoveropponents.Whilethesemethodsmodeltheopponentstrategy,ordistributionthereof,anduselook-aheadtofindoptimalresponsepolicies,theydonotaddressthelearningdynamicsofopponents.Forfurtherdetailswereferthereadertoexcellentreviewsonthesubject[6,19].Bycontrast,ZhangandLesser[46]carryoutpolicypredictionunderone-steplearningdynamics.However,theopponents’policyupdatesareassumedtobegivenandonlyusedtolearnabestresponsetotheanticipatedupdatedparameters.Bycontrast,aLOLAagentdirectlyshapesthepolicyupdatesofallopponentsinordertomaximiseitsownreward.Differentiatingthroughtheopponent’slearningstep,whichisuniquetoLOLA,iscrucialfortheemergenceoftit-for-tatandreciprocity.Tothebestofourknowledge,LOLAisthefirstmethodthataimstoshapethelearningofotheragentsinamulti-agentRLsetting.WithLOLA,eachagentdifferentiatesthroughtheopponents’policyupdate.SimilarideaswereproposedbyMetzetal.[31],whosetrainingmethodforgenerativeadversarialnetworksdif-ferentiatesthroughmultipleupdatestepsoftheopponent.Theirmethodreliesonanend-to-enddifferentiablelossfunction,andthusdoesnotworkinthegeneralRLsetting.However,theoverallresultsaresimilar:differentiatingthroughtheopponent’slearningprocessstabilisesthetrainingoutcomeinazerosumsetting.Outsideofpurelycomputationalstudiestheemergenceofco-operationanddefectioninRLsettingshasalsobeenstudiedandcomparedtohumandata[20].3NOTATIONOurworkassumesamulti-agenttaskthatiscommonlydescribedasastochasticgameG,specifiedbyatupleG=⟨S,U,P,r,Z,O,n,γ⟩.Herenagents,a∈A≡{1,...,n},chooseactions,ua∈U,ands∈Sisthestateoftheenvironment.Thejointactionu∈U≡UnleadstoastatetransitionbasedonthetransitionfunctionP(s′|s,u):S×U×S→[0,1].Therewardfunctionsra(s,u):S×U→Rspecifytherewardsandγ∈[0,1)isthediscountfactor.Wefurtherdefinethediscountedfuturereturnfromtimeton-wardasRat=˝∞l=0γlrat+lforeachagent,a.Inanaiveapproach,eachagentmaximisesitstotaldiscountedreturninexpectationseparately.Thiscanbedonewithpolicygradientmethods[42]suchasREINFORCE[44].Policygradientmethodsupdateanagent’spolicy,parameterisedbyθa,byperforminggradientascentonanestimateoftheexpecteddiscountedtotalrewardE(cid:2)Ra0(cid:3).Byconvention,boldlowercaselettersdenotecolumnvectors.4METHODSInthissection,wereviewthenaivelearner’sstrategyandintroducetheLOLAlearningrule.WefirstderivetheupdateruleswhenagentshaveaccesstoexactgradientsandHessiansoftheirexpecteddiscountedfuturereturninSections4.1and4.2.InSection4.3,wederivethelearningrulespurelybasedonpolicygradients,thusremovingaccesstoexactgradientsandHessians.ThisrendersLOLAsuitablefordeepRL.However,westillassumeagentshaveaccesstoopponents’policyparametersinpolicygradient-basedLOLA.Next,inSection4.4,weincorporateopponentmodelingintotheLOLAlearningrule,suchthateachLOLAagentonlyinferstheopponent’spolicyparameterfromexperience.Finally,wediscusshigherorderLOLAinSection4.5.Forsimplicity,hereweassumethenumberofagentsisn=2anddisplaytheupdaterulesforagent1only.Thesamederivationholdsforarbitrarynumbersofagents.4.1NaiveLearnerSupposeeachagent’spolicyπaisparameterisedbyθaandVa(θ1,θ2)istheexpectedtotaldiscountedreturnforagentaasafunctionofbothagents’policyparameters(θ1,θ2).ANaiveLearner(NL)iterativelyoptimisesforitsownexpectedtotaldiscountedreturn,suchthatattheithiteration,θaiisupdatedtoθai+1accordingtoθ1i+1=argmaxθ1V1(θ1,θ2i)θ2i+1=argmaxθ2V2(θ1i,θ2).Inthereinforcementlearningsetting,agentsdonothaveaccessto{V1,V2}overallparametervalues.Instead,weassumethatagentsonlyhaveaccesstothefunctionvaluesandgradientsat(θ1i,θ2i).Usingthisinformationthenaivelearnersapplythegradientascentupdaterulef1nl:θ1i+1=θ1i+f1nl(θ1i,θ2i),f1nl=∇θ1iV1(θ1i,θ2i)·δ,(4.1)whereδisthestepsize.4.2LearningwithOpponentLearningAwarenessALOLAlearneroptimisesitsreturnunderonesteplook-aheadofopponentlearning:Insteadofoptimizingtheexpectedreturnunderthecurrentparameters,V1(θ1i,θ2i),aLOLAagentoptimisesV1(θ1i,θ2i+∆θ2i),whichistheexpectedreturnaftertheopponentupdatesitspolicywithonenaivelearningstep,∆θ2i.Goingforwardwedropthesubscriptiforclarity.Assumingsmall∆θ2,afirst-orderTaylorexpansionresultsin:V1(θ1,θ2+∆θ2)≈V1(θ1,θ2)+(∆θ2)T∇θ2V1(θ1,θ2).(4.2)TheLOLAobjective(4.2)differsfrompriorwork,e.g.,ZhangandLesser[46],thatpredictstheopponent’spolicyparameterup-dateandlearnsabestresponse.LOLAlearnersattempttoactivelyinfluencetheopponent’sfuturepolicyupdate,andexplicitlydif-ferentiatethroughthe∆θ2withrespecttoθ1.SinceLOLAfocusesonthisshapingofthelearningdirectionoftheopponent,thede-pendencyof∇θ2V1(θ1,θ2)onθ1isdroppedduringthebackwardpass.Investigationofhowdifferentiatingthroughthistermwouldaffectthelearningoutcomesisleftforfuturework.Bysubstitutingtheopponent’snaivelearningstep:∆θ2=∇θ2V2(θ1,θ2)·η(4.3)into(4.2)andtakingthederivativeof(4.2)withrespecttoθ1,weobtainourLOLAlearningrule:θ1i+1=θ1i+f1lola(θ1i,θ2i),whichincludesasecondordercorrectiontermf1lola(θ1,θ2)=∇θ1V1(θ1,θ2)·δ+(cid:16)∇θ2V1(θ1,θ2)(cid:17)T∇θ1∇θ2V2(θ1,θ2)·δη,(4.4)wherethestepsizesδ,ηareforthefirstandsecondorderupdates.ExactLOLAandNLagents(LOLA-ExandNL-Ex)haveaccesstothegradientsandHessiansof{V1,V2}atthecurrentpolicyparameters(θ1i,θ2i)andcanevaluate(4.1)and(4.4)exactly.Figure1:a)showstheprobabilityofcooperationintheiteratedprisonersdilemma(IPD)attheendof50trainingrunsforbothagentsasafunctionofstateundernaivelearning(NL-Ex)andb)displaystheresultsforLOLA-Exwhenusingtheexactgradientsofthevaluefunction.c)showsthenormaliseddiscountedreturnforbothagentsinNL-Exvs.NL-ExandLOLA-Exvs.LOLA-Ex,withtheexactgradient.d)plotsthenormaliseddiscountedreturnforbothagentsinNL-PGvs.NL-PGandLOLA-PGvs.LOLA-PG,withpolicygradientapproximation.WeseethatNL-ExleadstoDD,resultinginanaveragerewardofca.−2.Incontrast,theLOLA-Exagentsplaytit-for-tatinb):Wheninthelastmoveagent1defectedandagent2cooperated(DC,greenpoints),mostlikelyinthenextmoveagent1willcooperateandagent2willdefect,indicatedbyaconcentrationofthegreenpointsinthebottomrightcorner.Similarly,theyellowpoints(CD),areconcentratedinthetopleftcorner.WhiletheresultsfortheNL-PGandLOLA-PGwithpolicygradientapproximationaremorenoisy,theyarequalitativelysimilar.Bestviewedincolour.4.3LearningviaPolicyGradientWhenagentsdonothaveaccesstoexactgradientsorHessians,wederivetheupdaterulesfnl,pgandflola,pgbasedonapproxi-mationsofthederivativesin(4.1)and(4.4).DenoteanepisodeofhorizonTasτ=(s0,u10,u20,r10,r20,...,r1T,r2T)anditscorrespondingdiscountedreturnforagentaattimesteptasRat(τ)=˝Tl=tγl−tral.Theexpectedepisodicreturnconditionedontheagents’policies(π1,π2),ER10(τ)andER20(τ),approximateV1andV2respectively,asdothegradientsandHessians.∇θ1ER10(τ)followsfromthepolicygradientderivation:∇θ1ER10(τ)=E(cid:2)R10(τ)∇θ1logπ1(τ)(cid:3)=Eh(cid:213)Tt=0∇θ1logπ1(u1t|st)·(cid:213)Tl=tγlr1li=Eh(cid:213)Tt=0∇θ1logπ1(u1t|st)γt(cid:0)R1t(τ)−b(st)(cid:1)i,whereb(st)isabaselineforvariancereduction.Thentheupdaterulefnl,pgforthepolicygradient-basednaivelearner(NL-PG)isf1nl,pg=∇θ1ER10(τ)·δ.(4.5)FortheLOLAupdate,wederivethefollowingestimatorofthesecond-ordertermin(4.4)basedonpolicygradients.Thederiva-tion(omitted)closelyresemblesthestandardproofofthepolicygradienttheorem,exploitingthefactthatagentssampleactionsindependently.Wefurthernotethatthissecondordertermisexactinexpectation:∇θ1∇θ2ER20(τ)=EhR20(τ)∇θ1logπ1(τ)(cid:0)∇θ2logπ2(τ)(cid:1)Ti=Eh(cid:213)Tt=0γtr2t·(cid:16)(cid:213)tl=0∇θ1logπ1(u1l|sl)(cid:17)(cid:16)(cid:213)tl=0∇θ2logπ2(u2l|sl)(cid:17)T(cid:21).(4.6)ThecompleteLOLAupdateusingpolicygradients(LOLA-PG)isf1lola,pg=∇θ1ER10(τ)·δ+(cid:0)∇θ2ER10(τ)(cid:1)T∇θ1∇θ2ER20(τ)·δη.(4.7)4.4LOLAwithOpponentModelingBothversions(4.4)and(4.7)ofLOLAlearningassumethateachagenthasaccesstotheexactparametersoftheopponent.However,inadversarialsettingstheopponent’sparametersaretypicallyob-scuredandhavetobeinferredfromtheopponent’sstate-actiontrajectories.Ourproposedopponentmodelingissimilartobe-havioralcloning[2,38].Insteadofaccessingagent2’struepolicyparametersθ2,agent1modelstheopponent’sbehaviorwithˆθ2,whereˆθ2isestimatedfromagent2’strajectoriesusingmaximumlikelihood:ˆθ2=argmaxθ2(cid:213)tlogπθ2(u2t|st).(4.8)Then,ˆθ2replacesθ2intheLOLAupdaterule,bothfortheexactversion(4.4)usingthevaluefunctionandthegradientbasedap-proximation(4.7).Wecomparetheperformanceofpolicy-gradientbasedLOLAagents(4.7)withandwithoutopponentmodelinginourexperiments.Inparticularwecanobtainˆθ2usingthepastaction-observationhistory.Inourexperimentsweincrementallyfittothemostrecentdatainordertoaddressthenon-stationarityoftheopponent.4.5Higher-OrderLOLABysubstitutingthenaivelearningrule(4.3)intotheLOLAobjec-tive(4.2),theLOLAlearningrulesofarassumesthattheopponentisanaivelearner.Wecallthissettingfirst-orderLOLA,whichcor-respondstothefirst-orderlearningruleoftheopponentagent.However,wecanalsoconsiderahigher-orderLOLAagentthatassumestheopponentappliesafirst-orderLOLAlearningrule,thusFigure2:a)theprobabilityofplayingheadsintheiteratedmatchingpennies(IMP)attheendof50independenttrainingrunsforbothagentsasafunctionofstateundernaivelearningNL-Ex.b)theresultsofLOLA-Exwhenusingtheexactgradientsofthevaluefunction.c)thenormaliseddiscountedreturnforbothagentsinNL-Exvs.NL-ExandLOLA-Exvs.LOLA-Exwithexactgradient.d)thenormaliseddiscountedreturnforbothagentsinNL-PGvs.NL-PGandLOLA-PGvs.LOLA-PGwithpolicygradientapproximation.Wecanseeina)thatNL-Exresultsinneardeterministicstrategies,indicatedbytheaccumulationofpointsinthecorners.Thesestrategiesareeasilyexploitablebyotherdeterministicstrategiesleadingtounstabletrainingandhighvarianceintherewardperstepinc).Incontrast,LOLAagentslearntoplaytheonlyNashstrategy,50%/50%,leadingtolowvarianceintherewardperstep.OneinterpretationisthatLOLAagentsanticipatethatexploitingadeviationfromNashincreasestheirimmediatereturn,butalsorendersthemmoreexploitablebytheopponent’snextlearningstep.Bestviewedincolour.replacing(4.3).Thisleadstothird-orderderivativesinthelearningrule.Whilethethird-ordertermsaretypicallydifficulttocomputeusingpolicygradientmethod,duetohighvariance,whentheexactvaluefunctionisavailableitistractable.Weexaminethebenefitsofhigher-orderLOLAinourexperiments.5EXPERIMENTALSETUPInthissection,wesummarisethesettingswherewecomparethelearningbehaviorofNLandLOLAagents.Thefirstsetting(Sec.5.1)consistsoftwoclassicinfinitelyiteratedgames,theiteratedprison-ersdilemma(IPD),[28]anditeratedmatchingpennies(IMP)[23].Eachroundinthesetwoenvironmentsrequiresasingleactionfromeachagent.Wecanobtainthediscountedfuturereturnofeachplayergivenbothplayers’policies,whichleadstoexactpolicyupdatesforNLandLOLAagents.Thesecondsetting(Sec.5.2),CoinGame,amoredifficulttwo-playerenvironment,whereeachroundrequirestheagentstotakeasequenceofactionsandexactdiscountedfuturerewardcannotbecalculated.Thepolicyofeachplayerisparameterisedwithadeeprecurrentneuralnetwork.InthepolicygradientexperimentswithLOLA,weassumeoff-linelearning,i.e.,agentsplaymany(batch-size)parallelepisodesusingtheirlatestpolicies.Policiesremainunchangedwithineachepisode,withlearninghappeningbetweenepisodes.Onesettinginwhichthiskindofofflinelearningnaturallyarisesiswhenpoli-ciesaretrainedonreal-worlddata.Forexample,inthecaseofautonomouscars,thedatafromafleetofcarsisusedtoperiodicallytrainanddispatchnewpolicies.5.1IteratedGamesWefirstreviewthetwoiteratedgames,theIPDandIMP,andexplainhowwecanmodeliteratedgamesasamemory-1two-agentMDP.CDC(-1,-1)(-3,0)D(0,-3)(-2,-2)Table1:Payoffmatrixofprisoners’dilemma.Table1showstheper-steppayoffmatrixoftheprisoners’dilemma.Inasingle-shotprisoners’dilemma,thereisonlyoneNashequi-librium[15],wherebothagentsdefect.Intheinfinitelyiteratedprisoners’dilemma,thefolktheorem[37]showsthatthereareinfinitelymanyNashequilibria.Twonotableonesarethealwaysdefectstrategy(DD),andtit-for-tat(TFT).InTFTeachagentstartsoutwithcooperationandthenrepeatsthepreviousactionoftheopponent.Theaveragereturnsperstepinself-playare−1and−2forTFTandDDrespectively.Matchingpennies[16]isazero-sumgame,withper-steppayoutsshowninTable2.ThisgameonlyhasasinglemixedstrategyNashequilibriumwhichisbothplayersplaying50%/50%heads/tails.HeadTailHead(+1,-1)(-1,+1)Tail(-1,+1)(+1,-1)Table2:Payoffmatrixofmatchingpennies.AgentsinboththeIPDandIMPcanconditiontheiractionsonpasthistory.AgentsinaniteratedgameareendowedwithamemoryoflengthK,i.e.,theagentsactbasedontheresultsofthelastKrounds.PressandDyson[35]provedthatagentswithagoodmemory-1strategycaneffectivelyforcetheiteratedgametobeplayedasmemory-1.Thus,weconsidermemory-1iteratedgames.Wemodelthememory-1IPDandIMPasatwo-agentMRP,wherethestateattime0isempty,denotedass0,andattimet≥1isthejointactionfromt−1:st=(u1t−1,u2t−1)fort>1.IPDIMP%TFTR(std)%NashR(std)NL-Ex.20.8-1.98(0.14)0.00(0.37)LOLA-Ex.81.0-1.06(0.19)98.80(0.02)NL-PG20.0-1.98(0.00)13.20(0.19)LOLA-PG66.4-1.17(0.34)93.20(0.06)Table3:WesummariseresultsforNLvs.NLandLOLAvs.LOLAsettingswitheitherexactgradientevaluation(-Ex)orpolicygradientapproximation(-PG).Shownistheprobabil-ityofagentsplayingTFTandNashfortheIPDandIMPrespectivelyaswellastheaveragerewardperstep,R,andstandarddeviation(std)attheendoftrainingfor50train-ingruns.Eachagent’spolicyisfullyspecifiedby5probabilities.ForagentainthecaseoftheIPD,theyaretheprobabilityofcooperationatgamestartπa(C|s0),andthecooperationprobabilitiesinthefourmemories:πa(C|CC),πa(C|CD),πa(C|DC),andπa(C|DD).Byanalyticallysolvingthemulti-agentMDPwecanderiveeachagent’sfuturediscountedrewardasananalyticalfunctionoftheagents’policiesandcalculatetheexactpolicyupdateforbothNL-ExandLOLA-Exagents.Wealsoorganisearound-robintournamentwherewecompareLOLA-Extoanumberofstate-of-the-artmulti-agentlearningalgo-rithms,bothontheandIMP.5.2CoinGameNext,westudyLOLAinamorecomplexsettingcalledCoinGame.Thisisasequentialgameandtheagents’policiesareparametrisedasdeepneuralnetworks.CoinGamewasfirstproposedbyLererandPeysakhovich[25]asahigherdimensionalalternativetotheIPDwithmulti-stepactions.AsshowninFigure3,inthissettingtwoagents,‘red’and‘blue’,aretaskedwithcollectingcoins.Thecoinsareeitherblueorred,andappearrandomlyonthegrid-world.Anewcoinwithrandomcolourandrandompositionappearsafterthelastoneispickedup.Agentspickupcoinsbymovingontothepositionwherethecoinislocated.Whileeveryagentreceivesapointforpickingupacoinofanycolour,wheneveranpicksupacoinofdifferentcolour,theotheragentloses2points.Asaresult,ifbothagentsgreedilypickupanycoinavailable,theyreceive0pointsinexpectation.Sincetheagents’policiesareparameterisedasarecurrentneuralnetwork,onecannotobtainthefuturediscountedrewardasafunctionofbothagents’policiesinclosedform.Policygradient-basedlearningisappliedforbothNLandLOLAagentsinourexperiments.Wefurtherincludeex-perimentsofLOLAwithopponentmodelling(LOLA-OM)inordertoexaminethebehaviorofLOLAagentswithoutaccesstotheopponent’spolicyparameters.5.3TrainingDetailsInpolicygradient-basedNLandLOLAsettings,wetrainagentswithanactor-criticmethod[41]andparameteriseeachagentwithapolicyactorand-criticforvariancereductionduringpolicyupdates.Figure3:IntheCoinGame,twoagents,‘red’and‘blue’,get1pointforpickingupanycoin.However,the‘redagent’loses2pointswhenthe‘blueagent’picksuparedcoinandviceversa.Effectively,thisisaworldwithanembeddedsocialdilemmawherecooperationanddefectionaretemporallyextended.Duringtraining,weusegradientdescentwithstepsize,δ,of0.005fortheactor,1forthecritic,andthebatchsize4000forrollouts.Thediscountrateγissetto0.96fortheprisoners’dilemmaandCoinGameand0.9formatchingpennies.ThehighvalueofγforCoinGameandtheIPDwaschoseninordertoallowforlongtimehorizons,whichareknowntoberequiredforcooperation.WefoundthatalowerγproducedmorestablelearningonIMP.ForCoinGametheagent’spolicyarchitectureisarecurrentneuralnetworkwith32hiddenunitsand2convolutionallayerswith3×3filters,stride1,andReLUactivationforinputprocessing.Theinputispresentedasa4-channelgrid,with2channelsencodingthepositionsofthe2agentsand2channelsfortheredandbluecoinsrespectively.Forthetournament,weusebaselinealgorithmsandthecorre-spondinghyperparametervaluesasprovidedintheliterature[3].Thetournamentisplayedinaround-robinfashionbetweenallpairsofagentsfor1000episodes,200stepseach.6RESULTSInthissection,wesummarisetheexperimentalresults.WedenoteLOLAandnaiveagentswithexactpolicyupdatesasLOLA-ExandNL-Exrespectively.WeabbreviateLOLAandnativeagentswithpolicyupdateswithLOLA-PGandNL-PG.Weaimtoanswerthefollowingquestions:(1)HowdopairsofLOLA-ExagentsbehaveiniteratedgamescomparedwithpairsofNL-Exagents?(2)Usingpolicygradientupdatesinstead,howtoLOLA-PGagentsandNL-PGagentsbehave?(3)HowdoLOLA-Exagentsfairinaroundrobintournamentinvolvingasetofmulti-agentlearningalgorithmsfromliterature?(4)DoesthelearningofLOLA-PGagentsscaletohigh-dimensionalsettingswheretheagents’policiesareparameterisedbydeepneuralnetworks?(5)DoesLOLA-PGmaintainitsbehaviorwhenreplacingaccesstotheexactparametersoftheopponentagentwithopponentmodeling?(6)CanLOLAagentsbeexploitedbyusinghigherordergradients,i.e.,doesLOLAleadtoanarmsraceofeverhigherordercorrectionsorisLOLA/LOLAstable?WeanswerthefirstthreequestionsinSec.6.1,thenexttwoquestionsinSec.6.2,andthelastoneinSec.6.3.02004006008001000Episode−2.5−2.0−1.5−1.0−0.5AveragerewardperstepNL-QJAL-QPHCWoLF-PHCNL-ExLOLA-Ex02004006008001000Episode−0.4−0.20.00.20.4NL-QJAL-QPHCWoLF-PHCNL-ExLOLA-ExFigure4:Normalisedreturnsofaround-robintournamentontheIPD(left)andIMP(right).LOLA-Exachievesthebestper-formanceintheIPDandiswithinerrorbarsforIMP.Shadingindicatesa95%confidenceintervalforthemean.Baselinesfrom[3]:naiveQ-learner(NL-Q),joint-actionQ-learner(JAL-Q),policyhill-climbing(PHC),and“WinorLearnFast”(WoLF).6.1IteratedGamesWefirstcomparethebehaviorsofLOLAagentswithNLagents,witheitherexactpolicyupdatesorpolicygradientupdates.Figures1aand1bshowthepolicyforbothagentsattheendoftrainingunderNL-ExandLOLA-ExwhentheagentshaveaccesstoexactgradientsandHessiansof{V1,V2}.HereweconsiderthesettingsofNL-Exvs.NL-ExandLOLA-Exvs.LOLA-Ex.WestudymixedlearningofoneLOLA-Exagentvs.anNL-ExagentinSection6.3.UnderNL-Ex,theagentslearntodefectinallstates,indicatedbytheaccumulationofpointsinthebottomleftcorneroftheplot.However,underLOLA-Ex,inmostcasestheagentslearnTFT.Inparticularagent1cooperatesinthestartingstates0,CCandDC,whileagent2cooperatesins0,CCandCD.Asaresult,Figure1c)showsthatthenormaliseddiscountedreward1iscloseto−1forLOLA-Exvs.LOLA-Ex,correspondingtoTFT,whileNL-Exvs.NL-Exresultsinannormaliseddiscountedrewardof−2,correspondingtothefullydefective(DD)equilibrium.Figure1d)showsthenormaliseddiscountedrewardforNL-PGandLOLA-PGwhereagentslearnviapolicygradient.LOLA-PGalsodemonstratescooperationwhileagentsdefectinNL-PG.WeconductthesameanalysisforIMPinFigure2.Inthisgame,undernaivelearningtheagents’strategiesfailtoconverge.Incontrast,underLOLAtheagents’policiesconvergetotheonlyNashequilibrium,playing50%/50%heads/tails.Table3summarisesthenumericalresultscomparingLOLAwithNLagentsinboththeexactandpolicygradientsettingsinthetwoiteratedgameenvironments.IntheIPD,LOLAagentslearnpoliciesconsistentwithTFTwithamuchhigherprobabilityandachievehighernormaliseddiscountedrewardsthanNL(−1.06vs−1.98).InIMP,LOLAagentsconvergetotheNashequilibriummorestablywhileNLagentsdonot.ThedifferenceinstabilityisillustratedbythehighvarianceofthenormaliseddiscountedreturnsforNLagentscomparedtothelowvarianceunderLOLA(0.37vs0.02).InFigure4weshowtheaveragenormalisedreturnofourLOLA-Exagentagainstasetoflearningalgorithmsfromtheliterature.WefindthatLOLA-Exreceivesthehighestnormalisedreturnin1Weusefollowingdefinitionforthenormaliseddiscountedreward:(1−γ)˝Tt=0γtrt.theIPD,indicatingthatitsuccessfullyshapesthelearningoutcomeofotheralgorithmsinthisgeneralsumsetting.IntheIMP,LOLA-Exachievesstableperformanceclosetothemiddleofthedistributionofresults.6.2CoinGameWesummariseourexperimentalresultsintheCoinGameenvi-ronment.ToexaminethescalabilityofLOLAlearningrules,wecompareNL-PGvs.NL-PGtoLOLA-PGvs.LOLA-PG.Figure5demonstratesthatNL-PGagentscollectcoinsindiscriminately,cor-respondingtodefection.Incontrast,LOLA-PGagentslearntopickupcoinspredominantly(around80%)oftheirowncolour,showingthattheLOLAlearningruleleadstocooperationintheCoinGame.Removingtheassumptionthatagentscanaccesstheexactpa-rametersofopponents,weexamineLOLAagentswithopponentmodeling(Section4.4).Figure5demonstratesthatwithoutaccesstotheopponent’spolicyparameters,LOLAagentswithopponentmodelingpickupcoinsoftheirowncolouraround60%ofthetime,inferiortotheperformanceofLOLA-PGagents.Weemphasisethatwithopponentmodelingneitheragentcanrecovertheexactpolicyparametersoftheopponent,sincethereisalargeamountofredundancyintheneuralnetworkparameters.Forexample,eachagentcouldpermutetheweightsoftheirfullyconnectedlay-ers.Opponentmodelingintroducesnoiseintheopponentagent’spolicyparameters,thusincreasingthevarianceandbiasofthegradients(4.7)duringpolicyupdates,whichleadstoinferiorper-formanceofLOLA-OMvs.LOLA-PGinFigure5.6.3ExploitabilityofLOLAInthissectionweaddresstheexploitabilityoftheLOLAlearningrule.WeconsidertheIPD,whereonecancalculatetheexactvaluefunctionofeachagentgiventhepolicies.Thus,wecanevaluatethehigher-orderLOLAterms.WepitchaNL-ExorLOLA-ExagentagainstNL-Ex,LOLA-Ex,anda2nd-orderLOLAagent.WecomparethenormaliseddiscountedreturnofeachagentinallsettingsandaddressthequestionofwhetherthereisanarmsracetoincorporateeverhigherordersofLOLAcorrectionterms.Table4showsthataLOLA-ExlearnercanachievehigherpayoutsagainstNL-Ex.Thus,thereisanincentiveforeitheragenttoswitch01000200030004000Iterations0.500.550.600.650.700.750.800.85P(own coin)NL-PGLOLA-PGLOLA-OM01000200030004000Iterations0246810121416PointsNL-PGLOLA-PGLOLA-OMFigure5:Thepercentageofallpickedupcoinsthatmatchincolour(left)andthetotalpointsobtainedperepisode(right)forapairofnaivelearnersusingpolicygradient(NL-PG),LOLA-agents(LOLA-PG),andapairofLOLA-agentswithopponentmodelling(LOLA-OM).Alsoshownisthestandarderrorofthemean(shading),basedon10trainingruns.WhileLOLA-PGandLOLA-OMagentslearntocooperate,LOLA-OMislessstableandobtainslowerreturnsthanLOLA-PG.Bestviewedincolour.fromnaivelearningtofirstorderLOLA.Furthermore,twoLOLA-ExagentsplayingagainsteachotherbothreceivehighernormaliseddiscountedrewardthanaLOLA-ExagentplayingagainstaNL-Ex.ThismakesLOLAadominantlearningruleintheIPDcomparedtonaivelearning.Wefurtherfindthat2nd-orderLOLAprovidesnoincrementalgainswhenplayingagainstaLOLA-Exagent,leadingtoareductioninpayoutsforbothagents.Theseexperimentswerecarriedoutwithaδof0.5.WhileitisbeyondthescopeofthisworktoprovethatLOLAvsLOLAisadominantlearningruleinthespaceofgradient-basedrules,theseinitialresultsareencouraging.NL-ExLOLA-Ex2nd-OrderNL-Ex(-1.99,-1.99)(-1.54,-1.28)(-1.46,-1.46)LOLA-Ex(-1.28,-1.54)(-1.04,-1.04)(-1.14,-1.17)Table4:Higher-orderLOLAresultsontheIPD.ALOLA-ExagentobtainshighernormalisedreturncomparedtoaNL-Exagent.Howeverinthissettingthereisnoincrementalgainfromusinghigher-orderLOLAinordertoexploitan-otherLOLAagentintheIPD.Infactbothagentsdoworsewiththe2ndorderLOLA(incl.3rdordercorrections).7CONCLUSIONS&FUTUREWORKWepresentedLearningwithOpponent-LearningAwareness(LOLA),alearningmethodformulti-agentsettingsthatconsidersthelearn-ingprocessesofotheragents.WeshowthatwhenbothagentshaveaccesstoexactvaluefunctionsandapplytheLOLAlearningrule,cooperationemergesbasedontit-for-tatintheinfinitelyrepeatediteratedprisoners’dilemmawhileindependentnaivelearnersde-fect.WealsofindthatLOLAleadstostablelearningoftheNashequilibriuminIMP.Inourround-robintournamentagainstothermulti-agentlearningalgorithmsweshowthatexactLOLAagentsachievethehighestaveragereturnsontheIPDandrespectableper-formanceonIMP.Wealsoderiveapolicygradient-basedversionofLOLA,applicabletoadeepRLsetting.ExperimentsontheIPDandIMPdemonstratesimilarlearningbehaviortothesettingwithexactvaluefunction.Inaddition,wescalethepolicygradient-basedversionofLOLAtotheCoinGame,amulti-stepgamethatrequiresdeeprecurrentpolicies.LOLAagentslearntocooperate,asagentspickupcoinsoftheirowncolourwithhighprobabilitywhilenaivelearnerspickupcoinsindiscriminately.Wefurtherremoveagents’accesstotheopponentagents’policyparametersandreplacewithopponentmodeling.Whilelessreliable,LOLAagentswithopponentmodelingalsolearntocooperate.WebrieflyaddresstheexploitabilityofLOLAagents.EmpiricalresultsshowthatintheIPDbothagentsareincentivisedtouseLOLA,whilehigherorderexploitsshownofurthergain.Inthefuture,wewouldliketocontinuetoaddresstheexploitabil-ityofLOLA,whenadversarialagentsexplicitlyaimtotakeadvan-tageofaLOLAlearnerusingglobalsearchmethodsratherthanjustgradient-basedmethods.JustasLOLAisawaytoexploitanaivelearner,thereshouldbemeansofexploitingLOLAlearnersinturn,unlessLOLAisitselfanequilibriumlearningstrategy.ACKNOWLEDGEMENTSThisprojecthasreceivedfundingfromtheEuropeanResearchCouncil(ERC)undertheEuropeanUnion’sHorizon2020researchandinnovationprogramme(grantagreementnumber637713)andfromtheNationalInstitutesofHealth(grantagreementnumberR01GM114311).ItwasalsosupportedbytheOxford-GoogleDeep-MindGraduateScholarshipandagenerousequipmentgrantfromNVIDIA.WewouldliketothankJaschaSohl-Dickstein,DavidBal-duzzi,KarlTuyls,MarcLanctot,MichaelBowling,IlyaSutskever,BobMcGrew,andPaulCristianoforfruitfuldiscussion.WewouldliketothankMichaelLittmanforprovidingfeedbackonanearlyversionofthemanuscript.Wewouldliketothankourreviewersforconstructiveandthoughtfulfeedback.REFERENCES[1]RobertMAxelrod.2006.Theevolutionofcooperation:revisededition.Basicbooks.[2]MariuszBojarski,DavideDelTesta,DanielDworakowski,BernhardFirner,BeatFlepp,PrasoonGoyal,LawrenceDJackel,MathewMonfort,UrsMuller,JiakaiZhang,etal.2016.Endtoendlearningforself-drivingcars.arXivpreprintarXiv:1604.07316(2016).[3]MichaelBowlingandManuelaVeloso.2002.Multiagentlearningusingavariablelearningrate.ArtificialIntelligence136,2(2002),215–250.[4]RonenI.BrafmanandMosheTennenholtz.2003.EfficientLearningEquilibrium.InAdvancesinNeuralInformationProcessingSystems,Vol.9.1635–1643.[5]GeorgeWBrown.1951.Iterativesolutionofgamesbyfictitiousplay.(1951).[6]LucianBusoniu,RobertBabuska,andBartDeSchutter.2008.Acomprehensivesurveyofmultiagentreinforcementlearning.IEEETransactionsonSystems,Man,AndCybernetics-PartC:ApplicationsandReviews,38(2),2008(2008).[7]DoranChakrabortyandPeterStone.2014.Multiagentlearninginthepresenceofmemory-boundedagents.Autonomousagentsandmulti-agentsystems28,2(2014),182–213.[8]CarolineClausandCraigBoutilier.1998.Thedynamicsofreinforcementlearningincooperativemultiagentsystems.AAAI/IAAI1998(1998),746–752.[9]VincentConitzerandTuomasSandholm.2007.AWESOME:Ageneralmultiagentlearningalgorithmthatconvergesinself-playandlearnsabestresponseagainststationaryopponents.MachineLearning67,1-2(2007),23–43.[10]JacobWCrandallandMichaelAGoodrich.2011.Learningtocompete,coordinate,andcooperateinrepeatedgamesusingreinforcementlearning.MachineLearning82,3(2011),281–314.[11]AbhishekDas,SatwikKottur,JoséMFMoura,StefanLee,andDhruvBatra.2017.LearningCooperativeVisualDialogAgentswithDeepReinforcementLearning.arXivpreprintarXiv:1703.06585(2017).[12]JakobFoerster,YannisMAssael,NandodeFreitas,andShimonWhiteson.2016.Learningtocommunicatewithdeepmulti-agentreinforcementlearning.InAdvancesinNeuralInformationProcessingSystems.2137–2145.[13]JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.2018.CounterfactualMulti-AgentPolicyGradients.InAAAI.[14]JakobFoerster,NantasNardelli,GregoryFarquhar,PhilipTorr,PushmeetKohli,ShimonWhiteson,etal.2017.Stabilisingexperiencereplayfordeepmulti-agentreinforcementlearning.In34thInternationalConferenceofMachineLearning.[15]DrewFudenbergandJeanTirole.1991.Gametheory,1991.Cambridge,Mas-sachusetts393(1991),12.[16]RobertGibbons.1992.Gametheoryforappliedeconomists.PrincetonUniversityPress.[17]JohannesHeinrichandDavidSilver.2016.Deepreinforcementlearningfromself-playinimperfect-informationgames.arXivpreprintarXiv:1603.01121(2016).[18]PabloHernandez-LealandMichaelKaisers.2017.Learningagainstsequentialopponentsinrepeatedstochasticgames.(2017).[19]PabloHernandez-Leal,MichaelKaisers,TimBaarslag,andEnriqueMunozdeCote.2017.ASurveyofLearninginMultiagentEnvironments:DealingwithNon-Stationarity.arXivpreprintarXiv:1707.09183(2017).[20]MaxKleiman-Weiner,MarkKHo,JosephLAusterweil,MichaelLLittman,andJoshuaBTenenbaum.2016.Coordinatetocooperateorcompete:abstractgoalsandjointintentionsinsocialinteraction.InCOGSCI.[21]MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,KarlTuyls,JulienPerolat,DavidSilver,andThoreGraepel.2017.AUnifiedGame-TheoreticApproachtoMultiagentReinforcementLearning.InAdvancesinNeuralInformationProcessingSystems(NIPS).[22]AngelikiLazaridou,AlexanderPeysakhovich,andMarcoBaroni.2016.Multi-agentcooperationandtheemergenceof(natural)language.arXivpreprintarXiv:1612.07182(2016).[23]KingLeeandKLouis.1967.TheApplicationofDecisionTheoryandDynamicProgrammingtoAdaptiveControlSystems.Ph.D.Dissertation.[24]JoelZLeibo,ViniciusZambaldi,MarcLanctot,JanuszMarecki,andThoreGraepel.2017.Multi-agentReinforcementLearninginSequentialSocialDilemmas.InProceedingsofthe16thConferenceonAutonomousAgentsandMultiAgentSystems.InternationalFoundationforAutonomousAgentsandMultiagentSystems,464–473.[25]AdamLererandAlexanderPeysakhovich.2017.Maintainingcooperationincomplexsocialdilemmasusingdeepreinforcementlearning.arXivpreprintarXiv:1707.01068(2017).[26]MichaelLLittman.2001.Friend-or-foeQ-learningingeneral-sumgames.InICML,Vol.1.322–328.[27]RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.2017.Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnvironments.arXivpreprintarXiv:1706.02275(2017).[28]RDuncanLuceandHowardRaiffa.1957.GamesandDecisions:IntroductionandCriticalSurvey.(1957).[29]RichardMealingandJonathanShapiro.2015.OpponentModellingbyExpectation-MaximisationandSequencePredictioninSimplifiedPoker.IEEETransactionsonComputationalIntelligenceandAIinGames(2015).[30]RichardMealingandJonathanLShapiro.2013.OpponentModellingbySequencePredictionandLookaheadinTwo-PlayerGames..InICAISC(2).385–396.[31]LukeMetz,BenPoole,DavidPfau,andJaschaSohl-Dickstein.2016.Unrolledgenerativeadversarialnetworks.arXivpreprintarXiv:1611.02163(2016).[32]IgorMordatchandPieterAbbeel.2017.EmergenceofGroundedCompositionalLanguageinMulti-AgentPopulations.arXivpreprintarXiv:1703.04908(2017).[33]EnriqueMunozdeCoteandMichaelL.Littman.2008.APolynomial-timeNashEquilibriumAlgorithmforRepeatedStochasticGames.In24thConferenceonUncertaintyinArtificialIntelligence(UAI’08).http://uai2008.cs.helsinki.fi/UAI_camera_ready/munoz.pdf[34]ShayeganOmidshafiei,JasonPazis,ChristopherAmato,JonathanPHow,andJohnVian.2017.DeepDecentralizedMulti-taskMulti-AgentRLunderPartialObservability.arXivpreprintarXiv:1703.06182(2017).[35]WilliamHPressandFreemanJDyson.2012.IteratedPrisonerâĂŹsDilemmacontainsstrategiesthatdominateanyevolutionaryopponent.ProceedingsoftheNationalAcademyofSciences109,26(2012),10409–10413.[36]NeilCRabinowitz,FrankPerbet,HFrancisSong,ChiyuanZhang,SMEs-lami,andMatthewBotvinick.2018.MachineTheoryofMind.arXivpreprintarXiv:1802.07740(2018).[37]BMyersonRoger.1991.Gametheory:analysisofconflict.(1991).[38]StéphaneRoss,GeoffreyJGordon,andJAndrewBagnell.2011.No-regretreduc-tionsforimitationlearningandstructuredprediction.InInAISTATS.Citeseer.[39]TuomasWSandholmandRobertHCrites.1996.Multiagentreinforcementlearningintheiteratedprisoner’sdilemma.Biosystems37,1-2(1996),147–166.[40]SainbayarSukhbaatar,RobFergus,etal.2016.Learningmultiagentcommunica-tionwithbackpropagation.InAdvancesinNeuralInformationProcessingSystems.2244–2252.[41]RichardSSuttonandAndrewGBarto.1998.Reinforcementlearning:Anintro-duction.Vol.1.MITpressCambridge.[42]RichardSSutton,DavidAMcAllester,SatinderPSingh,YishayMansour,etal.1999.Policygradientmethodsforreinforcementlearningwithfunctionapproxi-mation..InNIPS,Vol.99.1057–1063.[43]WilliamUtherandManuelaVeloso.1997.Adversarialreinforcementlearning.TechnicalReport.Technicalreport,CarnegieMellonUniversity,1997.Unpub-lished.[44]RonaldJWilliams.1992.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.Machinelearning8,3-4(1992),229–256.[45]MichaelWunder,MichaelLLittman,andMonicaBabes.2010.Classesofmultia-gentq-learningdynamicswithepsilon-greedyexploration.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10).1167–1174.[46]ChongjieZhangandVictorRLesser.2010.Multi-AgentLearningwithPolicyPrediction..InAAAI.[47]MartinZinkevich,AmyGreenwald,andMichaelLLittman.2006.CyclicequilibriainMarkovgames.InAdvancesinNeuralInformationProcessingSystems.1641–1648.LearningwithOpponent-LearningAwarenessSupplementaryMaterial1arXiv:1709.04326v4  [cs.AI]  19 Sep 2018AAppendixA.1DerivationofSecond-OrderderivativeInthissection,wederivethesecondorderderivativesofLOLAinthepolicygradientsetting.RecallthatanepisodeofhorizonTisτ=(s0,u10,u20,r10,r20,...,sT,u1T,u2T,r1T,r2T)andthecorrespondingdiscountedreturnforagentaattimesteptisRat(τ)=PTl=tγl−tral.WedenoteEπ1,π2,τastheexpectationtakenoverbothagents’policyandtheepisodeτ.Then,∇θ1∇θ2Eπ1,π2,τR10(τ)=∇θ1∇θ2Eτ"R10(τ)·TYl=0π1(u1l|sl,θ1)·TYl=0π2(u2l|sl,θ2)#=EτR10(τ)· ∇θ1(cid:0)TYl=0π1(u1l|sl,θ1)(cid:1)! ∇θ2(cid:0)TYl=0π2(u2l|sl,θ2)(cid:1)!T=EτR10(τ)· ∇θ1(cid:0)QTl=0π1(u1l|sl,θ1)(cid:1)QTl=0π1(u1l|sl,θ1)! ∇θ2(cid:0)QTl=0π2(u2l|sl,θ2)(cid:1)QTl=0π2(u2l|sl,θ2)!T·TYl=0π1(u1l|sl,θ1)·TYl=0π2(u2l|sl,θ2)#=Eπ1,π2,τR10(τ)· ∇θ1(cid:0)QTl=0π1(u1l|sl,θ1)(cid:1)QTl=0π1(u1l|sl,θ1)! ∇θ2(cid:0)QTl=0π2(u2l|sl,θ2)(cid:1)QTl=0π2(u2l|sl,θ2)!T=Eπ1,π2,τR10(τ)· ∇θ1log(cid:0)TYl=0π1(u1l|sl,θ1)(cid:1)! ∇θ2log(cid:0)TYl=0π2(u2l|sl,θ2)(cid:1)!T=Eπ1,π2,τ"R10(τ)·(cid:18)XTl=0∇θ1logπ1(u1l|sl,θ1)(cid:19)(cid:18)XTl=0∇θ2logπ2(u2l|sl,θ2)(cid:19)T#.Thesecondequalityisduetoπlisonlyafunctionofθl.Thethirdequalityismultiplyanddividetheprobabilityoftheepisodeτ.ThefourthequalityfactorstheprobabilityoftheepisodeτintotheexpectationEπ1,π2,τ.Theﬁfthandsixthequalitiesarestandardpolicygradientoperations.Similarderivationsleadtothethefollowingsecondordercross-termgradientforasinglerewardofagent1attimet∇θ1∇θ2Eπ1,π2,τr1t=Eπ1,π2,τ(cid:20)r1t·(cid:16)Xtl=0∇θ1logπ1(u1l|sl,θ1)(cid:17)(cid:16)Xtl=0∇θ2logπ2(u2l|sl,θ2)(cid:17)T(cid:21).Sumtherewardsovert,∇θ1∇θ2Eπ1,π2,τR10(τ)=Eπ1,π2,τ(cid:20)XTt=0γtr1t·(cid:16)Xtl=0∇θ1logπ1(u1l|sl,θ1)(cid:17)(cid:16)Xtl=0∇θ2logπ2(u2l|sl,θ2)(cid:17)T(cid:21),2whichisthe2ndordertermintheMethodsSection.A.2DerivationoftheexactvaluefunctionintheIteratedPrison-ers’dilemmaandIteratedMatchingPenniesInbothIPDandIMPtheactionspaceconsistsof2discreteactions.Thestateconsistsoftheunionofthelastactionofbothagents.Assuchthereareatotalof5possiblestates,1statebeingtheinitialstate,s0,andtheother4the2x2statesdependingonthelastactiontaken.Asaconsequencethepolicyofeachagentcanberepresentedby5parameters,θa,theprobabilitiesoftakingaction0ineachofthese5states.InthecaseoftheIPDtheseparameterscorrespondtotheprobabilityofcooperationins0,CC,CD,DCandDD:πa(C|s0)=θa,0,πa(D|s0)=1−θa,0,πa(C|CC)=θa,1,πa(D|CC)=1−θa,1,πa(C|CD)=θa,2,πa(D|CD)=1−θa,2,πa(C|DC)=θa,3,πa(D|DC)=1−θa,3,πa(C|DD)=θa,4,πa(D|DD)=1−θa,4,a∈{1,2}.Wedenoteθa=(θa,0,θa,1,θa,2,θa,3,θa,4).Inthesegamestheunionofπ1andπ2inducesastatetransitionfunctionP(s0|s)=P(u|s).Denotethedistributionofs0asp0:p0=(cid:0)θ1,0θ2,0,θ1,0(1−θ2,0),(1−θ1,0)θ2,0,(1−θ1,0)(1−θ2,0)(cid:1)T,thepayoutvectorasr1=(−1,−3,0,−2)Tandr2=(−1,0,−3,−2)T,andthetransitionmatrixisP=(cid:2)θ1θ2,θ1(1−θ2),(θ1−1)θ2,(1−θ1)(1−θ2)(cid:3)ThenV1,V2canberepresentedasV1(θ1,θ2)=pT0(cid:0)r1+X∞t=1γtPtr1(cid:1)V2(θ1,θ2)=pT0(cid:0)r2+X∞t=1γtPtr2(cid:1).Sinceγ<1andPisastochasticmatrix,theinﬁnitesumconvergesandV1(θ1,θ2)=pT0II−γPr1,V2(θ1,θ2)=pT0II−γPr2,whereIistheidentitymatrix.AnequivalentderivationholdsfortheIteratedMatchingPenniesgamewithr1=(−1,1,1,−1)Tandr2=−r1.3A.3Figures0.00.20.40.60.81.0P(cooperation | state)_agent 00.00.20.40.60.81.0P(cooperation | state)_agent 10.00.20.40.60.81.0P(cooperation | state)_agent 00.00.20.40.60.81.0CCCDDCDDP0050100150200Iterations2.01.81.61.41.21.00.8Average reward per stepa0 Lolaa1 Lolaa0 NLa1 NL(a)0.00.20.40.60.81.0P(Head | state)_agent 00.00.20.40.60.81.0P(Head | state)_agent 10.00.20.40.60.81.0P(Head | state)_agent 00.00.20.40.60.81.0HHHTTHTTP0050100150200Iterations0.20.10.00.10.2Average reward per stepa0 Lolaa1 Lolaa0 NLa1 NL(b)Figure1:Shownistheprobabilityofcooperationintheprisonersdilemma(a)andtheprobabilityofheadsinthematchingpenniesgame(b)attheendof50trainingrunsforbothagentsasafunctionofstateundernaivelearning(left)andLOLA(middle)whenusingtheexactgradientsofthevaluefunction.AlsoshownistheaveragereturnperstepfornaiveandLOLA(right)40.00.20.40.60.81.0P(cooperation | state)_agent 00.00.20.40.60.81.0P(cooperation | state)_agent 10.00.20.40.60.81.0P(cooperation | state)_agent 00.00.20.40.60.81.0CCCDDCDDP0020406080100Iterations2.01.51.0Average reward per stepa0 Lolaa1 Lolaa0 NLa1 NL(a)0.00.20.40.60.81.0P(Head | state)_agent 00.00.20.40.60.81.0P(Head | state)_agent 10.00.20.40.60.81.0P(Head | state)_agent 00.00.20.40.60.81.0HHHTTHTTP0020406080100Iterations0.30.20.10.00.10.20.3Average reward per stepa0 Lolaa1 Lolaa0 NLa1 NL(b)Figure2:SameasFigureA.3,butusingthepolicygradientapproximationforallterms.Clearlyresultsaremorenoisybyqualitativelyfollowtheresultsoftheexactmethod.5
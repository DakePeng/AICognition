7
1
0
2
c
e
D
4

]
I

A
.
s
c
[

3
v
6
2
3
7
0
.
3
0
7
1
:
v
i
X
r
a

One-ShotImitationLearningYanDuan†§,MarcinAndrychowicz‡,BradlyStadie†‡,JonathanHo†§,JonasSchneider‡,IlyaSutskever‡,PieterAbbeel†§,WojciechZaremba‡†BerkeleyAIResearchLab,‡OpenAI§WorkdonewhileatOpenAI{rockyduan,jonathanho,pabbeel}@eecs.berkeley.edu{marcin,bstadie,jonas,ilyasu,woj}@openai.comAbstractImitationlearninghasbeencommonlyappliedtosolvedifferenttasksinisolation.Thisusuallyrequireseithercarefulfeatureengineering,orasigniﬁcantnumberofsamples.Thisisfarfromwhatwedesire:ideally,robotsshouldbeabletolearnfromveryfewdemonstrationsofanygiventask,andinstantlygeneralizetonewsituationsofthesametask,withoutrequiringtask-speciﬁcengineering.Inthispaper,weproposeameta-learningframeworkforachievingsuchcapability,whichwecallone-shotimitationlearning.Speciﬁcally,weconsiderthesettingwherethereisaverylarge(maybeinﬁnite)setoftasks,andeachtaskhasmanyinstantiations.Forexample,ataskcouldbetostackallblocksonatableintoasingletower,anothertaskcouldbetoplaceallblocksonatableintotwo-blocktowers,etc.Ineachcase,differentinstancesofthetaskwouldconsistofdifferentsetsofblockswithdifferentinitialstates.Attrainingtime,ouralgorithmispresentedwithpairsofdemonstrationsforasubsetofalltasks.Aneuralnetistrainedsuchthatwhenittakesasinputtheﬁrstdemonstrationdemonstrationandastatesampledfromtheseconddemonstration,itshouldpredicttheactioncorrespondingtothesampledstate.Attesttime,afulldemonstrationofasingleinstanceofanewtaskispresented,andtheneuralnetisexpectedtoperformwellonnewinstancesofthisnewtask.Ourexperimentsshowthattheuseofsoftattentionallowsthemodeltogeneralizetoconditionsandtasksunseeninthetrainingdata.Weanticipatethatbytrainingthismodelonamuchgreatervarietyoftasksandsettings,wewillobtainageneralsystemthatcanturnanydemonstrationsintorobustpoliciesthatcanaccomplishanoverwhelmingvarietyoftasks.1IntroductionWeareinterestedinroboticsystemsthatareabletoperformavarietyofcomplexusefultasks,e.g.tidyingupahomeorpreparingameal.Therobotshouldbeabletolearnnewtaskswithoutlongsysteminteractiontime.Toaccomplishthis,wemustsolvetwobroadproblems.Theﬁrstproblemisthatofdexterity:robotsshouldlearnhowtoapproach,graspandpickupcomplexobjects,andhowtoplaceorarrangethemintoadesiredconﬁguration.Thesecondproblemisthatofcommunication:howtocommunicatetheintentofthetaskathand,sothattherobotcanreplicateitinabroadersetofinitialconditions.Demonstrationsareanextremelyconvenientformofinformationwecanusetoteachrobotstoover-comethesetwochallenges.Usingdemonstrations,wecanunambiguouslycommunicateessentiallyanymanipulationtask,andsimultaneouslyprovidecluesaboutthespeciﬁcmotorskillsrequiredtoperformthetask.Wecancomparethiswithanalternativeformofcommunication,namelynaturallanguage.Althoughlanguageishighlyversatile,effective,andefﬁcient,naturallanguageprocessing31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. 
 
 
 
 
 
(a) Traditional Imitation LearningTask Ae.g. stack blocks into towers of height 3ManydemonstrationsImitation Learning AlgorithmPolicy fortask AactionEnvironmentobsTask Be.g. stack blocks into towers of height 2ManydemonstrationsImitation Learning AlgorithmPolicy fortask BactionEnvironmentobsMany demonstrationsfor task AMeta Learning Algorithm…more demonstrations for more tasksOne-Shot Imitator(Neural Network)EnvironmentactionobsSingle demonstration for task FPolicy for task FMany demonstrationsfor task BMany demonstrationsfor task AsampleMany demonstrationsfor task B(b) One-Shot Imitation Learning(c) Training the One-Shot ImitatorOne-Shot Imitator(Neural Network)Supervised lossDemo1observation fromDemo2correspondingaction in Demo2predictedactionFigure1:(a)Traditionally,policiesaretask-speciﬁc.Forexample,apolicymighthavebeentrainedthroughanimitationlearningalgorithmtostackblocksintotowersofheight3,andthenanotherpolicywouldbetrainedtostackblocksintotowersofheight2,etc.(b)Inthispaper,weareinterestedintrainingnetworksthatarenotspeciﬁctoonetask,butrathercanbetold(throughasingledemonstration)whatthecurrentnewtaskis,andbesuccessfulatthisnewtask.Forexample,whenitisconditionedonasingledemonstrationfortaskF,itshouldbehavelikeagoodpolicyfortaskF.(c)Wecanphrasethisasasupervisedlearningproblem,wherewetrainthisnetworkonasetoftrainingtasks,andwithenoughexamplesitshouldgeneralizetounseen,butrelatedtasks.Totrainthisnetwork,ineachiterationwesampleademonstrationfromoneofthetrainingtasks,andfeedittothenetwork.Then,wesampleanotherpairofobservationandactionfromaseconddemonstrationofthesametask.Whenconditionedonboththeﬁrstdemonstrationandthisobservation,thenetworkistrainedtooutputthecorrespondingaction.systemsarenotyetatalevelwherewecouldeasilyuselanguagetopreciselydescribeacomplextasktoarobot.Comparedtolanguage,usingdemonstrationshastwofundamentaladvantages:ﬁrst,itdoesnotrequiretheknowledgeoflanguage,asitispossibletocommunicatecomplextaskstohumansthatdon’tspeakone’slanguage.Andsecond,therearemanytasksthatareextremelydifﬁculttoexplaininwords,evenifweassumeperfectlinguisticabilities:forexample,explaininghowtoswimwithoutdemonstrationandexperienceseemstobe,attheveryleast,anextremelychallengingtask.Indeed,learningfromdemonstrationshavehadmanysuccessfulapplications.However,sofartheseapplicationshaveeitherrequiredcarefulfeatureengineering,orasigniﬁcantamountofsysteminteractiontime.Thisisfarfromwhatwhatwedesire:ideally,wehopetodemonstrateacertaintaskonlyonceorafewtimestotherobot,andhaveitinstantlygeneralizetonewsituationsofthesametask,withoutlongsysteminteractiontimeordomainknowledgeaboutindividualtasks.Inthispaperweexploretheone-shotimitationlearningsettingillustratedinFig.1,wheretheobjectiveistomaximizetheexpectedperformanceofthelearnedpolicywhenfacedwithanew,previouslyunseen,task,andhavingreceivedasinputonlyonedemonstrationofthattask.Forthetasksweconsider,thepolicyisexpectedtoachievegoodperformancewithoutanyadditionalsysteminteraction,onceithasreceivedthedemonstration.Wetrainapolicyonabroaddistributionovertasks,wherethenumberoftasksispotentiallyinﬁnite.Foreachtrainingtaskweassumetheavailabilityofasetofsuccessfuldemonstrations.Ourlearnedpolicytakesasinput:(i)thecurrentobservation,and(ii)onedemonstrationthatsuccessfullysolvesadifferentinstanceofthesametask(thisdemonstrationisﬁxedforthedurationoftheepisode).Thepolicyoutputsthecurrentcontrols.Wenotethatanypairofdemonstrationsforthesametaskprovidesasupervisedtrainingexamplefortheneuralnetpolicy,whereonedemonstrationistreatedastheinput,whiletheotherastheoutput.2Tomakethismodelwork,wemadeessentialuseofsoftattention[6]forprocessingboththe(poten-tiallylong)sequenceofstatesandactionthatcorrespondtothedemonstration,andforprocessingthecomponentsofthevectorspecifyingthelocationsofthevariousblocksinourenvironment.Theuseofsoftattentionoverbothtypesofinputsmadestronggeneralizationpossible.Inparticular,onafamilyofblockstackingtasks,ourneuralnetworkpolicywasabletoperformwellonnovelblockconﬁgurationswhichwerenotpresentinanytrainingdata.Videosofourexperimentsareavailableathttp://bit.ly/nips2017-oneshot.2RelatedWorkImitationlearningconsiderstheproblemofacquiringskillsfromobservingdemonstrations.Surveyarticlesinclude[48,11,3].Twomainlinesofworkwithinimitationlearningarebehavioralcloning,whichperformssupervisedlearningfromobservationstoactions(e.g.,[41,44]);andinversereinforcementlearning[37],wherearewardfunction[1,66,29,18,22]isestimatedthatexplainsthedemonstrationsas(near)optimalbehavior.Whilethispastworkhasledtoawiderangeofimpressiveroboticsresults,itconsiderseachskillseparately,andhavinglearnedtoimitateoneskilldoesnotacceleratelearningtoimitatethenextskill.One-shotandfew-shotlearninghasbeenstudiedforimagerecognition[61,26,47,42],generativemodeling[17,43],andlearning“fast”reinforcementlearningagentswithrecurrentpolicies[16,62].Fastadaptationhasalsobeenachievedthroughfast-weights[5].Likeouralgorithm,manyoftheaforementionedapproachesareaformofmeta-learning[58,49,36],wherethealgorithmitselfisbeinglearned.Meta-learninghasalsobeenstudiedtodiscoverneuralnetworkweightoptimizationalgorithms[8,9,23,50,2,31].Thispriorworkonone-shotlearningandmeta-learning,however,istailoredtorespectivedomains(imagerecognition,generativemodels,reinforcementlearning,optimization)andnotdirectlyapplicableintheimitationlearningsetting.Recently,[19]proposeagenericframeworkformetalearningacrossseveralaforementioneddomains.Howevertheydonotconsidertheimitationlearningsetting.Reinforcementlearning[56,10]providesanalternativeroutetoskillacquisition,bylearningthroughtrialanderror.Reinforcementlearninghashadmanysuccesses,includingBackgammon[57],helicoptercontrol[39],Atari[35],Go[52],continuouscontrolinsimulation[51,21,32]andonrealrobots[40,30].However,reinforcementlearningtendstorequirealargenumberoftrialsandrequiresspecifyingarewardfunctiontodeﬁnethetaskathand.Theformercanbetime-consumingandthelattercanoftenbesigniﬁcantlymoredifﬁcultthanprovidingademonstration[37].Multi-taskandtransferlearningconsiderstheproblemoflearningpolicieswithapplicabilityandre-usebeyondasingletask.Successstoriesincludedomainadaptationincomputervision[64,34,28,4,15,24,33,59,14]andcontrol[60,45,46,20,54].However,whileacquiringamultitudeofskillsfasterthanwhatitwouldtaketoacquireeachoftheskillsindependently,theseapproachesdonotprovidetheabilitytoreadilypickupanewskillfromasingledemonstration.Ourapproachheavilyreliesonanattentionmodeloverthedemonstrationandanattentionmodeloverthecurrentobservation.Weusethesoftattentionmodelproposedin[6]formachinetranslations,andwhichhasalsobeensuccessfulinimagecaptioning[63].Theinteractionnetworksproposedin[7,12]alsoleveragelocalityofphysicalinteractioninlearning.Ourmodelisalsorelatedtothesequencetosequencemodel[55,13],asinbothcasesweconsumeaverylongdemonstrationsequenceand,effectively,emitalongsequenceofactions.3OneShotImitationLearning3.1ProblemFormalizationWedenoteadistributionoftasksbyT,anindividualtaskbyt⇠T,andadistributionofdemon-strationsforthetasktbyD(t).Apolicyissymbolizedby⇡✓(a|o,d),whereaisanaction,oisanobservation,disademonstration,and✓aretheparametersofthepolicy.Ademonstrationd⇠D(t)isasequenceofobservationsandactions:d=[(o1,a1),(o2,a2),...,(oT,aT)].WeassumethatthedistributionoftasksTisgiven,andthatwecanobtainsuccessfuldemonstrationsforeachtask.Weassumethatthereissomescalar-valuedevaluationfunctionRt(d)(e.g.abinaryvalue3indicatingsuccess)foreachtask,althoughthisisnotrequiredduringtraining.Theobjectiveistomaximizetheexpectedperformanceofthepolicy,wheretheexpectationistakenovertaskst2T,anddemonstrationsd2D(t).3.2BlockStackingTasksToclarifytheproblemsetting,wedescribeaconcreteexampleofadistributionofblockstackingtasks,whichwewillalsolaterstudyintheexperiments.Thecompositionalstructuresharedamongthesetasksallowsustoinvestigatenontrivialgeneralizationtounseentasks.Foreachtask,thegoalistocontrola7-DOFFetchroboticarmtostackvariousnumbersofcube-shapedblocksintoaspeciﬁcconﬁgurationspeciﬁedbytheuser.Eachconﬁgurationconsistsofalistofblocksarrangedintotowersofdifferentheights,andcanbeidentiﬁedbyastring.Forexample,abcdefghmeansthatwewanttostack4towers,eachwithtwoblocks,andwewantblockAtobeontopofblockB,blockContopofblockD,blockEontopofblockF,andblockGontopofblockH.Eachoftheseconﬁgurationscorrespondtoadifferenttask.Furthermore,ineachepisodethestartingpositionsoftheblocksmayvary,whichrequiresthelearnedpolicytogeneralizeevenwithinthetrainingtasks.Inatypicaltask,anobservationisalistof(x,y,z)objectpositionsrelativetothegripper,andinformationifgripperisopenedorclosed.Thenumberofobjectsmayvaryacrossdifferenttaskinstances.Wedeﬁneastageasasingleoperationofstackingoneblockontopofanother.Forexample,thetaskabcdefghhas4stages.3.3AlgorithmInordertotraintheneuralnetworkpolicy,wemakeuseofimitationlearningalgorithmssuchasbehavioralcloningandDAGGER[44],whichonlyrequiredemonstrationsratherthanrewardfunctionstobespeciﬁed.Thishasthepotentialtobemorescalable,sinceitisofteneasiertodemonstrateataskthanspecifyingawell-shapedrewardfunction[38].Westartbycollectingasetofdemonstrationsforeachtask,whereweaddnoisetotheactionsinordertohavewidercoverageinthetrajectoryspace.Ineachtrainingiteration,wesamplealistoftasks(withreplacement).Foreachsampledtask,wesampleademonstrationaswellasasmallbatchofobservation-actionpairs.Thepolicyistrainedtoregressagainstthedesiredactionswhenconditionedonthecurrentobservationandthedemonstration,byminimizingan`2orcross-entropylossbasedonwhetheractionsarecontinuousordiscrete.Ahigh-levelillustrationofthetrainingprocedureisgiveninFig.1(c).Acrossallexperiments,weuseAdamax[25]toperformtheoptimizationwithalearningrateof0.001.4ArchitectureWhile,inprinciple,agenericneuralnetworkcouldlearnthemappingfromdemonstrationandcurrentobservationtoappropriateaction,wefounditimportanttouseanappropriatearchitecture.Ourarchitectureforlearningblockstackingisoneofthemaincontributionsofthispaper,andwebelieveitisrepresentativeofwhatarchitecturesforone-shotimitationlearningcouldlooklikeinthefuturewhenconsideringmorecomplextasks.Ourproposedarchitectureconsistsofthreemodules:thedemonstrationnetwork,thecontextnetwork,andthemanipulationnetwork.AnillustrationofthearchitectureisshowninFig.2.Wewilldescribethemainoperationsperformedineachmodulebelow,andafullspeciﬁcationisavailableintheAppendix.4.1DemonstrationNetworkThedemonstrationnetworkreceivesademonstrationtrajectoryasinput,andproducesanembeddingofthedemonstrationtobeusedbythepolicy.Thesizeofthisembeddinggrowslinearlyasafunctionofthelengthofthedemonstrationaswellasthenumberofblocksintheenvironment.TemporalDropout:Forblockstacking,thedemonstrationscanspanhundredstothousandsoftimesteps,andtrainingwithsuchlongsequencescanbedemandinginbothtimeandmemoryusage.Hence,werandomlydiscardasubsetoftimestepsduringtraining,anoperationwecalltemporaldropout,analogousto[53,27].Wedenotepastheproportionoftimestepsthatarethrownaway.4Hidden layersHidden layersTemporal DropoutNeighborhood Attention+Temporal ConvolutionAttention overDemonstrationDemonstrationCurrent StateActionABlock#BCDEFGHIJAttention overCurrent StateContext NetworkDemonstration NetworkManipulation NetworkContext EmbeddingFigure2:Illustrationofthenetworkarchitecture.Inourexperiments,weusep=0.95,whichreducesthelengthofdemonstrationsbyafactorof20.Duringtesttime,wecansamplemultipledownsampledtrajectories,useeachofthemtocomputedownstreamresults,andaveragetheseresultstoproduceanensembleestimate.Inourexperience,thisconsistentlyimprovestheperformanceofthepolicy.NeighborhoodAttention:Afterdownsamplingthedemonstration,weapplyasequenceofopera-tions,composedofdilatedtemporalconvolution[65]andneighborhoodattention.Wenowdescribethissecondoperationinmoredetail.Sinceourneuralnetworkneedstohandledemonstrationswithvariablenumbersofblocks,itmusthavemodulesthatcanprocessvariable-dimensionalinputs.Softattentionisanaturaloperationwhichmapsvariable-dimensionalinputstoﬁxed-dimensionaloutputs.However,bydoingso,itmayloseinformationcomparedtoitsinput.Thisisundesirable,sincetheamountofinformationcontainedinademonstrationgrowsasthenumberofblocksincreases.Therefore,weneedanoperationthatcanmapvariable-dimensionalinputstooutputswithcomparabledimensions.Intuitively,ratherthanhavingasingleoutputasaresultofattendingtoallinputs,wehaveasmanyoutputsasinputs,andhaveeachoutputattendingtoallotherinputsinrelationtoitsowncorrespondinginput.Westartbydescribingthesoftattentionmoduleasspeciﬁedin[6].Theinputtotheattentionincludesaqueryq,alistofcontextvectors{cj},andalistofmemoryvectors{mj}.Theithattentionweightisgivenbywi vTtanh(q+ci),wherevisalearnedweightvector.Theoutputofattentionisaweightedcombinationofthememorycontent,wheretheweightsaregivenbyasoftmaxoperationovertheattentionweights.Formally,wehaveoutput Pimiexp(wi)Pjexp(wj).Notethattheoutputhasthesamedimensionasamemoryvector.Theattentionoperationcanbegeneralizedtomultiplequeryheads,inwhichcasetherewillbeasmanyoutputvectorsastherearequeries.Nowweturntoneighborhoodattention.WeassumethereareBblocksintheenvironment.Wedenotetherobot’sstateassrobot,andthecoordinatesofeachblockas(x1,y1,z1),...,(xB,yB,zB).Theinputtoneighborhoodattentionisalistofembeddingshin1,...,hinBofthesamedimension,whichcanbetheresultofaprojectionoperationoveralistofblockpositions,ortheoutputofapreviousneighborhoodattentionoperation.Giventhislistofembeddings,weusetwoseparatelinearlayerstocomputeaqueryvectorandacontextembeddingforeachblock:qi Linear(hini),andci Linear(hini).Thememorycontenttobeextractedconsistsofthecoordinatesofeachblock,concatenatedwiththeinputembedding.Theithqueryresultisgivenbythefollowingsoftattentionoperation:resulti SoftAttn(query:qi,context:{cj}Bj=1,memory:{((xj,yj,zj),hinj))}Bj=1).Intuitively,thisoperationallowseachblocktoqueryotherblocksinrelationtoitself(e.g.ﬁndtheclosestblock),andextractthequeriedinformation.Thegatheredresultsarethencombinedwitheachblock’sowninformation,toproducetheoutputembeddingperblock.Concretely,wehave5outputi Linear(concat(hini,resulti,(xi,yi,zi),srobot)).Inpractice,weusemultiplequeryheadsperblock,sothatthesizeofeachresultiwillbeproportionaltothenumberofqueryheads.4.2ContextnetworkThecontextnetworkisthecruxofourmodel.Itprocessesboththecurrentstateandtheembeddingproducedbythedemonstrationnetwork,andoutputsacontextembedding,whosedimensiondoesnotdependonthelengthofthedemonstration,orthenumberofblocksintheenvironment.Hence,itisforcedtocaptureonlytherelevantinformation,whichwillbeusedbythemanipulationnetwork.Attentionoverdemonstration:Thecontextnetworkstartsbycomputingaqueryvectorasafunctionofthecurrentstate,whichisthenusedtoattendoverthedifferenttimestepsinthedemonstrationembedding.Theattentionweightsoverdifferentblockswithinthesametimesteparesummedtogether,toproduceasingleweightpertimestep.Theresultofthistemporalattentionisavectorwhosesizeisproportionaltothenumberofblocksintheenvironment.Wethenapplyneighborhoodattentiontopropagatetheinformationacrosstheembeddingsofeachblock.Thisprocessisrepeatedmultipletimes,wherethestateisadvancedusinganLSTMcellwithuntiedweights.Attentionovercurrentstate:Thepreviousoperationsproduceanembeddingwhosesizeisinde-pendentofthelengthofthedemonstration,butstilldependentonthenumberofblocks.Wethenapplystandardsoftattentionoverthecurrentstatetoproduceﬁxed-dimensionalvectors,wherethememorycontentonlyconsistsofpositionsofeachblock,which,togetherwiththerobot’sstate,formsthecontextembedding,whichisthenpassedtothemanipulationnetwork.Intuitively,althoughthenumberofobjectsintheenvironmentmayvary,ateachstageofthemanipulationoperation,thenumberofrelevantobjectsissmallandusuallyﬁxed.Fortheblockstackingenvironmentspeciﬁcally,therobotshouldonlyneedtopayattentiontothepositionoftheblockitistryingtopickup(thesourceblock),aswellasthepositionoftheblockitistryingtoplaceontopof(thetargetblock).Therefore,aproperlytrainednetworkcanlearntomatchthecurrentstatewiththecorrespondingstageinthedemonstration,andinfertheidentitiesofthesourceandtargetblocksexpressedassoftattentionweightsoverdifferentblocks,whicharethenusedtoextractthecorrespondingpositionstobepassedtothemanipulationnetwork.Althoughwedonotenforcethisinterpretationintraining,ourexperimentanalysissupportsthisinterpretationofhowthelearnedpolicyworksinternally.4.3ManipulationnetworkThemanipulationnetworkisthesimplestcomponent.Afterextractingtheinformationofthesourceandtargetblocks,itcomputestheactionneededtocompletethecurrentstageofstackingoneblockontopofanotherone,usingasimpleMLPnetwork.1Thisdivisionoflaboropensupthepossibilityofmodulartraining:themanipulationnetworkmaybetrainedtocompletethissimpleprocedure,withoutknowingaboutdemonstrationsormorethantwoblockspresentintheenvironment.Weleavethispossibilityforfuturework.5ExperimentsWeconductexperimentswiththeblockstackingtasksdescribedinSection3.2.2Theseexperimentsaredesignedtoanswerthefollowingquestions:•HowdoestrainingwithbehavioralcloningcomparewithDAGGER?•Howdoesconditioningontheentiredemonstrationcomparetoconditioningontheﬁnalstate,evenwhenitalreadyhasenoughinformationtofullyspecifythetask?•Howdoesconditioningontheentiredemonstrationcomparetoconditioningona“snapshot”ofthetrajectory,whichisasmallsubsetofframesthataremostinformative?1Inprinciple,onecanreplacethismodulewithanRNNmodule.Butwedidnotﬁndthisnecessaryforthetasksweconsider.2AdditionalexperimentresultsareavailableintheAppendix,includingasimpleillustrativeexampleofparticlereachingtasksandfurtheranalysisofblockstacking6•Canourframeworkgeneralizetotasksthatithasneverseenduringtraining?Toanswerthesequestions,wecomparetheperformanceofthefollowingarchitectures:•BC:Weusethesamearchitectureasprevious,butandthepolicyusingbehavioralcloning.•DAGGER:Weusethearchitecturedescribedintheprevioussection,andtrainthepolicyusingDAGGER.•Finalstate:Thisarchitectureconditionsontheﬁnalstateratherthanontheentiredemon-strationtrajectory.Fortheblockstackingtaskfamily,theﬁnalstateuniquelyidentiﬁesthetask,andthereisnoneedforadditionalinformation.However,afulltrajectory,onewhichcontainsinformationaboutintermediatestagesofthetask’ssolution,canmakeiteasiertotraintheoptimalpolicy,becauseitcouldlearntorelyonthedemonstrationdirectly,withoutneedingtomemorizetheintermediatestepsintoitsparameters.Thisisrelatedtothewayinwhichrewardshapingcansigniﬁcantlyaffectperformanceinreinforcementlearning[38].Acomparisonbetweenthetwoconditioningstrategieswilltelluswhetherthishypothesisisvalid.WetrainthispolicyusingDAGGER.•Snapshot:Thisarchitectureconditionsona“snapshot”ofthetrajectory,whichincludesthelastframeofeachstagealongthedemonstrationtrajectory.Thisassumesthatasegmentationofthedemonstrationintomultiplestagesisavailableattesttime,whichgivesitanunfairadvantagecomparedtotheotherconditioningstrategies.Hence,itmayperformbetterthanconditioningonthefulltrajectory,andservesasareference,toinformuswhetherthepolicyconditionedontheentiretrajectorycanperformaswellasifthedemonstrationisclearlysegmented.Again,wetrainthispolicyusingDAGGER.Weevaluatethepolicyontasksseenduringtraining,aswellastasksunseenduringtraining.Notethatgeneralizationisevaluatedatmultiplelevels:thelearnedpolicynotonlyneedstogeneralizetonewconﬁgurationsandnewdemonstrationsoftasksseenalready,butalsoneedstogeneralizetonewtasks.Concretely,wecollect140trainingtasks,and43testtasks,eachwithadifferentdesiredlayoutoftheblocks.Thenumberofblocksineachtaskcanvarybetween2and10.Wecollect1000trajectoriespertaskfortraining,andmaintainaseparatesetoftrajectoriesandinitialconﬁgurationstobeusedforevaluation.Thetrajectoriesarecollectedusingahard-codedpolicy.5.1PerformanceEvaluation1234567NumberofStages0%20%40%60%80%100%AverageSuccessRatePolicyTypeDemoBCDAGGERSnapshotFinalstate(a)Performanceontrainingtasks.245678NumberofStages0%20%40%60%80%100%AverageSuccessRatePolicyTypeDemoBCDAGGERSnapshotFinalstate(b)Performanceontesttasks.Figure3:Comparisonofdifferentconditioningstrategies.Thedarkestbarshowstheperformanceofthehard-codedpolicy,whichunsurprisinglyperformsthebestmostofthetime.Forarchitecturesthatusetemporaldropout,weuseanensembleof10differentdownsampleddemonstrationsandaveragetheactiondistributions.Thenforallarchitecturesweusethegreedyactionforevaluation.Fig.3showstheperformanceofvariousarchitectures.Resultsfortrainingandtesttasksarepresentedseparately,wherewegrouptasksbythenumberofstagesrequiredtocompletethem.Thisisbecausetasksthatrequiremorestagestocompletearetypicallymorechallenging.Infact,evenourscriptedpolicyfrequentlyfailsonthehardesttasks.Wemeasuresuccessratepertaskbyexecutingthegreedypolicy(takingthemostconﬁdentactionateverytimestep)in100differentconﬁgurations,eachconditionedonadifferentdemonstrationunseenduringtraining.Wereporttheaveragesuccessrateoveralltaskswithinthesamegroup.7Fromtheﬁgure,wecanobservethatfortheeasiertaskswithfewerstages,allofthedifferentconditioningstrategiesperformequallywellandalmostperfectly.Asthedifﬁculty(numberofstages)increases,however,conditioningontheentiredemonstrationstartstooutperformconditioningontheﬁnalstate.Onepossibleexplanationisthatwhenconditionedonlyontheﬁnalstate,thepolicymaystruggleaboutwhichblockitshouldstackﬁrst,apieceofinformationthatisreadilyaccessiblefromdemonstration,whichnotonlycommunicatesthetask,butalsoprovidesvaluableinformationtohelpaccomplishit.Moresurprisingly,conditioningontheentiredemonstrationalsoseemstooutperformconditioningonthesnapshot,whichweoriginallyexpectedtoperformthebest.Wesuspectthatthisisduetotheregularizationeffectintroducedbytemporaldropout,whicheffectivelyaugmentsthesetofdemonstrationsseenbythepolicyduringtraining.AnotherinterestingﬁndingwasthattrainingwithbehavioralcloninghasthesamelevelofperformanceastrainingwithDAGGER,whichsuggeststhattheentiretrainingprocedurecouldworkwithoutrequiringinteractivesupervision.Inourpreliminaryexperiments,wefoundthatinjectingnoiseintothetrajectorycollectionprocesswasimportantforbehavioralcloningtoworkwell,henceinallexperimentsreportedhereweusenoiseinjection.Inpractice,suchnoisecancomefromnaturalhuman-inducednoisethroughtele-operation,orbyartiﬁciallyinjectingadditionalnoisebeforeapplyingitonthephysicalrobot.5.2VisualizationWevisualizetheattentionmechanismsunderlyingthemainpolicyarchitecturetohaveabetterunderstandingabouthowitoperates.Therearetwokindsofattentionwearemainlyinterestedin,onewherethepolicyattendstodifferenttimestepsinthedemonstration,andtheotherwherethepolicyattendstodifferentblocksinthecurrentstate.Fig.4showssomeoftheattentionheatmaps.(a)Attentionoverblocksinthecurrentstate.(b)Attentionoverdownsampleddemonstration.Figure4:Visualizingattentionsperformedbythepolicyduringanentireexecution.Thetaskbeingperformedisabcdefghij.Notethatthepolicyhasmultiplequeryheadsforeachtypeofattention,andonlyonequeryheadpertypeisvisualized.(a)Wecanobservethatthepolicyalmostalwaysfocusesonasmallsubsetoftheblockpositionsinthecurrentstate,whichallowsthemanipulationnetworktogeneralizetooperationsoverdifferentblocks.(b)Wecanobserveasparsepatternoftimestepsthathavehighattentionweights.Thissuggeststhatthepolicyhasessentiallylearnedtosegmentthedemonstrations,andonlyattendtoimportantkeyframes.Notethatthereareroughly6regionsofhighattentionweights,whichnicelycorrespondstothe6stagesrequiredtocompletethetask.6ConclusionsInthiswork,wepresentedasimplemodelthatmapsasinglesuccessfuldemonstrationofatasktoaneffectivepolicythatsolvessaidtaskinanewsituation.Wedemonstratedeffectivenessofthisapproachonafamilyofblockstackingtasks.Therearealotofexcitingdirectionsforfuturework.Weplantoextendtheframeworktodemonstrationsintheformofimagedata,whichwillallowmoreend-to-endlearningwithoutrequiringaseparateperceptionmodule.Wearealsointerestedinenablingthepolicytoconditiononmultipledemonstrations,incasewhereonedemonstrationdoesnotfullyresolveambiguityintheobjective.Furthermoreandmostimportantly,wehopetoscaleup8ourmethodonamuchlargerandbroaderdistributionoftasks,andexploreitspotentialtowardsageneralroboticsimitationlearningsystemthatwouldbeabletoachieveanoverwhelmingvarietyoftasks.7AcknowledgementWewouldliketothankourcolleaguesatUCBerkeleyandOpenAIforinsightfuldiscussions.ThisresearchwasfundedinpartbyONRthroughaPECASEaward.YanDuanwasalsosupportedbyaHuaweiFellowship.JonathanHowasalsosupportedbyanNSFFellowship.References[1]PieterAbbeelandAndrewNg.Apprenticeshiplearningviainversereinforcementlearning.InInternationalConferenceonMachineLearning(ICML),2004.[2]MarcinAndrychowicz,MishaDenil,SergioGomez,MatthewWHoffman,DavidPfau,TomSchaul,andNandodeFreitas.Learningtolearnbygradientdescentbygradientdescent.InNeuralInformationProcessingSystems(NIPS),2016.[3]BrennaDArgall,SoniaChernova,ManuelaVeloso,andBrettBrowning.Asurveyofrobotlearningfromdemonstration.Roboticsandautonomoussystems,57(5):469–483,2009.[4]YusufAytarandAndrewZisserman.Tabularasa:Modeltransferforobjectcategorydetection.In2011InternationalConferenceonComputerVision,pages2252–2259.IEEE,2011.[5]JimmyBa,GeoffreyEHinton,VolodymyrMnih,JoelZLeibo,andCatalinIonescu.Usingfastweightstoattendtotherecentpast.InNeuralInformationProcessingSystems(NIPS),2016.[6]DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.arXivpreprintarXiv:1409.0473,2014.[7]PeterBattaglia,RazvanPascanu,MatthewLai,DaniloJimenezRezende,etal.Interactionnetworksforlearningaboutobjects,relationsandphysics.InAdvancesinNeuralInformationProcessingSystems,pages4502–4510,2016.[8]SamyBengio,YoshuaBengio,JocelynCloutier,andJanGecsei.Ontheoptimizationofasynapticlearningrule.InOptimalityinArtiﬁcialandBiologicalNeuralNetworks,pages6–8,1992.[9]YoshuaBengio,SamyBengio,andJocelynCloutier.Learningasynapticlearningrule.UniversitédeMontréal,Départementd’informatiqueetderechercheopérationnelle,1990.[10]DimitriPBertsekasandJohnNTsitsiklis.Neuro-dynamicprogramming:anoverview.InDecisionandControl,1995.,Proceedingsofthe34thIEEEConferenceon,volume1,pages560–564.IEEE,1995.[11]SylvainCalinon.Robotprogrammingbydemonstration.EPFLPress,2009.[12]MichaelBChang,TomerUllman,AntonioTorralba,andJoshuaBTenenbaum.Acompositionalobject-basedapproachtolearningphysicaldynamics.InInt.Conf.onLearningRepresentations(ICLR),2017.[13]KyunghyunCho,BartVanMerriënboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,andYoshuaBengio.Learningphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation.arXivpreprintarXiv:1406.1078,2014.[14]JeffDonahue,YangqingJia,OriolVinyals,JudyHoffman,NingZhang,EricTzeng,andTrevorDarrell.Decaf:Adeepconvolutionalactivationfeatureforgenericvisualrecognition.InICML,pages647–655,2014.[15]LixinDuan,DongXu,andIvorTsang.Learningwithaugmentedfeaturesforheterogeneousdomainadaptation.arXivpreprintarXiv:1206.4660,2012.9[16]YanDuan,JohnSchulman,XiChen,PeterLBartlett,IlyaSutskever,andPieterAbbeel.Rl2:Fastreinforcementlearningviaslowreinforcementlearning.arXivpreprintarXiv:1611.02779,2016.[17]HarrisonEdwardsandAmosStorkey.Towardsaneuralstatistician.InternationalConferenceonLearningRepresentations(ICLR),2017.[18]ChelseaFinn,SergeyLevine,andPieterAbbeel.Guidedcostlearning:Deepinverseoptimalcontrolviapolicyoptimization.InProceedingsofthe33rdInternationalConferenceonMachineLearning,volume48,2016.[19]ChelseaFinn,PieterAbbeel,andSergeyLevine.Model-agnosticmeta-learningforfastadapta-tionofdeepnetworks.arXivpreprintarXiv:1703.03400,2017.[20]AbhishekGupta,ColineDevin,YuXuanLiu,PieterAbbeel,andSergeyLevine.Learninginvariantfeaturespacestotransferskillswithreinforcementlearning.InInt.Conf.onLearningRepresentations(ICLR),2017.[21]NicolasHeess,GregoryWayne,DavidSilver,TimLillicrap,TomErez,andYuvalTassa.Learningcontinuouscontrolpoliciesbystochasticvaluegradients.InAdvancesinNeuralInformationProcessingSystems,pages2944–2952,2015.[22]JonathanHoandStefanoErmon.Generativeadversarialimitationlearning.InAdvancesinNeuralInformationProcessingSystems,pages4565–4573,2016.[23]SeppHochreiter,AStevenYounger,andPeterRConwell.Learningtolearnusinggradientdescent.InInternationalConferenceonArtiﬁcialNeuralNetworks.Springer,2001.[24]JudyHoffman,ErikRodner,JeffDonahue,TrevorDarrell,andKateSaenko.Efﬁcientlearningofdomain-invariantimagerepresentations.arXivpreprintarXiv:1301.3224,2013.[25]DiederikP.KingmaandJimmyBa.Adam:Amethodforstochasticoptimization.InProceedingsofthe3rdInternationalConferenceonLearningRepresentations(ICLR),2014.[26]GregoryKoch.Siameseneuralnetworksforone-shotimagerecognition.ICMLDeepLearningWorkshop,2015.[27]DavidKrueger,TeganMaharaj,JánosKramár,MohammadPezeshki,NicolasBallas,NanRose-maryKe,AnirudhGoyal,YoshuaBengio,HugoLarochelle,AaronCourville,etal.Zoneout:Regularizingrnnsbyrandomlypreservinghiddenactivations.arXivpreprintarXiv:1606.01305,2016.[28]BrianKulis,KateSaenko,andTrevorDarrell.Whatyousawisnotwhatyouget:Domainadaptationusingasymmetrickerneltransforms.InComputerVisionandPatternRecognition(CVPR),2011IEEEConferenceon,pages1785–1792.IEEE,2011.[29]S.Levine,Z.Popovic,andV.Koltun.Nonlinearinversereinforcementlearningwithgaussianprocesses.InAdvancesinNeuralInformationProcessingSystems(NIPS),2011.[30]SergeyLevine,ChelseaFinn,TrevorDarrell,andPieterAbbeel.End-to-endtrainingofdeepvisuomotorpolicies.JournalofMachineLearningResearch,17(39):1–40,2016.[31]KeLiandJitendraMalik.Learningtooptimize.arXivpreprintarXiv:1606.01885,2016.[32]TimothyPLillicrap,JonathanJHunt,AlexanderPritzel,NicolasHeess,TomErez,YuvalTassa,DavidSilver,andDaanWierstra.Continuouscontrolwithdeepreinforcementlearning.arXivpreprintarXiv:1509.02971,2015.[33]MingshengLongandJianminWang.Learningtransferablefeatureswithdeepadaptationnetworks.CoRR,abs/1502.02791,1:2,2015.[34]YishayMansour,MehryarMohri,andAfshinRostamizadeh.Domainadaptation:Learningboundsandalgorithms.arXivpreprintarXiv:0902.3430,2009.10[35]VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533,2015.[36]DevangKNaikandRJMammone.Meta-neuralnetworksthatlearnbylearning.InInternationalJointConferenceonNeuralNetowrks(IJCNN),1992.[37]AndrewNgandStuartRussell.Algorithmsforinversereinforcementlearning.InInternationalConferenceonMachineLearning(ICML),2000.[38]AndrewYNg,DaishiHarada,andStuartRussell.Policyinvarianceunderrewardtransfor-mations:Theoryandapplicationtorewardshaping.InICML,volume99,pages278–287,1999.[39]AndrewYNg,HJinKim,MichaelIJordan,ShankarSastry,andShivBallianda.Autonomoushelicopterﬂightviareinforcementlearning.InNIPS,volume16,2003.[40]JanPetersandStefanSchaal.Reinforcementlearningofmotorskillswithpolicygradients.Neuralnetworks,21(4):682–697,2008.[41]DeanAPomerleau.Alvinn:Anautonomouslandvehicleinaneuralnetwork.InAdvancesinNeuralInformationProcessingSystems,pages305–313,1989.[42]SachinRaviandHugoLarochelle.Optimizationasamodelforfew-shotlearning.InUnderReview,ICLR,2017.[43]DaniloJimenezRezende,ShakirMohamed,IvoDanihelka,KarolGregor,andDaanWierstra.One-shotgeneralizationindeepgenerativemodels.InternationalConferenceonMachineLearning(ICML),2016.[44]StéphaneRoss,GeoffreyJGordon,andDrewBagnell.Areductionofimitationlearningandstructuredpredictiontono-regretonlinelearning.InAISTATS,volume1,page6,2011.[45]AndreiARusu,NeilCRabinowitz,GuillaumeDesjardins,HubertSoyer,JamesKirkpatrick,KorayKavukcuoglu,RazvanPascanu,andRaiaHadsell.Progressiveneuralnetworks.arXivpreprintarXiv:1606.04671,2016.[46]FereshtehSadeghiandSergeyLevine.(cad)2rl:Realsingle-imageﬂightwithoutasinglerealimage.2016.[47]AdamSantoro,SergeyBartunov,MatthewBotvinick,DaanWierstra,andTimothyLillicrap.Meta-learningwithmemory-augmentedneuralnetworks.InInternationalConferenceonMachineLearning(ICML),2016.[48]StefanSchaal.Isimitationlearningtheroutetohumanoidrobots?Trendsincognitivesciences,3(6):233–242,1999.[49]JurgenSchmidhuber.Evolutionaryprinciplesinself-referentiallearning.Onlearninghowtolearn:Themeta-meta-...hook.)Diplomathesis,Institutf.Informatik,Tech.Univ.Munich,1987.[50]JürgenSchmidhuber.Learningtocontrolfast-weightmemories:Analternativetodynamicrecurrentnetworks.NeuralComputation,1992.[51]JohnSchulman,SergeyLevine,PieterAbbeel,MichaelIJordan,andPhilippMoritz.Trustregionpolicyoptimization.InICML,pages1889–1897,2015.[52]DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-che,JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal.Mas-teringthegameofgowithdeepneuralnetworksandtreesearch.Nature,529(7587):484–489,2016.[53]NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-nov.Dropout:asimplewaytopreventneuralnetworksfromoverﬁtting.JournalofMachineLearningResearch,15(1):1929–1958,2014.11[54]BradlieStadie,PieterAbbeel,andIlyaSutskever.Thirdpersonimitationlearning.InInt.Conf.onLearningRepresentations(ICLR),2017.[55]IlyaSutskever,OriolVinyals,andQuocVLe.Sequencetosequencelearningwithneuralnetworks.InAdvancesinneuralinformationprocessingsystems,pages3104–3112,2014.[56]RichardSSuttonandAndrewGBarto.Reinforcementlearning:Anintroduction,volume1.MITpressCambridge,1998.[57]GeraldTesauro.Temporaldifferencelearningandtd-gammon.CommunicationsoftheACM,38(3):58–68,1995.[58]SebastianThrunandLorienPratt.Learningtolearn.SpringerScience&BusinessMedia,1998.[59]EricTzeng,JudyHoffman,NingZhang,KateSaenko,andTrevorDarrell.Deepdomainconfusion:Maximizingfordomaininvariance.arXivpreprintarXiv:1412.3474,2014.[60]EricTzeng,ColineDevin,JudyHoffman,ChelseaFinn,XingchaoPeng,PieterAbbeel,SergeyLevine,KateSaenko,andTrevorDarrell.Towardsadaptingdeepvisuomotorrepresentationsfromsimulatedtorealenvironments.arXivpreprintarXiv:1511.07111,2015.[61]OriolVinyals,CharlesBlundell,TimLillicrap,DaanWierstra,etal.Matchingnetworksforoneshotlearning.InNeuralInformationProcessingSystems(NIPS),2016.[62]JaneXWang,ZebKurth-Nelson,DhruvaTirumala,HubertSoyer,JoelZLeibo,RemiMunos,CharlesBlundell,DharshanKumaran,andMattBotvinick.Learningtoreinforcementlearn.arXivpreprintarXiv:1611.05763,2016.[63]KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCCourville,RuslanSalakhutdinov,RichardSZemel,andYoshuaBengio.Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.InICML,volume14,pages77–81,2015.[64]JunYang,RongYan,andAlexanderGHauptmann.Cross-domainvideoconceptdetectionusingadaptivesvms.InProceedingsofthe15thACMinternationalconferenceonMultimedia,pages188–197.ACM,2007.[65]FisherYuandVladlenKoltun.Multi-scalecontextaggregationbydilatedconvolutions.InInternationalConferenceonLearningRepresentations(ICLR),2016.[66]B.Ziebart,A.Maas,J.A.Bagnell,andA.K.Dey.Maximumentropyinversereinforcementlearning.InAAAIConferenceonArtiﬁcialIntelligence,2008.12AIllustrativeExample:ParticleReachingTheparticlereachingproblemisaverysimplefamilyoftasks.Ineachtask,wecontrolapointrobottoreachaspeciﬁclandmark,anddifferenttasksareidentiﬁedbydifferentlandmarks.AsillustratedinFig.1,onetaskcouldbetoreachtheorangesquare,andanothertaskcouldbetoreachthegreentriangle.Theagentreceivesitsown2Dlocation,aswellasthe2Dlocationsofeachofthelandmarks.Withineachtask,theinitialpositionoftheagent,aswellasthepositionsofallthelandmarks,canvaryacrossdifferentinstancesofthetask.Withoutademonstration,therobotdoesnotknowwhichlandmarkitshouldreach,andwillnotbeabletoaccomplishthetask.Hence,thissettingalreadygetsattheessenceofone-shotimitation,namelytocommunicatethetaskviaademonstration.Afterlearning,theagentshouldbeabletoidentifythetargetlandmarkfromthedemonstration,andreachthesamelandmarkinanewinstanceofthetask.Figure1:Therobotisapointmasscontrolledwith2-dimensionalforce.Thefamilyoftasksistoreachatargetlandmark.Theidentityofthelandmarkdiffersfromtasktotask,andthemodelhastoﬁgureoutwhichtargettopursuebasedonthedemonstration.(left)illustrationoftherobot;(middle)thetaskistoreachtheorangebox,(right)thetaskistoreachthegreentriangle.Weconsiderthreearchitecturesforthisproblem:•PlainLSTM:TheﬁrstarchitectureisasimpleLSTMwith512hiddenunits.Itreadsthedemonstrationtrajectory,theoutputofwhichisthenconcatenatedwiththecurrentstate,andfedtoamulti-layerperceptron(MLP)toproducetheaction.•LSTMwithattention:Inthisarchitecture,theLSTMoutputsaweightingoverthedif-ferentlandmarksfromthedemonstrationsequence.Then,itappliesthisweightinginthetestscene,andproducesaweightedcombinationoverlandmarkpositionsgiventhecurrentstate.This2Doutputisthenconcatenatedwiththecurrentagentposition,andfedtoanMLPtoproducetheaction.•Finalstatewithattention:Ratherthanlookingattheentiredemonstrationtrajectory,thisarchitectureonlylooksattheﬁnalstateinthedemonstration(whichisalreadysufﬁcienttocommunicatethetask),andproduceaweightingoverlandmarks.Itthenproceedslikethepreviousarchitecture.Noticethatthesethreearchitecturesareincreasinglymorespecializedtothespeciﬁcparticlereach-ingsetting,whichsuggestsapotentialtrade-offbetweenexpressivenessandgeneralizability.TheexperimentresultsareshowninFig.2.Weobservethatasthearchitecturebecomesmorespecialized,weachievemuchbettergeneralizationperformance.Forthissimpletask,itappearsthatconditioningontheentiredemonstrationhurtsgeneralizationperformance,andconditioningonjusttheﬁnalstateperformsthebestevenwithoutexplicitregularization.Thismakesintuitivesense,sincetheﬁnalstatealreadysufﬁcientlycharacterizesthetaskathand.However,thesameconclusiondoesnotappeartoholdasthetaskbecomesmorecomplicated,asshownbytheblockstackingtasksinthemaintext.Fig.3showsthelearningcurvesforthethreearchitecturesdesignedfortheparticlereachingtasks,asthenumberoflandmarksisvaried,byrunningthepoliciesover100differentconﬁgurations,andcomputingsuccessratesoverbothtrainingandtestdata.WecanclearlyobservethatbothLSTM-basedarchitecturesexhibitoverﬁttingasthenumberoflandmarksincreases.Ontheotherhand,usingattentionclearlyimprovesgeneralizationperformance,andwhenconditioningononlytheﬁnalstate,itachievesperfectgeneralizationinallscenarios.Itisalsointerestingtoobservethat12345678910Numberoflandmarks0%20%40%60%80%100%SuccessratePlainLSTM(Train)PlainLSTM(Test)LSTMwithattention(Train)LSTMwithattention(Test)Finalstatewithattention(Train)Finalstatewithattention(Test)Figure2:Successratesofdifferentarchitecturesforparticlereaching.The“Train”curvesshowthesuccessrateswhenconditionedondemonstrationsseenduringtraining,andrunningthepolicyoninitialconditionsseenduringtraining,whilethe“Test”curvesshowthesuccessrateswhenconditionedonnewtrajectoriesandoperatinginnewsituations.Bothattention-basedarchitecturesachieveperfecttrainingsuccessrates,andthecurvesareoverlapped.learningundergoesaphasetransition.Intuitively,thismaybewhenthenetworkislearningtoinferthetaskfromthedemonstration.Oncethisisﬁnished,thelearningofcontrolpolicyisalmosttrivial.Table1andTable2showtheexactperformancenumbersforreference.#LandmarksPlainLSTMLSTMwithattentionFinalstatewithattention2100.0%100.0%100.0%3100.0%100.0%100.0%4100.0%100.0%100.0%5100.0%100.0%100.0%699.0%100.0%100.0%7100.0%100.0%100.0%8100.0%100.0%100.0%9100.0%100.0%100.0%1091.9%100.0%100.0%Table1:Successratesofparticlereachingconditionedonseendemonstrations,andrunningonseeninitialconﬁgurations.BFurtherDetailsonBlockStackingB.1FullDescriptionofArchitectureWenowspecifythearchitectureinpseudocode.Weomitimplementationdetailswhichinvolvehan-dlingaminibatchofdemonstrationsandobservation-actionpairs,aswellasnecessarypaddingandmaskingtohandledataofdifferentdimensions.Weuseweightnormalizationwithdata-dependentinitializationSalimansandKingma[2016]foralldenseandconvolutionoperations.202004006008001000Epoch0%20%40%60%80%100%SuccessRateNumberofParticles2345678910(a)PlainLSTM(Train)02004006008001000Epoch0%20%40%60%80%100%SuccessRateNumberofParticles2345678910(b)PlainLSTM(Test)02004006008001000Epoch0%20%40%60%80%100%SuccessRateNumberofParticles2345678910(c)LSTMwithattention(Train)02004006008001000Epoch0%20%40%60%80%100%SuccessRateNumberofParticles2345678910(d)LSTMwithattention(Test)02004006008001000Epoch0%20%40%60%80%100%SuccessRateNumberofParticles2345678910(e)Finalstatewithattention(Train)02004006008001000Epoch0%20%40%60%80%100%SuccessRateNumberofParticles2345678910(f)Finalstatewithattention(Test)Figure3:Learningcurvesforparticlereachingtasks.Shownsuccessratesaremovingaveragesofpast10epochsforsmoothercurves.Eachpolicyistrainedforupto1000epochs,whichtakesuptoanhourusingaTitanXPascalGPU(ascanbeseenfromtheplot,mostexperimentscanbeﬁnishedsooner).B.1.1DemonstrationNetworkAssumethatthedemonstrationhasTtimestepsandwehaveBblocks.Ourarchitectureonlymakeuseoftheobservationsintheinputdemonstrationbutnottheactions.Eachobservationisa(3B+2)-dimensionalvector,containingthe(x,y,z)coordinatesofeachblockrelativetothecurrentpositionofthegripper,aswellasa2-dimensionalgripperstateindicatingwhetheritisopenorclosed.ThefullsequenceofoperationsisgiveninModule1.Weﬁrstapplytemporaldropoutasdescribedinthemaintext.Thenwesplittheobservationintoinformationabouttheblockandinformationabouttherobot,wheretheﬁrstdimensionistimeandtheseconddimensionistheblockID.Therobotstateisbroadcastedacrossdifferentblocks.Hencetheshapeofoutputsshouldbe˜T⇥B⇥3and˜T⇥B⇥2,respectively.Then,weperforma1⇥1convolutionovertheblockstatestoprojectthemtothesamedimensionastheper-blockembedding.Thenweperformasequenceofneighborhoodattentionoperationsand1⇥1convolutions,wheretheinputtotheconvolutionistheconcatenationoftheattentionresult,3Module1DemonstrationNetworkInput:Demonstrationd2RT⇥(3B+2)Hyperparameters:p=0.95,D=64Output:Demonstrationembedding2R˜T⇥B⇥D,where˜T=dT(1 p)eisthelengthofthedown-sampledtrajectory.d’ TemporalDropout(d,probability=p)blockstate,robotstate Split(d’)h Conv1D(blockstate,kernelsize=1,channels=D)fora2{1,2,4,8}do//Residualconnectionsh’ ReLU(h)attnresult NeighborhoodAttention(h’)h’ Concat({h’,blockstate,robotstate},axis=-1)h’ Conv1D(h’,kernelsize=2,channels=D,dilation=a)h’ ReLU(h’)h h+h’endfordemoembedding hthecurrentblockposition,andtherobot’sstate.Thisallowseachblocktoquerythestateofotherblocks,andreasonaboutthequeryresultincomparisonwithitsownstateandtherobot’sstate.Weuseresidualconnectionsduringthisprocedure.B.1.2ContextNetworkThepseudocodeisshowninModule2.Weperformaseriesofattentionoperationsoverthedemon-stration,followedbyattentionoverthecurrentstate,andweapplythemrepeatedlythroughanLSTMwithdifferentweightspertimestep(wefoundthistobeslightlyeasiertooptimize).Then,intheendweapplyaﬁnalattentionoperationwhichproducesaﬁxed-dimensionalembeddingin-dependentofthelengthofthedemonstrationorthenumberofblocksintheenvironment.B.1.3ManipulationNetworkGiventhecontextembedding,thismoduleissimplyamultilayerperceptron.PseudocodeisgiveninModule3.B.2EvaluatingPermutationInvarianceDuringtrainingandinthepreviousevaluations,weonlyselectonetaskperequivalenceclass,wheretwotasksareconsideredequivalentiftheyarethesameuptopermutingdifferentblocks.Thisisbasedontheassumptionthatourarchitectureisinvarianttopermutationsamongdifferentblocks.Forexample,ifthepolicyisonlytrainedonthetaskabcd,itshouldperformwellontaskdcba,givenasingledemonstrationofthetaskdcba.Wenowexperimentallyverifythispropertybyﬁxingatrainingtask,andevaluatingthepolicy’sperformanceunderallequivalentpermutationsofit.AsFig.4shows,althoughthepolicyhasonlyseenthetaskabcd,itachievesthesamelevelofperformanceonallotherequivalenttasks.B.3EffectofEnsemblingWenowevaluatetheimportanceofsamplingmultipledownsampleddemonstrationsduringevalu-ation.Fig.5showstheperformanceacrossalltrainingandtesttasks,asthenumberofensemblesvariesfrom1to20.Weobservethatmoreensembleshelpsthemostfortaskswithfewerstages.Ontheotherhand,itconsistentlyimprovesperformanceforthehardertasks,althoughthegapissmaller.Wesuspectthatthisisbecausethepolicyhaslearnedtoattendtoframesinthedemonstra-tiontrajectorywheretheblocksarealreadystackedtogether.Intaskswithonly1stage,forexample,itisveryeasyfortheseframestobedroppedinasingledownsampleddemonstration.Ontheotherhand,intaskswithmorestages,itbecomesmoreresilienttomissingframes.Usingmorethan104Module2ContextNetworkInput:Demonstrationembeddinghin2R˜T⇥B⇥D,currentstates2R3B+2Hyperparameters:D=64,tlstm=4,H=2Output:Contextembedding2R2+6H//Splitthecurrentstateintoblockstate2RB⇥3androbotstatebroadcastedtoallblocks2RB⇥2blockstate,robotstate SplitSingle(s)//InitializeLSTMoutput2RB⇥Dandstate(includinghiddenandcellstate)2RB⇥2Doutput,state InitLSTMState(size=B,hiddendim=D)fort=1totlstmdo//Temporalattention:everyblockattendtothesametimestepx outputift>1thenx ReLU(x)endif//Computingqueryforattentionoverdemonstration2RB⇥Dq Dense(x,outputdim=D)//Computeresultfromattention2RH⇥B⇥Dtemp SoftAttention(query=q,context=hin,memory=hin,numheads=H)//ReorganizeresultintoshapeB⇥(HD)temp Reshape(Transpose(temp,(1,0,2)),(B,H*D))//Spatialattention:eachblockattendtoadifferentblockseparatelyx outputift>1thenx ReLU(x)endifx Concat({x,temp},axis=-1)//Computingcontextforattentionovercurrentstate2RB⇥Dctx Dense(x,outputdim=D)//Computingqueryforattentionovercurrentstate2RB⇥Dq Dense(x,outputdim=D)//Computingmemoryforattentionovercurrentstate2RB⇥(HD+3)mem Concat({blockstate,temp},axis=-1)//Computeresultfromattention2RB⇥H⇥(HD+3)spatial SoftAttention(query=q,context=ctx,memory=mem,numheads=H)//ReorganizeresultintoshapeB⇥H(HD+3)spatial Reshape(spatial,(B,H*(H*D+3)))//ForminputtotheLSTMcell2RB⇥(H(HD+3)+HD+8)input Concat({robotstate,blockstate,spatial,temp},axis=-1)//RunonestepofanLSTMwithuntiedweights(meaningthatweusedifferentweightspertimestepoutput,state LSTMOneStep(input=input,state=state)endfor//Finalattentionoverthecurrentstate,compressinganO(B)representationdowntoO(1)//Computethequeryvector.Weuseaﬁxed,trainablequeryvectorindependentoftheinputdata,withsize2R2⇥D(weusetwoqueries,originallyintendedtohaveoneforthesourceblockandoneforthetargetblock)q GetFixedQuery()//Getattentionresult,whichshouldbeofshape2⇥H⇥3r SoftAttention(query=q,context=output,memory=blockstate,numheads=H)//Formtheﬁnalcontextembedding(wepicktheﬁrstrobotstatesincenoneedtobroadcasthere)contextembedding Concat({robotstate[0],Reshape(r,2*H*3)})5Module3ManipulationNetworkInput:Contextembeddinghin2R2+6HHyperparameters:H=2Output:Predictedactiondistribution2R|A|h ReLU(Dense(hin,outputdim=256))h ReLU(Dense(h,outputdim=256))actiondist Dense(h,outputdim=|A|)abcdabdcacbdacdbadbcadcbbacdbadcbcadbcdabdacbdcacabdcadbcbadcbdacdabcdbadabcdacbdbacdbcadcabdcbaTask0%20%40%60%80%AverageSuccessRateFigure4:Performanceofpolicyonasetoftasksequivalentuptopermutations.ensemblesappearstoprovidenosigniﬁcantimprovements,andhenceweused10ensemblesinourmainevaluation.12345678NumberofStages0%20%40%60%80%100%AverageSuccessRateNumberofEnsembles1251020Figure5:Performanceofvariousnumberofensembles.6B.4BreakdownofFailureCasesTounderstandthelimitationsofthecurrentapproach,weperformabreakdownanalysisofthefailurecases.Weconsiderthreefailurescenarios:“Wrongmove”meansthatthepolicyhasarrangedalayoutincompatiblewiththedesiredlayout.Thiscouldbebecausethepolicyhasmisinterpretedthedemonstration,orduetoanaccidentalbadmovethathappenstoscrambletheblocksintothewronglayout.“Manipulationfailure”meansthatthepolicyhasmadeanirrecoverablefailure,forexampleiftheblockisshakenoffthetable,whichthecurrenthard-codedpolicydoesnotknowhowtohandle.“Recoverablefailure”meansthatthepolicyrunsoutoftimebeforeﬁnishingthetask,whichmaybeduetoanaccidentalfailureduringtheoperationthatwouldhavebeenrecoverablegivenmoretime.AsshowninFig.6,conditioningononlytheﬁnalstatemakesmorewrongmovescomparedtootherarchitectures.Apartfromthat,mostofthefailurecasesareactuallyduetomanipulationfailuresthataremostlyirrecoverable.1Thissuggeststhatbettermanipulationskillsneedtobeacquiredtomakethelearnedone-shotpolicymorereliable.Figure6:Breakdownofthesuccessandfailurescenarios.Theareathateachcoloroccupiesrepresenttheratioofthecorrespondingscenario.B.5LearningCurvesFig.7showsthelearningcurvesfordifferentarchitecturesdesignedfortheblockstackingtasks.Theselearningcurvesdonotreﬂectﬁnalperformance:foreachevaluationpoint,wesampletasksanddemonstrationsfromtrainingdata,resettheenvironmenttothestartingpointofsomeparticularstage(sothatsomeblocksarealreadystacked),andonlyrunthepolicyforuptoonestage.IfthetrainingalgorithmisDAGGER,thesesampledtrajectoriesareannotatedandaddedtothetrainingset.Hencethisevaluationdoesnotevaluategeneralization.Wedidnotperformfullevaluationastrainingproceeds,becauseitisverytimeconsuming:eachevaluationrequirestensofthousandsof1Notethattheactualratioofmisinterpreteddemonstrationsmaybedifferent,sincetherunsthathavecausedamanipulationfailurecouldlaterleadtoawrongmove,wereitsuccessfullyexecuted.Ontheotherhand,byvisuallyinspectingthevideos,weobservedthatmostofthetrajectoriescategorizedas“WrongMove”areactuallyduetomanipulationfailures(exceptforpolicyconditioningontheﬁnalstate,whichdoesseemtooccasionallyexecuteanactualwrongmove).7trajectoriesacrossover>100tasks.However,theseﬁguresarestillusefultoreﬂectsomerelativetrend.Fromtheseﬁgures,wecanobservethatwhileconditioningonfulltrajectoriesgivesthebestper-formancewhichwasshowninthemaintext,itrequiresmuchlongertrainingtime,simplybecauseconditioningontheentiredemonstrationrequiresmorecomputation.Inaddition,thismayalsobeduetothehighvarianceofthetrainingprocessduetodownsamplingdemonstrations,aswellasthefactthatthenetworkneedstolearntoproperlysegmentthedemonstration.Itisalsointerestingthatconditioningonsnapshotsseemstolearnfasterthanconditioningonjusttheﬁnalstate,whichagainsuggeststhatconditioningonintermediateinformationishelpful,notonlyfortheﬁnalpolicy,butalsotofacilitatetraining.Wealsoobservethatlearninghappensmostrapidlyfortheinitialstages,andmuchslowerforthelaterstages,sincemanipulationbecomesmorechallenginginthelaterstages.Inaddition,therearefewertaskswithmorestages,andhencethelaterstagesarenotsampledasfrequentlyastheearlierstagesduringevaluation.B.6ExactPerformanceNumbersExactperformancenumbersarepresentedforreference:•Table3andTable4showthesuccessratesofdifferentarchitecturesontrainingandtesttasks,respectively;•Table5showsthesuccessratesacrossalltasksasthenumberofensemblesisvaried;•Table6showsthesuccessratesoftasksthatareequivalenttoabcduptopermutations;•Table7,Table8,Table9,Table10,andTable11showthebreakdownofdifferentsuccessandfailurescenariosforallconsideredarchitectures.B.7MoreVisualizationsFig.8andFig.9showthefullsetofheatmapsofattentionweights.Interestingly,inFig.8,weobservethatratherthanattendingtotwoblocksatatime,asweoriginallyexpected,thepolicyhaslearnedtomostlyattendtoonlyoneblockatatime.Thismakessensebecauseduringeachofthegraspingandtheplacingphaseofasinglestackingoperation,thepolicyneedstoonlypayattentiontothesingleblockthatthegrippershouldaimtowards.Forcontext,Fig.10andFig.11showkeyframesoftheneuralnetworkpolicyexecutingthetask.ReferencesTimSalimansandDiederikPKingma.Weightnormalization:Asimplereparameterizationtoaccel-eratetrainingofdeepneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pages901–901,2016.8#LandmarksPlainLSTMLSTMwithattentionFinalstatewithattention2100.0%100.0%100.0%3100.0%100.0%100.0%499.0%100.0%100.0%598.0%100.0%100.0%699.0%100.0%100.0%798.0%100.0%100.0%893.9%99.0%100.0%983.8%94.9%100.0%1050.5%85.9%100.0%Table2:Successratesofparticlereachingconditionedonunseendemonstrations,andrunningonunseeninitialconﬁgurations.#StagesDemoDAGGERBCSnapshotFinalstate199.1%99.1%99.1%97.2%98.8%295.6%94.3%93.7%92.6%86.7%388.5%88.0%86.9%86.7%84.8%478.6%78.2%76.7%76.4%71.9%567.3%65.9%65.4%62.5%60.6%655.7%51.5%52.4%47.0%43.6%742.8%34.3%37.5%31.4%31.5%Table3:Successratesofdifferentarchitecturesontrainingtasksofblockstacking.#StagesDemoDAGGERBCSnapshotFinalstate295.8%94.9%95.9%92.8%94.1%477.6%77.0%74.8%77.2%75.8%565.9%65.9%64.3%61.1%51.9%649.4%50.6%46.5%42.6%35.9%746.5%36.5%38.5%32.8%32.0%829.0%18.0%24.0%19.0%20.0%Table4:Successratesofdifferentarchitecturesontesttasksofblockstacking.#Stages1Ens.2Ens.5Ens.10Ens.20Ens.191.9%95.4%98.8%99.1%98.7%292.3%92.2%94.5%94.6%94.1%386.0%86.8%87.9%88.0%87.9%476.6%77.4%77.9%78.0%78.3%565.1%65.0%65.3%65.9%65.5%649.0%50.4%50.1%51.3%50.8%734.4%36.1%36.0%34.9%36.8%820.0%21.0%21.0%18.0%20.0%Table5:SuccessratesofvaryingnumberofensemblesusingtheDAGGERpolicyconditionedonfulltrajectories,acrossbothtrainingandtesttasks.9TaskIDSuccessRateabcd83.0%abdc86.0%acbd92.0%acdb84.0%adbc91.0%adcb88.0%bacd92.0%badc90.0%bcad92.0%bcda88.0%bdac94.0%bdca88.0%cabd82.0%cadb87.0%cbad95.0%cbda87.0%cdab91.0%cdba93.0%dabc90.0%dacb92.0%dbac88.0%dbca90.0%dcab91.0%dcba84.0%Table6:Successratesofasetoftasksthatareequivalentuptopermutations,usingtheDAGGERpolicyconditionedonfulltrajectories.#StagesSuccessRecoverablefailureManipulationfailureWrongmove199.3%0.0%0.7%0.0%295.9%0.4%3.7%0.0%389.1%0.7%10.1%0.1%479.2%1.2%19.4%0.1%567.5%1.4%30.9%0.2%655.2%1.4%43.1%0.3%744.6%1.7%53.2%0.6%830.9%4.3%64.9%0.0%Table7:BreakdownofsuccessandfailurescenariosforDemopolicy.#StagesSuccessRecoverablefailureManipulationfailureWrongmove199.4%0.0%0.6%0.0%295.3%0.9%3.8%0.0%389.1%1.9%8.8%0.1%479.5%3.5%16.7%0.3%569.1%5.0%25.6%0.3%655.8%7.3%36.4%0.5%739.0%8.6%51.5%0.8%821.2%14.1%62.4%2.4%Table8:BreakdownofsuccessandfailurescenariosforDAGGERpolicy.100100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(a)AllStages0100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(b)Stage00100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(c)Stage10100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(d)Stage20100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(e)Stage30100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(f)Stage40100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(g)Stage50100200300400500EvaluationEpoch0%20%40%60%80%100%SingleStageSuccessRatePolicyTypeDAGGERBCSnapshotFinalstate(h)Stage6Figure7:Learningcurvesofblockstackingtask.Theﬁrstplotshowstheaveragesuccessratesoverinitialconﬁgurationsofallstages.Thesubsequentﬁguresshowsthebreakdownofeachstage.Forinstance,“Stage3”meansthattheﬁrst3stackingoperationsarealreadycompleted,andthepolicyisevaluatedonitsabilitytoperformthe4thstackingoperation.11#StagesSuccessRecoverablefailureManipulationfailureWrongmove199.6%0.0%0.4%0.0%295.6%1.1%3.2%0.1%388.1%2.2%9.5%0.2%478.5%4.5%16.8%0.2%567.2%6.6%25.7%0.4%653.9%8.3%37.1%0.6%740.6%9.8%48.7%0.9%827.0%13.5%58.4%1.1%Table9:BreakdownofsuccessandfailurescenariosforBCpolicy.#StagesSuccessRecoverablefailureManipulationfailureWrongmove199.1%0.0%0.9%0.0%294.5%1.6%3.8%0.1%388.0%2.5%9.3%0.2%478.9%4.6%16.2%0.3%565.6%8.0%25.8%0.6%650.8%8.3%40.2%0.7%736.1%9.2%54.2%0.4%821.6%11.4%65.9%1.1%Table10:BreakdownofsuccessandfailurescenariosforSnapshotpolicy.#StagesSuccessRecoverablefailureManipulationfailureWrongmove199.2%0.0%0.8%0.0%295.1%1.3%3.6%0.0%386.7%2.5%9.7%1.1%475.2%4.0%18.3%2.5%560.5%4.3%31.2%4.0%645.5%4.7%45.5%4.3%734.9%5.6%57.3%2.2%824.1%3.6%72.3%0.0%Table11:BreakdownofsuccessandfailurescenariosforFinalstatepolicy.(a)Head0(b)Head1(c)Head2(d)Head3Figure8:Heatmapofattentionweightsoverdifferentblocksofall4queryheads.12(a)Head0(b)Head1(c)Head2(d)Head3(e)Head4(f)Head5Figure9:Heatmapofattentionweightsoverdownsampleddemonstrationtrajectoryofall6queryheads.Thereare2queryheadsperstepofLSTM,and3stepsofLSTMareperformed.13Figure10:Illustrationofthetaskusedforthevisualizationofattentionheatmaps(ﬁrsthalf).Thetaskisabcdefghij.Theleftsideshowsthekeyframesinthedemonstration.Therightsideshowshow,afterseeingtheentiredemonstration,tthepolicyreproducesthesamelayoutinanewinitializationofthesametask.14Figure11:Illustrationofthetaskusedforthevisualizationofattentionheatmaps(secondhalf).Thetaskisabcdefghij.Theleftsideshowsthekeyframesinthedemonstration.Therightsideshowshow,afterseeingtheentiredemonstration,tthepolicyreproducesthesamelayoutinanewinitializationofthesametask.15
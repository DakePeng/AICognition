7
1
0
2

n
u
J

6

]

G
L
.
s
c
[

2
v
3
7
2
4
0
.
1
1
6
1
:
v
i
X
r
a

PublishedasaconferencepaperatICLR2017ONTHEQUANTITATIVEANALYSISOFDECODER-BASEDGENERATIVEMODELSYuhuaiWuDepartmentofComputerScienceUniversityofTorontoywu@cs.toronto.eduYuriBurdaOpenAIyburda@openai.comRuslanSalakhutdinovSchoolofComputerScienceCarnegieMellonUniversityrsalakhu@cs.cmu.eduRogerGrosseDepartmentofComputerScienceUniversityofTorontorgrosse@cs.toronto.eduABSTRACTThepastseveralyearshaveseenremarkableprogressingenerativemodelswhichproduceconvincingsamplesofimagesandothermodalities.Asharedcomponentofmanypowerfulgenerativemodelsisadecodernetwork,aparametricdeepneuralnetthatdeﬁnesagenerativedistribution.Examplesincludevariationalautoencoders,generativeadversarialnetworks,andgenerativemomentmatchingnetworks.Unfortunately,itcanbedifﬁculttoquantifytheperformanceofthesemodelsbecauseoftheintractabilityoflog-likelihoodestimation,andinspectingsamplescanbemisleading.WeproposetouseAnnealedImportanceSamplingforevaluatinglog-likelihoodsfordecoder-basedmodelsandvalidateitsaccuracyusingbidirectionalMonteCarlo.Theevaluationcodeisprovidedathttps://github.com/tonywu95/eval_gen.Usingthistechnique,weanalyzetheperformanceofdecoder-basedmodels,theeffectivenessofexistinglog-likelihoodestimators,thedegreeofoverﬁtting,andthedegreetowhichthesemodelsmissimportantmodesofthedatadistribution.1INTRODUCTIONInrecentyears,deepgenerativemodelshavedramaticallypushedforwardthestate-of-the-artingenerativemodellingbygeneratingconvincingsamplesofimages(Radfordetal.,2016),achievingstate-of-the-artsemi-supervisedlearningresults(Salimansetal.,2016),andenablingautomaticimagemanipulation(Zhuetal.,2016).Manyofthemostsuccessfulapproachesaredeﬁnedintermsofaprocesswhichsampleslatentvariablesfromasimpleﬁxeddistribution(suchasGaussianoruniform)andthenappliesalearneddeterministicmappingwhichwewillrefertoasadecodernetwork.Importantexamplesincludevariationalautoencoders(VAEs)(Kingma&Welling,2014;Rezendeetal.,2014),generativeadversarialnetworks(GANs)(Goodfellowetal.,2014),generativemomentmatchingnetworks(GMMNs)(Li&Swersky,2015;Dziugaiteetal.,2015),andnonlinearindependentcomponentsestimation(Dinhetal.,2014).Werefertothissetofmodelscollectivelyasdecoder-basedmodels,alsoknownasdensitynetworks(MacKay&Gibbs,1998).Whilemanydecoder-basedmodelsareabletoproduceconvincingsamples(Dentonetal.,2015;Radfordetal.,2016),rigorousevaluationremainsachallenge.Comparingmodelsbyinspectingsamplesislabor-intensive,andpotentiallymisleading(Theisetal.,2016).Whilealternativequantita-tivecriteriahavebeenproposed(Bounliphoneetal.,2016;Imetal.,2016;Salimansetal.,2016),log-likelihoodofheld-outtestdataremainsoneofthemostimportantmeasuresofagenerativemodel’sperformance.Unfortunately,unlessthedecoderisdesignedtobereversible(Dinhetal.,2014;2016),log-likelihoodestimationindecoder-basedmodelsistypicallyintractable.InthecaseofVAE-basedmodels,alearnedencodernetworkgivesatractablelowerbound,butforGANsandGMMNsitisnotobvioushoweventocomputeagoodlowerbound.Evenwhenlowerboundsareavailable,theiraccuracymaybehardtodetermine.Becauseofthedifﬁcultyoflog-likelihood1 
 
 
 
 
 
PublishedasaconferencepaperatICLR2017(a)GAN-10;LLD:328.7(b)GAN-50,epoch200;LLD:543.5(c)GAN-50,epoch1000;LLD:625.5Figure1:(a)samplesfromaGANwith10latentdimensions,(b)and(c)samplesfromaGANwith50latentdimensionsatdifferentepochsoftraining.Whileitisdifﬁculttovisuallydiscerndifferencesbetweenthesethreemodels,theirlog-likelihood(LLD)valuesspanalmost300nats.evaluation,itishardtoanswerbasicquestionssuchaswhetherthenetworksaresimplymemorizingtrainingexamples,orwhethertheyaremissingimportantmodesofthedatadistribution.Themostwidelyusedestimatoroflog-likelihoodforGANsandGMMNsistheKernelDensityEstimator(KDE)(Parzen,1962).Itestimatesthelikelihoodunderanapproximationtothemodel’sdistributionobtainedbysimulatingfromthemodelandconvolvingthesetofsampleswithakernel(typicallyGaussian).Unfortunately,KDEisnotoriouslyinaccurateforestimatinglikelihoodinhighdimensions,becauseitishardtotileahigh-dimensionalmanifoldwithsphericalGaussians(Theisetal.,2016).Inthispaper,weproposetouseannealedimportancesampling(AIS;(Neal,2001))toestimatelog-likelihoodsofdecoder-basedgenerativemodelsandtoobtainapproximateposteriorsamples.Importantly,wevalidatethisapproachusingBidirectionalMonteCarlo(BDMC)(Grosseetal.,2015),whichprovablyboundsthelog-likelihoodestimationerrorandtheKLdivergencefromthetrueposteriordistributionfordatasimulatedfromamodel.Formostmodelsweconsider,weﬁndthatAISistwoordersofmagnitudemoreaccuratethanKDE,andisaccurateenoughtoperformﬁne-grainedcomparisonsbetweengenerativemodels.InthecaseofVAEs,weshowthatAIScanbefurtherspedupbyusingtherecognitionnetworktodeterminetheinitialdistribution;thisyieldsanestimatorwhichisfastenoughtoberunrepeatedlyduringtraining.Usingtheproposedmethod,weanalyzeseveralscientiﬁcquestionscentraltounderstandingdecoder-basedgenerativemodels.First,wemeasuretheaccuracyofKDEandoftheimportanceweightingboundwhichiscommonlyusedtoevaluateVAEs.WeﬁndthattheKDEerrorislargerthanthe(quitesigniﬁcant)log-likelihooddifferencesbetweendifferentmodels,andthatKDEcanleadtomisleadingconclusions.Theimportanceweightedbound,whilereasonablyaccurate,canalsoyieldmisleadingresultsinsomecases.Second,wecomparethelog-likelihoodsofVAEs,GANs,andGMMNs,andﬁndthatVAEsachievelog-likelihoodsseveralhundrednatshigherthantheothermodels(eventhoughKDEconsidersallthreemodelstohaveroughlythesamelog-likelihood).Third,weanalyzethedegreeofoverﬁttinginVAEs,GANs,andGMMNs.Contrarytoacommonlyproposedhypothesis,weﬁndthatGANsandGMMNsarenotsimplymemorizingtheirtrainingdata;infact,theirlog-likelihoodgapsbetweentrainingandtestdataaremuchsmallerrelativetocomparably-sizedVAEs.Finally,byvisualizing(approximate)posteriorsamplesobtainedfromAIS,weobservethatGANsmissimportantmodesofthedatadistribution,evenoneswhicharerepresentedinthetrainingdata.WeemphasizethatnoneoftheabovephenomenacanbemeasuredusingKDEortheimportanceweightedbound,orbyinspectingsamples.(SeeFig.1foranexamplewhereitistrickytocomparemodelsbasedonsamples.)Whilelog-likelihoodisbynomeansaperfectmeasure,weﬁndthattheabilitytoaccuratelyestimatelog-likelihoodsofdecoder-basedmodelsyieldscrucialinsightintotheirbehaviorandsuggestsdirectionsforimprovingthem.2BACKGROUND2.1DECODER-BASEDGENERATIVEMODELSIngenerativemodelling,adecodernetworkisoftenusedtodeﬁneagenerativedistributionbytransformingsamplesfromsomesimpledistribution(e.g.normal)tothedatamanifold.Inthis2PublishedasaconferencepaperatICLR2017paper,weconsiderthreekindsofdecoder-basedgenerativemodels:VariationalAutoencoder(VAE)(Kingma&Welling,2014),GenerativeAdversarialNetwork(GAN)(Goodfellowetal.,2014),andGenerativeMomentMatchingNetwork(GMMN)(Li&Swersky,2015;Dziugaiteetal.,2015).2.1.1VARIATIONALAUTOENCODERAvariationalautoencoder(VAE)(Kingma&Welling,2014)isaprobabilisticdirectedgraphicalmodel.Itisdeﬁnedbyajointdistributionoverasetoflatentrandomvariableszandtheobservedvariablesx:p(x,z)=p(x|z)p(z).Theprioroverthelatentrandomvariables,p(z),isusuallychosentobeastandardGaussiandistribution.Thedatalikelihoodp(x|z)isusuallyaGaussianorBernoullidistributionwhoseparametersdependonzthroughadeepneuralnetwork,knownasthedecodernetwork.Italsousesanapproximateinferencemodelcalledanencoderorrecognitionnetwork,thatservesasavariationalapproximationq(z|x)totheposteriorp(z|x).Thedecodernetworkandtheencodernetworksarejointlytrainedtomaximizetheevidencelowerbound(ELBO):logp(x)≥Eq(z|x)[logp(x|z)]−KL(q(z|x)||p(z))(1)Inaddition,thereparametrizationtrickisusedtoreducethevarianceofthegradientestimate.2.1.2GENERATIVEADVERSARIALNETWORK(GAN)Agenerativeadversarialnetwork(GAN)(Goodfellowetal.,2014)isagenerativemodeltrainedbyagamebetweenadecodernetworkandadiscriminatornetwork.Itdeﬁnesthegenerativemodelbysamplingthelatentvariablezfromsomesimplepriordistributionp(z)(e.g.,Gaussian)followedthroughthedecodernetwork.ThediscriminatornetworkD(·)outputsaprobabilityofagivensamplecomingfromthedatadistribution.Itstaskistodistinguishsamplesfromthegeneratordistributionfromrealdata.Thedecodernetwork,ontheotherhand,triestoproducesamplesasrealisticaspossible,inordertofoolthediscriminatorintoacceptingitsoutputsasbeingreal.Thecompetitionbetweenthetwonetworksresultsinthefollowingminimaxproblem:minGmaxDEx∼pdata[logD(x)]+Ez∼p(z)[log(1−D(G(z))](2)UnlikeVAE,theobjectiveisnotexplicitlyrelatedtothelog-likelihoodofthedata.Moreover,thegenerativedistributionisadeterministicmapping,i.e.,p(x|z)isaDiracdeltadistribution,parametrizedbythedeterministicdecoder.Thiscanmakedatalikelihoodill-deﬁned,astheprobabilitydensityofanyparticularpointxcanbeeitherinﬁnite,orexactlyzero.2.1.3GENERATIVEMOMENTMATCHINGNETWORK(GMMN)Generativemomentmatchingnetworks(GMMNs)(Li&Swersky,2015;Dziugaiteetal.,2015)adoptmaximummeandiscrepancy(MMD)asthetrainingobjective,amomentmatchingcriterionwherekernelmeanembeddingtechniquesareusedtoavoidunnecessaryassumptionsofthedistributions.IthasthesameissueasGANinthatthelog-likelihoodisundeﬁned.2.2ANNEALEDIMPORTANCESAMPLINGWeareinterestedinestimatingtheprobabilityp(x)=Rp(z)p(x|z)dzamodelassignstoanobservationx.Thisisequivalenttocomputingthenormalizingconstantoftheunnormalizeddistributionf(z)=p(z,x).Onenaïveapproachislikelihoodweighting,whereonesamples{z(k)}Kk=1∼p(z)andaveragestheconditionallikelihoodsp(x|z(k)).Thisisjustiﬁedbythefollowingidentity:p(x)=Zp(x,z)p(z)p(z)dz=Ez∼p(z)[p(x|z)](3)Likelihoodweightingcanbeviewedassimpleimportancesampling,wheretheproposaldistributionisthepriorp(z)andthetargetdistributionistheposteriorp(z|x).Unfortunately,importancesamplingworkswellonlywhentheproposaldistributionisagoodmatchforthetargetdistribution.Forthemodelsconsideredinthispaper,the(verybroad)priorcanbedrasticallydifferentthanthe(highlyconcentrated)posterior,leadingtoinaccurateestimatesofthelikelihood.Annealedimportancesampling(AIS;Neal,2001)isaMonteCarloalgorithmcommonlyusedtoestimate(ratiosof)normalizingconstants.Roughlyspeaking,itcomputesasequenceofimportance3PublishedasaconferencepaperatICLR2017samplingbasedestimates,eachofwhichisstablebecauseitinvolvestwodistributionswhichareverysimilar.Inparticular,supposeoneisinterestedinestimatingthenormalizingconstantZ=Rf(z)dzofanunnormalizeddistributionf(z).(Inthelikelihoodestimationsetting,f(z)=p(z,x)andZ=p(x).)Onemustspecifyasequenceofdistributionsq1,...,qT,whereqt=ft/Zt,andfT=fisthetargetdistribution.Itisrequiredthatonecanobtainoneormoreexactsamplesfromtheinitialdistributionq1.OnemustalsospecifyasequenceofreversibleMCMCtransitionoperatorsT1,...,TT,whereTtleavesqtinvariant.AISproducesa(nonnegative)unbiasedestimateoftheratioZT/Z1asfollows:ﬁrst,wesamplearandominitialstatez1∼q1andsettheinitialweightw1=1.Foreverystaget≥2weupdatetheweightwandsamplethestateztaccordingtowt←wt−1ft(zt−1)ft−1(zt−1)zt∼Tt(z|zt−1)(4)AsdemonstratedbyNeal(2001),thisprocedureproducesanonnegativeweightwTsuchthatE[wT]=ZT/Z1.Typically,Z1isknown,soonecomputesmultipleindependentAISweights{w(K)T}Kk=1andobtainstheunbiasedestimateˆZT=Z11KPKk=1w(K)T.Inthelikelihoodestimationsetting,Z1=1andZT=p(x),sowedenotethisestimatorasˆp(x).Typically,theunnormalizedintermediatedistributionsaresimplydeﬁnedtobegeometricaveragesft(z)=f1(z)1−βtfT(z)βt,wheretheβtaremonotonicallyincreasingparameterswithβ1=0andβT=1.Forf1(z)=p(z)andfT(z)=p(z,x),thisgivesft(z)=p(z)p(x|z)βt.(5)AsshownbyNeal(2001),undercertainregularityconditions,thevarianceofˆZTtendstozeroasthenumberofintermediatedistributionsisincreased.AISisveryeffectiveinpractice,andhasbeenusedtoestimatenormalizingconstantsofcomplexhigh-dimensionaldistributions(Salakhutdinov&Murray,2008).2.3BIDIRECTIONALMONTECARLOAISprovidesanonnegativeunbiasedestimateˆp(x)ofp(x).However,itisoftenmorepracticaltoestimatep(x)inthelogspace,i.e.logp(x),becauseofunderﬂowproblemofdealingwithmanyproductsofprobabilitymeasure.Ingeneral,wenotethatlogarithmofanonnegativeunbiasedestimateisastochasticlowerboundofthelogestimand(Grosseetal.,2015).Inparticular,logˆp(x)isastochasticlowerboundonlogp(x),satisfyingE[logˆp(x)]≤logp(x)andPr(logˆp(x)>logp(x)+b)<e−b.Grosseetal.(2015)pointedoutthatifAISisruninreversestartingfromanexactposteriorsample,ityieldsanunbiasedestimateof1/p(x),which(bytheaboveargument)canbeseenasastochasticupperboundonlogp(x).ThecombinationoflowerandupperboundsfromforwardandreverseAISisknownasbidirectionalMonteCarlo(BDMC).Inmanycases,thecombinationofboundscanpinpointthetruevaluequiteprecisely.Whileposteriorsamplingisjustashardaslog-likelihoodestimation(Jerrumetal.,1986),inthecaseoflog-likelihoodestimationforsimulateddata,onehasavailableasingleexactposteriorsample:theparametersand/orlatentvariableswhichgeneratedthedata.Becausethistrickisonlyapplicabletosimulateddata,BDMCismostusefulformeasuringtheaccuracyofalog-likelihoodestimatoronsimulateddata.Grosseetal.(2016)observedthatBDMCcanalsobeusedtovalidateposteriorinferencealgorithms,asthegapbetweenupperandlowerboundsisitselfaboundontheKLdivergenceofapproximatesamplesfromthetrueposteriordistribution.3METHODOLOGYForagivengenerativedistributionp(x,z)=p(z)p(x|z),ourtaskistomeasurethelog-likelihoodoftestexampleslogp(xtest).Weﬁrstdiscusshowwedeﬁnethegenerativedistributionfordecoder-basednetworks.ForVAE,thegenerativedistributionisdeﬁnedinthestandardway,wherep(z)isastandardnormaldistributionandp(x|z)isanormaldistributionparametrizedbymeanµθ(z)andσθ(z),predictedbythegeneratorgiventhelatentcode.However,theobservationdistributionforGANsandGMMNsistypicallytakentobeadeltafunction,sothatthemodel’sdistributioncovers4PublishedasaconferencepaperatICLR2017onlyasubmanifoldofthespaceofobservables.Inorderforthelikelihoodtobewell-deﬁned,wefollowthesameassumptionmadewhenevaluatingusingKernelDensityEstimator(Parzen,1962):weassumeaGaussianobservationmodelwithaﬁxedvariancehyperparameterσ2.WewillrefertothedistributiondeﬁnedbythisGaussianobservationmodelaspσ.ObservethattheKDEestimateisgivenbyˆpσ(x)=1KKXk=1pσ(x|z(k)),(6)where{z(k)}Kk=1aresamplesfromthepriorp(z).Thisisequivalenttolikelihoodweightingforthedistributionpσ,whichisaninstanceofsimpleimportancesampling(SIS).BecauseSISisanunbiasedestimatorofthelikelihood,logˆpσ(x)isastochasticlowerboundonlogpσ(x)(Grosseetal.,2015).Unfortunately,SIScanresultinverypoorestimateswhentheevidencehaslowpriorprobability(i.e.theposteriorisverydissimilartotheprior).ThissuggeststhatAISmightbeabletoyieldmuchmoreaccuratelog-likelihoodestimatesunderpσ.WenotethatKDEcanbeviewedasaspecialcaseofAISwherethenumberofintermediatedistributionsissetto0.WenowdescribespeciﬁcallyhowwecarryoutevaluationusingAIS.Inmostofourexperiments,wechoosetheinitialdistributionofAIStobep(z),thesamepriordistributionusedintrainingdecoder-basedmodels.Ifthemodelprovidesanencodernetwork(e.g.,VAE),wecantaketheapproximateddistributionpredictedbytheencoderq(z|x)astheinitialdistributionoftheAISchain.Forcontinuousdata,wedeﬁnetheunnormalizeddensityoftargetdistributiontobethejointgenerativedistributionwiththeGaussiannoisemodel,pσ(x,z)=pσ(x|z)p(z).Forthesmallsubsetofexperimentsdoneonthebinarydata,wedeﬁnetheobservationmodeltobeaBernoullimodelwithmeanpredictedbythedecoder.Ourintermediatedistributionsaregeometricaveragesofthepriorandposterior,asinEqn.5.Sinceallofourexperimentsaredoneusingcontinuouslatentspace,weuseHamiltonianMonteCarlo(Neal,2010)asthetransitionoperatorforsamplinglatentsamplesalongannealing.Theevaluationcodeisprovidedathttps://github.com/tonywu95/eval_gen.4RELATEDWORKAISisknowntobeapowerfultechniqueofestimatingthepartitionfunctionofthemodel.OneinﬂuentialexamplewastheuseofAIStoevaluatedeepbeliefnetworks(Salakhutdinov&Murray,2008).Althoughweusedthesametechnique,theproblemweconsideriscompletelydifferent.Firstofall,themodeltheyconsiderisundirectedgraphicalmodels,whereasdecoder-basedmodelsaredirectedgraphicalmodels.Secondly,theirmodelhasawell-deﬁnedprobabilisticdensityfunctionintermsofenergyfunction,whereasweneedtoconsiderdifferentprobabilisticmodelforoneinwhichthethelikelihoodisill-deﬁned.Inaddition,wevalidateourestimatesusingBDMC.Theisetal.(2016)giveanin-depthanalysisofissuesthatmightcomeupinevaluatinggenerativemodels.Theyalsopointoutthatamodelthatcompletelyfailsatmodellingtheproportionofmodesofthedistributionmightstillachieveahighlikelihoodscore.Salimansetal.(2016)proposeanimage-qualitymeasurewhichtheyﬁndtobehighlycorrelatedwithhumanvisualjudgement.Theyproposetofeedthesamplesxofthemodeltothe“inception”modeltoobtainaconditionallabeldistributionp(y|x),andevaluatethescoredeﬁnedbyexpExKL(p(y|x)||p(y)),whichismotivatedbyhavingalowentropyofp(y|x)butalargeentropyofp(y).However,themeasureislargelybasedonvisualqualityofthesample,andwearguethatthevisualqualitycanbeamisleadingwaytoevaluateamodel.5EXPERIMENTS5.1DATASETSAllofourexperimentswereperformedontheMNISTdatasetofimagesofhandwrittendigits(LeCunetal.,1998).Forconsistencywithpriorworkonevaluatingdecoder-basedmodels,mostofourexperimentsusedthecontinuousinputs.WedequantizedthedatafollowingUriaetal.(2013),byaddingauniformnoiseof1256tothedataandrescalingittobein[0,1]Dafterdequantization.WeusethestandardsplitofMNISTinto60,000trainingand10,000testexamples,andused50,000imagesfromthetrainingsetfortraining,andremaining10,000imagesforvalidation.Inaddition,5PublishedasaconferencepaperatICLR2017someofourexperimentsusedthebinarizedMNISTdatasetwithaBernoulliobservationmodel(Salakhutdinov&Murray,2008).5.2MODELSFormostofourexperiments,weconsideredtwodecoderarchitectures:asmallonewith10latentdimensions,andalargeronewith50latentdimensions.WeusestandardNormaldistributionaspriorfortrainingallofourmodels.Alllayerswerefullyconnected,andthenumberofunitsineachlayerwas10–64–256–256-1024–784forthesmallerarchitectureand50–1024–1024–1024–784forthelargerone.WetrainedbotharchitecturesusingtheVAE,GAN,andGMMNobjectives,resultinginsixnetworkswhichwerefertoasVAE-10,VAE-50,etc.Ingeneral,thelargerarchitectureperformedsubstantiallybetteronboththetrainingandtestsets,butweanalyzethesmallerarchitectureaswellbecauseitbetterhighlightssomeofthedifferencesbetweenthetrainingcriteria.AdditionalarchitecturaldetailsaregiveninAppendixA.1.Inordertoenableadirectcomparisonbetweentrainingcriteria,allmodelsusedasphericalGaussianobservationmodelwithﬁxedvariance.ThisisconsistentwithpreviousprotocolsforevaluatingGANsandGMMNs.However,wenotethatthisobservationmodelisanontrivialconstraintontheVAEs,whichcouldinsteadbetrainedwithamoreﬂexiblediagonalGaussianobservationmodelwherethevariancesdependonthelatentstate.Suchobservationmodelscaneasilyachievemuchhigherlog-likelihoodscores,forinstancebynoticingthatboundarypixelsarealwayscloseto0.(E.g.,wetrainedaVAEwiththemoregeneralobservationmodelwhichachievedalog-likelihoodofatleast2200natsoncontinuousMNIST.)Therefore,thelog-likelihoodvalueswereportshouldnotbecompareddirectlyagainstnetworkswhichhaveamoreﬂexibleobservationmodel.5.3VALIDATIONOFLOG-LIKELIHOODESTIMATESBeforeweanalyzetheperformanceofthetrainednetworks,wemustﬁrstdeterminetheaccuracyofthelog-likelihoodestimators.Inthissection,wevalidatetheaccuracyofourAIS-basedestimatesusingBDMC.WethenanalyzetheerrorintheKDEandIWAEestimatesandhighlightsomecaseswherethesemeasuresmissimportantphenomena.5.3.1VALIDATIONOFAISWeusedAIStoestimatelog-likelihoodsforallmodelsunderconsideration.Exceptwhereotherwisespeciﬁed,allAISestimateswereobtainedusing16independentchains,10,000intermediatedistri-butionsoftheforminEqn.5,andatransitionoperatorconsistingofoneproposedHMCtrajectorywith10leapfrogsteps.1FollowingRanzatoetal.(2010),theHMCstepsizewastunedtoachieveanacceptancerateof0.65(asrecommendedbyNeal(2010)).Forallsixmodels,weevaluatedtheaccuracyofthisestimationprocedureusingBDMCondatasampledfromthemodel’sdistributionon1000simulatedexamples.Thegapbetweenthelog-likelihoodestimatesproducedbyforwardAIS(whichgivesalowerbound)andreverseAIS(whichgivesanupperbound)boundstheerroroftheAISestimatesonsimulateddata.WerefertothisgapastheBDMCgap.Forﬁveofthesixnetworksunderconsideration,wefoundtheBDMCgaptobelessthan1nat.Fortheremainingmodel(GAN-50),thegapwasabout10nats.Bothgapsaremuchsmallerthanourmeasuredlog-likelihooddifferencesbetweenmodels.Ifthesegapsarerepresentativeofthetrueerrorintheestimatesontherealdata,thenthisindicatesAISisaccurateenoughtomakeﬁne-grainedcomparisonsbetweenmodelsandtobenchmarkotherlog-likelihoodestimators.(TheBDMCgapisnotguaranteedtoholdfortherealdata,althoughGrosseetal.(2016)foundthebehaviorofAIStomatchcloselybetweenrealandsimulateddata.)5.3.2HOWACCURATEISKERNELDENSITYESTIMATION?Kerneldensityestimation(KDE)(Parzen,1962)iswidelyusedtoevaluatedecoder-basedmodels(Goodfellowetal.,2014;Li&Swersky,2015),andavariantwasproposedinthesettingofevaluatingBoltzmannmachines(Bengioetal.,2013).PapersreportingKDEestimatesoftencautionthatthe1WeusedtheHMCimplementationfromhttp://deeplearning.net/tutorial/deeplearning.pdf6PublishedasaconferencepaperatICLR20170.0050.0100.0150.0200.025Variance−400−2000200400600Log-likelihoodGAN50 with varying varianceTrain AISValid AISTrain KDEValid KDE101102103Seconds50100150200250300350Log-likelihoodAIS vs: KDEKDEAIS forwardAIS backward101102103104Seconds−88.0−87.5−87.0−86.5−86.0−85.5Log-likelihoodAIS vs: IWAEIWAEAISAIS+encoder(a)GAN-50:LLDvs.Variance(b)GMMN-10:LLDvs.Evaluationtime(c)IWAE:LLDvs.EvaluationtimeFigure2:(a)Log-likelihoodofGAN-50,underdifferentchoicesofvarianceparameter.(b)Log-likelihoodofGMMN-10on100simulatedexamplesevaluatedbyAISandKDEvs.thecorrespondingrunningtime.WeshowtheBDMCgapconvergestoalmostzeroasweincreasetherunningtime.(c)Log-likelihoodofIWAEon10,000testexamplesevaluatedbyAISandIWAEboundvs.runningtime.(a),(b)areresultsoncontinuousMNIST,and(c)isonbinarizedMNIST.NotethatAIS/AIS+encoderdominatestheotherestimateinbothestimationaccuracyandrunningtime.(Nats)AISAIS+encoderIWAEbound#distAIS#distAIS+encoder#samplesIWAE-85.679-85.754-86.902100010010000-85.619-85.621-86.464100001000100000Table1:AISvs.IWAEboundon10,000testexamplesofbinarizedMNIST.“#dist”denotesthenumberofintermediatedistributionsusedforevalution.WeﬁndAISestimateisconsistently1nathigherthanIWAEbound;AIS+encodercanachieveaboutthesameestimateasAIS,butwith1orderofmagnitudelessnumberofintermediatedistributions.KDEisnotmeanttobeappliedinhigh-dimensionalspacesandthattheresultsmightthereforebeinaccurate.Nevertheless,KDEremainsthestandardprotocolforevaluatingdecoder-basedmodels.WeanalyzedtheaccuracyoftheKDEestimatesbycomparingagainstAIS.Bothestimatesarestochasticlowerboundsonthetruelog-likelihood(seeSection3),solargervaluesareguaranteed(withhighprobability)tobemoreaccurate.Foreachestimator,wevariedoneparameterinﬂuencingthecomputationalbudget;forAIS,thiswasthenumberofintermediatedistributions(chosenfrom{100,500,1000,2000,10000}),andforKDE,itwasthenumberofsamples(chosenfrom{10000,100000,500000,1000000,2000000}).UsingGMMN-10forillustration,weplotbothlog-likelihoodestimates100simulatedexamplesasafunctionofevaluationtimeinFig.2(b).WealsoplottheupperboundoflikelihoodgivenbyrunningAISinreversedirection.WeseethattheBDMCgapapproachestozero,validatingtheaccuracyofAIS.WealsoseethattheAISestimatorachievesmuchmoreaccurateestimatesduringsimilarevaluationtime.Furthermore,theKDEestimatesappeartoleveloff,suggestingonecannotobtainaccurateresultsevenusingordersofmagnitudemoresamples.TheKDEestimationerroralsoimpactstheestimateoftheobservationnoiseσ,sincealargevalueofσisneededforthesamplestocoverthefulldistribution.Wecomparedthelog-likelihoodsestimatedbyAISandKDEwithvaryingchoicesofσon100trainingandvalidationexamplesofMNIST.Weused1millionsimulatedsamplesforKDEevaluation,whichtakesalmostthesametimeasrunningAISestimation.InFig.2(a),weshowthelog-likelihoodofGAN-50estimatedbyKDEandAISasafunctionofσ.BecausetheaccuracyofKDEdeclinessharplyforsmallσvalues,itcreatesastrongbiastowardslargeσ.5.3.3HOWACCURATEISTHEIWAEBOUND?Inprinciple,onecouldestimateVAElikelihoodsusingtheVAEobjectivefunction(whichisalowerboundonthetruelog-likelihood).However,itismorecommontouseimportanceweighting,wheretheproposaldistributioniscomputedbytherecognitionnetwork.ThisisprovablymoreaccuratethantheVAEbound(Burdaetal.,2016).BecausetheimportanceweightedestimatecorrespondstotheobjectivefunctionusedbytheImportanceWeightedAutoencoder(IWAE)(Burdaetal.,2016),wewillrefertoitastheIWAEbound.OncontinuousMNIST,theIWAEboundunderestimatedthetruelog-likelihoodsbyatleast33.2natsonthetrainingsetand187.4natsonthetestset.WhilethisisconsiderablymoreaccuratethanKDE,theerrorisstillsigniﬁcant.Interestingly,thisresultalsosuggeststhattherecognitionnetworkoverﬁtsthetrainingdata.7PublishedasaconferencepaperatICLR2017(Nats)AISTestAISTrainBDMCgapKDETestIWAETestVAE-50991.435±6.4771298.830±0.8631.540351.213826.325GAN-50627.297±8.813648.283±21.11510.045300.331/GMMN-50593.472±8.591607.272±1.4511.146277.193/VAE-10705.375±7.411791.029±0.8100.832408.659486.466GAN-10328.772±5.538346.640±4.2600.934259.673/GMMN-10346.679±5.860358.943±6.4850.605262.73/Table2:Modelcomparisonson1000testandtrainingexamplesofcontinuousMNIST.Conﬁdenceintervalsreﬂectthevariabilityfromthechoiceoftrainingortestexamples(whichappearstobethedominantsourceoferrorfortheAISvalues).AIS,KDE,andIWAEareallstochasticlowerboundsonthelog-likelihood.SinceVAEandIWAEresultshavecustomarilybeenreportedonbinarizedMNIST,weadditionallytrainedanIWAEinthissetting.ThetrainingdetailsaregiveninAppendixA.2.Toshowthepracticalityofourmethod,weevaluatedtheIWAEonthefull10000testusingAISandIWAEbound,withdifferentchoicesofintermediatedistributionandnumberofsimulatedsamples,showninTable1.WealsoevaluateAISwiththeinitialdistributiondeﬁnedbyencodersofVAEs,denotedasAIS+encoder.WeﬁndthattheIWAEboundunderestimatesthetruevaluebyatleast1nat,whichisalargedifferencebythestandardsofbinarizedMNIST.(E.g.,itrepresentsabouthalfofthegapbetweenastate-of-the-artpermutation-invariantmodel(Tranetal.,2016)andonewhichexploitsstructure(vandenOordetal.,2016).)TheAISandIWAEestimatesarecomparedintermsofevaluationtimeinFig.2(c).5.4SCIENTIFICFINDINGSHavingvalidatedtheaccuracyofAIS,wenowuseittoanalyzetheeffectivenessofvarioustrainingcriteria.Wealsohighlightphenomenawhichwouldnotbeobservableusingexistinglog-likelihoodestimatorsorbyinspectingsamples.Forallexperimentsinthissection,weused10,000intermediatedistributionsforAIS,1millionsimulatedsamplesforKDE,and200,000importancesamplesfortheIWAEbound.(Thesesettingsresultedinsimilarcomputationtimeforallthreeestimators.)5.4.1MODELLIKELIHOODCOMPARISONWeevaluatedthetrainedmodelsusingAISandKDEon1000testexamplesofMNIST;resultsareshowninTable2.Weﬁndthatforallthreetrainingcriteria,thelargerarchitecturesconsistentlyoutperformedthesmallerones.Wealsoﬁndthatforboththe10-and50-dimensionalarchitectures,theVAEsachievedsubstantiallyhigherlog-likelihoodsthanGANsorGMMNs.ItisnotsurprisingthattheVAEsachievedhigherlikelihood,becausetheyweretrainedusingalikelihood-basedobjectivewhiletheGANsandGMMNswerenot.However,itisinterestingthatthedifferenceinlog-likelihoodswassolarge;intherestofthissection,weattempttoanalyzewhatexactlyiscausingthislargedifference.WenotethattheKDEerrorswereofthesameorderofmagnitudeasthedifferencesbetweenmodels,indicatingthatitcannotbeusedreliablytocomparelog-likelihoods.Furthermore,KDEdidnotidentifythecorrectorderingofmodels;forinstance,itestimatedalowerlog-likelihoodforVAE-50thanforVAE-10,eventhoughitstruelog-likelihoodwasalmost300natshigher.KDEalsounderestimatedbyanorderofmagnitudethelog-likelihoodimprovementsthatresultedfromusingthelargerarchitectures.(E.g.,itestimateda15natdifferencebetweenGMMN-10andGMMN-50,eventhoughthetruedifferencewas247natsasestimatedbyAIS.)Thesedifferencesarealsohardtoobservesimplybylookingatsamples;forinstance,wewereunabletovisuallydistinguishthequalityofsamplesforGAN-10andGAN-50(seeFig.1),eventhoughtheirlog-likelihoodsdifferedbyalmost300natsonboththetrainingandtestsets.5.4.2MEASURINGTHEDEGREEOFOVERFITTINGOnequestionthatarisesinevaluationofdecoder-basedgenerativemodelsiswhethertheymemorizepartsofthetrainingdataset.Onecannottestthisbylookingonlyatmodelsamples.Thecommonlyreportednearest-neighborsfromthetrainingsetcanbemisleading(Theisetal.,2016),andinterpola-tioninthelatentspacebetweendifferentsamplescanbevisuallyappealing,butdoesnotprovideaquantitativemeasureofthedegreeofgeneralization.8PublishedasaconferencepaperatICLR20171002004006008001000number of Epochs200250300350400450500550600650Log-likelihoodGAN50 training curvesTrain AISValid AISTrain KDEValid KDE1002004006008001000number of Epochs40060080010001200Log-likelihoodVAE50 training curvesTrain AISValid AISTrain KDEValid KDETrain IWAEValid IWAE200040006000800010000number of Epochs200300400500600Log-likelihoodGMMN50 training curvesTrain AISValid AISTrain KDEValid KDE(a)GAN-50:LLDvs.Numepochs(b)VAE-50:LLDvs.Numepochs(c)GMMN-50:LLDvs.NumepochsFigure3:Trainingcurvesfor(a)GAN-50,(b)VAE-50,and(c)GMMN-10,asmeasuredbyAIS,KDE,and(ifapplicable)theIWAElowerbound.Allestimatesshownherearelowerbounds.In(c),thegapbetweentrainingandvalidationlog-likelihoodsisnotfairlysmall(seeTable2).Toanalyzethedegreeofoverﬁtting,Fig.3showstrainingcurvesforthreenetworksasmeasuredbyAIS,KDE,andtheIWAEbound.WeobservethatGAN-50’strainingandtestlog-likelihoodsarenearlyidenticalthroughouttraining,disconﬁrmingthehypothesisthatitwasmemorizingtrainingdata.BothGAN-50andGMMN-50overﬁtlessthanVAE-50.Wealsoobservedtwophenomenawhichcouldnotbemeasuredusingexistingtechniques.First,inthecaseofVAE-50,theIWAElowerboundstartstodeclineafter200epochs,whiletheAISestimatesholdsteady,suggestingitistherecognitionnetworkratherthanthegenerativenetworkwhichisoverﬁttingmost.Second,theGMMN-50trainingandvalidationerrorcontinuetoimproveat10,000epochs,eventhoughKDEerroneouslyindicatesthatperformancehasleveledoff.5.4.3HOWAPPROPRIATEISTHEOBSERVATIONMODEL?AppendixBaddressesthequestionsofwhetherthesphericalGaussianobservationmodelisagoodﬁtandwhetherthelog-likelihooddifferencescouldbeanartifactoftheobservationmodel.Weﬁndthatallofthemodelscanbesubstantiallyimprovedbyaccountingfornon-Gaussianity,butthatthiseffectisinsufﬁcienttoexplainthegapbetweentheVAEsandtheothermodels.5.4.4ARETHENETWORKSMISSINGMODES?ItwaspreviouslyobservedthatoneofthepotentialfailuremodesofBoltzmannmachinesistofailtogenerateoneormoremodesofadistributionortodrasticallymisallocateprobabilitymassbetweenmodes(Salakhutdinov&Murray,2008).Hereweanalyzethisfordecoder-basedmodels.First,weaskacoarse-grainedversionofthisquestion:dothenetworksallocateprobabilitymasscorrectlybetweenthe10digitclasses,andifnot,canthisexplainthedifferenceinlog-likelihoodscores?InFig.1,weseethatGAN-50’sdistributionofdigitclasseswasheavilyskewed:outof100samples,itgenerated37imagesof1’s,butonlyasingle2.Thisappearstobealargeeffect,butitdoesnotexplainthemagnitudeofthelog-likelihooddifferencefromVAEs.Inparticular,iftheallocationofdigitclasseswereoffbyafactorof10,thiseffectbyitselfcouldcostatmostlog10≈2.3natsoflog-likelihood.SinceVAE-50outperformedGAN-50by364nats,thiseffectcannotexplainthedifference.However,MNISThasmanyfactorsofvariabilitybeyondsimplythe10digitclasses.Inordertodeterminewhetheranyofthemodelsmissedmoreﬁne-grainedmodes,wevisualizedposteriorsamplesforeachmodelconditionedontrainingandtestimages.Inparticular,foreachimagexunderconsideration,weusedAIStoapproximatelysamplezfromtheposteriordistributionp(z|x),andthenranthedecoderonz.Whilethesesamplesareapproximate,Grosseetal.(2016)pointoutthattheBDMCgapalsoboundstheKLdivergenceofapproximatesamplesfromthetrueposterior.WiththeexceptionofGAN-50,ourBDMCgapswereontheorderof1nat,suggestingourapproximateposteriorsamplesarefairlyrepresentative.TheresultsareshowninFig.4.Furtherposteriorvisualizationsfordigitclass2(themostdifﬁcultforthemodelsweconsidered)areshowninAppendixC.BothVAEs’posteriorsamplesmatchtheobservationsalmostperfectly.(Weobservedafewpoorlyreconstructedexamplesonthetestset,butnotonthetrainingset.)TheGANsandGMMNsfailto9PublishedasaconferencepaperatICLR2017DataGAN10VAE10GMMN10GAN50VAE50GMMN50(a)Thevisualizationofposteriorof10trainingexamples(b)Thevisualizationofposteriorof10validationexamples(c)Thevisualizationofposteriorof10ex-amplesofdigit“2"oftrainingsetFigure4:(a)and(b)showvisualizationofposteriorsamplesof10training/validationexamples.(c)showsvisualizationofposteriorsamplesof10trainingexamplesofdigit“2".Eachcolumnof10digitscomesfromtruedataandthesixmodels.Theorderofvisualizationis:Truedata,GAN-10,VAE-10,GMMN-10,GAN-50,VAE-50,GMMN-50.reconstructsomeoftheexamplesonboththetrainingandvalidationsets,suggestingthattheyfailedtolearnsomemodesofthedistribution.ACKNOWLEDGMENTSWeliketothankYujiaLiforprovidinghisoriginalGMMNmodelandcodebase,andthankJimmyBaforadviceontrainingGANs.RuslanSalakhutdinovissupportedinpartbyDisneyandONRgrantN000141310721.WealsothankthedevelopersofLasagne(Battenbergetal.,2014)andTheano(Al-Rfouetal.,2016).REFERENCESRamiAl-Rfou,GuillaumeAlain,AmjadAlmahairi,andetal.Theano:Apythonframeworkforfastcomputationofmathematicalexpressions,2016.EricBattenberg,SanderDieleman,DanielNouri,EbenOlson,AäronvandenOord,ColinRaffel,JanSchlüter,andSørenKaaeSønderby.lasagne.https://github.com/Lasagne/Lasagne,2014.Y.Bengio,L.Yao,andK.Cho.Boundingthetestlog-likelihoodofgenerativemodels.arXiv:1311.6184,2013.WachaBounliphone,EugeneBelilovsky,MatthewB.Blaschko,IoannisAntonoglou,andArthurGretton.Atestofrelativesimilarityformodelselectioningenerativemodels.InICLR.2016.YuriBurda,RogerGrosse,andRuslanSalakhutdinov.Importanceweightedautoencoders.InICLR,2016.E.Denton,S.Chintala,A.Szlam,andR.Fergus.Deepgenerativeimagemodelsusingalaplacianpyramidofadversarialnetworks.InNIPS,2015.LaurentDinh,DavidKrueger,andYoshuaBengio.Nice:Non-linearindependentcomponentsestimation.arXivpreprintarXiv:1410.8516,2014.LaurentDinh,JaschaSohl-Dickstein,andSamyBengio.Densityestimationusingrealnvp.arXiv:1605.08803,2016.GintareKarolinaDziugaite,DanielM.Roy,andZoubinGhahramani.TraininggenerativeneuralnetworksviaMaximumMeanDiscrepancyoptimization.InUAI.2015.IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronCourville,andYoshuaBengio.Generativeadversarialnets.InZ.Ghahramani,M.Welling,C.Cortes,N.D.Lawrence,andK.Q.Weinberger(eds.),AdvancesinNeuralInformationProcess-ingSystems27,pp.2672–2680.CurranAssociates,Inc.,2014.URLhttp://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.10PublishedasaconferencepaperatICLR2017RogerGrosse,SiddharthAncha,andDanielM.Roy.MeasuringthereliabilityofMCMCinferencewithbidirectionalMonteCarlo.InNIPS,2016.RogerB.Grosse,ZoubinGhahramani,andRyanP.Adams.Sandwichingthemarginallikelihoodusingbidirectionalmontecarlo.arXivpreprintarXiv:1511.02543,2015.DanielJiwoongIm,ChrisDongjooKim,HuiJiang,andRolandMemisevic.Generatingimageswithrecurrentadversarialnetworks.arXivpreprintarXiv:1602.05110,2016.MarkR.Jerrum,LeslieG.Valiant,andVijayV.Vazirani.Randomgenerationofcombinatorialstructuresfromauniformdistribution.TheoreticalComputerScience,43:169–188,1986.DiederikP.KingmaandMaxWelling.Auto-encodingvariationalbayes.InICLR,2014.Y.LeCun,L.Bottou,Y.Bengio,andP.Haffner.Gradient-basedlearningappliedtodocumentrecognition.ProceedingsoftheIEEE,86(11):2278–2324,November1998.YujiaLiandKevinSwersky.Generativemomentmatchingnetworks.InInICML32,2015.D.J.C.MacKayandM.N.Gibbs.Densitynetworks.InJ.W.KayandD.M.Titterington(eds.),StatisticsandNeuralNetworks,pp.129–146.O.U.P.,1998.RadfordM.Neal.Annealedimportancesampling.StatisticsandComputing,11(2):125–139,April2001.ISSN0960-3174.doi:10.1023/A:1008923215028.URLhttp://dx.doi.org/10.1023/A:1008923215028.RadfordM.Neal.MCMCusingHamiltoniandynamics.HandbookofMarkovChainMonteCarlo,54:113–162,2010.EmanuelParzen.Onestimationofaprobabilitydensityfunctionandmode.TheAnnalsofMathe-maticalStatistics,33(3):pp.1065–1076,1962.ISSN00034851.URLhttp://www.jstor.org/stable/2237880.AlecRadford,LukeMetz,andSoumithChintala.Unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.InICLR,2016.Marc’AurelioRanzato,AlexKrizhevsky,andGeoffreyEHinton.Factored3-wayrestrictedBoltz-mannmachinesformodelingnaturalimages.InInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),pp.621–628,2010.DaniloJ.Rezende,ShakirMohamed,andDaanWierstra.Stochasticbackpropagationandapproxi-mateinferenceindeepgenerativemodels.InTonyJebaraandEricP.Xing(eds.),Proceedingsofthe31stInternationalConferenceonMachineLearning(ICML-14),pp.1278–1286.JMLRWorkshopandConferenceProceedings,2014.URLhttp://jmlr.org/proceedings/papers/v32/rezende14.pdf.RuslanSalakhutdinovandIainMurray.OnthequantitativeanalysisofDeepBeliefNetworks.InAndrewMcCallumandSamRoweis(eds.),Proceedingsofthe25thAnnualInternationalConferenceonMachineLearning(ICML2008),pp.872–879.Omnipress,2008.TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.Improvedtechniquesfortraininggans.InNIPS,2016.LucasTheis,AäronvandenOord,andMatthiasBethge.Anoteontheevaluationofgenerativemodels.InICLR,2016.DustinTran,RajeshRanganath,andDavidM.Blei.ThevariationalGaussianprocess.InICLR,2016.BenignoUria,IainMurray,andHugoLarochelle.RNADE:Thereal-valuedneuralautoregressivedensity-estimator.InAdvancesinNeuralInformationProcessingSystems26,pp.2175–2183.2013.URLhttp://www.benignouria.com/en/research/papers/Uria2013.pdf.AäronvandenOord,NalKalchbrenner,andKorayKavukcuoglu.Pixelrecurrentneuralnetworks.InICML,2016.11PublishedasaconferencepaperatICLR2017MartinJ.WainwrightandEeroP.Simoncelli.ScalemixturesofGaussiansandthestatisticsofnaturalimages.InNIPS,1999.Jun-YanZhu,PhilippKrähenbühl,EliShechtman,andAlexeiA.Efros.Generativevisualmanipula-tiononthenaturalimagemanifold.InProceedingsofEuropeanConferenceonComputerVision(ECCV),2016.ANETWORKARCHITECTURES/TRAININGA.1MODELSONCONTINUOUSMNISTThedecodershaveallfullyconnectedlayers,andthenumberofunitsineachlayerwas10–64–256–256-1024–784forthesmallerarchitectureand50–1024–1024–1024–784forthelargerone.Otherarchitecturedetailsaresummarizedasfollows.•ForGAN-10,weusedadiscriminatorwiththearchitecture784-512-256-1,whereeachlayeruseddropoutwithparameter0.5.ForGAN-50,weusedadiscriminatorwitharchitecture784-4096-4096-4096-4096-1.Allhiddenlayersuseddropoutwithparameter0.8.Allhiddenlayersinbothnetworksusedthetanhactivationfunction,andtheoutputlayersusedthelogisticfunction.•Thelargermodelusesanencoderofanarchitecture784-1024-1024-1024-100.Weadddropoutlayerbetweeneachhiddenlayer,withadropoutrateof0.2.Thesmallermodelusesanencoderofanarchitecture784-256-64-20.Generator’shiddenlayersusetanhactivationfunction,andtheoutputlayerusessigmoidunit.Encoder’shiddenlayersusetanhactivationfunction,andtheoutputlayeruseslinearactivation.•GMMN:ThehiddenlayersuseReLUactivationfunction,andtheoutputlayerusessigomidunit.FortrainingGAN/VAE,weuseourownimplementation.WeuseAdamforoptimization,andperformgridsearchoflearningratefrom{0.001,0.0001,0.00001}.FortrainingGMMN,wetaketheimplementationfromhttps://github.com/yujiali/gmmn.git.Followingtheimplementation,weuseSGDwithmomentumforoptimization,andperformgridsearchoflearningratefrom{0.1,0.5,1,2},withmomentum0.9.A.2MODELSONBINARIZEDMNISTItsdecoderhasthearchitecture50-200-200-784withalltanhhiddenlayersandsigmoidoutputlayer,anditsencoderissymmetricinarchitecture,withlinearoutputlayer.Wetaketheimplementationathttps://github.com/yburda/iwae.gitfortrainingtheIWAEmodel.TheIWAEboundwascomputedwith50samplesduringtraining.Wekeepallthehyperparameterchoicesthesameasintheimplementation.BHOWPROBLEMATICISTHEGAUSSIANOBSERVATIONMODEL?(Nats)TrainValidOptimalFixedImprovementOptimalFixedImprovementGAN-50711.405620.49890.907702.699623.49279.207GMMN-50655.807571.80384.004661.652594.61267.040GAN-10376.788318.94857.840368.585316.61451.971GMMN-10393.976345.17748.799371.325332.36038.965Table3:Optimalvariancevs.FixedvarianceInthissection,weconsiderwhetherthedifferenceinlog-likelihoodbetweenmodelscouldbeanartifactoftheGaussiannoisemodel(whichweknowtobeapoorﬁt).Inprinciple,theGaussiannoiseassumptioncouldbeunfairtotheGANsandGMMNs,becausetheVAEtrainingusesthecorrect12PublishedasaconferencepaperatICLR2017observationmodel,whiletheGANandGMMNobjectivesdonothaveanyparticularobservationmodelbuiltin.Todeterminethesizeofthiseffect,weevaluatedthemodelsunderadifferentregimewhere,insteadofchoosingaﬁxedvalueoftheobservationnoiseσonavalidationset,σwastunedindependentlyforeachexample.2Thisisnotapropergenerativemodel,butitcanbeviewedasanupperboundonthelog-likelihoodthatwouldbeachievablewithaheavy-tailedandradiallysymmetricnoisemodel.3ResultsareshowninTable3.Weseethatadaptingσforeachexampleresultsinalog-likelihoodimprovementbetween30and100natsforallofthenetworks.Ingeneral,theexampleswhichshowthelargestperformancejumpareimagesof1’s(whichprefersmallerσ)and2’s(whichpreferlargerσ).Thisisasigniﬁcanteffect,andsuggeststhatonecouldsigniﬁcantlyimprovethelog-likelihoodscoresbypickingabetterobservationmodel.However,thiseffectissmallerinmagnitudethanthedifferencesbetweenVAEandGAN/GMMNlog-likelihoods,soitfailstoexplainthelikelihooddifference.CPOSTERIORVISUALIZATIONOFDIGIT“2"Accordingtothelog-likelihoodevaluation,weﬁnddigit“2"isthehardestdigitformodelling.Inthissectionweinvestigatethequalityofmodelling“2"ofeachmodel.Werandomlysampledaﬁxedsetof100samplesofdigit“2"fromtrainingdataandcomparewhethermodelcapturethismode.Weshowtheplotsof“2"forGAN-10,GAN-50,VAE-10andtruedatainthefollowingﬁguresforillustration.WeseethatGAN-10failsatcapturingmanyinstancesofdigit“2"inthetrainingdata!Weseeinsteadofgenerating“2",ittriestogeneratedigit“1",“7"“9",“4",“8"fromreconstruction.GAN-50doesmuchbetter,itsreconstructionarealldigit“2"andthereisonlysomestyledifferencefromthetruedata.VAE-10totallydominatesthiscompetition,whereitperfectlyreconstructsallthesamplesofdigit“2".Weemphasizeifdirectlysamplingfromeachmodel,sampleslookvisuallyindistinguishable(seeFig.1),butwecanclearlyseedifferencesinposteriorsamples.2Wepickthebestvarianceparameteramong{0.005,0.01,0.015,0.02,0.025}foreachtraining/validationexampleswhenevaluatingGAN-50andGMMN-50and{0.015,0.02,0.025,0.03,0.035}whenevaluatingGAN-10andGMMN-10.3Inparticular,heavy-tailedradiallysymmetricdistributionscanbeviewedasGaussianscalemixtures(Wainwright&Simoncelli,1999).I.e.,onehasapriordistributiononσ(possiblylearned)andintegratesitoutforeachtestexample.Clearlytheprobabilityundersuchamixturecannotexceedthemaximumvaluewithrespecttoσ.13PublishedasaconferencepaperatICLR2017Figure5:Posteriorsamplesofdigit“2"forGAN-10.14PublishedasaconferencepaperatICLR2017Figure6:Posteriorsamplesofdigit“2"forGAN-50.15PublishedasaconferencepaperatICLR2017Figure7:Posteriorsamplesofdigit“2"forVAE-10.16PublishedasaconferencepaperatICLR2017Figure8:100digit“2"fromtrainingdata.17
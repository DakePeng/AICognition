6
1
0
2

v
o
N
0
1

]
I

A
.
s
c
[

2
v
9
7
7
2
0
.
1
1
6
1
:
v
i
X
r
a

UnderreviewasaconferencepaperatICLR2017RL2:FASTREINFORCEMENTLEARNINGVIASLOWREINFORCEMENTLEARNINGYanDuan†‡,JohnSchulman†‡,XiChen†‡,PeterL.Bartlett†,IlyaSutskever‡,PieterAbbeel†‡†UCBerkeley,DepartmentofElectricalEngineeringandComputerScience‡OpenAI{rocky,joschu,peter}@openai.com,peter@berkeley.edu,{ilyasu,pieter}@openai.comABSTRACTDeepreinforcementlearning(deepRL)hasbeensuccessfulinlearningsophis-ticatedbehaviorsautomatically;however,thelearningprocessrequiresahugenumberoftrials.Incontrast,animalscanlearnnewtasksinjustafewtrials,bene-ﬁtingfromtheirpriorknowledgeabouttheworld.Thispaperseekstobridgethisgap.Ratherthandesigninga“fast”reinforcementlearningalgorithm,weproposetorepresentitasarecurrentneuralnetwork(RNN)andlearnitfromdata.Inourproposedmethod,RL2,thealgorithmisencodedintheweightsoftheRNN,whicharelearnedslowlythroughageneral-purpose(“slow”)RLalgorithm.TheRNNreceivesallinformationatypicalRLalgorithmwouldreceive,includingob-servations,actions,rewards,andterminationﬂags;anditretainsitsstateacrossepisodesinagivenMarkovDecisionProcess(MDP).TheactivationsoftheRNNstorethestateofthe“fast”RLalgorithmonthecurrent(previouslyunseen)MDP.WeevaluateRL2experimentallyonbothsmall-scaleandlarge-scaleproblems.Onthesmall-scaleside,wetrainittosolverandomlygeneratedmulti-armedban-ditproblemsandﬁniteMDPs.AfterRL2istrained,itsperformanceonnewMDPsisclosetohuman-designedalgorithmswithoptimalityguarantees.Onthelarge-scaleside,wetestRL2onavision-basednavigationtaskandshowthatitscalesuptohigh-dimensionalproblems.1INTRODUCTIONInrecentyears,deepreinforcementlearninghasachievedmanyimpressiveresults,includingplayingAtarigamesfromrawpixels(Guoetal.,2014;Mnihetal.,2015;Schulmanetal.,2015),andacquiringadvancedmanipulationandlocomotionskills(Levineetal.,2016;Lillicrapetal.,2015;Watteretal.,2015;Heessetal.,2015;Schulmanetal.,2015;2016).However,manyofthesuccessescomeattheexpenseofhighsamplecomplexity.Forexample,thestate-of-the-artAtariresultsrequiretensofthousandsofepisodesofexperience(Mnihetal.,2015)pergame.Tomasteragame,onewouldneedtospendnearly40daysplayingitwithnorest.Incontrast,humansandanimalsarecapableoflearninganewtaskinaverysmallnumberoftrials.Continuingthepreviousexample,thehumanplayerinMnihetal.(2015)onlyneeded2hoursofexperiencebeforemasteringagame.Wearguethatthereasonforthissharpcontrastislargelyduetothelackofagoodprior,whichresultsinthesedeepRLagentsneedingtorebuildtheirknowledgeabouttheworldfromscratch.AlthoughBayesianreinforcementlearningprovidesasolidframeworkforincorporatingpriorknowledgeintothelearningprocess(Strens,2000;Ghavamzadehetal.,2015;Kolter&Ng,2009),exactcomputationoftheBayesianupdateisintractableinallbutthesimplestcases.Thus,practi-calreinforcementlearningalgorithmsoftenincorporateamixtureofBayesiananddomain-speciﬁcideastobringdownsamplecomplexityandcomputationalburden.Notableexamplesincludeguidedpolicysearchwithunknowndynamics(Levine&Abbeel,2014)andPILCO(Deisenroth&Ras-mussen,2011).Thesemethodscanlearnataskusingafewminutestoafewhoursofrealexperience,comparedtodaysorevenweeksrequiredbypreviousmethods(Schulmanetal.,2015;2016;Lilli-crapetal.,2015).However,thesemethodstendtomakeassumptionsabouttheenvironment(e.g.,instrumentationforaccesstothestateatlearningtime),orbecomecomputationallyintractableinhigh-dimensionalsettings(Wahlstr¨ometal.,2015).1 
 
 
 
 
 
UnderreviewasaconferencepaperatICLR2017Ratherthanhand-designingdomain-speciﬁcreinforcementlearningalgorithms,wetakeadifferentapproachinthispaper:weviewthelearningprocessoftheagentitselfasanobjective,whichcanbeoptimizedusingstandardreinforcementlearningalgorithms.TheobjectiveisaveragedacrossallpossibleMDPsaccordingtoaspeciﬁcdistribution,whichreﬂectsthepriorthatwewouldliketodistillintotheagent.Westructuretheagentasarecurrentneuralnetwork,whichreceivespastrewards,actions,andterminationﬂagsasinputsinadditiontothenormallyreceivedobservations.Furthermore,itsinternalstateispreservedacrossepisodes,sothatithasthecapacitytoperformlearninginitsownhiddenactivations.Thelearnedagentthusalsoactsasthelearningalgorithm,andcanadapttothetaskathandwhendeployed.Weevaluatethisapproachontwosetsofclassicalproblems,multi-armedbanditsandtabularMDPs.Theseproblemshavebeenextensivelystudied,andthereexistalgorithmsthatachieveasymptoti-callyoptimalperformance.Wedemonstratethatourmethod,namedRL2,canachieveperformancecomparablewiththesetheoreticallyjustiﬁedalgorithms.Next,weevaluateRL2onavision-basednavigationtaskimplementedusingtheViZDoomenvironment(Kempkaetal.,2016),showingthatRL2canalsoscaletohigh-dimensionalproblems.2METHOD2.1PRELIMINARIESWedeﬁneadiscrete-timeﬁnite-horizondiscountedMarkovdecisionprocess(MDP)byatupleM=(S,A,P,r,ρ0,γ,T),inwhichSisastateset,Aanactionset,P:S×A×S→R+atransitionprobabilitydistribution,r:S×A→[−Rmax,Rmax]aboundedrewardfunction,ρ0:S→R+aninitialstatedistribution,γ∈[0,1]adiscountfactor,andTthehorizon.Inpolicysearchmethods,wetypicallyoptimizeastochasticpolicyπθ:S×A→R+parametrizedbyθ.Theobjectiveistomaximizeitsexpecteddiscountedreturn,η(πθ)=Eτ[PTt=0γtr(st,at)],whereτ=(s0,a0,...)denotesthewholetrajectory,s0∼ρ0(s0),at∼πθ(at|st),andst+1∼P(st+1|st,at).2.2FORMULATIONWenowdescribeourformulation,whichcastslearninganRLalgorithmasareinforcementlearningproblem,andhencethenameRL2.WeassumeknowledgeofasetofMDPs,denotedbyM,andadistributionoverthem:ρM:M→R+.Weonlyneedtosamplefromthisdistribution.WeusentodenotethetotalnumberofepisodesallowedtospendwithaspeciﬁcMDP.WedeﬁneatrialtobesuchaseriesofepisodesofinteractionwithaﬁxedMDP.Episode 1Episode 2s0s1s2h0h1a0r0,d0h2h3s3a1r1,d1a2r2,d2s0s1s2h4h5a0r0,d0h6a1r1,d1AgentMDP 1Episode 1s0s1…h0h1a0r0,d0…a1AgentMDP 2………Trial 1Trial 2Figure1:Procedureofagent-environmentinteractionThisprocessofinteractionbetweenanagentandtheenvironmentisillustratedinFigure1.Here,eachtrialhappenstoconsistoftwoepisodes,hencen=2.Foreachtrial,aseparateMDPisdrawnfromρM,andforeachepisode,afreshs0isdrawnfromtheinitialstatedistributionspeciﬁctothecorrespondingMDP.Uponreceivinganactionatproducedbytheagent,theenvironmentcomputesrewardrt,stepsforward,andcomputesthenextstatest+1.Iftheepisodehasterminated,itsetsterminationﬂagdtto1,whichotherwisedefaultsto0.Together,thenextstatest+1,action2UnderreviewasaconferencepaperatICLR2017at,rewardrt,andterminationﬂagdt,areconcatenatedtoformtheinputtothepolicy1,which,conditionedonthehiddenstateht+1,generatesthenexthiddenstateht+2andactionat+1.Attheendofanepisode,thehiddenstateofthepolicyispreservedtothenextepisode,butnotpreservedbetweentrials.Theobjectiveunderthisformulationistomaximizetheexpectedtotaldiscountedrewardaccumu-latedduringasingletrialratherthanasingleepisode.Maximizingthisobjectiveisequivalenttominimizingthecumulativepseudo-regret(Bubeck&Cesa-Bianchi,2012).SincetheunderlyingMDPchangesacrosstrials,aslongasdifferentstrategiesarerequiredfordifferentMDPs,theagentmustactdifferentlyaccordingtoitsbeliefoverwhichMDPitiscurrentlyin.Hence,theagentisforcedtointegratealltheinformationithasreceived,includingpastactions,rewards,andtermi-nationﬂags,andadaptitsstrategycontinually.Hence,wehavesetupanend-to-endoptimizationprocess,wheretheagentisencouragedtolearna“fast”reinforcementlearningalgorithm.Forclarityofexposition,wehavedeﬁnedthe“inner”problem(ofwhichtheagentseesneachtrials)tobeanMDPratherthanaPOMDP.However,themethodcanalsobeappliedinthepartially-observedsettingwithoutanyconceptualchanges.Inthepartiallyobservedsetting,theagentisfacedwithasequenceofPOMDPs,anditreceivesanobservationotinsteadofstatestattimet.ThevisualnavigationexperimentinSection3.3,isactuallyaninstanceofthethisPOMDPsetting.2.3POLICYREPRESENTATIONWerepresentthepolicyasageneralrecurrentneuralnetwork.Eachtimestep,itreceivesthetuple(s,a,r,d)asinput,whichisembeddedusingafunctionφ(s,a,r,d)andprovidedasinputtoanRNN.ToalleviatethedifﬁcultyoftrainingRNNsduetovanishingandexplodinggradients(Bengioetal.,1994),weuseGatedRecurrentUnits(GRUs)(Choetal.,2014)whichhavebeendemonstratedtohavegoodempiricalperformance(Chungetal.,2014;J´ozefowiczetal.,2015).TheoutputoftheGRUisfedtoafullyconnectedlayerfollowedbyasoftmaxfunction,whichformsthedistributionoveractions.WehavealsoexperimentedwithalternativearchitectureswhichexplicitlyresetpartofthehiddenstateeachepisodeofthesampledMDP,butwedidnotﬁndanyimprovementoverthesimplearchi-tecturedescribedabove.2.4POLICYOPTIMIZATIONAfterformulatingthetaskasareinforcementlearningproblem,wecanreadilyusestandardoff-the-shelfRLalgorithmstooptimizethepolicy.Weuseaﬁrst-orderimplementationofTrustRegionPolicyOptimization(TRPO)(Schulmanetal.,2015),becauseofitsexcellentempiricalperfor-mance,andbecauseitdoesnotrequireexcessivehyperparametertuning.Formoredetails,wereferthereadertotheoriginalpaper.Toreducevarianceinthestochasticgradientestimation,weuseabaselinewhichisalsorepresentedasanRNNusingGRUsasbuildingblocks.WeoptionallyapplyGeneralizedAdvantageEstimation(GAE)(Schulmanetal.,2016)tofurtherreducethevariance.3EVALUATIONWedesignedexperimentstoanswerthefollowingquestions:•CanRL2learnalgorithmsthatachievegoodperformanceonMDPclasseswithspecialstructure,relativetoexistingalgorithmstailoredtothisstructurethathavebeenproposedintheliterature?•CanRL2scaletohigh-dimensionaltasks?Fortheﬁrstquestion,weevaluateRL2ontwosetsoftasks,multi-armedbandits(MAB)andtabularMDPs.Theseproblemshavebeenstudiedextensivelyinthereinforcementlearningliterature,andthisbodyofworkincludesalgorithmswithguaranteesofasymptoticoptimality.Wedemonstratethatourapproachachievescomparableperformancetothesetheoreticallyjustiﬁedalgorithms.1Tomakesurethattheinputshaveaconsistentdimension,weuseplaceholdervaluesfortheinitialinputtothepolicy.3UnderreviewasaconferencepaperatICLR2017Forthesecondquestion,weevaluateRL2onavision-basednavigationtask.Ourexperimentsshowthatthelearnedpolicymakeseffectiveuseofthelearnedvisualinformationandalsoshort-terminformationacquiredfrompreviousepisodes.3.1MULTI-ARMEDBANDITSMulti-armedbanditproblemsareasubsetofMDPswheretheagent’senvironmentisstateless.Speciﬁcally,therearekarms(actions),andateverytimestep,theagentpullsoneofthearms,sayi,andreceivesarewarddrawnfromanunknowndistribution:ourexperimentstakeeacharmtobeaBernoullidistributionwithparameterpi.Thegoalistomaximizethetotalrewardobtainedoveraﬁxednumberoftimesteps.Thekeychallengeisbalancingexplorationandexploitation—“exploring”eacharmenoughtimestoestimateitsdistribution(pi),buteventuallyswitchingoverto“exploitation”ofthebestarm.Despitethesimplicityofmulti-armbanditproblems,theirstudyhasledtoarichtheoryandacollectionofalgorithmswithoptimalityguarantees.UsingRL2,wecantrainanRNNpolicytosolvebanditproblemsbytrainingitonagivendistributionρM.Ifthelearningissuccessful,theresultingpolicyshouldbeabletoperformcompetitivelywiththetheoreticallyoptimalalgorithms.Werandomlygeneratedbanditproblemsbysamplingeachparameterpifromtheuniformdistributionon[0,1].AftertrainingtheRNNpolicywithRL2,wecompareditagainstthefollowingstrategies:•Random:thisisabaselinestrategy,wheretheagentpullsarandomarmeachtime.•Gittinsindex(Gittins,1979):thismethodgivestheBayesoptimalsolutioninthedis-countedinﬁnite-horizoncase,bycomputinganindexseparatelyforeacharm,andtakingthearmwiththelargestindex.Whilethisworkshowsitissufﬁcienttoindependentlycom-puteanindexforeacharm(henceavoidingcombinatorialexplosionwiththenumberofarms),itdoesn’tshowhowtotractablycomputetheseindividualindicesexactly.Wefol-lowthepracticalapproximationsdescribedinGittinsetal.(2011),Chakravorty&Mahajan(2013),andWhittle(1982),andchoosethebest-performingapproximationforeachsetup.•UCB1(Auer,2002):thismethodestimatesanupper-conﬁdencebound,andpullsthearmwiththelargestvalueofucbi(t)=ˆµi(t−1)+cq2logtTi(t−1),whereˆµi(t−1)istheestimatedmeanparameterfortheitharm,Ti(t−1)isthenumberoftimestheitharmhasbeenpulled,andcisatunablehyperparameter(Audibert&Munos,2011).Weinitializethestatisticswithexactlyonesuccessandonefailure,whichcorrespondstoaBeta(1,1)prior.•Thompsonsampling(TS)(Thompson,1933):thisisasimplemethodwhich,ateachtimestep,samplesalistofarmmeansfromtheposteriordistribution,andchoosethebestarmaccordingtothissample.IthasbeendemonstratedtocomparefavorablytoUCB1empir-ically(Chapelle&Li,2011).Wealsoexperimentwithanoptimisticvariant(OTS)(Mayetal.,2012),whichsamplesNtimesfromtheposterior,andtakestheonewiththehighestprobability.•(cid:15)-Greedy:inthisstrategy,theagentchoosesthearmwiththebestempiricalmeanwithprobability1−(cid:15),andchoosesarandomarmwithprobability(cid:15).Weusethesameinitial-izationasUCB1.•Greedy:thisisaspecialcaseof(cid:15)-Greedywith(cid:15)=0.TheBayesianmethods,GittinsindexandThompsonsampling,takeadvantageofthedistributionρM;andweprovidethesemethodswiththetruedistribution.Foreachmethodwithhyperparame-ters,wemaximizethescorewithaseparategridsearchforeachoftheexperimentalsettings.ThehyperparametersusedforTRPOareshownintheappendix.TheresultsaresummarizedinTable1.LearningcurvesforvarioussettingsareshowninFigure2.Weobservethatourapproachachievesperformancethatisalmostasgoodasthethereferencemeth-ods,whichwere(human)designedspeciﬁcallytoperformwellonmulti-armedbanditproblems.Itisworthnotingthatthepublishedalgorithmsaremostlydesignedtominimizeasymptoticregret(ratherthanﬁnitehorizonregret),hencetheretendstobealittlebitofroomtooutperformthemintheﬁnitehorizonsettings.4UnderreviewasaconferencepaperatICLR2017Table1:MABResults.Eachgridcellrecordsthetotalrewardaveragedover1000differentinstancesofthebanditproblem.Weconsiderk∈{5,10,50}banditsandn∈{10,100,500}episodesofinteraction.Wehighlightthebest-performingalgorithmsineachsetupaccordingtothecomputedmean,andwealsohighlighttheotheralgorithmsinthatrowwhoseperformanceisnotsigniﬁcantlydifferentfromthebestone(determinedbyaone-sidedt-testwithp=0.05).SetupRandomGittinsTSOTSUCB1(cid:15)-GreedyGreedyRL2n=10,k=55.06.65.76.56.76.66.66.7n=10,k=105.06.65.56.26.76.66.66.7n=10,k=505.16.55.25.56.66.56.56.8n=100,k=549.978.374.777.978.075.474.878.7n=100,k=1049.982.876.781.482.477.477.183.5n=100,k=5049.885.264.567.784.378.378.084.9n=500,k=5249.8405.8402.0406.7405.8388.2380.6401.6n=500,k=10249.0437.8429.5438.9437.1408.0395.0432.5n=500,k=50249.6463.7427.2437.6457.6413.6402.8438.90300Iteration01Normalized total rewardk = 5k = 10k = 50Gittins(a)n=100600Iteration01Normalized total rewardk = 5k = 10k = 50Gittins(b)n=1000600Iteration01Normalized total rewardk = 5k = 10k = 50Gittins(c)n=500Figure2:RL2learningcurvesformulti-armedbandits.PerformanceisnormalizedsuchthatGittinsindexscores1,andrandompolicyscores0.WeobservethatthereisanoticeablegapbetweenGittinsindexandRL2inthemostchallengingscenario,with50armsand500episodes.Thisraisesthequestionwhetherbetterarchitecturesorbetter(slow)RLalgorithmsshouldbeexplored.Todeterminethebottleneck,wetrainedthesamepolicyarchitectureusingsupervisedlearning,usingthetrajectoriesgeneratedbytheGittinsindexapproachastrainingdata.Wefoundthatthelearnedpolicy,whenexecutedintestdomains,achievedthesamelevelofperformanceastheGittinsindexapproach,suggestingthatthereisroomforimprovementbyusingbetterRLalgorithms.3.2TABULARMDPSThebanditproblemprovidesanaturalandsimplesettingtoinvestigatewhetherthepolicylearnstotradeoffbetweenexplorationandexploitation.However,theproblemitselfinvolvesnosequen-tialdecisionmaking,anddoesnotfullycharacterizethechallengesinsolvingMDPs.Hence,weperformfurtherexperimentsusingrandomlygeneratedtabularMDPs,wherethereisaﬁnitenum-berofpossiblestatesandactions—smallenoughthatthetransitionprobabilitydistributioncanbeexplicitlygivenasatable.Wecompareourapproachwiththefollowingmethods:•Random:theagentchoosesanactionuniformlyatrandomforeachtimestep;•PSRL(Strens,2000;Osbandetal.,2013):thisisadirectgeneralizationofThompsonsam-plingtoMDPs,whereatthebeginningofeachepisode,wesampleanMDPfromthepos-teriordistribution,andtakeactionsaccordingtotheoptimalpolicyfortheentireepisode.Similarly,weincludeanoptimisticvariant(OPSRL),whichhasalsobeenexploredinOs-band&VanRoy(2016).•BEB(Kolter&Ng,2009):thisisamodel-basedoptimisticalgorithmthataddsanexplo-rationbonusto(thusfar)infrequentlyvisitedstatesandactions.5UnderreviewasaconferencepaperatICLR2017•UCRL2(Jakschetal.,2010):thisalgorithmcomputes,ateachiteration,theoptimalpol-icyagainstanoptimisticMDPunderthecurrentbelief,usinganextendedvalueiterationprocedure.•(cid:15)-Greedy:thisalgorithmtakesactionsoptimalagainsttheMAPestimateaccordingtothecurrentposterior,whichisupdatedonceperepisode.•Greedy:aspecialcaseof(cid:15)-Greedywith(cid:15)=0.Table2:RandomMDPResultsSetupRandomPSRLOPSRLUCRL2BEB(cid:15)-GreedyGreedyRL2n=10100.1138.1144.1146.6150.2132.8134.8156.2n=25250.2408.8425.2424.1427.8377.3368.8445.7n=50499.7904.4930.7918.9917.8823.3769.3936.1n=75749.91417.11449.21427.61422.61293.91172.91428.8n=100999.41939.51973.91942.11935.11778.21578.51913.7ThedistributionoverMDPsisconstructedwith|S|=10,|A|=5.TherewardsfollowaGaus-siandistributionwithunitvariance,andthemeanparametersaresampledindependentlyfromNormal(1,1).ThetransitionsaresampledfromaﬂatDirichletdistribution.ThisconstructionmatchesthecommonlyusedpriorinBayesianRLmethods.WesetthehorizonforeachepisodetobeT=10,andanepisodealwaysstartsontheﬁrststate.010005000Iteration01Normalized total rewardn = 10n = 25n = 50n = 75n = 100OPSRLFigure3:RL2learningcurvesfortabularMDPs.PerformanceisnormalizedsuchthatOPSRLscores1,andrandompolicyscores0.TheresultsaresummarizedinTable2,andthelearningcurvesareshowninFigure3.Wefollowthesameevaluationprocedureasinthebanditcase.Weexperimentwithn∈{10,25,50,75,100}.Forfewerepisodes,ourapproachsurprisinglyoutperformsexistingmethodsbyalargemargin.Theadvantageisreversedasnincreases,suggestingthatthereinforcementlearningproblemintheouterloopbecomesmorechallengingtosolve.Wethinkthattheadvantageforsmallncomesfromtheneedformoreaggressiveexploitation:sincethereare140degreesoffreedomtoestimateinordertocharacterizetheMDP,andbythe10thepisode,wewillnothaveenoughsamplestoformagoodestimateoftheentiredynamics.BydirectlyoptimizingtheRNNinthissetting,ourapproachshouldbeabletocopewiththisshortageofsamples,anddecidestoexploitsoonercomparedtothereferencealgorithms.3.3VISUALNAVIGATIONTheprevioustwotasksbothonlyinvolveverylow-dimensionalstatespaces.Toevaluatethefea-sibilityofscalingupRL2,wefurtherexperimentwithachallengingvision-basedtask,wherethe6UnderreviewasaconferencepaperatICLR2017agentisaskedtonavigatearandomlygeneratedmazetoﬁndarandomlyplacedtarget2.Theagentreceivesa+1rewardwhenitreachesthetarget,−0.001whenithitsthewall,and−0.04pertimesteptoencourageittoreachtargetsfaster.Itcaninteractwiththemazeformultipleepisodes,dur-ingwhichthemazestructureandtargetpositionareheldﬁxed.Theoptimalstrategyistoexplorethemazeefﬁcientlyduringtheﬁrstepisode,andafterlocatingthetarget,actoptimallyagainstthecurrentmazeandtargetbasedonthecollectedinformation.AnillustrationofthetaskisgiveninFigure4.(a)Sampleobservation(b)Layoutofthe5×5mazein(a)(c)Layoutofa9×9mazeFigure4:Visualnavigation.Thetargetblockisshowninred,andoccupiesanentiregridinthemazelayout.Visualnavigationaloneisachallengingtaskforreinforcementlearning.Theagentonlyreceivesverysparserewardsduringtraining,anddoesnothavetheprimitivesforefﬁcientexplorationatthebeginningoftraining.Italsoneedstomakeefﬁcientuseofmemorytodecidehowitshouldexplorethespace,withoutforgettingaboutwhereithasalreadyexplored.Previously,Ohetal.(2016)havestudiedsimilarvision-basednavigationtasksinMinecraft.However,theyusehigher-levelactionsforefﬁcientnavigation.Similarhigh-levelactionsinourtaskwouldeachrequirearound5low-levelactionscombinedintherightway.Incontrast,ourRL2agentneedstolearnthesehigher-levelactionsfromscratch.Weuseasimpletrainingsetup,whereweusesmallmazesofsize5×5,with2episodesofinterac-tion,eachwithhorizonupto250.Herethesizeofthemazeismeasuredbythenumberofgridcellsalongeachwallinadiscreterepresentationofthemaze.Duringeachtrial,wesample1outof1000randomlygeneratedconﬁgurationsofmaplayoutandtargetpositions.Duringtesting,weevaluateon1000separatelygeneratedconﬁgurations.Inaddition,wealsostudyitsextrapolationbehavioralongtwoaxes,by(1)testingonlargemazesofsize9×9(seeFigure4c)and(2)runningtheagentforupto5episodesinbothsmallandlargemazes.Forthelargemaze,wealsoincreasethehorizonperepisodeby4xduetotheincreasedsizeofthemaze.Table3:Resultsforvisualnavigation.ThesemetricsarecomputedusingthebestrunamongallrunsshowninFigure5.In3c,wemeasuretheproportionofmazeswherethetrajectorylengthinthesecondepisodedoesnotexceedthetrajectorylengthintheﬁrstepisode.(a)AveragelengthofsuccessfultrajectoriesEpisodeSmallLarge152.4±1.3180.1±6.0239.1±0.9151.8±5.9342.6±1.0169.3±6.3443.5±1.1162.3±6.4543.9±1.1169.3±6.5(b)%SuccessEpisodeSmallLarge199.3%97.1%299.6%96.7%399.7%95.8%499.4%95.6%599.6%96.1%(c)%ImprovedSmallLarge91.7%71.4%2Videosforthetaskareavailableathttps://goo.gl/rDDBpb.7UnderreviewasaconferencepaperatICLR20170500100015002000250030003500Iteration1614121086420Total rewardFigure5:RL2learningcurvesforvisualnavigation.Eachcurveshowsadifferentrandominitial-izationoftheRNNweights.Performancevariesgreatlyacrossdifferentinitializations.TheresultsaresummarizedinTable3,andthelearningcurvesareshowninFigure5.Weobservethatthereisasigniﬁcantreductionintrajectorylengthsbetweentheﬁrsttwoepisodesinboththesmallerandlargermazes,suggestingthattheagenthaslearnedhowtouseinformationfrompastepisodes.Italsoachievesreasonableextrapolationbehaviorinfurtherepisodesbymaintainingitsperformance,althoughthereisasmalldropintherateofsuccessinthelargermazes.Wealsoobservethatonlargermazes,theratioofimprovedtrajectoriesislower,likelybecausetheagenthasnotlearnedhowtoactoptimallyinthelargermazes.Still,evenonthesmallmazes,theagentdoesnotlearntoperfectlyreusepriorinformation.Anillustrationoftheagent’sbehaviorisshowninFigure6.Theintendedbehavior,whichoccursmostfrequently,asshownin6aand6b,isthattheagentshouldrememberthetarget’slocation,andutilizeittoactoptimallyinthesecondepisode.However,occasionallytheagentforgetsaboutwherethetargetwas,andcontinuestoexploreinthesecondepisode,asshownin6cand6d.Webelievethatbetterreinforcementlearningtechniquesusedastheouter-loopalgorithmwillimprovetheseresultsinthefuture.(a)Goodbehavior,1stepisode(b)Goodbehavior,2ndepisode(c)Badbehavior,1stepisode(d)Badbehavior,2ndepisodeFigure6:Visualizationoftheagent’sbehavior.Ineachscenario,theagentstartsatthecenteroftheblueblock,andthegoalistoreachanywhereintheredblock.4RELATEDWORKTheconceptofusingpriorexperiencetospeedupreinforcementlearningalgorithmshasbeenex-ploredinthepastinvariousforms.Earlierstudieshaveinvestigatedautomatictuningofhyper-parameters,suchaslearningrateandtemperature(Ishiietal.,2002;Schweighofer&Doya,2003),asaformofmeta-learning.Wilsonetal.(2007)usehierarchicalBayesianmethodstomaintainaposterioroverpossiblemodelsofdynamics,andapplyoptimisticThompsonsamplingaccordingtotheposterior.Manyworksinhierarchicalreinforcementlearningproposetoextractreusableskillsfromprevioustaskstospeedupexplorationinnewtasks(Singh,1992;Perkinsetal.,1999).WereferthereadertoTaylor&Stone(2009)foramorethoroughsurveyonthemulti-taskandtransferlearningaspects.8UnderreviewasaconferencepaperatICLR2017Morerecently,Fuetal.(2015)proposeamodel-basedapproachontopofiLQGwithunknowndynamics(Levine&Abbeel,2014),whichusessamplescollectedfromprevioustaskstobuildaneuralnetworkpriorforthedynamics,andcanperformone-shotlearningonnew,butrelatedtasksthankstoreducedsamplecomplexity.Therehasbeenagrowinginterestinusingdeepneuralnetworksformulti-tasklearningandtransferlearning(Parisottoetal.,2015;Rusuetal.,2015;2016a;Devinetal.,2016;Rusuetal.,2016b).Inthebroadercontextofmachinelearning,therehasbeenalotofinterestinone-shotlearningforobjectclassiﬁcation(Vilalta&Drissi,2002;Fei-Feietal.,2006;Larochelleetal.,2008;Lakeetal.,2011;Koch,2015).Ourworkdrawsinspirationfromaparticularlineofwork(Youngeretal.,2001;Santoroetal.,2016;Vinyalsetal.,2016),whichformulatesmeta-learningasanoptimizationproblem,andcanthusbeoptimizedend-to-endviagradientdescent.Whiletheseworkappliestothesupervisedlearningsetting,ourworkappliesinthemoregeneralreinforcementlearningsetting.Althoughthereinforcementlearningsettingismorechallenging,theresultingbehaviorisfarricher:ouragentmustnotonlylearntoexploitexistinginformation,butalsolearntoexplore,aproblemthatisusuallynotafactorinsupervisedlearning.Anotherlineofwork(Hochreiteretal.,2001;Youngeretal.,2001;Andrychowiczetal.,2016;Li&Malik,2016)studiesmeta-learningovertheoptimizationprocess.There,themeta-learnermakesexplicitupdatestoaparametrizedmodel.Incomparison,wedonotuseadirectlyparametrizedpolicy;instead,therecurrentneuralnetworkagentactsasthemeta-learnerandtheresultingpolicysimultaneously.OurformulationessentiallyconstructsapartiallyobservableMDP(POMDP)whichissolvedintheouterloop,wheretheunderlyingMDPisunobservedbytheagent.ThisreductionofanunknownMDPtoaPOMDPcanbetracedbacktodualcontroltheory(Feldbaum,1960),where“dual”referstothefactthatoneiscontrollingboththestateandthestateestimate.Feldbaumpointedoutthatthesolutioncaninprinciplebecomputedwithdynamicprogramming,butdoingsoisusuallyim-practical.POMDPswithsuchstructurehavealsobeenstudiedunderthename“mixedobservabilityMDPs”(Ongetal.,2010).However,themethodproposedtheresuffersfromtheusualchallengesofsolvingPOMDPsinhighdimensions.5DISCUSSIONThispapersuggestsadifferentapproachfordesigningbetterreinforcementlearningalgorithms:insteadofactingasthedesignersourselves,learnthealgorithmend-to-endusingstandardrein-forcementlearningtechniques.Thatis,the“fast”RLalgorithmisacomputationwhosestateisstoredintheRNNactivations,andtheRNN’sweightsarelearnedbyageneral-purpose“slow”re-inforcementlearningalgorithm.Ourmethod,RL2,hasdemonstratedcompetencecomparablewiththeoreticallyoptimalalgorithmsinsmall-scalesettings.Wehavefurthershownitspotentialtoscaletohigh-dimensionaltasks.Intheexperiments,wehaveidentiﬁedopportunitiestoimproveuponRL2:theouter-loopreinforce-mentlearningalgorithmwasshowntobeanimmediatebottleneck,andwebelievethatforsettingswithextremelylonghorizons,betterarchitecturemayalsoberequiredforthepolicy.Althoughwehaveusedgenericmethodsandarchitecturesfortheouter-loopalgorithmandthepolicy,doingthisalsoignorestheunderlyingepisodicstructure.Weexpectalgorithmsandpolicyarchitecturesthatexploittheproblemstructuretosigniﬁcantlyboosttheperformance.ACKNOWLEDGMENTSWewouldliketothankourcolleaguesatBerkeleyandOpenAIforinsightfuldiscussions.ThisresearchwasfundedinpartbyONRthroughaPECASEaward.YanDuanwasalsosupportedbyaBerkeleyAIResearchlabFellowshipandaHuaweiFellowship.XiChenwasalsosupportedbyaBerkeleyAIResearchlabFellowship.WegratefullyacknowledgethesupportoftheNSFthroughgrantIIS-1619362andoftheARCthroughaLaureateFellowship(FL110100281)andthroughtheARCCentreofExcellenceforMathematicalandStatisticalFrontiers.REFERENCESMarcinAndrychowicz,MishaDenil,SergioGomez,MatthewWHoffman,DavidPfau,TomSchaul,andNandodeFreitas.Learningtolearnbygradientdescentbygradientdescent.arXivpreprint9UnderreviewasaconferencepaperatICLR2017arXiv:1606.04474,2016.Jean-YvesAudibertandR´emiMunos.Introductiontobandits:Algorithmsandtheory.ICMLTutorialonbandits,2011.PeterAuer.Usingconﬁdenceboundsforexploitation-explorationtrade-offs.JournalofMachineLearningResearch,3(Nov):397–422,2002.YoshuaBengio,PatriceSimard,andPaoloFrasconi.Learninglong-termdependencieswithgradientdescentisdifﬁcult.IEEEtransactionsonneuralnetworks,5(2):157–166,1994.S´ebastienBubeckandNicoloCesa-Bianchi.Regretanalysisofstochasticandnonstochasticmulti-armedbanditproblems.arXivpreprintarXiv:1204.5721,2012.JhelumChakravortyandAdityaMahajan.Multi-armedbandits,gittinsindex,anditscalculation.MethodsandApplicationsofStatisticsinClinicalTrials:Planning,Analysis,andInferentialMethods,2:416–435,2013.OlivierChapelleandLihongLi.Anempiricalevaluationofthompsonsampling.InAdvancesinneuralinformationprocessingsystems,pp.2249–2257,2011.KyunghyunCho,BartVanMerri¨enboer,DzmitryBahdanau,andYoshuaBengio.Onthepropertiesofneuralmachinetranslation:Encoder-decoderapproaches.arXivpreprintarXiv:1409.1259,2014.JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio.Empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodeling.arXivpreprintarXiv:1412.3555,2014.MarcDeisenrothandCarlERasmussen.Pilco:Amodel-basedanddata-efﬁcientapproachtopolicysearch.InProceedingsofthe28thInternationalConferenceonmachinelearning(ICML-11),pp.465–472,2011.ColineDevin,AbhishekGupta,TrevorDarrell,PieterAbbeel,andSergeyLevine.Learningmodularneuralnetworkpoliciesformulti-taskandmulti-robottransfer.arXivpreprintarXiv:1609.07088,2016.LiFei-Fei,RobFergus,andPietroPerona.One-shotlearningofobjectcategories.IEEEtransactionsonpatternanalysisandmachineintelligence,28(4):594–611,2006.AAFeldbaum.Dualcontroltheory.i.AvtomatikaiTelemekhanika,21(9):1240–1249,1960.JustinFu,SergeyLevine,andPieterAbbeel.One-shotlearningofmanipulationskillswithonlinedynamicsadaptationandneuralnetworkpriors.arXivpreprintarXiv:1509.06841,2015.MohammadGhavamzadeh,ShieMannor,JoellePineau,AvivTamar,etal.Bayesianreinforcementlearning:asurvey.WorldScientiﬁc,2015.JohnGittins,KevinGlazebrook,andRichardWeber.Multi-armedbanditallocationindices.JohnWiley&Sons,2011.JohnCGittins.Banditprocessesanddynamicallocationindices.JournaloftheRoyalStatisticalSociety.SeriesB(Methodological),pp.148–177,1979.XiaoxiaoGuo,SatinderSingh,HonglakLee,RichardLLewis,andXiaoshiWang.Deeplearningforreal-timeatarigameplayusingofﬂinemonte-carlotreesearchplanning.InAdvancesinneuralinformationprocessingsystems,pp.3338–3346,2014.NicolasHeess,GregoryWayne,DavidSilver,TimLillicrap,TomErez,andYuvalTassa.Learningcontinuouscontrolpoliciesbystochasticvaluegradients.InAdvancesinNeuralInformationProcessingSystems,pp.2944–2952,2015.SeppHochreiter,AStevenYounger,andPeterRConwell.Learningtolearnusinggradientdescent.InInternationalConferenceonArtiﬁcialNeuralNetworks,pp.87–94.Springer,2001.10UnderreviewasaconferencepaperatICLR2017ShinIshii,WakoYoshida,andJunichiroYoshimoto.Controlofexploitation–explorationmeta-parameterinreinforcementlearning.Neuralnetworks,15(4):665–687,2002.ThomasJaksch,RonaldOrtner,andPeterAuer.Near-optimalregretboundsforreinforcementlearning.JournalofMachineLearningResearch,11(Apr):1563–1600,2010.RafalJ´ozefowicz,WojciechZaremba,andIlyaSutskever.Anempiricalexplorationofrecur-rentnetworkarchitectures.InProceedingsofthe32ndInternationalConferenceonMachineLearning,ICML2015,Lille,France,6-11July2015,pp.2342–2350,2015.URLhttp://jmlr.org/proceedings/papers/v37/jozefowicz15.html.MichałKempka,MarekWydmuch,GrzegorzRunc,JakubToczek,andWojciechJa´skowski.Viz-doom:Adoom-basedairesearchplatformforvisualreinforcementlearning.arXivpreprintarXiv:1605.02097,2016.GregoryKoch.Siameseneuralnetworksforone-shotimagerecognition.PhDthesis,UniversityofToronto,2015.JZicoKolterandAndrewYNg.Near-bayesianexplorationinpolynomialtime.InProceedingsofthe26thAnnualInternationalConferenceonMachineLearning,pp.513–520.ACM,2009.BrendenMLake,RuslanSalakhutdinov,JasonGross,andJoshuaBTenenbaum.Oneshotlearningofsimplevisualconcepts.InProceedingsofthe33rdAnnualConferenceoftheCognitiveScienceSociety,volume172,pp.2,2011.HugoLarochelle,DumitruErhan,andYoshuaBengio.Zero-datalearningofnewtasks.InAAAI,volume1,pp.3,2008.SergeyLevineandPieterAbbeel.Learningneuralnetworkpolicieswithguidedpolicysearchunderunknowndynamics.InAdvancesinNeuralInformationProcessingSystems,pp.1071–1079,2014.SergeyLevine,ChelseaFinn,TrevorDarrell,andPieterAbbeel.End-to-endtrainingofdeepvisuo-motorpolicies.JournalofMachineLearningResearch,17(39):1–40,2016.KeLiandJitendraMalik.Learningtooptimize.arXivpreprintarXiv:1606.01885,2016.TimothyPLillicrap,JonathanJHunt,AlexanderPritzel,NicolasHeess,TomErez,YuvalTassa,DavidSilver,andDaanWierstra.Continuouscontrolwithdeepreinforcementlearning.arXivpreprintarXiv:1509.02971,2015.BenedictCMay,NathanKorda,AnthonyLee,andDavidSLeslie.Optimisticbayesiansamplingincontextual-banditproblems.JournalofMachineLearningResearch,13(Jun):2069–2106,2012.VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBelle-mare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533,2015.JunhyukOh,ValliappaChockalingam,SatinderSingh,andHonglakLee.Controlofmemory,activeperception,andactioninminecraft.arXivpreprintarXiv:1605.09128,2016.SylvieCWOng,ShaoWeiPng,DavidHsu,andWeeSunLee.Planningunderuncertaintyforrobotictaskswithmixedobservability.TheInternationalJournalofRoboticsResearch,29(8):1053–1068,2010.IanOsbandandBenjaminVanRoy.Whyisposteriorsamplingbetterthanoptimismforreinforce-mentlearning.arXivpreprintarXiv:1607.00215,2016.IanOsband,DanRusso,andBenjaminVanRoy.(more)efﬁcientreinforcementlearningviaposte-riorsampling.InAdvancesinNeuralInformationProcessingSystems,pp.3003–3011,2013.EmilioParisotto,JimmyLeiBa,andRuslanSalakhutdinov.Actor-mimic:Deepmultitaskandtransferreinforcementlearning.arXivpreprintarXiv:1511.06342,2015.11UnderreviewasaconferencepaperatICLR2017TheodoreJPerkins,DoinaPrecup,etal.Usingoptionsforknowledgetransferinreinforcementlearning.UniversityofMassachusetts,Amherst,MA,USA,Tech.Rep,1999.AndreiARusu,SergioGomezColmenarejo,CaglarGulcehre,GuillaumeDesjardins,JamesKirk-patrick,RazvanPascanu,VolodymyrMnih,KorayKavukcuoglu,andRaiaHadsell.Policydistil-lation.arXivpreprintarXiv:1511.06295,2015.AndreiARusu,NeilCRabinowitz,GuillaumeDesjardins,HubertSoyer,JamesKirkpatrick,KorayKavukcuoglu,RazvanPascanu,andRaiaHadsell.Progressiveneuralnetworks.arXivpreprintarXiv:1606.04671,2016a.AndreiARusu,MatejVecerik,ThomasRoth¨orl,NicolasHeess,RazvanPascanu,andRaiaHadsell.Sim-to-realrobotlearningfrompixelswithprogressivenets.arXivpreprintarXiv:1610.04286,2016b.AdamSantoro,SergeyBartunov,MatthewBotvinick,DaanWierstra,andTimothyLillicrap.One-shotlearningwithmemory-augmentedneuralnetworks.arXivpreprintarXiv:1605.06065,2016.JohnSchulman,SergeyLevine,PhilippMoritz,MichaelIJordan,andPieterAbbeel.Trustregionpolicyoptimization.CoRR,abs/1502.05477,2015.JohnSchulman,PhilippMoritz,SergeyLevine,MichaelJordan,andPieterAbbeel.High-dimensionalcontinuouscontrolusinggeneralizedadvantageestimation.InInternationalCon-ferenceonLearningRepresentations(ICLR2016),2016.NicolasSchweighoferandKenjiDoya.Meta-learninginreinforcementlearning.NeuralNetworks,16(1):5–9,2003.SatinderPalSingh.Transferoflearningbycomposingsolutionsofelementalsequentialtasks.MachineLearning,8(3-4):323–339,1992.MalcolmStrens.Abayesianframeworkforreinforcementlearning.InICML,pp.943–950,2000.MatthewETaylorandPeterStone.Transferlearningforreinforcementlearningdomains:Asurvey.JournalofMachineLearningResearch,10(Jul):1633–1685,2009.WilliamRThompson.Onthelikelihoodthatoneunknownprobabilityexceedsanotherinviewoftheevidenceoftwosamples.Biometrika,25(3/4):285–294,1933.RicardoVilaltaandYoussefDrissi.Aperspectiveviewandsurveyofmeta-learning.ArtiﬁcialIntelligenceReview,18(2):77–95,2002.OriolVinyals,CharlesBlundell,TimothyLillicrap,KorayKavukcuoglu,andDaanWierstra.Match-ingnetworksforoneshotlearning.arXivpreprintarXiv:1606.04080,2016.NiklasWahlstr¨om,ThomasBSch¨on,andMarcPeterDeisenroth.Frompixelstotorques:Policylearningwithdeepdynamicalmodels.arXivpreprintarXiv:1502.02251,2015.ManuelWatter,JostSpringenberg,JoschkaBoedecker,andMartinRiedmiller.Embedtocontrol:Alocallylinearlatentdynamicsmodelforcontrolfromrawimages.InAdvancesinNeuralInformationProcessingSystems,pp.2746–2754,2015.PeterWhittle.Optimizationovertime.JohnWiley&Sons,Inc.,1982.AaronWilson,AlanFern,SoumyaRay,andPrasadTadepalli.Multi-taskreinforcementlearning:ahierarchicalbayesianapproach.InProceedingsofthe24thinternationalconferenceonMachinelearning,pp.1015–1022.ACM,2007.AStevenYounger,SeppHochreiter,andPeterRConwell.Meta-learningwithbackpropagation.InNeuralNetworks,2001.Proceedings.IJCNN’01.InternationalJointConferenceon,volume3.IEEE,2001.12UnderreviewasaconferencepaperatICLR2017APPENDIXADETAILEDEXPERIMENTSETUPCommontoallexperiments:asmentionedinSection2.2,weuseplaceholdervalueswhenneces-sary.Forexample,att=0thereisnopreviousaction,reward,orterminationﬂag.Sinceallofourexperimentsusediscreteactions,weusetheembeddingoftheaction0asaplaceholderforactions,and0forboththerewardsandterminationﬂags.ToformtheinputtotheGRU,weusethevaluesfortherewardsandterminationﬂagsas-is,andembedthestatesandactionsasdescribedseparatelybelowforeachexperiments.Thesevaluesarethenconcatenatedtogethertoformthejointembedding.Fortheneuralnetworkarchitecture,Weuserectiﬁedlinearunitsthroughouttheexperimentsasthehiddenactivation,andweapplyweightnormalizationwithoutdata-dependentinitialization(Sali-mans&Kingma,2016)toallweightmatrices.Thehidden-to-hiddenweightmatrixusesanorthog-onalinitialization(Saxeetal.,2013),andallotherweightmatricesuseXavierinitialization(Glorot&Bengio,2010).Weinitializeallbiasvectorsto0.Unlessotherwisementioned,thepolicyandthebaselineusesseparateneuralnetworkswiththesamearchitectureuntiltheﬁnallayer,wherethenumberofoutputsdiffer.AllexperimentsareimplementedusingTensorFlow(Abadietal.,2016)andrllab(Duanetal.,2016).WeusetheimplementationsofclassicalgorithmsprovidedbytheTabulaRLpackage(Os-band,2016).A.1MULTI-ARMEDBANDITSTheparametersforTRPOareshowninTable1.Sincetheenvironmentisstateless,weuseaconstantembedding0asaplaceholderinplaceofthestates,andaone-hotembeddingfortheactions.Table1:HyperparametersforTRPO:multi-armedbanditsDiscount0.99GAEλ0.3PolicyItersUpto1000#GRUUnits256MeanKL0.01Batchsize250000A.2TABULARMDPSTheparametersforTRPOareshowninTable2.Weuseaone-hotembeddingforthestatesandactionsseparately,whicharethenconcatenatedtogether.Table2:HyperparametersforTRPO:tabularMDPsDiscount0.99GAEλ0.3PolicyItersUpto10000#GRUUnits256MeanKL0.01Batchsize250000A.3VISUALNAVIGATIONTheparametersforTRPOareshowninTable3.Forthistask,weuseaneuralnetworktoformthejointembedding.Werescaletheimagestohavewidth40andheight30withRGBchannelspreserved,andwerecentertheRGBvaluestoliewithinrange[−1,1].Then,thispreprocessed13UnderreviewasaconferencepaperatICLR2017imageispassedthrough2convolutionlayers,eachwith16ﬁltersofsize5×5andstride2.Theactionisﬁrstembeddedintoa256-dimensionalvectorwheretheembeddingislearned,andthenconcatenatedwiththeﬂattenedoutputoftheﬁnalconvolutionlayer.Thejointvectoristhenfedtoafullyconnectedlayerwith256hiddenunits.Unlikepreviousexperiments,weletthepolicyandthebaselinesharethesameneuralnetwork.Wefoundthistoimprovethestabilityoftrainingbaselinesandalsotheendperformanceofthepolicy,possiblyduetoregularizationeffectsandbetterlearnedfeaturesimposedbyweightsharing.Similarweight-sharingtechniqueshavealsobeenexploredin(Mnihetal.,2016).Table3:HyperparametersforTRPO:visualnavigationDiscount0.99GAEλ0.99PolicyItersUpto5000#GRUUnits256MeanKL0.01Batchsize50000REFERENCESMartınAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregSCorrado,AndyDavis,JeffreyDean,MatthieuDevin,etal.Tensorﬂow:Large-scalemachinelearningonheterogeneousdistributedsystems.arXivpreprintarXiv:1603.04467,2016.YanDuan,XiChen,ReinHouthooft,JohnSchulman,andPieterAbbeel.Benchmarkingdeepreinforcementlearningforcontinuouscontrol.arXivpreprintarXiv:1604.06778,2016.XavierGlorotandYoshuaBengio.Understandingthedifﬁcultyoftrainingdeepfeedforwardneuralnetworks.InAistats,volume9,pp.249–256,2010.VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyPLillicrap,TimHarley,DavidSilver,andKorayKavukcuoglu.Asynchronousmethodsfordeepreinforcementlearning.arXivpreprintarXiv:1602.01783,2016.IanOsband.TabulaRL.https://github.com/iosband/TabulaRL,2016.TimSalimansandDiederikPKingma.Weightnormalization:Asimplereparameterizationtoac-celeratetrainingofdeepneuralnetworks.arXivpreprintarXiv:1602.07868,2016.AndrewMSaxe,JamesLMcClelland,andSuryaGanguli.Exactsolutionstothenonlineardynam-icsoflearningindeeplinearneuralnetworks.arXivpreprintarXiv:1312.6120,2013.14
6
1
0
2

v
o
N
5
2

]

G
L
.
s
c
[

3
v
2
5
8
3
0
.
1
1
6
1
:
v
i
X
r
a

A Connection Between Generative Adversarial
Networks, Inverse Reinforcement Learning, and
Energy-Based Models

Chelsea Finn∗, Paul Christiano∗, Pieter Abbeel, Sergey Levine
University of California, Berkeley
{cbfinn,paulfchristiano,pabbeel,svlevine}@eecs.berkeley.edu

Abstract

Generative adversarial networks (GANs) are a recently proposed class of genera-
tive models in which a generator is trained to optimize a cost function that is being
simultaneously learned by a discriminator. While the idea of learning cost func-
tions is relatively new to the ﬁeld of generative modeling, learning costs has long
been studied in control and reinforcement learning (RL) domains, typically for
imitation learning from demonstrations. In these ﬁelds, learning the cost function
underlying observed behavior is known as inverse reinforcement learning (IRL) or
inverse optimal control. While at ﬁrst the connection between cost learning in RL
and cost learning in generative modeling may appear to be a superﬁcial one, we
show in this paper that certain IRL methods are in fact mathematically equivalent
to GANs. In particular, we demonstrate an equivalence between a sample-based
algorithm for maximum entropy IRL and a GAN in which the generator’s density
can be evaluated and is provided as an additional input to the discriminator. Inter-
estingly, maximum entropy IRL is a special case of an energy-based model. We
discuss the interpretation of GANs as an algorithm for training energy-based mod-
els, and relate this interpretation to other recent work that seeks to connect GANs
and EBMs. By formally highlighting the connection between GANs, IRL, and
EBMs, we hope that researchers in all three communities can better identify and
apply transferable ideas from one domain to another, particularly for developing
more stable and scalable algorithms: a major challenge in all three domains.

1 Introduction

Generative adversarial networks (GANs) are a recently proposed class of generative models in which
a generator is trained to optimize a cost function that is being simultaneously learned by a discrimi-
nator [8]. While the idea of learning objectives is relatively new to the ﬁeld of generative modeling,
learning cost or reward functions has long been studied in control [5] and was popularized in 2000 for
reinforcement learning problems [15]. In these ﬁelds, learning the cost function underlying demon-
strated behavior is referred to as inverse reinforcement learning (IRL) or inverse optimal control
(IOC). At ﬁrst glance, the connection between cost learning in RL and cost learning for generative
models may appear to be superﬁcial; however, if we apply GANs to a setting where the generator
density can be efﬁciently evaluated, the result is exactly equivalent to a sample-based algorithm
for maximum entropy (MaxEnt) IRL. Interestingly, as MaxEnt IRL is an energy-based model, this
connection suggests a method for using GANs to train a broader class of energy-based models.

MaxEnt IRL is a widely-used objective for IRL, proposed by Ziebart et al. [27]. Sample-based
algorithms for performing maximum entropy (MaxEnt) IRL have scaled cost learning to scenarios

∗ Indicates equal contribution.

 
 
 
 
 
 
with unknown dynamics, using nonlinear function classes, such as neural networks [4, 11, 7]. We
show that the gradient updates for the cost and the policy in these methods can be viewed as the
updates for the discriminator and generator in GANs, under a speciﬁc form of the discriminator.
The key difference to a generic discriminator is that we need to be able evaluate the density of the
generator, which we integrate into the discriminator in a natural way.

Traditionally, GANs are used to train generative models for which it is not possible to evaluate the
density. When it is possible to evaluate the density, for example in an autoregressive model, it is
typical to maximize the likelihood of the data directly. By considering the connection to IRL, we
ﬁnd that GAN training may be appropriate even when density values are available. For example,
suppose we are interested in modeling a complex multimodal distribution, but our model does not
have enough capacity to represent the distribution. Then maximizing likelihood will lead to a dis-
tribution which “covers” all of the modes, but puts most of its mass in parts of the space that have
negligible density under the data distribution. These might be images that look extremely unrealistic,
nonsensical sentences, or suboptimal robot behavior. A generator trained adversarially will instead
try to “ﬁll in” as many of modes as it can, without putting much mass in the space between modes.
This results in lower diversity, but ensures that samples “look like” they could have been from the
original data.

By drawing an exact correspondence between adaptive, sample-based algorithms for MaxEnt IRL
and GAN training, we show that this phenomenon occurs and is practically important: GAN train-
ing can signiﬁcantly improve the quality of samples even when the generator density can be exactly
evaluated. This is precisely analogous to the observed ability of inverse reinforcement learning to im-
itate behaviors that cannot be successfully learned through behavioral cloning [21], direct maximum
likelihood regression to the demonstrated behavior.

Interestingly, the maximum entropy formulation of IRL is a special case of an energy-based model
(EBM) [26]. The learned cost in MaxEnt IRL corresponds to the energy function, and is trained
via maximum likelihood. Hence, we can also show how a particular form of GANs can be used
to train EBMs. Recent works have recognized a connection between EBMs and GANs [12, 25].
In this work, we particularly focus on EBMs trained with maximum likelihood, and expand upon
the connection recognized by Kim & Bengio [12] for the case where the generator’s density can be
computed. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that
researchers in all three areas can better identify and apply transferable ideas from one domain to
another.

2 Background

In this section, we formally deﬁne generative adversarial networks (GANs), energy-based models
(EBMs), and inverse reinforcement learning (IRL), and introduce notation.

2.1 Generative Adversarial Networks

Generative adversarial networks are an approach to generative modeling where two models are
trained simultaneously: a generator G and a discriminator D. The discriminator is tasked with
classifying its inputs as either the output of the generator, or actual samples from the underlying
data distribution p(x). The goal of the generator is to produce outputs that are classiﬁed by the
discriminator as coming from the underlying data distribution [8].

Formally, the generator takes noise as input and outputs a sample x ∼ G, while the discriminator
takes as input a sample x and outputs the probability D(x) that the sample was from the data distri-
bution. The discriminator’s loss is the average log probability it assigns to the correct classiﬁcation,
evaluated on an equal mixture of real samples and outputs from the generator:

Ldiscriminator(D) = Ex∼p[− log D(x)] + Ex∼G[− log(1 − D(x))].
The generator’s loss can be deﬁned one of several similar ways. The simplest deﬁnition, originally
proposed in [8], is simply the opposite of the discriminator’s loss. However, this provides very
little training signal if the generator’s output can be easily distinguished from the real samples. It is
common to instead use the log of the discriminator’s confusion [8]. We will deﬁne the generator’s
loss as the sum of these two variants:

Lgenerator(G) = Ex∼G[− log D(x)] + Ex∼G[log(1 − D(x))].

2

2.2 Energy-Based Models

Energy-based models [14] associate an energy value Eq (x) with a sample x, modeling the data as a
Boltzmann distribution:

pq (x) =

(1)
The energy function parameters q are often chosen to maximize the likelihood of the data; the
main challenge in this optimization is evaluating the partition function Z, which is an intractable
sum or integral for most high-dimensional problems. A common approach to estimating Z requires
sampling from the Boltzmann distribution pq (x) within the inner loop of learning.

exp(−Eq (x))

1
Z

Sampling from pq (x) can be approximated by using Markov chain Monte Carlo (MCMC) methods;
however, these methods face issues when there are several distinct modes of the distribution and, as
a result, can take arbitrarily large amounts of time to produce a diverse set of samples. Approximate
inference methods can also be used during training, though the energy function may incorrectly
assign low energy to some modes if the approximate inference method cannot ﬁnd them [14].

2.3 Inverse Reinforcement Learning

The goal of inverse reinforcement learning is to infer the cost function underlying demonstrated
behavior [15]. It is typically assumed that the demonstrations come from an expert who is behaving
near-optimally under some unknown cost. In this section, we discuss MaxEnt IRL and guided cost
learning, an algorithm for MaxEnt IRL.

2.3.1 Maximum entropy inverse reinforcement learning

Maximum entropy inverse reinforcement learning models the demonstrations using a Boltzmann
distribution, where the energy is given by the cost function cq :

pq (t ) =

1
Z

exp(−cq (t )),

t = {x1, u1, . . . , xT , uT } is a trajectory; cq (t ) = (cid:229)

t cq (xt , ut ) is a learned cost function
Here,
parametrized by q ; xt and ut are the state and action at time step t; and the partition function Z
is the integral of exp(−cq (t )) over all trajectories that are consistent with the environment dynam-
ics.2

Under this model, the optimal trajectories have the highest likelihood, and the expert can generate
suboptimal trajectories with a probability that decreases exponentially as the trajectories become
more costly. As in other energy-based models, the parameters q are optimized to maximize the like-
lihood of the demonstrations. Estimating the partition function Z is difﬁcult for large or continuous
domains, and presents the main computational challenge. The ﬁrst applications of this model com-
puted Z exactly with dynamic programming [27]. However, this is only practical in small, discrete
domains, and is impossible in domains where the system dynamics p(xt+1|xt, ut ) are unknown.

2.3.2 Guided cost learning

Guided cost learning introduces an iterative sample-based method for estimating Z in the Max-
Ent IRL formulation, and can scale to high-dimensional state and action spaces and nonlinear cost
functions [7]. The algorithm estimates Z by training a new sampling distribution q(t ) and using
importance sampling:

Lcost(q ) = Et ∼p[− log pq (t )] = Et ∼p[cq (t )] + log Z
exp(−cq (t ))
q(t )

= Et ∼p[cq (t )] + log

Et ∼q

(cid:18)

(cid:20)

.
(cid:21)(cid:19)

Guided cost learning alternates between optimizing cq using this estimate, and optimizing q(t ) to
minimize the variance of the importance sampling estimate.

2This formula assumes that xt+1 is a deterministic function of the previous history. A more general form
of this equation can be derived for stochastic dynamics [26]. However, the analysis largely remains the same:
the probability of a trajectory can be written as the product of conditional probabilities, but the conditional
probabilities of the states xt are not affected by q and so factor out of all likelihood ratios.

3

exp(−cq (t ))dt
The optimal importance sampling distribution for estimating the partition function
is q(t ) (cid:181)
the sampling policy
q(t ) is updated to match this distribution by minimizing the KL divergence between q(t ) and
Z exp(−cq (t )), or equivalently minimizing the learned cost and maximizing entropy.
1

| exp(−cq (t ))| = exp(−cq (t )). During guided cost learning,

R

Lsampler(q) = Et ∼q[cq (t )] + Et ∼q[log q(t )]
Conveniently, this optimal sampling distribution is the demonstration distribution for the true cost
function. Thus, this training procedure results in both a learned cost function, characterizing the
demonstration distribution, and a learned policy q(t ), capable of generating samples from the
demonstration distribution.

(2)

This importance sampling estimate can have very high variance if the sampling distribution q fails
to cover some trajectories t with high values of exp(−cq (t )). Since the demonstrations will have
low cost (as a result of the IRL objective), we can address this coverage problem by mixing the
demonstration data samples with the generated samples. Let m = 1
2 q be the mixture distribution
p(t ) be a rough estimate for the density of the demonstrations; for
over trajectory roll-outs. Let
example we could use the current model pq , or we could use a simpler density model trained using
another method. Guided cost learning uses m
2 q(t ) as the
importance weights:

for importance sampling3, with 1
2

p(t ) + 1

2 p + 1

e

Lcost(q ) = Et ∼p[cq (t )] + log

Et ∼m

exp(−cq (t ))
p(t ) + 1
1
2

2 q(t ) #!

,

"

e

2.4 Direct Maximum Likelihood and Behavioral Cloning
e

A simple approach to imitation learning and generative modeling is to train a generator or policy
to output a distribution over the data, without learning a discriminator or energy function. For
tractability, the data distribution is typically factorized using a directed graphical model or Bayesian
network. In the ﬁeld of generative modeling, this approach has most commonly been applied to
speech and language generation tasks [23, 18], but has also been applied to image generation [22].
Like most EBMs, these models are trained by maximizing the likelihood of the observed data points.

When a generative model does not have the capacity to represent the entire data distribution, max-
imizing likelihood directly will lead to a moment-matching distribution that tries to “cover” all of
the modes, leading to a solution that puts much of its mass in parts of the space that have negligible
probability under the true distribution. In many scenarios, it is preferable to instead produce only re-
alistic, highly probable samples, by “ﬁlling in” as many modes as possible, at the trade-off of lower
diversity. Since EBMs are also trained with maximum likelihood, the energy function in an EBM
will exhibit the same moment-matching behavior when it has limited capacity. However, designing
a ﬂexible energy function to represent a distribution’s density function is generally much easier than
designing a tractable generator with the same ﬂexibility, that can to generate samples without a com-
plex iterative inference procedure. Moreover, once we have a trained energy function, the generator
is trained to be mode-seeking, by minimizing the KL divergence between the generator’s distribu-
tion and the distribution induced by the energy function. As a result, even if the generator has the
same capacity as a generative model trained with direct maximum likelihood, the generator trained
with an EBM will exhibit mode-seeking behavior as long as the energy function is more ﬂexible than
the generator. Of course, this phenomenon is often achieved at the cost of tractability, as generating
samples from an energy function requires training a generator which, in the case of IRL, is forward
policy optimization.

In sequential decision-making domains, using direct maximum likelihood is known as behavioral
cloning, where the policy is trained with supervised learning to match the actions of the demonstrat-
ing agent, conditioned on the corresponding observations. While this approach is simple and often
effective for small problems, the moment-matching behavior of direct maximum likelihood can pro-
duce particularly ineffective trajectories because of compounding errors. When the policy makes a
small mistake, it deviates from the state distribution seen during training, making it more likely to
make a mistake again. This issue compounds and eventually, the agent reaches a state far from the

3In RL settings, where generating samples requires executing a policy in the real world, such as in robotics,
old samples from old generators are typically retained for efﬁciency. In this case, the density q can be computed
using a fusion distribution over the past generator densities.

4

 
training distribution and makes a catastrophic error [21]. Generative modeling also faces this issue
when generating variables sequentially. A popular approach for handling this involves incremen-
tally sampling more from the model and drawing less from the data distribution during training [21].
This requires that the true data distribution can be sampled from during training, corresponding to
a human or algorithmic expert. Bengio et al. proposed an approximate solution, termed scheduled
sampling, that does not require querying the data distribution [3]. However, while these approaches
alleviate the issue, they do not solve it completely.

3 GANs and IRL

We now show how generative adversarial modeling has implicitly been applied to the setting of in-
verse reinforcement learning, where the data-to-be-modeled is a set of expert demonstrations. The
derivation requires a particular form of discriminator, which we discuss ﬁrst in Section 3.1. After
making this modiﬁcation to the discriminator, we obtain an algorithm for IRL, as we show in Sec-
tion 3.2, where the discriminator involves the learned cost and the generator represents the policy.

3.1 A special form of discriminator

For a ﬁxed generator with a [typically unknown] density q(t ), the optimal discriminator is the fol-
lowing [8]:

D∗(t ) =

p(t )
p(t ) + q(t )

,

(3)

where p(t ) is the actual distribution of the data.
In the traditional GAN algorithm, the discriminator is trained to directly output this value. When
the generator density q(t ) can be evaluated, the traditional GAN discriminator can be modiﬁed
to incorporate this density information. Instead of having the discriminator estimate the value of
Equation 3 directly, it can be used to estimate p(t ), ﬁlling in the value of q(t ) with its known value.
In this case, the new form of the discriminator Dq with parameters q

is

Dq (t ) =

˜pq (t )
˜pq (t ) + q(t )

.

In order to make the connection to MaxEnt IRL, we also replace the estimated data density with
the Boltzmann distribution. As in MaxEnt IRL, we write the energy function as cq
to designate the
learned cost. Now the discriminator’s output is:
1

Dq (t ) =

Z exp(−cq (t ))
Z exp(−cq (t )) + q(t )

1

.

The resulting architecture for the discriminator is very similar to a typical model for binary classi-
ﬁcation, with a sigmoid as the ﬁnal layer and log Z as the bias of the sigmoid. We have adjusted
the architecture only by subtracting log q(t ) from the input to the sigmoid. This modest change
allows the optimal discriminator to be completely independent of the generator: the discriminator is
Z exp(−cq (t )) = p(t ). Independence between the generator and the optimal discrim-
optimal when 1
inator may signiﬁcantly improve the stability of training.
This change is very simple to implement and is applicable in any setting where the density q(t )
can be cheaply evaluated. Of course this is precisely the case where we could directly maximize
likelihood, and we might wonder whether it is worth the additional complexity of GAN training.
But the experience of researchers in IRL has shown that maximizing log likelihood directly is not
always the most effective way to learn complex behaviors, even when it is possible to implement. As
we will show, there is a precise equivalence between MaxEnt IRL and this type of GAN, suggesting
that the same phenomenon may occur in other domains: GAN training may provide advantages even
when it would be possible to maximize likelihood directly.

3.2 Equivalence between generative adversarial networks and guided cost learning

In this section, we show that GANs, when applied to IRL problems, optimize the same objective as
MaxEnt IRL, and in fact the variant of GANs described in the previous section is precisely equivalent
to guided cost learning.

5

Recall that the discriminator’s loss is equal to

Ldiscriminator(Dq ) = Et ∼p[− log Dq (t )] + Et ∼q[− log(1 − Dq (t ))]

= Et ∼p

− log

"

1

Z exp(−cq (t ))
Z exp(−cq (t )) + q(t ) #

1

+ Et ∼q

− log

"

q(t )
Z exp(−cq (t )) + q(t ) #

1

In maximum entropy IRL, the log-likelihood objective is:

Lcost(q ) = Et ∼p[cq (t )] + log

Et ∼ 1

2 p+ 1
2 q

= Et ∼p[cq (t )] + log

Et ∼m

"

2 q(t ) #!

"

exp(−cq (t ))
p(t ) + 1
1
2
exp(−cq (t ))
e
2Z exp(−cq (t )) + 1

1

2 q(t ) #!

(4)

(5)

,

where we have substituted
estimate the importance weights.

p(t ) = pq (t ) = 1

Z exp(−cq (t )), i.e. we are using the current model to

We will establish the following facts, which together imply that GANs optimize precisely the Max-
Ent IRL problem:

e

1. The value of Z which minimizes the discriminator’s loss is an importance-sampling estima-

tor for the partition function, as described in Section 2.3.2.

2. For this value of Z, the derivative of the discriminator’s loss with respect to q

is equal to

the derivative of the MaxEnt IRL objective.

3. The generator’s loss is exactly equal to the cost cq minus the entropy of q(t ), i.e.

the

MaxEnt policy loss deﬁned in Equation 2 in Section 2.3.2.

is the mixture distribution between p and q. Write

Recall that m
Note that when q and Z are optimized, 1
hence

m (t ) is an estimate for the density of m .

2 q(t ).
m (t ) = 1
Z exp(−cq (t )) is an estimate for the density of p(t ), and

2Z exp(−cq (t )) + 1

e

m (t )]

(6)

(7)

(8)

e

3.2.1 Z estimates the partition function

e

We can compute the discriminator’s loss:

Ldiscriminator(Dq ) =Et ∼p

− log

"

1

Z exp(−cq (t ))
m (t )

#

+ Et ∼q

− log

q(t )
m (t )

= log Z + Et ∼p[cq (t )] + Et ∼p[log
= log Z + Et ∼p[cq (t )] − Et ∼q[log q(t )] + 2Et ∼m [log

e

e

m (t )].

(cid:20)

(cid:21)
m (t )] − Et ∼q[log q(t )] + Et ∼q[log

e

Only the ﬁrst and last terms depend on Z. At the minimizing value of Z, the derivative of these term
with respect to Z will be zero:

e

¶ ZLdiscriminator(Dq ) = 0

1
Z

= Et ∼m

Z = Et ∼m

1

Z2 exp(−cq (t ))
m (t )

#

exp(−cq (t ))
e
m (t )

.

(cid:21)

"

(cid:20)

Thus the minimizing Z is precisely the importance sampling estimate of the partition function in
Equation 4.

e

3.2.2 cq optimizes the IRL objective

We return to the discriminator’s loss as computed in Equation 8, and consider the derivative with
respect to the parameters q . We will show that this is exactly the same as the derivative of the IRL
objective.

6

 
 
Only the second and fourth terms in the sum depend on q . When we differentiate those terms we
obtain:

¶ q Ldiscriminator(Dq ) = Et ∼p[¶ q cq (t )] − Et ∼m

1

Z exp(−cq (t ))¶ q cq (t )
m (t )

#

"

On the other hand, when we differentiate the MaxEnt IRL objective, we obtain:

e

¶ q Lcost(q ) = Et ∼p[¶ q cq (t )] + ¶ q log

(cid:18)
Et ∼m

= Et ∼p[¶ q cq (t )] +

(cid:18)

= Et ∼p[¶ q cq (t )] − Et ∼m

"

= ¶ q Ldiscriminator(Dq ).

(cid:20)

Et ∼m

exp(−cq (t ))
m (t )
(cid:21)(cid:19)
− exp(−cq (t ))¶ q cq (t )
m (t )
e
(cid:20)
Z exp(−cq (t ))¶ q cq (t )
1
e
m (t )

#

e

Et ∼m

(cid:21)(cid:30)

(cid:20)

exp(−cq (t ))
m (t )

(cid:21)(cid:19)

e

In the third equality, we used the deﬁnition of Z as an importance sampling estimate. Note that in the
m (t ) as a constant rather than as a quantity that depends on q . This
second equality, we have treated
m (t ) as the
is because the IRL optimization is minimizing log Z = log (cid:229)
weights for an importance sampling estimator of Z. For this purpose we do not want to differentiate
through the importance weights.

t exp(−cq (t )) and using

e

e

3.3 The generator optimizes the MaxEnt IRL objective

Finally, we compute the generator’s loss:

Lgenerator(q) = Et ∼q[log(1 − D(t )) − log(D(t ))]

1

log

− log

= Et ∼q

q(t )
m (t )
= Et ∼q[log q(t ) + log Z + cq (t )]
e
= log Z + Et ∼q[cq (t )] + Et ∼q[log q(t )] = log Z + Lsampler(q).

Z exp(−cq (t ))
m (t )

"

#

e

The term log Z is a parameter of the discriminator that is held ﬁxed while optimizing the generator,
this loss is exactly equivalent the sampler loss from MaxEnt IRL, deﬁned in Equation 2.

3.4 Discussion

There are many apparent differences between MaxEnt IRL and the GAN optimization problem. But,
we have shown that after making a single key change—using a generator q(t ) for which densities
can be evaluated efﬁciently, and incorporating this information into the discriminator in a natural
way—generative adversarial networks can be viewed as a sample-based algorithm for the MaxEnt
IRL problem. By connecting GANs to the empirical literature on inverse reinforcement learning [7],
this demonstrates that GAN training can improve the quality of samples even when the generator’s
density can be evaluated exactly. By generalizing this connection, we can derive a new adversarial
training strategy for energy-based models, which we discuss in the next section.

4 GANs for training EBMs

Now that we have highlighted the connection between GANs and guided cost learning, the appli-
cation of GANs to EBMs follows directly. As discussed in Section 2.2, the primary challenge in
training EBMs is estimating the partition function, which is done by approximately sampling from
the distribution induced by the energy Eq . Two recent papers have proposed to use adversarial train-
ing to derive fast estimates of the partition function [12, 25]. In particular, these methods alternate
between training a generator to produce samples with minimal energy Eq (x), and optimizing the
parameters of the energy function using the samples to estimate the partition function.

7

When the density of the generator is available, however, we can derive an unbiased estimate of the
partition function as

Z = Ex∼m

exp(−Eq (x))
p(x) + 1
1
2

2 q(x) #

"

where m denotes an equal mixture of generated and real data points, q(x) denotes the density under
e
the generator, and

p(x) denotes an estimate for the data density.

This gives a loss function

e
Lenergy(q ) = Ex∼p[− log pq (x)]

= Ex∼p[−Eq (x)] − log

Ex∼m

exp(−Eq (x))
1
p(x) + 1
2

.
2 q(x) #!

"

As before, the generator is updated to minimize energy and maximize entropy:

e

Lgenerator(q) = Ex∼q[Eq (x)] + Ex∼q[log q(x)]

p(x) = pq (x), the resulting model is a special case of a GAN which is straightforward to
If we set
implement. The discriminator’s output is s (Eq (x) − logq(x)), where s
is a sigmoid with a trainable
bias. The discriminator’s loss is the log probability and the generator’s loss is the discriminator’s
log odds, as deﬁned in Section 2.1.

e

Kim & Bengio proposed a similar energy-based model for generative image modeling, but did not
assume they could compute the generator’s density [12]. As a result, they do not use importance
weights, and work with a biased estimator of the partition function which converges to the true
partition function when the generator correctly samples from the energy-based model. In contrast,
by using the generator density, we can get an unbiased estimate of the partition function that does
not rely on any assumptions about the generator. Thus, even if the generator cannot learn to sample
exactly from the data distribution, our training procedure is consistent.

Zhao et al. also proposed an energy-based GAN model with an autoencoder discriminator where
the energy is given by the mean-squared error between the data example (generated or real) and the
discriminator’s reconstruction [25]. The energy function is optimized with a margin loss, and the
generator is trained to minimize energy. This method also did not use the form of discriminator
presented above. An interesting direction for future exploration is to consider combining the GAN
training algorithm discussed here with an objective other than log-likelihood, such as one used with
EBMs [14] or different f -divergences used with GANs [17].

5 Related Work

Ho et al. [10, 9] previously presented a GAN-like algorithm for imitation learning, where the goal
is to recover a policy that matches the expert demonstrations. The proposed algorithm, called gener-
ative adversarial imitation learning (GAIL), has an adversarial structure. The analysis in this paper
provides additional insight into what GAIL is doing. As discussed above, GANs are optimizing the
same objective as MaxEnt IRL. Thus, the GAIL policy is being trained to optimize a cost learned
through MaxEnt IRL. Unlike guided cost learning [7], however, Ho & Ermon use the typical uncon-
strained form of the discriminator [9] and do not use the generator’s density. In this case, the cost
function remains implicit within the discriminator and cannot be recovered. Hence, in GAIL, the
discriminator is discarded and the policy is the end result.

Bachman & Precup [1] suggested that data generation can be converted into a sequential decision-
making problem and solved with a reinforcement learning method. Several recent works have pro-
posed methods for merging maximum likelihood objectives and known reward functions for training
sequential language generation models and rely on surrogate reward function such as BLEU score
or edit distance [20, 16, 2]. In this work, we assume that the reward function is unknown.

Yu et al. proposed to learn a cost function for sequential data generation using GANs, where the cost
is deﬁned as the probability of the discriminator classifying the generated sequence as coming from
the data distribution [24]. The discriminator does not take advantage of the policy’s density values,
despite the fact that they are known (and are used during pre-training). Their experiments also ﬁnd

8

 
that max-likelihood pre-training is crucial for good performance, suggesting that recurrent genera-
tors that can’t afford such pre-training (e.g. because they don’t have densities) are less practical to
train.

Pfau & Vinyals drew a connection between the optimization problems in GANs and actor-critic
methods in reinforcement learning, suggesting how ideas for stabilizing training in one domain
could be beneﬁcial for the other [19]. As the authors point out, these optimization tricks could also
be useful for imitation learning algorithms with the same two-level optimization structure.

6 Discussion

In this work, we showed an equivalence between generative adversarial modeling and an algorithm
for performing maximum entropy inverse reinforcement learning. Our derivation used a special
form of discriminator that leverages likelihood values from the generator, leading to an unbiased
estimate of the underlying energy function. A natural direction for future work is to experiment
with combining deep generators that can provide densities, such as autoregressive models [13, 22]
or models that use invertible transformations [6], with generative adversarial modeling. Such an
approach may provide more stable training, better generators, and wider applicability to discrete
problems such as language.

This work also suggests a new algorithm for training energy-based models using generative adver-
sarial networks, that trains a neural network model to sample from the distribution induced by the
current energy. This method could reduce the computational challenges of existing MCMC-based
solutions.

Acknowledgments

The authors would like to thank Ian Goodfellow and Joan Bruna for insightful discussions.

References

[1] P. Bachman and D. Precup. Data generation as sequential decision making. In Neural Infor-

mation Processing Systems (NIPS), 2015.

[2] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An

actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.

[3] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction
with recurrent neural networks. In Neural Information Processing Systems (NIPS), 2015.

[4] A. Boularias, J. Kober, and J. Peters. Relative entropy inverse reinforcement learning.

In

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2011.

[5] S. Boyd and L. Vandenberghe. Convex optimization, 2004.

[6] L. Dinh, J. Sohl-Dickstein, and S. Bengio. Density estimation using real nvp. arXiv preprint

arXiv:1605.08803, 2016.

[7] C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via

policy optimization. International Conference on Machine Learning (ICML), 2016.

[8] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In Neural Information Processing Systems (NIPS),
2014.

[9] J. Ho and S. Ermon. Generative adversarial imitation learning. Neural Information Processing

Systems (NIPS), 2016.

[10] J. Ho, J. K. Gupta, and S. Ermon. Model-free imitation learning with policy optimization. In

International Conference on Machine Learning (ICML), 2016.

[11] M. Kalakrishnan, P. Pastor, L. Righetti, and S. Schaal. Learning objective functions for manip-

ulation. In International Conference on Robotics and Automation (ICRA), 2013.

9

[12] T. Kim and Y. Bengio. Deep directed generative models with energy-based probability estima-

tion. ICLR Workshop Track, 2016.

[13] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2011.

[14] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based

learning. Predicting structured data, 1:0, 2006.

[15] A. Ng, S. Russell, et al. Algorithms for inverse reinforcement learning.

In International

Conference on Machine Learning (ICML), 2000.

[16] M. Norouzi, S. Bengio, Z. Chen, N. Jaitly, M. Schuster, Y. Wu, and D. Schuurmans. Reward
augmented maximum likelihood for neural structured prediction. Neural Information Process-
ing Systems (NIPS), 2016.

[17] S. Nowozin, B. Cseke, and R. Tomioka.

f-gan: Training generative neural samplers using

variational divergence minimization. Neural Information Processing Systems (NIPS), 2016.

[18] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,
A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint
arXiv:1609.03499, 2016.

[19] D. Pfau and O. Vinyals. Connecting generative adversarial networks and actor-critic methods.

arXiv preprint arXiv:1610.01945, 2016.

[20] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent
neural networks. International Conference on Learning Representations (ICLR), 2016.

[21] S. Ross, G. Gordon, and A. Bagnell. A reduction of imitation learning and structured prediction

to no-regret online learning. Journal of Machine Learning Research, 15, 2011.

[22] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks.

International Conference on Machine Learning (ICML), 2016.

[23] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, et al. Google’s neural machine translation system: Bridging the gap between
human and machine translation. arXiv preprint arXiv:1609.08144, 2016.

[24] L. Yu, W. Zhang, J. Wang, and Y. Yu. Seqgan: Sequence generative adversarial nets with

policy gradient. arXiv preprint arXiv:1609.05473, 2016.

[25] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. arXiv

preprint arXiv:1609.03126, 2016.

[26] B. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal

entropy. PhD thesis, Carnegie Mellon University, 2010.

[27] B. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement

learning. In AAAI Conference on Artiﬁcial Intelligence, 2008.

10


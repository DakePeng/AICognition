7
1
0
2

g
u
A
8
1

]

G
L
.
s
c
[

2
v
4
4
1
5
0
.
8
0
7
1
:
v
i
X
r
a

Scalabletrust-regionmethodfordeepreinforcementlearningusingKronecker-factoredapproximationYuhuaiWu∗UniversityofTorontoVectorInstituteywu@cs.toronto.eduElmanMansimov∗NewYorkUniversitymansimov@cs.nyu.eduShunLiaoUniversityofTorontoVectorInstitutesliao3@cs.toronto.eduRogerGrosseUniversityofTorontoVectorInstitutergrosse@cs.toronto.eduJimmyBaUniversityofTorontoVectorInstitutejimmy@psi.utoronto.caAbstractInthiswork,weproposetoapplytrustregionoptimizationtodeepreinforce-mentlearningusingarecentlyproposedKronecker-factoredapproximationtothecurvature.WeextendtheframeworkofnaturalpolicygradientandproposetooptimizeboththeactorandthecriticusingKronecker-factoredapproximatecurvature(K-FAC)withtrustregion;hencewecallourmethodActorCriticusingKronecker-FactoredTrustRegion(ACKTR).Tothebestofourknowledge,thisistheﬁrstscalabletrustregionnaturalgradientmethodforactor-criticmethods.Itisalsoamethodthatlearnsnon-trivialtasksincontinuouscontrolaswellasdiscretecontrolpoliciesdirectlyfromrawpixelinputs.WetestedourapproachacrossdiscretedomainsinAtarigamesaswellascontinuousdomainsintheMu-JoCoenvironment.Withtheproposedmethods,weareabletoachievehigherrewardsanda2-to3-foldimprovementinsampleefﬁciencyonaverage,comparedtopreviousstate-of-the-arton-policyactor-criticmethods.Codeisavailableathttps://github.com/openai/baselines.1IntroductionAgentsusingdeepreinforcementlearning(deepRL)methodshaveshowntremendoussuccessinlearningcomplexbehaviourskillsandsolvingchallengingcontroltasksinhigh-dimensionalrawsensorystate-space[25,18,13].DeepRLmethodsmakeuseofdeepneuralnetworkstorepresentcontrolpolicies.Despitetheimpressiveresults,theseneuralnetworksarestilltrainedusingsimplevariantsofstochasticgradientdescent(SGD).SGDandrelatedﬁrst-ordermethodsexploreweightspaceinefﬁciently.ItoftentakesdaysforthecurrentdeepRLmethodstomastervariouscontinuousanddiscretecontroltasks.Previously,adistributedapproachwasproposed[18]toreducetrainingtimebyexecutingmultipleagentstointeractwiththeenvironmentsimultaneously,butthisleadstorapidlydiminishingreturnsofsampleefﬁciencyasthedegreeofparallelismincreases.SampleefﬁciencyisadominantconcerninRL;roboticinteractionwiththerealworldistypicallyscarcerthancomputationtime,andeveninsimulatedenvironmentsthecostofsimulationoftendominatesthatofthealgorithmitself.Onewaytoeffectivelyreducethesamplesizeistousemoreadvancedoptimizationtechniquesforgradientupdates.Naturalpolicygradient[11]usesthetechniqueofnaturalgradientdescent[1]toperformgradientupdates.Naturalgradientmethods∗Equalcontribution. 
 
 
 
 
 
Figure1:PerformancecomparisonsonsixstandardAtarigamestrainedfor10milliontimesteps(1timestepequals4frames).Theshadedregiondenotesthestandarddeviationover2randomseeds.followthesteepestdescentdirectionthatusestheFishermetricastheunderlyingmetric,ametricthatisbasednotonthechoiceofcoordinatesbutratheronthemanifold(i.e.,thesurface).However,theexactcomputationofthenaturalgradientisintractablebecauseitrequiresinvertingtheFisherinformationmatrix.Trust-regionpolicyoptimization(TRPO)[22]avoidsexplicitlystoringandinvertingtheFishermatrixbyusingFisher-vectorproducts[21].However,ittypicallyrequiresmanystepsofconjugategradienttoobtainasingleparameterupdate,andaccuratelyestimatingthecurvaturerequiresalargenumberofsamplesineachbatch;henceTRPOisimpracticalforlargemodelsandsuffersfromsampleinefﬁciency.Kronecker-factoredapproximatedcurvature(K-FAC)[16,7]isascalableapproximationtonaturalgradient.Ithasbeenshowntospeeduptrainingofvariousstate-of-the-artlarge-scaleneuralnetworks[2]insupervisedlearningbyusinglargermini-batches.UnlikeTRPO,eachupdateiscomparableincosttoanSGDupdate,anditkeepsarunningaverageofcurvatureinformation,allowingittousesmallbatches.ThissuggeststhatapplyingK-FACtopolicyoptimizationcouldimprovethesampleefﬁciencyofthecurrentdeepRLmethods.Inthispaper,weintroducetheactor-criticusingKronecker-factoredtrustregion(ACKTR;pro-nounced“actor”)method,ascalabletrust-regionoptimizationalgorithmforactor-criticmethods.TheproposedalgorithmusesaKronecker-factoredapproximationtonaturalpolicygradientthatallowsthecovariancematrixofthegradienttobeinvertedefﬁciently.Tobestofourknowledge,wearealsotheﬁrsttoextendthenaturalpolicygradientalgorithmtooptimizevaluefunctionsviaGauss-Newtonapproximation.Inpractice,theper-updatecomputationcostofACKTRisonly10%to25%higherthanSGD-basedmethods.Empirically,weshowthatACKTRsubstantiallyimprovesbothsampleefﬁciencyandtheﬁnalperformanceoftheagentintheAtarienvironments[4]andtheMuJoCo[27]taskscomparedtothestate-of-the-arton-policyactor-criticmethodA2C[18]andthefamoustrustregionoptimizerTRPO[22].Wemakeoursourcecodeavailableonlineathttps://github.com/openai/baselines.2Background2.1Reinforcementlearningandactor-criticmethodsWeconsideranagentinteractingwithaninﬁnite-horizon,discountedMarkovDecisionProcess(X,A,γ,P,r).Attimet,theagentchoosesanactionat∈Aaccordingtoitspolicyπθ(a|st)givenitscurrentstatest∈X.Theenvironmentinturnproducesarewardr(st,at)andtransitionstothenextstatest+1accordingtothetransitionprobabilityP(st+1|st,at).Thegoaloftheagentistomaximizetheexpectedγ-discountedcumulativereturnJ(θ)=Eπ[Rt]=Eπ[P∞i≥0γir(st+i,at+i)]withrespecttothepolicyparametersθ.Policygradientmethods[30,26]directlyparameterizeapolicyπθ(a|st)andupdateparameterθsoastomaximizetheobjectiveJ(θ).Initsgeneralform,2thepolicygradientisdeﬁnedas[23],∇θJ(θ)=Eπ[∞Xt=0Ψt∇θlogπθ(at|st)],whereΨtisoftenchosentobetheadvantagefunctionAπ(st,at),whichprovidesarelativemeasureofvalueofeachactionatatagivenstatest.Thereisanactivelineofresearch[23]ondesigninganadvantagefunctionthatprovidesbothlow-varianceandlow-biasgradientestimates.Asthisisnotthefocusofourwork,wesimplyfollowtheasynchronousadvantageactorcritic(A3C)method[18]anddeﬁnetheadvantagefunctionasthek-stepreturnswithfunctionapproximation,Aπ(st,at)=k−1Xi=0(cid:0)γir(st+i,at+i)+γkVπφ(st+k)(cid:1)−Vπφ(st),whereVπφ(st)isthevaluenetwork,whichprovidesanestimateoftheexpectedsumofrewardsfromthegivenstatefollowingpolicyπ,Vπφ(st)=Eπ[Rt].Totraintheparametersofthevaluenetwork,weagainfollow[18]byperformingtemporaldifferenceupdates,soastominimizethesquareddifferencebetweenthebootstrappedk-stepreturnsˆRtandthepredictionvalue12||ˆRt−Vπφ(st)||2.2.2NaturalgradientusingKronecker-factoredapproximationTominimizeanonconvexfunctionJ(θ),themethodofsteepestdescentcalculatestheupdate∆θthatminimizesJ(θ+∆θ),subjecttotheconstraintthat||∆θ||B<1,where||·||Bisthenormdeﬁnedby||x||B=(xTBx)12,andBisapositivesemideﬁnitematrix.Thesolutiontotheconstraintoptimizationproblemhastheform∆θ∝−B−1∇θJ,where∇θJisthestandardgradient.WhenthenormisEuclidean,i.e.,B=I,thisbecomesthecommonlyusedmethodofgradientdescent.However,theEuclideannormofthechangedependsontheparameterizationθ.Thisisnotfavorablebecausetheparameterizationofthemodelisanarbitrarychoice,anditshouldnotaffecttheoptimizationtrajectory.ThemethodofnaturalgradientconstructsthenormusingtheFisherinformationmatrixF,alocalquadraticapproximationtotheKLdivergence.Thisnormisindependentofthemodelparameterizationθontheclassofprobabilitydistributions,providingamorestableandeffectiveupdate.However,sincemodernneuralnetworksmaycontainmillionsofparameters,computingandstoringtheexactFishermatrixanditsinverseisimpractical,sowehavetoresorttoapproximations.ArecentlyproposedtechniquecalledKronecker-factoredapproximatecurvature(K-FAC)[16]usesaKronecker-factoredapproximationtotheFishermatrixtoperformefﬁcientapproximatenaturalgradientupdates.Weletp(y|x)denotetheoutputdistributionofaneuralnetwork,andL=logp(y|x)denotethelog-likelihood.LetW∈RCout×Cinbetheweightmatrixintheℓthlayer,whereCoutandCinarethenumberofoutput/inputneuronsofthelayer.Denotetheinputactivationvectortothelayerasa∈RCin,andthepre-activationvectorforthenextlayerass=Wa.Notethattheweightgradientisgivenby∇WL=(∇sL)a⊺.K-FACutilizesthisfactandfurtherapproximatestheblockFℓcorrespondingtolayerℓasˆFℓ,Fℓ=E[vec{∇WL}vec{∇WL}⊺]=E[aa⊺⊗∇sL(∇sL)⊺]≈E[aa⊺]⊗E[∇sL(∇sL)⊺]:=A⊗S:=ˆFℓ,whereAdenotesE[aa⊺]andSdenotesE[∇sL(∇sL)⊺].Thisapproximationcanbeinterpretedasmakingtheassumptionthatthesecond-orderstatisticsoftheactivationsandthebackpropagatedderivativesareuncorrelated.Withthisapproximation,thenaturalgradientupdatecanbeefﬁcientlycomputedbyexploitingthebasicidentities(P⊗Q)−1=P−1⊗Q−1and(P⊗Q)vec(T)=PTQ⊺:vec(∆W)=ˆF−1ℓvec{∇WJ}=vec(cid:0)A−1∇WJS−1(cid:1).FromtheaboveequationweseethattheK-FACapproximatenaturalgradientupdateonlyrequirescomputationsonmatricescomparableinsizetoW.GrosseandMartens[7]haverecentlyextendedtheK-FACalgorithmtohandleconvolutionalnetworks.Baetal.[2]laterdevelopedadistributedversionofthemethodwheremostoftheoverheadismitigatedthroughasynchronouscomputa-tion.DistributedK-FACachieved2-to3-timesspeed-upsintraininglargemodernclassiﬁcationconvolutionalnetworks.3Figure2:IntheAtarigameofAtlantis,ouragent(ACKTR)quicklylearnstoobtainrewardsof2millionin1.3hours,600episodesofgames,2.5milliontimesteps.Thesameresultisachievedbyadvantageactorcritic(A2C)in10hours,6000episodes,25milliontimesteps.ACKTRis10timesmoresampleefﬁcientthanA2Conthisgame.3Methods3.1Naturalgradientinactor-criticNaturalgradientwasproposedtoapplytothepolicygradientmethodmorethanadecadeagobyKakade[11].Buttherestilldoesn’texistascalable,sample-efﬁcient,andgeneral-purposeinstantiationofthenaturalpolicygradient.Inthissection,weintroducetheﬁrstscalableandsample-efﬁcientnaturalgradientalgorithmforactor-criticmethods:theactor-criticusingKronecker-factoredtrustregion(ACKTR)method.WeuseKronecker-factoredapproximationtocomputethenaturalgradientupdate,andapplythenaturalgradientupdatetoboththeactorandthecritic.TodeﬁnetheFishermetricforreinforcementlearningobjectives,onenaturalchoiceistousethepolicyfunctionwhichdeﬁnesadistributionovertheactiongiventhecurrentstate,andtaketheexpectationoverthetrajectorydistribution:F=Ep(τ)[∇θlogπ(at|st)(∇θlogπ(at|st))⊺],wherep(τ)isthedistributionoftrajectories,givenbyp(s0)QTt=0π(at|st)p(st+1|st,at).Inpractice,oneapproximatestheintractableexpectationovertrajectoriescollectedduringtraining.Wenowdescribeonewaytoapplynaturalgradienttooptimizethecritic.Learningthecriticcanbethoughtofasaleast-squaresfunctionapproximationproblem,albeitonewithamovingtarget.Inthesettingofleast-squaresfunctionapproximation,thesecond-orderalgorithmofchoiceiscommonlyGauss-Newton,whichapproximatesthecurvatureastheGauss-NewtonmatrixG:=E[JTJ],whereJistheJacobianofthemappingfromparameterstooutputs[19].TheGauss-NewtonmatrixisequivalenttotheFishermatrixforaGaussianobservationmodel[15];thisequivalenceallowsustoapplyK-FACtothecriticaswell.Speciﬁcally,weassumetheoutputofthecriticvisdeﬁnedtobeaGaussiandistributionp(v|st)∼N(v;V(st),σ2).TheFishermatrixforthecriticisdeﬁnedwithrespecttothisGaussianoutputdistribution.Inpractice,wecansimplysetσto1,whichisequivalenttothevanillaGauss-Newtonmethod.Iftheactorandcriticaredisjoint,onecanseparatelyapplyK-FACupdatestoeachusingthemetricsdeﬁnedabove.Buttoavoidinstabilityintraining,itisoftenbeneﬁcialtouseanarchitecturewherethetwonetworkssharelower-layerrepresentationsbuthavedistinctoutputlayers[18,28].Inthiscase,wecandeﬁnethejointdistributionofthepolicyandthevaluedistributionbyassumingindependenceofthetwooutputdistributions,i.e.,p(a,v|s)=π(a|s)p(v|s),andconstructtheFishermetricwithrespecttop(a,v|s),whichisnodifferentthanthestandardK-FACexceptthatweneedtosamplethenetworks’outputsindependently.WecanthenapplyK-FACtoapproximatetheFishermatrixEp(τ)[∇logp(a,v|s)∇logp(a,v|s)T]toperformupdatessimultaneously.Inaddition,weusethefactorizedTikhonovdampingapproachdescribedby[16].Wealsofollow[2]andperformtheasynchronouscomputationofsecond-orderstatisticsandinversesrequiredbytheKroneckerapproximationtoreducecomputationtime.43.2Step-sizeSelectionandtrust-regionoptimizationTraditionally,naturalgradientisperformedwithSGD-likeupdates,θ←θ−ηF−1∇θL.ButinthecontextofdeepRL,Schulmanetal.[22]observedthatsuchanupdaterulecanresultinlargeupdatestothepolicy,causingthealgorithmtoprematurelyconvergetoanear-deterministicpolicy.Theyadvocateinsteadusingatrustregionapproach,wherebytheupdateisscaleddowntomodifythepolicydistribution(intermsofKLdivergence)byatmostaspeciﬁedamount.Therefore,weadoptthetrustregionformulationofK-FACintroducedby[2],choosingtheeffectivestepsizeηtobemin(ηmax,q2δ∆θ⊺ˆF∆θ),wherethelearningrateηmaxandtrustregionradiusδarehyperparameters.Iftheactorandthecriticaredisjoint,thenweneedtotuneadifferentsetofηmaxandδseparatelyforboth.ThevarianceparameterforthecriticoutputdistributioncanbeabsorbedintothelearningrateparameterforvanillaGauss-Newton.Ontheotherhand,iftheysharerepresentations,weneedtotuneonesetofηmax,δ,andalsotheweightingparameterofthetraininglossofthecritic,withrespecttothatoftheactor.4RelatedworkNaturalgradient[1]wasﬁrstappliedtopolicygradientmethodsbyKakade[11].BagnellandSchneider[3]furtherprovedthatthemetricdeﬁnedin[11]isacovariantmetricinducedbythepath-distributionmanifold.PetersandSchaal[20]thenappliednaturalgradienttotheactor-criticalgorithm.Theyproposedperformingnaturalpolicygradientfortheactor’supdateandusingaleast-squarestemporaldifference(LSTD)methodforthecritic’supdate.However,therearegreatcomputationalchallengeswhenapplyingnaturalgradientmethods,mainlyassociatedwithefﬁcientlystoringtheFishermatrixaswellascomputingitsinverse.Fortractability,previousworkrestrictedthemethodtousingthecompatiblefunctionapproximator(alinearfunctionapproximator).Toavoidthecomputationalburden,TrustRegionPolicyOptimization(TRPO)[22]approximatelysolvesthelinearsystemusingconjugategradientwithfastFishermatrix-vectorproducts,similartotheworkofMartens[14].Thisapproachhastwomainshortcomings.First,itrequiresrepeatedcomputationofFishervectorproducts,preventingitfromscalingtothelargerarchitecturestypicallyusedinexperimentsonlearningfromimageobservationsinAtariandMuJoCo.Second,itrequiresalargebatchofrolloutsinordertoaccuratelyestimatecurvature.K-FACavoidsbothissuesbyusingtractableFishermatrixapproximationsandbykeepingarunningaverageofcurvaturestatisticsduringtraining.AlthoughTRPOshowsbetterper-iterationprogressthanpolicygradientmethodstrainedwithﬁrst-orderoptimizerssuchasAdam[12],itisgenerallylesssampleefﬁcient.SeveralmethodswereproposedtoimprovethecomputationalefﬁciencyofTRPO.ToavoidrepeatedcomputationofFisher-vectorproducts,Wangetal.[28]solvetheconstrainedoptimizationproblemwithalinearapproximationofKLdivergencebetweenarunningaverageofthepolicynetworkandthecurrentpolicynetwork.Insteadofthehardconstraintimposedbythetrustregionoptimizer,Heessetal.[9]andSchulmanetal.[24]addedaKLcosttotheobjectivefunctionasasoftconstraint.Bothpapersshowsomeimprovementovervanillapolicygradientoncontinuousanddiscretecontroltasksintermsofsampleefﬁciency.Thereareotherrecentlyintroducedactor-criticmodelsthatimprovesampleefﬁciencybyintroducingexperiencereplay[28],[8]orauxiliaryobjectives[10].Theseapproachesareorthogonaltoourwork,andcouldpotentiallybecombinedwithACKTRtofurtherenhancesampleefﬁciency.5ExperimentsWeconductedaseriesofexperimentstoinvestigatethefollowingquestions:(1)HowdoesACKTRcomparewiththestate-of-the-arton-policymethodandcommonsecond-orderoptimizerbaselineintermsofsampleefﬁciencyandcomputationalefﬁciency?(2)Whatmakesabetternormforoptimizationofthecritic?(3)HowdoestheperformanceofACKTRscalewithbatchsizecomparedtotheﬁrst-ordermethod?Weevaluatedourproposedmethod,ACKTR,ontwostandardbenchmarkplatforms.WeﬁrstevaluateditonthediscretecontroltasksdeﬁnedinOpenAIGym[5],simulatedbyArcadeLearningEnvironment[4],asimulatorforAtari2600gameswhichiscommonlyusedasadeepreinforcementlearningbenchmarkfordiscretecontrol.Wethenevaluateditonavarietyofcontinuouscontrol5ACKTRA2CTRPO(10M)DomainHumanlevelRewardsEpisodeRewardsEpisodeRewardsEpisodeBeamrider5775.013581.432798148.18930670.0N/ABreakout31.8735.74094581.61446414.7N/APong9.320.990419.94768-1.2N/AQ-bert13455.021500.3642215967.419168971.8N/ASeaquest20182.01776.0N/A1754.0N/A810.4N/ASpaceInvaders1652.019723.0146961757.2N/A465.1N/ATable1:ACKTRandA2Cresultsshowingthelast100averageepisoderewardsattainedafter50milliontimesteps,andTRPOresultsafter10milliontimesteps.ThetablealsoshowstheepisodeN,whereNdenotestheﬁrstepisodeforwhichthemeanepisoderewardovertheNthgametothe(N+100)thgamecrossesthehumanperformancelevel[17],averagedover2randomseeds.benchmarktasksdeﬁnedinOpenAIGym[5],simulatedbytheMuJoCo[27]physicsengine.Ourbaselinesare(a)asynchronousandbatchedversionoftheasynchronousadvantageactorcriticmodel(A3C)[18],henceforthcalledA2C(advantageactorcritic),and(b)TRPO[22].ACKTRandthebaselinesusethesamemodelarchitectureexceptfortheTRPObaselineonAtarigames,withwhichwearelimitedtousingasmallerarchitecturebecauseofthecomputingburdenofrunningaconjugategradientinner-loop.Seetheappendixforotherexperimentdetails.5.1DiscretecontrolWeﬁrstpresentresultsonthestandardsixAtari2600gamestomeasuretheperformanceimprovementobtainedbyACKTR.TheresultsonthesixAtarigamestrainedfor10milliontimestepsareshowninFigure1,withcomparisontoA2CandTRPO2.ACKTRsigniﬁcantlyoutperformedA2Cintermsofsampleefﬁciency(i.e.,speedofconvergencepernumberoftimesteps)byasigniﬁcantmargininallgames.WefoundthatTRPOcouldonlylearntwogames,SeaquestandPong,in10milliontimesteps,andperformedworsethanA2Cintermsofsampleefﬁciency.InTable1wepresentthemeanofrewardsofthelast100episodesintrainingfor50milliontimesteps,aswellasthenumberofepisodesrequiredtoachievehumanperformance[17].Notably,onthegamesBeamrider,Breakout,Pong,andQ-bert,A2Crequiredrespectively2.7,3.5,5.3,and3.0timesmoreepisodesthanACKTRtoachievehumanperformance.Inaddition,oneoftherunsbyA2CinSpaceInvadersfailedtomatchhumanperformance,whereasACKTRachieved19723onaverage,12timesbetterthanhumanperformance(1652).OnthegamesBreakout,Q-bertandBeamrider,ACKTRachieved26%,35%,and67%largerepisoderewardsthanA2C.WealsoevaluatedACKTRontherestoftheAtarigames;seeAppendixBforfullresults.WecomparedACKTRwithQ-learningmethods,andwefoundthatin36outof44benchmarks,ACKTRisonparwithQ-learningmethodsintermsofsampleefﬁciency,andconsumedalotlesscomputationtime.Remarkably,inthegameofAtlantis,ACKTRquicklylearnedtoobtainrewardsof2millionin1.3hours(600episodes),asshowninFigure2.IttookA2C10hours(6000episodes)toreachthesameperformancelevel.5.2ContinuouscontrolWeranexperimentsonthestandardbenchmarkofcontinuouscontroltasksdeﬁnedinOpenAIGym[5]simulatedinMuJoCo[27],bothfromlow-dimensionalstate-spacerepresentationanddirectlyfrompixels.IncontrasttoAtari,thecontinuouscontroltasksaresometimesmorechallengingduetohigh-dimensionalactionspacesandexploration.TheresultsofeightMuJoCoenvironmentstrainedfor1milliontimestepsareshowninFigure3.OurmodelsigniﬁcantlyoutperformedbaselinesonsixoutofeightMuJoCotasksandperformedcompetitivelywithA2Contheothertwotasks(Walker2dandSwimmer).WefurtherevaluatedACKTRfor30milliontimestepsoneightMuJoCotasksandinTable2wepresentmeanrewardsofthetop10consecutiveepisodesintraining,aswellasthenumberof2TheA2CandTRPOAtaribaselineresultsareprovidedtousbytheOpenAIteam,https://github.com/openai/baselines-results.6Figure3:PerformancecomparisonsoneightMuJoCoenvironmentstrainedfor1milliontimesteps(1timestepequals4frames).Theshadedregiondenotesthestandarddeviationover3randomseeds.Figure4:Performancecomparisonson3MuJoCoenvironmentsfromimageobservationstrainedfor40milliontimesteps(1timestepequals4frames).episodestoreachacertainthresholddeﬁnedin[8].AsshowninTable2,ACKTRreachesthespeciﬁedthresholdfasteronalltasks,exceptforSwimmerwhereTRPOachieves4.1timesbettersampleefﬁciency.AparticularlynotablecaseisAnt,whereACKTRis16.4timesmoresampleefﬁcientthanTRPO.Asforthemeanrewardscore,allthreemodelsachieveresultscomparablewitheachotherwiththeexceptionofTRPO,whichintheWalker2denvironmentachievesa10%betterrewardscore.Wealsoattemptedtolearncontinuouscontrolpoliciesdirectlyfrompixels,withoutprovidinglow-dimensionalstatespaceasaninput.Learningcontinuouscontrolpoliciesfrompixelsismuchmorechallengingthanlearningfromthestatespace,partiallyduetotheslowerrenderingtimecomparedtoAtari(0.5secondsinMuJoCovs0.002secondsinAtari).Thestate-of-the-artactor-criticmethodA3C[18]onlyreportedresultsfrompixelsonrelativelysimpletasks,suchasPendulum,Pointmass2D,andGripper.AsshowninFigure4wecanseethatourmodelsigniﬁcantlyoutperformsA2Cintermsofﬁnalepisoderewardaftertrainingfor40milliontimesteps.Morespeciﬁcally,onReacher,HalfCheetah,andWalker2dourmodelachieveda1.6,2.8,and1.7timesgreaterﬁnalrewardcomparedtoA2C.Thevideosoftrainedpoliciesfrompixelscanbefoundathttps://www.youtube.com/watch?v=gtM87w1xGoM.Pretrainedmodelweightsareavailableathttps://github.com/emansim/acktr.5.3Abetternormforcriticoptimization?Thepreviousnaturalpolicygradientmethodappliedanaturalgradientupdateonlytotheactor.Inourwork,weproposealsoapplyinganaturalgradientupdatetothecritic.Thedifferenceliesinthenormwithwhichwechoosetoperformsteepestdescentonthecritic;thatis,thenorm||·||Bdeﬁnedinsection2.2.Inthissection,weappliedACKTRtotheactor,andcomparedusingaﬁrst-ordermethod(i.e.,Euclideannorm)withusingACKTR(i.e.,thenormdeﬁnedbyGauss-Newton)forcriticoptimization.Figures5(a)and(b)showtheresultsonthecontinuouscontroltaskHalfCheetahandtheAtarigameBreakout.Weobservethatregardlessofwhichnormweusetooptimizethecritic,thereareimprovementsbroughtbyapplyingACKTRtotheactorcomparedtothebaselineA2C.7ACKTRA2CTRPO(10M)DomainThresholdRewardsEpisodesRewardsEpisodesRewardsEpisodesAnt3500(6000)4621.636604870.51061865095.060156HalfCheetah4700(4800)5586.3129805343.7211525704.721033Hopper2000(3800)3915.9170333915.3334813755.039426InvertedPendulum950(950)1000.068311000.0109821000.029267InvertedDoublePendulum9100(9100)9356.0419969356.1826949320.078519Reacher-7(-3.75)-1.53325-1.720591-2.014940Swimmer90(360)138.06475140.711516136.41571Walker2d3000(N/A)6198.8150435874.9268286874.127720Table2:ACKTR,A2C,andTRPOresults,showingthetop10averageepisoderewardsattainedwithin30milliontimesteps,averagedoverthe3bestperformingrandomseedsoutof8randomseeds.“Episode”denotesthesmallestNforwhichthemeanepisoderewardovertheNthtothe(N+10)thgamecrossesacertainthreshold.ThethresholdsforallenvironmentsexceptforInvertedPendulumandInvertedDoublePendulumwerechosenaccordingtoGuetal.[8],andinbracketsweshowtherewardthresholdneededtosolvetheenvironmentaccordingtotheOpenAIGymwebsite[5].However,theimprovementsbroughtbyusingtheGauss-Newtonnormforoptimizingthecriticaremoresubstantialintermsofsampleefﬁciencyandepisoderewardsattheendoftraining.Inaddition,theGauss-Newtonnormalsohelpsstabilizethetraining,asweobservelargervarianceintheresultsoverrandomseedswiththeEuclideannorm.RecallthattheFishermatrixforthecriticisconstructedusingtheoutputdistributionofthecritic,aGaussiandistributionwithvarianceσ.InvanillaGauss-Newton,σissetto1.WeexperimentedwithestimatingσusingthevarianceoftheBellmanerror,whichresemblesestimatingthevarianceofthenoiseinregressionanalysis.WecallthismethodadaptiveGauss-Newton.However,weﬁndadaptiveGauss-Newtondoesn’tprovideanysigniﬁcantimprovementovervanillaGauss-Newton.(SeedetailedcomparisonsonthechoicesofσinAppendixD).5.4HowdoesACKTRcomparewithA2Cinwall-clocktime?WecomparedACKTRtothebaselinesA2CandTRPOintermsofwall-clocktime.Table3showstheaveragetimestepspersecondoversixAtarigamesandeightMuJoCo(fromstatespace)environments.Theresultisobtainedwiththesameexperimentsetupaspreviousexperiments.NotethatinMuJoCotasksepisodesareprocessedsequentially,whereasintheAtarienvironmentepisodesareprocessedinparallel;hencemoreframesareprocessedinAtarienvironments.FromthetableweseethatACKTRonlyincreasescomputingtimebyatmost25%pertimestep,demonstratingitspracticalitywithlargeoptimizationbeneﬁts.(Timesteps/Second)AtariMuJoCobatchsize801606401000250025000ACKTR712753852519551582A2C101010381162624650651TRPO160161177593619637Table3:Comparisonofcomputationalcost.TheaveragetimestepspersecondoversixAtarigamesandeightMuJoCotasksduringtrainingforeachalgorithms.ACKTRonlyincreasescomputingtimeatmost25%overA2C.5.5HowdoACKTRandA2Cperformwithdifferentbatchsizes?Inalarge-scaledistributedlearningsetting,largebatchsizeisusedinoptimization.Therefore,insuchasetting,itispreferabletouseamethodthatcanscalewellwithbatchsize.Inthissection,wecomparehowACKTRandthebaselineA2Cperformwithrespecttodifferentbatchsizes.Weexperimentedwithbatchsizesof160and640.Figure5(c)showstherewardsinnumberoftimesteps.WefoundthatACKTRwithalargerbatchsizeperformedaswellasthatwithasmallerbatchsize.However,withalargerbatchsize,A2Cexperiencedsigniﬁcantdegradationintermsofsampleefﬁciency.ThiscorrespondstotheobservationinFigure5(d),whereweplottedthetrainingcurve8(a)(b)(c)(d)Figure5:(a)and(b)compareoptimizingthecritic(valuenetwork)withaGauss-Newtonnorm(ACKTR)againstaEuclideannorm(ﬁrstorder).(c)and(d)compareACKTRandA2Cwithdifferentbatchsizes.intermsofnumberofupdates.WeseethatthebeneﬁtincreasessubstantiallywhenusingalargerbatchsizewithACKTRcomparedtowithA2C.Thissuggeststhereispotentialforlargespeed-upswithACKTRinadistributedsetting,whereoneneedstouselargemini-batches;thismatchestheobservationin[2].6ConclusionInthisworkweproposedasample-efﬁcientandcomputationallyinexpensivetrust-region-optimizationmethodfordeepreinforcementlearning.WeusedarecentlyproposedtechniquecalledK-FACtoapproximatethenaturalgradientupdateforactor-criticmethods,withtrustregionoptimizationforstability.Tothebestofourknowledge,wearetheﬁrsttoproposeoptimizingboththeactorandthecriticusingnaturalgradientupdates.WetestedourmethodonAtarigamesaswellastheMuJoCoenvironments,andweobserved2-to3-foldimprovementsinsampleefﬁciencyonaveragecomparedwithaﬁrst-ordergradientmethod(A2C)andaniterativesecond-ordermethod(TRPO).Becauseofthescalabilityofouralgorithm,wearealsotheﬁrsttotrainseveralnon-trivialtasksincontinuouscontroldirectlyfromrawpixelobservationspace.ThissuggeststhatextendingKronecker-factorednaturalgradientapproximationstootheralgorithmsinreinforcementlearningisapromisingresearchdirection.AcknowledgementsWewouldliketothanktheOpenAIteamfortheirgeneroussupportinprovidingbaselineresultsandAtarienvironmentpreprocessingcodes.WealsowanttothankJohnSchulmanforhelpfuldiscussions.References[1]S.I.Amari.Naturalgradientworksefﬁcientlyinlearning.NeuralComputation,10(2):251–276,1998.[2]J.Ba,R.Grosse,andJ.Martens.Distributedsecond-orderoptimizationusingKronecker-factoredapproximations.InICLR,2017.[3]J.A.BagnellandJ.G.Schneider.Covariantpolicysearch.InIJCAI,2003.[4]M.G.Bellemare,Y.Naddaf,J.Veness,andM.Bowling.Thearcadelearningenvironment:Anevaluationplatformforgeneralagents.JournalofArtiﬁcialIntelligenceResearch,47:253–279,2013.[5]G.Brockman,V.Cheung,L.Pettersson,J.Schneider,J.Schulman,J.Tang,andW.Zaremba.OpenAIGym.arXivpreprintarXiv:1606.01540,2016.[6]D.A.Clevert,T.Unterthiner,andS.Hochreiter.Fastandaccuratedeepnetworklearningbyexponentiallinearunits(ELUs).InICLR,2016.[7]R.GrosseandJ.Martens.AKronecker-factoredapproximateFishermatrixforconvolutionallayers.InICML,2016.9[8]S.Gu,T.Lillicrap,Z.Ghahramani,R.E.Turner,andS.Levine.Q-prop:Sample-efﬁcientpolicygradientwithanoff-policycritic.InICLR,2017.[9]N.Heess,D.TB,S.Sriram,J.Lemmon,J.Merel,G.Wayne,Y.Tassa,T.Erez,Z.Wang,S.M.A.Eslami,M.Riedmiller,andD.Silver.Emergenceoflocomotionbehavioursinrichenvironments.arXivpreprintarXiv:1707.02286.[10]M.Jaderberg,V.Mnih,W.M.Czarnecki,T.Schaul,J.Z.Leibo,D.Silver,andK.Kavukcuoglu.Reinforcementlearningwithunsupervisedauxiliarytasks.InICLR,2017.[11]S.Kakade.Anaturalpolicygradient.InAdvancesinNeuralInformationProcessingSystems,2002.[12]D.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.ICLR,2015.[13]T.P.Lillicrap,J.J.Hunt,A.Pritzel,N.Heess,T.Erez,Y.Tassa,D.Silver,andD.Wierstra.Continuouscontrolwithdeepreinforcementlearning.InICLR,2016.[14]J.Martens.DeeplearningviaHessian-freeoptimization.InICML-10,2010.[15]J.Martens.Newinsightsandperspectivesonthenaturalgradientmethod.arXivpreprintarXiv:1412.1193,2014.[16]J.MartensandR.Grosse.Optimizingneuralnetworkswithkronecker-factoredapproximatecurvature.[17]V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,S.Petersen,C.Beattie,A.Sadik,I.Antonoglou,H.King,D.Kumaran,D.Wierstra,S.Legg,andD.Hassabis.Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533,2015.[18]V.Mnih,A.PuigdomenechBadia,M.Mirza,A.Graves,T.P.Lillicrap,T.Harley,D.Silver,andK.Kavukcuoglu.Asynchronousmethodsfordeepreinforcementlearning.InICML,2016.[19]J.NocedalandS.Wright.NumericalOptimization.Springer,2006.[20]J.PetersandS.Schaal.Naturalactor-critic.Neurocomputing,71(7-9):1180–1190,2008.[21]N.N.Schraudolph.Fastcurvaturematrix-vectorproductsforsecond-ordergradientdescent.NeuralComputation,2002.[22]J.Schulman,S.Levine,P.Abbeel,M.I.Jordan,andP.Moritz.Trustregionpolicyoptimization.InProceedingsofthe32ndInternationalConferenceonMachineLearning(ICML),2015.[23]J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel.High-dimensionalcontinuouscontrolusinggeneralizedadvantageestimation.InProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR),2016.[24]J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov.Proximalpolicyoptimizationalgorithms.arXivpreprintarXiv:1707.06347.[25]D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.VanDenDriessche,J.Schrittwieser,I.Antonoglou,V.Panneershelvam,M.Lanctot,S.Dieleman,D.Grewe,J.Nham,N.Kalch-brenner,I.Sutskever,T.Lillicrap,M.Leach,K.Kavukcuoglu,T.Graepel,andD.Hassabis.MasteringthegameofGowithdeepneuralnetworksandtreesearch.Nature,529(7587):484–489,2016.[26]R.S.Sutton,D.A.McAllester,S.Singh,andY.Mansour.Policygradientmethodsforreinforce-mentlearningwithfunctionapproximation.InAdvancesinNeuralInformationProcessingSystems12,2000.[27]E.Todorov,T.Erez,andY.Tassa.MuJoCo:Aphysicsengineformodel-basedcontrol.IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,2012.10[28]Z.Wang,V.Bapst,N.Heess,V.Mnih,R.Munos,K.Kavukcuoglu,andN.deFreitas.Sampleefﬁcientactor-criticwithexperiencereplay.InICLR,2016.[29]Z.Wang,T.Schaul,M.Hessel,H.vanHasselt,M.Lanctot,andN.deFreitas.Duelingnetworkarchitecturesfordeepreinforcementlearning.InProceedingsoftheInternationalConferenceonMachineLearning(ICML).[30]R.J.Williams.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.MachineLearning,8(3):229–256,1992.AExperimentaldetailsA.1DiscretecontrolForexperimentsontheAtariEnvironment,weadoptedthesameinputpreprocessingprocedureasin[17],withaslightmodiﬁcationtothearchitecture.Speciﬁcally,weusedasharednetworktoparameterizethepolicyandvaluefunction:Theﬁrstconvolutionallayerisof32ﬁltersofsize8×8withstride4followedbyanotherconvolutionallayerwith64ﬁltersofsize4×4andstride2,followedbyaﬁnalconvolutionallayerwith32ﬁltersofsize3×3withstride1,followedbyafullyconnectedlayerofsize512,followedbyonesoftmaxoutputlayerthatparameterizesthepolicyandalinearoutputlayerthatpredictsthevalue.Weused32ﬁltersinthethirdconvolutionallayerbecausewefoundthatitsavedtimeforcomputingtheFishermatrixinversewithoutanydegradationinperformance.(Onealternativewouldbetousethedoublyfactoredapproximation[2]withall64ﬁlters.)ForthebaselineA2C,weusedthesamearchitectureasin[17].ForTRPO,becauseofitshighper-iterationexpense,weusedasmallerarchitecture,with2convolutionallayersfollowedbyafullyconnectedlayerwith128units.Theﬁrstconvolutionallayerhad8ﬁltersofsize8×8withstride4,followedbyanotherconvolutionallayerwith16ﬁltersofsize4×4withstride2.Wetunedthemaximumlearningrateηmaxusingagridsearchover{0.7,0.2,0.07,0.02}onthegameofBreakout,withthetrustregionradiusδsetto0.001.WeusedthesamehyperparametersforallAtariexperiments.Boththebaseline(A2C)andourmethodusedalinearscheduleforthelearningrateoverthecourseoftraining,andentropyregularizationwithweight0.01.Following[17],theagentistrainedoneachgameusing50milliontimestepsor200millionframes.Unlessotherwisestated,weusedabatchsizeof640forACKTR,80forA2C,and512forTRPO.Thebatchsizeswerechosentoachievebettersampleefﬁciency.A.2ContinuouscontrolForexperimentswithlow-dimensionalstatespaceasaninputweusedtwoseparateneuralnetworkswith64hiddenunitsperlayerinatwo-layernetwork.WeusedTanhandELU[6]nonlinearitiesforthepolicynetworkandvaluenetwork,respectively,foralllayersexcepttheoutputlayer,whichdidn’thaveanynonlinearity.ThelogstandarddeviationofaGaussianpolicywasparameterizedasabiasinaﬁnallayerofpolicynetworkthatdidn’tdependoninputstate.Forallexperiments,weusedACKTRandA2Ctrainedwithbatchsizesof2500andTRPOtrainedwithabatchsizeof25000.ThebatchsizeswerechosentobeconsistentwiththeexperimentaldesignoftheresultsprovidedbytheOpenAIteam.Forexperimentsusingpixelsasaninputwepassedina42×42RGBimagealongwiththepreviousframetoaconvolutionalneuralnetwork.Thetwoconvolutionallayerscontained32ﬁltersofsize3×3withstrideof2followedbyafullyconnectedlayerof256hiddenunits.IncontrasttoourAtariexperiments,wefoundthatseparatingthepolicynetworkandvaluefunctionintotwoseparatenetworksresultedinbetterempiricalperformanceinbothACKTRandA2C.WeusedReLUnonlinearityforthepolicynetworkandELU[6]nonlinearityforthevaluefunction.Wealsofoundthatitisimportanttouseorthogonalinitializationforbothnetworks,otherwisetheA2Cbaselinefailedtoimproveitsepisodereward.Allmodelsweretrainedwithbatchsizeof8000.Wetunedthemaximumlearningrateηmaxusingagridsearchover{0.3,0.03,0.003}onthetasksofReacherandHopper,withthetrustregionradiusδsetto0.001.WeﬁxedhyperparametersforallMuJoCoexperiments.11Table4:Rawscoresacrossallgames,startingwith30no-opactions.Otherscoresfrom[29].GAMESHUMANDQNDDQNDUELPRIOR.PRIOR.DUEL.ACKTRAlien7,127.71,620.03,747.74,461.44,203.83,941.03197.1Amidar1,719.5978.01,793.32,354.51,838.92,296.81059.4Assault742.04,280.45,393.24,621.07,672.111,477.010,777.7Asterix8,503.34,359.017,356.528,188.031,527.0375,080.031,583.0Asteroids47,388.71,364.5734.72,837.72,654.31,192.734,171.6Atlantis29,028.1279,987.0106,056.0382,572.0357,324.0395,762.03,433,182.0BankHeist753.1455.01,030.61,611.91,054.61,503.11,289.7BattleZone37,187.529,900.031,700.037,150.031,530.035,520.08910.0Beamrider16,926.58,627.513,772.812,164.023,384.230,276.513,581.4Berzerk2,630.4585.61,225.41,472.61,305.63,409.0927.2Bowling160.750.468.165.547.946.724.3Boxing12.188.091.699.495.698.91.45Breakout30.5385.5418.5345.3373.9366.0735.7Centipede12,017.04,657.75,409.47,561.44,463.27,687.57,125.28CrazyClimber35,829.4110,763.0117,282.0143,570.0141,161.0162,224.0150,444.0DemonAttack1,971.012,149.458,044.260,813.371,846.472,878.6274,176.7DoubleDunk-16.4-6.6-5.50.118.5-12.5-0.54Enduro860.5729.01,211.82,258.22,093.02,306.40.0FishingDerby-38.7-4.915.546.439.541.333.73Freeway29.630.833.30.033.733.00.0Gopher2,412.58,777.414,840.815,718.432,487.2104,368.247,730.8IceHockey0.9-1.9-2.70.51.3-0.4-4.2JamesBond302.8768.51,358.01,312.55,148.0812.0490.0Kangaroo3,035.07,259.012,992.014,854.016,200.01,792.03,150.0Krull2,665.58,422.37,920.511,451.99,728.010,374.49,686.9Kung-FuMaster22,736.326,059.029,710.034,294.039,581.048,375.034,954.0Phoenix7,242.68,485.212,252.523,092.218,992.770,324.3133,433.7Pitfall!6,463.7-286.1-29.90.0-356.50.0-1.1Pong14.619.520.921.020.620.920.9Q-bert13,455.013,117.315,088.519,220.316,256.518,760.323,151.5RiverRaid17,118.07,377.614,884.521,162.614,522.320,607.617,762.8RoadRunner7,845.039,544.044,127.069,524.057,608.062,151.053,446.0Robotank11.963.965.165.362.627.516.5Seaquest42,054.75,860.616,452.750,254.226,357.8931.61,776.0Solaris12,326.73,482.83,067.82,250.84,309.0133.42,368.6SpaceInvaders1,668.71,692.32,525.56,427.32,865.815,311.519,723.0StarGunner10,250.054,282.060,142.089,238.063,302.0125,117.082,920.0TimePilot5,229.24,870.08,339.011,666.09,197.07,553.022,286.0Tutankham167.668.1218.4211.4204.6245.9314.3UpandDown11,693.29,989.922,972.244,939.616,154.133,879.1436,665.8VideoPinball17,667.9196,760.4309,941.998,209.5282,007.3479,197.0100,496.6WizardOfWor4,756.52,704.07,492.07,855.04,802.012,352.0702.0Yars’Revenge54,576.918,098.911,712.649,622.111,357.069,618.1125,169.0Zaxxon9,173.35,363.010,163.012,944.010,469.013,886.017,448.0BResultsontheremainingAtarigamesInthissectionwepresentresultsontherestoftheAtarigamesinTable4.Thescorereportedforourmethodisthemeanofthelast100episoderewardsafter50milliontimesteps.Eachepisodeisstartedwith30no-opactions.WeﬁndthatthereisnoresultreportedinA3C[18]orA2Cusingthesamemetric.HencewecompareourresultswithotherQ-learningmethodsobtainedfrom[29].Duetolimitedcomputationalresources,wewereonlyabletoevaluateACKTRonasubsetofthegames.Ourresultsareobtainedwithasinglerandomseedandwehavenottunedanyhyperparameters.Althoughweuseonlyonerandomseed,ourresultsareonparwithQ-learningmethods,whichuseoff-policytechniquessuchasexperiencereplay.Q-learningmethodsusuallytakedaystoﬁnishonetraining,whereasourmethodtakesonly16hoursonamodernGPU.12CMuJoCoresultswithcomparisonstoOpenAIbaselinesWecomparedACKTRwiththeresultsofA2CandTRPOsenttousbytheOpenAIteam(https://github.com/openai/baselines-results).Wefollowedtheirexperimentalprotocolascloselyaspossible.Likeourbaselines,A2CandTRPOweretrainedwiththesametwo-layerarchitecturewith64hiddenunitsineachlayeronbatchsizesof2500and25000respectively.However,incontrasttoourbaselines,thevaluefunctionusedTanhnonlinearitiesandwas“softly”updatedbycalculatingtheweightedaverageofthevaluefunctionbeforeandaftertheupdate.ComparedtoourimplementationoftheA2Cbaseline,A2CimplementedbyOpenAIperformedbetterontheHopper,InvertedPendulum,Swimmer,andWalker2dtaskswhileperformingworseontheReacherandHalfCheetahtasks.TRPObyOpenAIperformedworsethantheTRPOtrainedbyusonHopperwhileachievingthesameperformanceontherestofthetasks.ResultsareshowninFigure6.‘Figure6:PerformancecomparisonsonsevenMuJoCoenvironmentstrainedfor1milliontimesteps(1timestepequals4frames).ThenumbersforA2CandTRPOwereprovidedtousbytheOpenAIteam.Theshadedregiondenotesthestandarddeviationover3randomseeds.DAdaptiveGauss-Newton?InthissectionweinvestigatewhethertrainingthecriticusingadaptiveGauss-Newton(i.e.,keepinganestimateofthestandarddeviationoftheBellmanerrorasthestandarddeviationofthecriticoutputdistribution)providesanyimprovementovervanillaGauss-Newton(bothdeﬁnedinSection3.1).WeranadaptiveGauss-NewtononallsixstandardAtarigamesandeightMuJoCotasks.TheresultsareshowninFigure7andFigure8.WeseethatinAtarigames,adaptiveGauss-NewtonhurtstheperformanceintermsofsampleefﬁciencyinBeamrider,Q-bertandSeaquest,andshowsonlyaslightimprovementinthegameofPong.InMuJoCotasks,adaptiveGauss-NewtongivesaslightimprovementonthetasksofInvertedDoublePendulum,Swimmer,Walker2d,andAntwhileperformingonparonthetasksofInvertedPendulumandReacher,andworkingconsiderablyworseontheHalfCheetahtaskcomparedtovanillaGauss-Newton.EHowwelldoestheKronecker-factoredquadraticapproximationmatchtheexactKL?WeindirectlytestedhowaccuratetheKronecker-factoredapproximationtothecurvatureisbymeasuringtheexactKLchangesduringtraining,whileperformingtrustregionoptimizationusingaKronecker-factoredquadraticmodel.WetestedthisintwoMujocoenvironments,HalfCheetahandReacher.ThevaluesofapproximatedKLandexactKLareshowninFigure9.Fromtheplotwesee13Figure7:PerformancecomparisonsofcritictrainedwithadaptiveGauss-NewtonandvanillaGauss-NewtononsixAtarienvironmentstrainedfor10milliontimesteps(1timestepequals4frames).Theshadedregiondenotesthestandarddeviationover2randomseeds.Figure8:PerformancecomparisonsofcritictrainedwithadaptiveGauss-NewtonandvanillaGauss-NewtononeightMuJoCoenvironmentstrainedfor1milliontimesteps(1timestepequals4frames).Theshadedregiondenotesthestandarddeviationover3randomseeds.thatexactKLisclosetothetrustregionradius,showingtheeffectivenessoftrustregionoptimizationviaKronecker-factoredapproximation.Figure9:TheplotshowstheexactKLchangesduringtrainingwithtrustregionoptimizationusingACKTR.TheactualKLisclosetothetrustregionradius,showingtheeffectivenessoftrustregionoptimizationviaKronecker-factoredapproximation.14
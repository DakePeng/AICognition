Computational Limitations in Robust Classiﬁcation and Win-Win
Results∗

Akshay Degwekar

Preetum Nakkiran

Vinod Vaikuntanathan

June 6, 2019

Abstract

We continue the study of statistical/computational tradeoﬀs in learning robust classiﬁers,
following the recent work of Bubeck, Lee, Price and Razenshteyn who showed examples of
classiﬁcation tasks where (a) an eﬃcient robust classiﬁer exists, in the small-perturbation regime;
(b) a non-robust classiﬁer can be learned eﬃciently; but (c) it is computationally hard to learn
a robust classiﬁer, assuming the hardness of factoring large numbers. Indeed, the question of
whether a robust classiﬁer for their task exists in the large perturbation regime seems related
to important open questions in computational number theory.
In this work, we extend their work in three directions.
First, we demonstrate classiﬁcation tasks where computationally eﬃcient robust classiﬁcation
is impossible, even when computationally unbounded robust classiﬁers exist. For this, we rely
on the existence of average-case hard functions, requiring no cryptographic assumptions.

Second, we show hard-to-robustly-learn classiﬁcation tasks in the large-perturbation regime.
Namely, we show that even though an eﬃcient classiﬁer that is very robust (namely, tolerant to
large perturbations) exists, it is computationally hard to learn any non-trivial robust classiﬁer.
Our ﬁrst construction relies on the existence of one-way functions, a minimal assumption in
cryptography, and the second on the hardness of the learning parity with noise problem. In
the latter setting, not only does a non-robust classiﬁer exist, but also an eﬃcient algorithm
that generates fresh new labeled samples given access to polynomially many training examples
(termed as generation by Kearns et. al. (1994)).

Third, we show that any such counterexample implies the existence of cryptographic prim-
itives such as one-way functions or even forms of public-key encryption. This leads us to a
win-win scenario: either we can quickly learn an eﬃcient robust classiﬁer, or we can construct
new instances of popular and useful cryptographic primitives.

9
1
0
2

n
u
J

5

]
L
M

.
t
a
t
s
[

2
v
6
8
0
1
0
.
2
0
9
1
:
v
i
X
r
a

∗This work is a merge of [DV19] and [Nak19].

 
 
 
 
 
 
Contents

1 Introduction

2 Our Results

2.1 Existence (World 2)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Learnability (World 4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 A Win-Win Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Related Work.

1

2
2
3
3
4

3 Our Techniques

3.1 The BLPR Classiﬁcation Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Deﬁnitions: Robust Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Unlearnability From Pseudorandom Functions and Error Correcting Codes
. . . . .
3.4 Non-Existence of Robust Classiﬁers from Average-Case Hardness . . . . . . . . . . .
3.5 From Hardness of Decoding under Noise. . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Converse: Cryptography from Hardness of Robust Classiﬁcation.

5
5
6
6
7
8
. . . . . . . . . . . 10

4 Deﬁnitions

11
4.1 Learning & Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.2 Hardness of Eﬃcient Robust Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . 12

5 Learning Parity with Noise

13
5.1 Assumption Deﬁnition and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2 No Eﬃcient Robust Classiﬁer Exists . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5.3 Eﬃcient Robust Classiﬁer Exists but is Hard to Learn . . . . . . . . . . . . . . . . . 17

6 Learning with Errors

19
6.1 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2 No Eﬃcient Robust Classiﬁer Exists . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.3 An Eﬃcient Robust Classiﬁer Exists but is Hard to Learn . . . . . . . . . . . . . . . 20

7 Using Pseudorandom Functions and Error Correcting Codes

8 Using Average-Case Hardness and Error Correcting Codes

9 Cryptography from Robustly Hard Tasks

A A Description of BLPR Example and the Blum-Blum-Shub PRG.

22

24

26

30

i

1

Introduction

The basic task in learning theory is to learn a classiﬁer given a dataset. Namely, given a labeled
dataset {(Xi, f (Xi))}i∈[n] where f is the unknown ground-truth and Xi are drawn i.i.d.
from a
distribution D, learn a classiﬁer h so as to (approximately) minimize

δ := P

X∼D

[h(X) (cid:54)= f (X)]

Adversarial machine learning is harder in that the learned classiﬁer is required to be robust. Namely,
it has to produce the right answer even under bounded perturbations (under some distance measure)
of the sample X ∼ D. That is, the goal is to learn a classiﬁer h so as to (approximately) minimize

δ := P

X∼D

[∃Y ∈ B(X, ε) s.t. h(Y ) (cid:54)= f (Y )]

where B(X, ε) = {Y : d(X, Y ) ≤ (cid:15)} and d is the distance measure in question.

Learning robust classiﬁers is an important question given a large number of attacks against
practical machine learning systems that show how to minimally perturb a sample X so that clas-
siﬁers output the wrong prediction with high probability. Such attacks were ﬁrst discovered in the
context of spam ﬁltering and malware classiﬁcation [DDS+04, LM05, BR18] and more recently,
following [GSS, SZS+13], in image classiﬁcation, voice recognition and many other domains.

This state of aﬀairs raises a slew of questions in learning theory. Fix a concept class F and a
distribution D for which eﬃcient (non-robust) learning is possible. Do there exist robust classiﬁers
for F ? Do there exist eﬃciently computable robust classiﬁers for F ? Pushing the envelope further,
can such classiﬁers be learned with small sample-complexity? and ﬁnally, is the learning algorithm
computationally eﬃcient? The answer to these questions give rise to ﬁve possible worlds of robust
learning, ﬁrst postulated in two recent works [BPR18] and [BLPR18], henceforth referred to as
BPR and BLPR respectively.1 This is the starting point of our work.

World 1. No robust classiﬁers exist, regardless of computational or sample-eﬃciency considera-
tions. [FFF18] show a learning task in this world, namely one where computationally eﬃcient
non-robust classiﬁcation is possible, no robust classiﬁers exist. On the other hand, for natural
learning tasks, humans seem to be robust classiﬁers that tolerate non-zero error rate (cid:15), indeed
even eﬃcient robust classiﬁers; see [BPR18] for a more detailed discussion.

World 2. Robust classiﬁers exist, but they are computationally ineﬃcient. We demonstrate learn-

ing tasks in this world.

World 3. Computationally eﬃcient robust classiﬁers exist, but learning them incurs large sample
complexity. [SST+18] show a learning task where a computationally eﬃcient robust classiﬁer
exists, but learning it requires polynomially more samples than non-robust learning. On the
other hand, [BPR18] show that this gap cannot be more than linear in the dimension; see
[BPR18] for a more detailed discussion.

World 4. Computationally eﬃcient robust classiﬁers exist, and can be learned sample-eﬃciently,
but training is computationally ineﬃcient.
[BLPR18] show a learning task in this world.
However, as we observe below, their computationally eﬃcient robust classiﬁer only recovers
from a very small number (indeed, a constant number) of perturbations. Whether there

1To be precise, [BPR18] postulated four worlds, namely worlds 1 and 3–5. Subsequent work of [BLPR18] added

the second world.

1

exists an eﬃcient robust classiﬁer for their task that recovers from large perturbations seems
related to long-standing open questions in computational number theory [Hen19, Gre13]. As
our second result, we show two examples of learning tasks that live in this world; more details
in Section 2.

World 5. The best world of all, in which there exist eﬃcient algorithms both for classiﬁcation and
training, and the sample complexity is small (but it could be that we haven’t discovered the
right algorithm just yet.)

We want to understand – are we likely to ﬁnd learning tasks such as the ones [BLPR18] and
we demonstrate in the wild? To that end, our third result is a win-win statement: namely,
any such learning task gives rise to a cryptographic object– either a simple one like a one-way
function or a complex one like public-key encryption.

We proceed to describe the three results in more detail.
But before we do so, a word of warning. We and [BLPR18] deﬁne these ﬁve worlds in a coarse
way using polynomial-time as a proxy for computational eﬃciency, and a large constant accuracy
as a proxy for successful classiﬁcation. (We should also mention that [BPR18] use SQ-learning as
a diﬀerent proxy for computationally eﬃcient learning.) One could be more careful and speak of
running-time/accuracy tradeoﬀs in the diﬀerent worlds, but since our goal here is to show broad
counterexamples, we do not attempt to do such a ﬁne-grained distinction.

2 Our Results

We explore the relationship of computational constraints and eﬃcient robust classiﬁcation. The
setting we consider consists of two distributions D0, D1 and the classiﬁer has to correctly classify
inputs from both. We consider the two facets to eﬃcient robust classiﬁcation: (1) existence: do
eﬃcient robust classiﬁers exist? (corresponds to World 2) and (2) learnbility: can we learn robust
classiﬁers eﬃciently? We show three sets results on which we elaborate below.

2.1 Existence (World 2)

In terms of feasibility, we show that there are learning tasks where while ineﬃcient robust classi-
ﬁcation is possible, no eﬃcient robust classiﬁers exist. That is, we demonstrate learning tasks in
World 2. We can show the following:

Theorem 2.1 (Informal). There exist classiﬁcation tasks over where (1) eﬃcient non-robust clas-
siﬁers exist, (2) no eﬃcient robust classiﬁer exists, but (3) ineﬃcient robust classiﬁers exist.

This result does not require cryptographic assumptions, and relies only on the existence of
average-case hard functions and good error-correcting codes. In fact, this result scales down to
more ﬁne-grained notions of eﬃciency than polynomial-time. All that is required is a function that
is average-case hard for the “eﬃcient” class, but computable by the “ineﬃcient” class.

We give several examples of such learning tasks, including some examples that require crypto-
graphic assumptions but obtain other desirable properties (such as obtaining tasks with eﬃciently-
samplable distributions). More details are given in Sections 3.4, 3.5, 5, 6 and 8.

2

2.2 Learnability (World 4)

We want to understand the hardness of learning an eﬃcient robust classiﬁer when it exists. The
starting point of this work was the BLPR work [BLPR18]. They showed that under cryptographic
assumptions, there exists a learning task which admits eﬃcient robust classiﬁers, but it is com-
putationally infeasible to train such a classiﬁer. More precisely, they showed that there exists a
classiﬁcation task (over {0, 1}n) where (a) learning any non-trivial robust classiﬁer is computation-
ally infeasible while (b) an eﬃcient robust classiﬁer exists.

Unfortunately, we observe that their robust classiﬁer is eﬃcient only when correcting a constant
number of errors.
Indeed, as we explain in Section 3.1, the question of whether there exists a
computationally eﬃcient robust classiﬁer for their task correcting even ω(1) bits of error is an
important open question in computational number theory that has received some attention in the
cryptanalysis community [Gre13, Hen19].

The BPLR construction can be rescued using error correcting codes to enable eﬃcient robust
classiﬁers robust to large (constant fraction) perturbations. Our results strengthen theirs in two
ways: we can weaken the required cryptographic assumption to that one-way functions exist and
demonstrate tasks where the gap between learning and robust classiﬁcation is more: in that eﬃcient
learning algorithms can learn to not only classify, but also to generate fresh samples from the
distributions.

Theorem 2.2 (Informal). Under the minimal cryptographic assumption that one-way functions,
there exist classiﬁcation tasks over {0, 1}m where (1) it is easy to learn a non-robust classiﬁer (2) an
eﬃcient robust classiﬁer that tolerates m/8-sized perturbations exists, and (3) it is computationally
hard to learn any non-trivial robust classiﬁer.

Theorem 2.3 (Informal). Assuming Learning Parity with Noise (or Learning with Errors) in the
“public-key” regime of parameters, there exist classiﬁcation tasks on {0, 1}m where (1) it is easy to
learn a non-robust classiﬁer. (2) an eﬃcient robust classiﬁer tolerating O(
m)-errors exists, and
(3) it is computationally hard to learn any non-trivial robust classiﬁer.

√

Furthermore, it is easy to learn generators/evaluators for the non-robust distributions.2

We elaborate on the diﬀerences between the two theorems in the techniques section. Brieﬂy,
there are three key diﬀerences: Theorem 2.3 requires a stronger assumption, but gives a more
“natural” example where the resulting distributions are “more easier” to learn non-robustly. In
particular, it is easy to learn how to generate fresh samples from the two distributions, something
that the one-way function based example cannot support. This is important because we want
to separate the complexity of learning the distribution from that of robust classiﬁcation. And
here, these distributions can be learned in a stronger sense while still being hard to classify under
adversarial perturbations.

2.3 A Win-Win Result

Finally, we want to understand – Are we likely to ﬁnd such learning tasks in the wild? To that
end, we show a converse to our results. Namely,

Theorem 2.4 (Informal). Any computational task where an eﬃcient robust classiﬁer exists, but is
hard to learn one in polynomial time implies one-way functions, and hence symmetric key cryptog-
raphy.

2Generators and Evaluators [KMR+94], are algorithms that can sample from the distribution and output the pdf

of the distribution respectively.

3

Furthermore, if the learning task satisﬁes certain natural properties, it gives us (a certain weaker

form of) public-key cryptography as well!

It would be very surprising to us if public-key cryptography (and even one-way functions)
arise out of natural classiﬁcation tasks on, say, images. Thus, perhaps uncharacteristically for
cryptographers, we oﬀer a possible (optimistic) interpretation of this state of aﬀairs: namely, that
for natural learning tasks where there exists a robust classifer, it can also be eﬃciently found, we
just haven’t ﬁgured out the right algorithm yet.

An important caveat is due here: our deﬁnition of hardness of learning a robust classiﬁer
is a strong one:
it requires that the perturbing adversary be constructive and universal. Our
classiﬁcation tasks do satisfy this deﬁnition, and that only makes them stronger. On the other
hand, it does make our converse weaker. More details are given in Section 3.

2.4 Related Work.

The works closest to ours are [BPR18, BLPR18]. We discuss them last.

Adversarial Examples. The problem of adversarial classiﬁcation ws ﬁrst considered by
[DDS+04]. Starting with [SZS+13], there is a large body of work demonstrating the existence
of small adversarial perturbations in neural networks that cause them to misclassify examples with
high conﬁdence. There have been various approaches proposed against such perturbations and
many of them have been broken (see [CW17, ACW18] and references therein).

A line of work which [GMF+18, FFF18, MM18] shows that for certain learning tasks and dis-
tributions (eg spheres in Rn or product distributions), due to concentration of measure adversarial
examples exist close to points in the distribution and can at times be found eﬃciently for classiﬁers
that are not perfectly correct, pointing to the challenges of robust classiﬁcation in this setting.
These works show evidence for World 1: that for certain speciﬁc models and training algorithms,
robust classiﬁers don’t exist. In our learning tasks, robust classiﬁcation is possible, albeit compu-
tationally ineﬃcient.

[SST+18] demonstrate simple classiﬁcation tasks (distinguishing between high dimensional gaus-
sians) where the sample complexity of robust learning is higher than that of classical learning by
[BPR18] show that this gap is es-
a polynomial factor. Hence they show evidence for world 3.
sentially tight. This work is similar in spirit to ours, with the resource being sample complexity
instead of computational complexity in our case. In the case of computational complexity, we can
essentially show exponential gap between the running time required for learning non-robustly vs
learning robust classiﬁers.

BPR/BLPR [BPR18, BLPR18].

In BPR, they showed two results. First, that in the
world of polynomial sample complexity with no bounds on running time, learning a non-robust
classiﬁer and learning a robust classifer have the comparable sample complexity, if such a robust
classiﬁer exists. Second, they exhibit a learning task where while learning a robust classiﬁer was
information-theoretically easy with polynomial sample complexity, but doing so was diﬃcult in the
SQ model and it required exponentially many queries. This gives rise to a task where learning a
robust classiﬁer in a computationally eﬃcient manner (in the SQ model) was a lot harder than
doing so ineﬃciently.

In a followup work, BLPR they considered strengthening the second BPR result to show that
under cryptographic assumptions, there exists a learning task which admitted eﬃcient robust classi-
ﬁers, but it was computationally infeasible to do so. They showed that there exists a classiﬁcation
task (over {0, 1}n) where learning any non-trivial robust classiﬁer is computationally infeasible

4

while an eﬃcient robust classiﬁer exists that can correct O(1)-bit error. A description of their
construction is given in Section 3.1 and Appendix A.

3 Our Techniques

In this section, we give a high level description of our techniques. We begin by describing the BLPR
classiﬁcation task and its limitations. Then we describe the deﬁnition of robust classiﬁcation and
non-existence/unlearnability of such classiﬁers. We then describe several recipes for constructing
tasks where robust classiﬁcation is computationally intractable.
In the ﬁrst recipe, based on
one-way functions, we show tasks where while eﬃcient robust classiﬁers exist, but are hard to
learn, thus proving Theorem 2.2. The second recipe assuming average-case hard functions proves
Theorem 2.1, where no eﬃcient robust classiﬁers exist. The ﬁnal recipe is based on hardness
assumptions on decoding noisy codewords / lattices, namely Learning Parity with Noise (LPN)
and Learning with Errors (LWE) and proves Theorems 2.1 and 2.3 in diﬀerent parameter regimes.

3.1 The BLPR Classiﬁcation Task

We sketch the [BLPR18] classiﬁcation task where it is diﬃcult to learn a robust classiﬁer. A more
detailed description of their construction is given in Appendix A.

The key object in their construction is a “trapdoor pseudorandom generator”. A pseudorandom
generator PRG : {0, 1}n → {0, 1}2n is an expanding function whose outputs are indistinguishable
(cid:8)y : y ← {0, 1}2n(cid:9).3 A trapdoor
from truly random strings. That is, {PRG(x) : x ← {0, 1}n} ≈c
pseudorandom generator has a hard-to-ﬁnd trapdoor trap that allows distinguishing the output of
the PRG from random outputs. That is, there exists a distinguisher D such that (say),

P
x←{0,1}n

[D(trap, TrapPRG(x)) = 1] −

P
y←{0,1}2n

[D(trap, y) = 1] > 0.99

They show that the Blum-Blum-Shub Pseudorandom generator [BBS86] has such a trapdoor. Given
a trapdoor PRG, their learning task D0, D1 is the following:

D0 = {(0, TrapPRG(x)) : x ← {0, 1}n} and, D1 = (cid:8)(1, y) : y ← {0, 1}2n(cid:9) .

The ﬁrst bit enables easy non-robust classiﬁcation. The fact that there exists an ineﬃcient robust
classiﬁer follows from a volume argument – that the there are a few PRG outputs in a large domain.
This implies that there is an ineﬃcient robust classiﬁer that tolerates O(
n)-sized perturbations.
That a robust classiﬁer is hard to learn follows from the perturbing adversary that sets the ﬁrst
bit to 0. A robust classiﬁer has to distinguish between outputs of the PRG from random strings,
without the trapdoor. This is infeasible by the security guarantee of the PRG.

√

Finally, what needs to be proved is that the trapdoor enables robust classiﬁcation. The trapdoor
indeed does enable a robust classiﬁer that tolerates constant-sized pertubations (i.e., if any constant
number of bits are altered) simply by exhaustive search among the polynomially many possible sets
of perturbed bits. For a constant c, the robust classiﬁer given input y goes over all nc words in the
Hamming ball y(cid:48) ∈ B(y, c) and checks if the distinguisher D(trap, y(cid:48)) = 1. If yes, output D0 else
output D1. But this approach does not give a classiﬁer beyond constant-sized errors because the
running time is exponential in the number of errors corrected.

3We say that two families of distributions {Xn}n∈N an {Yn}n∈N are computationally indistinguishable (denoted by
{Xn} ≈c {Yn} or X ≈c Y for brevity) if for every polynomial time distinguisher D, | Px←Xn [D(x)] − Py←Yn [D(y)]| ≤
negl(n).

5

The primary limitation of trapdoor PRGs is that the trapdoor does not enable decoding the
PRG output from the perturbed samples, only distinguishes PRG outputs from random strings.
Indeed, for the Blum-Blum-Shub trapdoor PRG (and related constructions such as the one of
Micali and Schnorr [MS91]) considered in BLPR, the question of whether there is any trapdoor
that permits robust inversion is an open question in computational number theory. We refer the
reader to Appendix A for discussions regarding related questions. To enable eﬃcient decoding,
their construction can be modiﬁed by using an error correcting code to make it robust to larger
pertubations.

3.2 Deﬁnitions: Robust Classiﬁcation

We start by describing the notion of robust classiﬁcation and hardness of robust classiﬁcation used.

Robust Classiﬁer. When we state that a robust classiﬁer exists (for given ε), we show the
strongest notion: that there exists a classiﬁer R (eﬃcient or ineﬃcient, as speciﬁed) that classiﬁes
all input close to a random sample correctly:

For b ∈ {0, 1},

P
x←Db

(cid:2)R(x(cid:48)) = b for all x(cid:48) ∈ B(x, ε)(cid:3) > 0.99 .

Non-Existence/Unlearnability of Robust Classiﬁers. When we describe the non-existence
(or unlearnability) of robust classiﬁcation, we satisfy the strongest notion: that there exists a poly-
time perturbation adversary P whose perturbed examples cannot be classiﬁed better than chance
by any eﬃcient (or eﬃciently learned) classiﬁer. That is, for any eﬃcient R (or R ← learnD0,D1(1n)),

(cid:2)R(PD0,D1(x)) = b(cid:3) < 0.5 + negl(n) ,

P
x←Db

where a negl : N → R is a function such that negl(n) < n−c for all c ∈ N for large enough
n. See Deﬁnition 4.5 for a formal deﬁnition of hard to learn robustly. This deﬁnition has two
key properties:
it is constructive (adversarial perturbations are found) and universal (the same
adversary works for all algorithms).

The negation of the robust classiﬁcation deﬁnition suggests the following deﬁnition:

for eﬃ-
(cid:2)B(x, ε) (cid:54)⊆ R−1(b)(cid:3) < 0.99. This deﬁnition is unsatisfying be-
ciently learned classiﬁers R, Px←Db
cause it says nothing about the hardness of ﬁnding such misclassiﬁed examples. In particular, if
such adversarial perturbations existed but were computationally hard to ﬁnd, then the existence of
adversarial examples is not an issue. Hence, we choose a constructive deﬁnition that requires such
examples to be eﬃciently found. The fact that the adversary is universal only makes the counter
examples stronger.

3.3 Unlearnability From Pseudorandom Functions and Error Correcting Codes

In this section, we construct a learning task where classiﬁcation is easy, robust classiﬁer exits, but
is hard to learn. The primary ingredients of this construction are pseudorandom functions and
error correcting codes. We introduce both the primitives and build the construction in stages. A
pseudorandom function family (PRF) [GGM86] is a family of keyed functions Fk : {0, 1}n → {0, 1}
where the key k ← {0, 1}n, that are indistinguishable from uniformly random functions to any
polynomial time algorithm. That is, for every poly time algorithm A,

P
k←{0,1}n

(cid:2)AFk (1n)(cid:3) ≈ P
Un

(cid:2)AUn(1n)(cid:3)

6

where Un : {0, 1}n → {0, 1} is a uniformly random function. PRFs can be constructed from one-
way fucntions. Kearns and Valiant [KV94] constructed a hard to learn classiﬁcation task using
pseudorandom functions as follows:

D0 = (x, Fk(x)) and, D1 = (x, 1 − Fk(x))

The task essentially asks to eﬃciently learn a predictor for the pseudorandom function which is
diﬃcult. To transform this task to one that is hard to learn robustly, while an eﬃcient robust
classiﬁer exists, we use error correcting codes. Recall that an error correcting code has two al-
gorithms (Encode, Decode) where Encode returns a redundant encoding of the message that the
Decode algorithm can eﬃciently recovers the encoded message even when the encoded codeword is
tampered adversarially to some degree. So, consider the following classiﬁcation task: distinguish
between error-corrected versions of the PRF:

D0 = Encode(x, Fk(x)) and, D1 = Encode(x, 1 − Fk(x)) .

Note that this task has the following properties: (1) A robust classiﬁer exits and, (2) a robust
classiﬁer is hard to learn. For the ﬁrst property, consider the following robust classiﬁer: the
classiﬁer given the secret key, ﬁrst decodes the perturbed sample using the Decode algorithm and
then checks if is of the form (x, Fk(x)) or (x, 1−Fk(x)) and outputs which case it is. The robustness
follows from the error correcting code. The fact that no classiﬁer is learnable follows from the fact
that the PRF is hard to predict, and thats exactly what the classiﬁer has to do. Finally, we want
the task to be easy to classify non-robustly. Here we use the “BPR trick” ([BPR18]). That is, we
additionally append to each sample a bit indicating which distribution it was sampled from. That
is,

D0 = (0, Encode(x, Fk(x))) and, D1 = (1, Encode(x, 1 − Fk(x))) .

Now the samples are easy to classify non-robustly, simply output the ﬁrst bit. Learning a robust
classiﬁer is hard, for that, consider the perturbing adversary that erases the ﬁrst bit. For these
samples, robust classiﬁcation is identical to predicting the output of the PRF. This is diﬃcult for
any eﬃciently learned classiﬁer. Hence, this gives us a task that that is easy to classify, has an
eﬃcient robust classifer and yet, any non-trivial robust classiﬁer is hard to learn.

Note that because we have excellent error correcting codes, this recipe is maximally robust. We
can pick a code that tolerates a constant fraction ( 1
4 − ε) errors and still enable correct decryption
[GI01]. This can be further boosted to ( 1
2 − ε) by using list decoding instead of unique decoding
and increasing the output size of the PRF to n-bits. We do not formally write this construction.

3.4 Non-Existence of Robust Classiﬁers from Average-Case Hardness

This section describes a learning task for which no computationally eﬃcient robust classiﬁer exists,
even though ineﬃcient ones do, based on average-case hard functions, thus proving Theorem 2.1.
Let g : {0, 1}n → {0, 1} be a function that is average-case hard, such that no polynomial-
time nonuniform algorithm can compute z (cid:55)→ g(z) noticeably better than random guessing. For
example, taking g to be a random function {0, 1}n → {0, 1} suﬃces. Let (Encode, Decode) be a
good error correcting code, capable of decoding from a constant fraction of errors. Now, construct
distributions D0, D1 as follows:

D0 = (0, Encode(x, g(x))) and D1 = (1, Encode(x, 1 − g(x)))

for x ← {0, 1}n uniformly. Note that these distributions are trivially distinguishable non-robustly.
However, with a perturbation adversary that destroys the ﬁrst coordinate, distinguishing D0 from

7

D1 essentially requires computing the function g(x), which cannot be done eﬃciently. Thus, there
is no eﬃcient robust classiﬁer. Moreover, an ineﬃcient robust classiﬁer exists, since one can decode
the error correcting code (correcting any adversarial errors) and compute g(x).

When using an average-case hard function, one limitation here is that the algorithm generating
the samples from distributions D0, D1 is ineﬃcient. This can be remedied by using one-way func-
tions, because generating D0, D1 requires the algorithm to perform the simpler task of sampling
(z, g(z)) for random z’s, and not computing g(z) given z, that the classiﬁer has to do. In fact,
this is precisely the diﬀerence between average-case NP hardness, which requires us to generate
hard instances, and one-way functions, which require generating hard instances along with their
solutions. See Section 3.4 for more details.

3.5 From Hardness of Decoding under Noise.

This section describes a proof sketch for Theorems 2.1 and 2.3. The problems Learning Parity with
Noise (LPN) and Learning with Errors (LWE) have the following ﬂavor: In both the problems a
random code C (over Z2 in LPN, Zq for a large prime in LWE) is speciﬁed by a matrix A:

C = {sT A} or, the dual form C = {y : Ay = 0} .

Then the computational task is to distinguish a point close to the code from a uniformly random
point in the space. The conjectured hardness of these problems can be used to construct a variety
of cryptographic primitives.
In the overview, we will describe the construction with the LPN
assumption. The LWE construction is conceptually identical.

The Classiﬁcation Task. We begin by describing the classiﬁcation task and then the rationale.
The task consists of two distributions on samples D0, D1 picked as follows: Pick a random linear
code over Z2, C : {0, 1}n → {0, 1}8n, (described by the generator matrix A or the parity check
matrix H). Then,

D0 = {y : y ← C} and, D1 = {y + 1 : y ← C} .
So, the task is to distinguish codewords of C from their aﬃne shift (1 represents the all-ones
vector). The distributions are easy to classify non-robustly. There exists an ineﬃcient robust
classiﬁer because the distance between the two codes C and C + 1 is large.

To show that a robust classiﬁer is hard to learn, consider the perturbation adversary that adds
random noise of varying size to the two distributions. Learning a robust classiﬁer for this adversary
is equivalent to distinguishing LPN samples from random. Hence any computationally eﬃcient
adversary cannot classify these examples better than chance.

Finally, we need to show that for a certain perturbation regime, no eﬃcient robust classiﬁer
exists while for a diﬀerent perturbation regime, an eﬃcient robust classiﬁer does exist. The latter
is accomplished by the notion of “trapdoor sampling” where the code is sampled with a trapdoor
that enables decoding noisy codewords (and hence robust classiﬁcation too).

Below we describe the example in more detail and give a sketch of the arguments needed.

Formal proofs are given in Sections 5 and 6.

LPN Assumption. The LPN hardness assumption states that: for m = poly(n),

(A, sT A + eT ) mod 2 ≈c (A, r) mod 2

where A ← Zn×m
2 is drawn unifomly at random and
each coordinate of error e ← Ber(r)m drawn from a Bernoulli distribution with error rate r, i.e.,
probability of drawing 1 is r.

describes a random code, the secret s ← Zn

2

8

Hardness Regimes and Trapdoors. The key parameter which controls the hardness of LPN
is the distance of the close point from the code in the appropriate norm.4 As the distance increases,
the problem becomes harder. In the case of LPN, this distance is approximately m · r where r is the
error rate. For most non-trivial parameter settings of the distance parameter, these two problems
are believed to be computationally intractable. That is, an eﬃcient algorithm given a description of
the random code cannot distinguish random points close to the code from random points in the
space.

Along with their conjectured computational hardness, we are interested in another property
of these problems, the existence of a trapdoor : that is, can we sample the code along with some
polynomial-size side information that lets us distinguish eﬃciently random points from points close
to the code. This information usually is a “short basis” for the dual code. The trapdoor property
has two important regimes: the “public-key” regime and the “private-key” regime. In the case of
LPN, the public-key regime corresponds to error rate r = O(1/
n) while the private-key regime
translates to constant error rates, e.g., r = 0.1. The public key regime of parameters enables
construction of advanced cryptographic primitives, including public key encryption. On the other
hand, in the private-key regime, we know constructions of one-way functions and symmetric key
cryptography, but not much more.

√

Importantly for us, in “public-key” parameter regime, such a trapdoors exists and can be
sampled eﬃciently. On the other hand, in the private-key regime, it is conjectured that no such
trapdoor exists. Traditionally this problem is studied as the problem of decoding linear codes with
preprocessing (for LPN) and closest vector problem with preprocessing (for LWE). In the problem
of decoding linear codes with preprocessing, an ineﬃcient algorithm Preprocess performs arbitrary
preprocessing on the given linear code (described by the matrix A) and has to come up with a
short polynomial-sized trapdoor for the code. Later the Decode algorithm has to use this trapdoor
to eﬃciently ﬁnd the codeword close to a given input. This problem and the closest vector problem
(is the same problem, on lattices instead of codes) are NP-hard to approximate in the worst-case
[BN90, Mic01, Reg04].

We require an average-case variant of the problem termed as the hardness of LPN with Prepro-
cessing. The assumption is stated more formally in Assumption 5.4. This assumption can be used
to construct a task where no eﬃcient robust classiﬁer exists. The task is very similar to the one
below where a eﬃcient robust classiﬁer exists but is hard to ﬁnd, except with higher levels of noise.
More details in Section 5.

Task with an Hard-to-Learn Eﬃcient Robust Classiﬁer. We now turn to the problem
of constructing learning tasks where an eﬃcient robust classiﬁer exists, but is hard to learn. It
consists of distinguishing between codewords (D0 = {sA}) from an aﬃne shift of the codewords
(D1 = {sA + 1} where 1 is the all-ones vector). That is, for a random matrix A ∈ Zn×m
where
m = 8n,

2

D0 = {sT A : s ∈ {0, 1}n} and, D1 = {sT A + 1 : s ∈ {0, 1}n}

We want to show that this task exhibits an eﬃcient robust classiﬁer. For that, we need access to a
trapdoor. In the case of LWE, such algorithms are known [GPV08] and proven to be exremely fruit-
ful (see [Pei16] and references therein). This LWE trapdoor is a zero-one matrix T ⊆ {0, 1}m×m−n
over Zq such that AT = 0 (mod q). Because T is a zero-one matrix, given any adversarially per-
turbed sample ˜y = sT A + eT , multiplying by T results in ˜yT = eT T which is a vector with small
entries in each coordinate. And this can be checked to distinguish LWE samples from random.

4The speciﬁc norm is not crucial for the discussion below. Hamming is used for LPN while for LWE, the norm is

obtained by embedding Zq in Z as {−(cid:98)q/2(cid:99), . . . 0, 1, . . . (cid:98)q/2(cid:99)} and take the (cid:96)∞ norm on Z.

9

In the case of LPN, we don’t know how to perform such trapdoor sampling: where a uniformly
Instead we rely on a computational
random matrix A is sampled along with such a trapdoor.
variant of this. We can sample a matrix H ∈ Zn×8n
that is indistinguishable from a random matrix
along with such a “short” trapdoor: a matrix E where each row and colum of E has hamming
weight O(
m). See Lemma 5.5 for more details. This then allows for a similar construction. The
perturbing adversary P again adds random noise, this time of a lower magnitude though.

√

2

P(y) : Output ˜y = y + e where e ← Zm

2,Ham=0.1

√

m .

Note that earlier, we added 0.1m bits of noise, instead here we are adding 0.1
m bits. This
level of noise places the problem in the “public-key” regime of parameters. Furthermore, given
the trapdoor, in this case, we can recover which distribution the unperturbed sample was sampled
from, giving us the required robust classiﬁer. See Section 5 for more details.

√

Again, it is clear that learning a non-robust classiﬁer is easy. The hardness of LPN assumption
implies that it is hard to learn a robust classiﬁer. This is in contrast to the previous construction
where no eﬃcient robust existed. Here, the trapdoor gives us an eﬃcient robust classiﬁer, but
the hardness of LPN implies that such a classiﬁer is hard to learn. In fact, any eﬃciently learned
classiﬁer cannot do better than chance.

One thing to note is that this eﬃcient robust classiﬁer is not “maximally robust”, meaning
that while an ineﬃcient robust classiﬁer can tolerate 0.1m bits of noise and still classify correctly,
the eﬃcient classiﬁer can tolerate 0.1
m bits of noise. This is similar in the case of LWE as well,
where there is a gap between noise the trapdoor can support (about q/m in the (cid:96)∞ norm) against
the maximally robust limit (Ω(q)). This is not surprising because decoding random linear codes is
harder than decoding speciﬁcally designed codes and hence the trapdoors do not achieve optimal
decoding.

√

A feature of this construction is that an eﬃcient algorithm can learn to not only distinguish
the samples from distributions D0 and D1, it can easily learn to generate samples from the two
distributions as well.

Comparing Recipes. There are three key diﬀerences between the recipes. The ﬁrst diﬀerence
is in the underlying hardness assumption. The ﬁrst two constructions are based on weaker assump-
tions: namely general assumptions that one-way functions exist (or average-case hard function
respectively) rather than the speciﬁc assumptions of LWE and LPN.

The second diﬀerence is that the distributions based on LWE/LPN facilitate learning in a
stronger sense, that it is possible to sample from the non-robust distributions after seeing poly-
nomially many samples. In construction I based on one-way functions, we do not learn either D0
or D1 in that strong sense.
In fact, after seeing polynomially many samples, eﬃcient sampling
algorithms have no non-trivial advantage with the other recipes. As pointed out in in construction
II, it is possible to support generation, albeit using a slightly stronger assumption that one-way
functions exist.

The third diﬀerence is that of naturalness: we feel that the LWE/LPN recipe gives a more
natural learning task. This is obviously a subjective notion. This learning task of distinguishing
noisy codewords from random has existed independent of the notion of robust classiﬁcation and
arises naturally in other contexts.

3.6 Converse: Cryptography from Hardness of Robust Classiﬁcation.

In this section, we describe how Theorem 2.4 is proved. The key result we rely on here is that we
can construct one-way functions from any pair of samplable distributions that are statistically far

10

and computationally indistinguishable.

Theorem 3.1. Given a pair of distributions (X0, X1) ← F over X that are statistically far,
i.e., dT V (X0, X1) > 0.9 and computationally indistinguishable. That is for every polynomial time
adversary A that gets sample access to the distributions,

E
x←X0

A(X0,X1)(x) − E

x←X1

A(X0,X1)(x) < 0.1

Then one-way functions exist.5

In order to construct such distributions, we rely on the learning task (given by (D0, D1)) and

the perturbation adversary P. The distributions we consider are

X0 = {P(x) : x ← D0} and, X1 = {P(x) : x ← D1} .

Note that because eﬃcient robust classiﬁers are hard to learn, no eﬃcient algorithm A (that knows
P and gets access to the distributions D0, D1) can distinguish between the two distributions . On
the other hand, because a robust classiﬁer exists, these two distributions are statistically far from
each other. This implies that one-way functions exist.

4 Deﬁnitions

We use lowercase letters for values, uppercase for random variables, uppercase calligraphic letters
(e.g., U) to denote sets, boldface for vectors (e.g., x), and uppercase sans-serif (e.g., A) for algo-
rithms (i.e., Turing Machines). We let poly denote the set all polynomials. A function ν : N → [0, 1]
is negligible, denoted ν(n) = negl(n), if ν(n) < 1/p(n) for every p ∈ poly and large enough n. Given
a random variable X, we write x ← X to indicate that x is selected according to X. Similarly,
given a ﬁnite set S, we let s ← S denote that s is selected according to the uniform distribution
on S. For an algorithm A, we denote by x ← A the experiment where x is sampled by feeding
a uniformly random input to A from its input domain. We say that two families of distributions
{Xn}n∈N an {Yn}n∈N are computationally indistinguishable (denoted by {Xn} ≈c {Yn} or X ≈c Y
for brevity) if for every polynomial time distinguisher D,

| P
x←Xn

[D(x)] − P

y←Yn

[D(y)]| ≤ negl(n)

4.1 Learning & Classiﬁcation

Deﬁnition 4.1 (Classiﬁcation). For a family of classiﬁcation tasks F over X is easy to classify
if there exists a learning algorithm that given poly(n) i.i.d. samples from a pair of distributions
(D0, D1) ∈ F supported on X , outputs an eﬃciently computable classiﬁer A : X → {0, 1} such that,

P
X←Db

[A(x) = b] ≥ 0.99

We want to consider other notions of learning distributions as well, in order to make more reﬁned
distinctions between learning distributions. The following deﬁnition for learnability of discrete
distributions is from [KMR+94].

5The constants in the equations are fairly arbitrary. We can replace them by any constants α, β where α > β and

the result holds (see [NR06, BDRV19]).

11

Deﬁnition 4.2. For a distribution D over a discrete domain X ,

1. Generator. A circuit G : {0, 1}m → X is an ε-good generator for D if

KL(D(cid:107)G(U )) ≤ ε

where G(U ) denotes the distribution obtained by evaluating G on a uniformly random input.

2. Evaluator. A circuit E : X → R≥0 is an ε-good evaluator for D if

KL(D(cid:107)E) ≤ ε

where E denotes the distribution obtained by sampling with probability density function E.

Deﬁnition 4.3. A class of distributions F = {Fn} over a discrete domain X = {Xn} is (ε, δ)-
eﬃciently learnable with a generator (or evaluator resp.) if there exists a polynomial time algorithm
Gen that given oracle access to any Dn ∈ Fn runs in time poly(n, 1/δ, 1/ε) and outputs G (or E
resp.) such that with probability ≥ 1 − δ over the randomness of Gen and samples, G (E resp.) is
an ε-good generator (evaluator resp.) of D.

In our examples, we seek to ﬁnd distributions where the gap between ease of learning the actual

distributions and that of the adversarially perturbed distributions is maximized.

4.2 Hardness of Eﬃcient Robust Classiﬁcation

We start by recalling the notion of robust classiﬁcation. Then, we consider two ways of formalizing
the diﬃculty of eﬃcient robust classiﬁcation: (1) no eﬃciently computable robust classiﬁer exists,
(2) an eﬃcient robust classifer exists, but it is hard to learn one eﬃciently.

Deﬁnition 4.4. Consider a classiﬁcation task given by two distributions D0, D1 over X n. Let (cid:107) · (cid:107)
be a norm over the space X n and ε > 0. Let R : X n → {0, 1} be a classiﬁer. The classiﬁer R is
ε-robust if

P
X←Db

[R(˜x) = b for all ˜x ∈ B(x, ε)] ≥ 0.99

In the deﬁnition above, B(x, ε) is all points that are ε distance from x in the given norm. Here,

we will generally be concerned with Hamming distance and the (cid:96)1 norm.

Deﬁnition 4.5 (Hardness of Robust Classiﬁcation). Consider a family of classiﬁcation tasks,
deﬁned by two distributions D0, D1 over X n sampled from a distribution over learning tasks Samp.
Let (cid:107) · (cid:107) be a norm over the space X n and ε > 0. We consider the following notions of diﬃculty of
robust classiﬁcation:

1. No eﬃcient ε-robust classiﬁer exists. There exists a polynomial-sized perturbation al-
gorithm P, such that for every polynomial sized classiﬁer R, the perturbed samples are hard
to classify. That is,

P(cid:2)RD0,D1(˜x) = b(cid:3) ≤

+ negl(n)

1
2

where the perturbed sample ˜x is generated by sampling x ← Db for a random b ← {0, 1} and
is then perturbing ˜x ← PD0,D1(x).

12

2. Eﬃcient ε-robust classiﬁer is hard to learn. There exists a polynomial-sized pertur-
bation algorithm P, such that every polynomial-time learning algorithm learn that outputs a
polynomial sized classiﬁer R, the perturbed samples are hard to classify for R. That is, for
a learning task D0, D1 sampled by Samp and robust classifer R ← learnD0,D1(1n) output by
learn,

P[R(˜x) = b] ≤

+ negl(n)

1
2

where the perturbed sample ˜x is generated by sampling x ← Db for a random b ← {0, 1} and
is then perturbing ˜x ← PD0,D1(x). The probability is over the entire experiment from sampling
the learning tasks to the randomness of the perturbation algorithm and the classiﬁer.

Discussion. An alternate deﬁnition of hard to classify robustly would be the negation of robust
classiﬁcation. That deﬁnition takes a following form:

P
x←Db

[∃˜x ∈ B(x, ε) such that, R(˜x) (cid:54)= b] ≥ 0.5

This deﬁnition is unsatisfactory because it does not say anything about how diﬃcult it is to ﬁnd
such perturbations. In the event when such examples are not eﬃciently discoverable, we do not
have to worry about these.

In the deﬁnitions used, the perturbing adversary is both eﬃcient and universal. Eﬃciency is a
very natural property to have, in that if the adversarial examples are computationally hard to ﬁnd,
then they are less of a concern. The universality property says that there is a single perturbation
adversary that succeeds against all eﬃcient classiﬁers. This is a strong requirement. This makes
our robustly hard to learn tasks better: that they have a unique perturbation adversary that is
independent of which classiﬁcation algorithm is used. On the other hand, it makes our converse
results constructing one-way functions from hard to learn robust tasks weaker, because they only
hold for such robustly hard to learn tasks, with universal perturbation adversaries.

It is possible to have a perturbation adversary P that is eﬃcient but not universal. The pertur-
bation adversary gets oracle access to the classiﬁer and has to then output a misclassiﬁed example.
This is a weaker requirement than Deﬁnition 4.5. We do not know if such a deﬁnition also implies
cryptography.

5 Learning Parity with Noise

5.1 Assumption Deﬁnition and Discussion

2 with Hamming weight exactly t. We will consider Hamming

Let Zm
2,Ham=t denote vectors in Zm
weight as our norm in this setting.
Deﬁnition 5.1 (Learning Parity with Noise Problem (LPN)). For n, m, t ∈ N, an LPN sample is
obtained by sampling a matrix A ← Zn×m
2,Ham=t and
outputting (A, sT A + eT ).

2 , and an error vector (cid:15) ∈ Zm

, a secret s ← Zn

2

We say that an algorithm solves LPNn,m,t if it distinguishes an LPN sample from a random

sample distributed as Zn×m

2

× Z1×m
2

.

Assumption 5.2 (Learning Parity with Noise Assumption). The Learning Parity with Noise
(LPN) assumption assumes that for m = poly(n) and t = θ(m/
n), the LPN samples are in-
distinguishable from random. That is, for every eﬃcient distinguisher D,
[D(A, r) = 1](cid:12)

(cid:2)D(A, sT A + eT ) = 1(cid:3) − P

√

(cid:12) < negl(n)

(cid:12)
(cid:12)

P
s←Zn
2

e←Zm

2,Ham=t

r←Zm
2

13

This regime of parameters m = poly(n) and t = θ(m/

n) is what is traditionally used to
construct public key encryption from the LPN assumption. Next, we consider the LPN problem
with preprocessing:
in this variant of the problem, an ineﬃcient algorithm Preprocess is allowed
to process the matrix A arbitrarily to construct a “trapdoor”. Then the distinguisher is asked to
distinguish LPN sample (A, sT A + e) from random. The assumption states that this is diﬃcult
for higher error rates.

√

Deﬁnition 5.3 (LPN with Preprocessing Problem (LPNP)). We say that a pair of algorithms
(Preprocess, D) where Preprocess is possibly ineﬃcient and D is eﬃcient, solves LPNn,m,t if D can
distinguish an LPN sample from a random sample given the trapdoor trap generated by Preprocess(A).

The Learning Parity with Noise problem is hard even with preprocessing in the constant noise

regime.

Assumption 5.4 (LPN with Preprocessing (LPNP)). Let m = poly(n) and t = r · m for any con-
stant r. For every pair of algorithms (Preprocess, D) with a possibly ineﬃcient algorithm Preprocess
and eﬃcient D, the following experiment is performed: Sample A ← Zn×m
and get trap ←
Preprocess(A). Then, the distinguisher D given trap cannot distinguish the LPN samples from
random. That is for large enough n,

2

(cid:12)
(cid:12)
(cid:12)

P
s←Zn
2

e←Zm

2,Ham=t

(cid:2)D(trap, A, sT A + eT ) = 1(cid:3) − P

r←Zm
2

[D(trap, A, r) = 1]

(cid:12)
(cid:12)
(cid:12) < negl(n)

where the probability is over the code A, s, e, r and the randomness of the distinguisher D.

Discussion. The most important parameter of the LPN problem is its error rate, that is r = t/m.
The higher the error rate, the more diﬃcult the problem. There are two important regimes of the
error rate: r is a constant and r = o( 1√
n ). When the error rate is a constant, the hardness of LPN in
this regime implies one-way functions and hence symmetric key cryptography. We do not know how
to base public key encryption on error rates in this regime. When the error rate decreases below
O( 1√
n ), we can construct public key encryption from this problem. For error rates below log n/n,
the problem becomes easy. The best known algorithms for solving LPN are due to Blum Kalai
and Wasserman [BKW03] which solves LPN in time 2O(n/ log n) requiring 2O(n/ log n) samples; and
Lyubashevsky [Lyu05] which solves LPN in time 2O(n/ log log n) with polynomially many samples.
For structured LPN samples, more eﬃcient algorithms are known [AG11]. Our error distributions
are not structured.

Note that the lesser used variant of LPN is used here, in that we insist that the Hamming weight
of the error vector is exactly t instead of a random variable. This is equivalent to the standard
formulation [JKPT12].6 This is done for convenience and the example can be translated to the
deﬁnition of LPN where the error vector is drawn from a product distribution.

We also consider a the preprocessing variant of Learning Parity with Noise. In this variant, the
adversary is allowed to preprocess the code and generate a small “trapdoor” to the code. Then an
eﬃcient adversary is tasked with distinguishing the LPN samples from random. The preprocessing
variant of LPN assumption states that even this is hard in the constant error regime, that is when
t/m is a constant. It is known that decoding linear codes is NP-hard in the worst case [BN90].
The search analog of LPN is precisely the average-case variant of this question and is conjectured
to be hard in the regime of constant noise rate.

6In the search version of the problem where the adversary has to ﬁnd s given A, sT A + eT , these two versions
are equivalent as t takes polynomially many values, hence we can go over all polynomially-many and try solving each
exact version).

14

In the public key regime, we want to show that trapdoors
Trapdoor for Eﬃcient Decoding.
exist that enable eﬃent distinguishing of LPN samples. We state the result next: that there is a
way to sample a random matrix H that is indistinguishable from a random matrix such that it has
a trapdoor that enables eﬃcient distinguishing.

Lemma 5.5 (Computational Trapdoor Sampling). Consider the following algorithm LPNTrapSamp
such that, LPNTrapSamp on input (n, t) with t = θ(

n) does the following:

√

LPNTrapSamp(n, t):

1. Sample A ← Zn×m
2
(cid:20) A
SA + E

2. Output H =

(cid:21)

, E

, S ← Zn×n

2

and E ∈ Zn×m

2

where e(i,·) ← Zm

2,Ham=t.

The algorithm has the following properties:

1. rowspan(E) ⊂ rowspan(H) where rowspan(T) = {sT : s ∈ Zn
2. The matrix H is computationally indistinguishable from uniformly random matries. That is,

2 }.

{H ← LPNTrapSamp(n, t)} ≈c

(cid:8)U ← Z2n×8n

2

(cid:9)

3. With overwhelming probability over the randomness of the algorithm, it outputs E such that
every column of E has Hamming weight at most t and every row of E has Hamming weight
exactly t.

The notion of Trapdoor sampling is very widely used in the context of learning with errors
assumption. A trapdoor sampling algorithm samples along with the public matrix A which is
statistically close to a random matrix (representing the code/lattice), a secret “trapdoor”. This
trapdoor enables solving the bounded distance decoding problem, that is given a point close to a
codeword in the code, ﬁnds the close codeword. As we know, without this trapdoor, this problem
is conjectured to be hard. But the trapdoor enables solving this problem.

We have a computational analog of that property for LPN in the “public-key” regime of param-
eters. We construct that below. Because E is a sparse matrix, it can be used to solve the problem
of distinguishing LPN samples from random and decoding noisy codewords.

Proof. By deﬁnition, rowspan(E) ⊂ rowspan(H) and that each row of E has Hamming weight
exactly t. We need to show that H is indistinguishable from random and that every column of
E has at most t ones. The former follows from the Learning Parity with Noise combined with a
hybrid argument and the latter from a Chernoﬀ bound.

Claim 5.5.1. The output distribution of H is computationally indistinguishable from uniform.
That is,

{H ← LPNTrapSamp(n, t)} ≈c

(cid:8)U ← Z2n×8n

(cid:9)

2

Proof. Observe that the LPN assumption can be restated as, The LPN assumption assumes that
the following two distributions are indistinguishable:

(cid:20)
A
sT A + eT

(cid:21)

≈c

(cid:110)

U : U ← Z(n+1)×m

2

(cid:111)

2 , A ← Zn×m

where s ← Zn
2 is a random vector of Hamming weight t. The claim then
follows by applying a hybrid argument to each of the rows of SA+E and replacing them by random
vectors, by viewing them as s(i,·)A + e(i,·) where s(i,·), e(i,·) denote the i-th row of matrix S and E
and using the LPN assumption.

, e ∈ Zm

2

15

Claim 5.5.2. Let e(·,j) denote the j-th column of matrix E. Then,

P
E←LPNTrapSamp(n,t)

(cid:2)∃j, such that, (cid:107)e(·,j)(cid:107)Ham > t(cid:3) < 8n · e− 7t

24 .

Proof. The proof follows from Chernoﬀ bound and a union bound. For any ﬁxed column j, each
coordinate ei,j = 1 independently with probability t/8n where the probability is over i. Hence, for
any column j, the expected Hamming weight is t
8 . By a Chernoﬀ bound, we can observe
the following:

8n · n = t

P
E←LPNTrapSamp(n,t)

(cid:34)

(cid:88)

i

(cid:35)

ei,j > t

=

P
E←LPNTrapSamp(n,t)

≤ e−7· t

8 · 1

3

(cid:34)

(cid:88)

i

ei,j > (1 + 7) · E(

(cid:35)

ei,j)

(cid:88)

i

where the inequality follows from the Chernoﬀ bound in the following form: Let X1, X2, . . . Xn be
independent random variables taking values in {0, 1}. Let X be their sum and µ = E X. For any
δ ≥ 1,

P[X ≥ (1 + δ)µ] ≤ e− δµ

3

.

A union bound over all j gives us the required bound.

Because t =

√

n, the failure probability is negligible.

5.2 No Eﬃcient Robust Classiﬁer Exists

Next, we describe a learning task where while it is possible to ineﬃciently perform robust classiﬁ-
cation, no eﬃcient robust classiﬁer exists.

Theorem 5.6. For an n, let m = 8n, t = 2n − 1, ε = 2n. Consider the following learning task. Let
A ← Zm×n

. Deﬁne D0, D1 as:

2

D(A)

0 = (cid:8)sT A : s ← {0, 1}n(cid:9) and, D(A)

1 = (cid:8)sT A + 1 : s ← {0, 1}n(cid:9) .

The learning task has the following properties.

1. (Learnability) A classiﬁer to distinguish D0 from D1 can be learned from the samples eﬃ-

ciently. Furthermore, it is easy to learn a generator/ evaluator for these distributions.

2. (No Eﬃcient Robust Classiﬁer Exists) There exists a perturbation algorithm P such that there

exists no eﬃcient robust classiﬁer R such that,

P(cid:2)R(˜y) ∈ R−1(b)(cid:3) ≥ 0.5 + negl(n)

where the perturbed sample ˜y is generated by sampling y ← Db for a random b and is then
perturbing ˜y ← PD0,D1(y) such that (cid:107)y − ˜y(cid:107) ≤ ε.

Proof. Learnability of this task is trivial. Given enough samples, the entire subspace spanned by
A is learned and can be sampled from.

In order to show that no eﬃcient robust classiﬁer exists for ε = 2n, we rely on the diﬃculty of

LPN with Preprocessing (Assumption 5.4). Consider the following perturbing adversary P:

P(y) : Output ˜y = y + e where e ← Zm

2,Ham=t

16

Consider the following pair of algorithms Preprocess, D: Preprocess(A) ineﬃciently ﬁnds the best
possible eﬃcient robust classiﬁer R and returns that as the trapdoor trap = R. The distinguisher
D simply runs the robust classiﬁer R and returns the answer. It can do this in polynomial time
because R is also polynomial time computable.

The LPNP assumption implies that for this pair of algorithms (Preprocess, D), LPN is hard to

solve. That is, for A ← Zn×m

2

, R ← Preprocess(A),

(cid:12)
(cid:12)
(cid:12)

P
s←Zn
2

e←Zm

2,Ham=t

(cid:2)R(A, sT A + eT ) = 1(cid:3) − P

r←Zm
2

(cid:12)
(cid:12)
(cid:12) < negl(n)
[R(A, r) = 1]

(1)

Now a hybrid argument ﬁnishes the proof as the following distributions are computationally indis-
tinguishable for R:

(A, sT A + eT ) ≈c (A, r) ≡ (A, r + 1) ≈c (A, sT A + eT + 1)

where the two ≈c statements follow from Eq. (1) and the ≡ follows from the fact that adding any
ﬁxed vector to the uniform distribution still remains uniform.

This completes the argument.

5.3 Eﬃcient Robust Classiﬁer Exists but is Hard to Learn

In this section, we describe a learning task where a robust classiﬁer exists, but it is hard to learn.
Consider the following classiﬁcation task : Given a matrix H ∈ Z2n×8n

, deﬁne D0, D1 as:

2

D(H)

0 = (cid:8)y ∈ Z8n

2

: Hy = 0 mod 2(cid:9) and, D(H)

1 = (cid:8)y + 1 ∈ Z8n

2

: Hy = 0 mod 2(cid:9)

where both are uniform distributions on the sets and 1 is the all ones vector on 8n dimensions.

Theorem 5.7. For an n, let t = 2(cid:98)
task. Let (H, E) ← LPNTrapSamp(n, t). Given a matrix H ∈ Z2n×8n

n/6(cid:99) − 1, such that t is odd. Consider the following learning

, deﬁne D0, D1 as:

2

√

D(H)

0 = (cid:8)y ∈ Z8n

2

: Hy = 0 mod 2(cid:9) and, D(H)

1 = (cid:8)y + 1 ∈ Z8n

2

: Hy = 0 mod 2(cid:9)

The learning task has the following properties.

1. (Learnability) A classiﬁer to distinguish D0 from D1 can be learned from the samples eﬃ-

ciently. Furthermore, it is easy to learn a generator/ evaluator for these distributions.

2. (Existence of an Eﬃcient Robust Classiﬁer) There exists an eﬃcient robust classiﬁer R such

that,

where ε = (cid:98)

P
y←Db
n(cid:99) and B(y, ε) = {y(cid:48) : (cid:107)y − y(cid:48)(cid:107)Ham ≤ ε}.

(cid:2)B(y, ε) ∈ R−1(b)(cid:3) ≥ 0.99

√

3. (Unlearnability of Robust Classiﬁer) There exists a perturbation algorithm such that no eﬃ-

ciently learned classiﬁer can classify better than chance.

We drop H from the notation to avoid clutter and denote the distributions as D0, D1. Here H
functions as the parity check matrix of the code D0 and D1 is a shift of the code. Observe that
Part (1): distinguishing between D0 and D1 is easily done by Gaussian elimination.

We want to show that (2) a robust classiﬁer exists, and, (3) it is diﬃcult to ﬁnd any robust

classiﬁer eﬃciently. We argue this in the subsequent claims.

17

Lemma 5.8 (Existence of Robust Classiﬁer). Consider the following robust classiﬁer:

Robust Classifer RE(˜y):

1. Compute z = E˜y mod 2.
2. If (cid:107)z(cid:107)Ham ≤ n output 0 otherwise, output 1.

Then, the following holds:

P
y←Db

[R(˜y) = b for all ˜y ∈ B(y, ε)] ≥ 0.99

√

for ε = (cid:98)

n(cid:99) and B(y, ε) = {y : (cid:107)˜y − y(cid:107)Ham ≤ ε}.

Proof. The correctness of the robust classiﬁer follows from the fact that E is a sparse matrix where
each column has Hamming weight at most t. Consider the case when y ← D0, the other case is
analogous. Observe that,

where (cid:107)(cid:15)(cid:107)Ham ≤ ε ≤

√

n. Hence,

˜y = y + (cid:15) mod 2

E˜y mod 2 = E(y + (cid:15)) = E(cid:15)

(mod 2)

where the second equality follows from the fact that Hy = 0 mod 2 and that rowspan(E) ⊆
rowspan(H). Observe that each column of E has at most t ones and that the Hamming weight of
(cid:15) is at most ε. As, E(cid:15) = (cid:80)
j:(cid:15)j =1 e(·,j), we can bound the Hamming weight (cid:107)E(cid:15)(cid:107)Ham ≤ t(cid:107)(cid:15)(cid:107)Ham ≤
t · ε ≤ n/3. Hence the classiﬁer would always correctly classify adversarially perturbed samples
from D0.

In the other case when b = 1 observe that E · 1 = 1 because each row of E has Hamming weight
t which is odd. Hence the Hamming weight of z is at least 2n − n/3 > n in this case and would be
classiﬁed correctly. This proves that a robust classiﬁer exists.

Lemma 5.9 (Hardness of Learning a Robust Classiﬁer). There exists a perturbation algorithm
P such that for every polynomial time learner L, the learner L has no advantage over chance in
classifying examples perturbed by P. That is,







P

H, E ← LPNTrapSamp(n, t);
y ← D(H)
where b ← {0, 1}
b
˜y ← PD0,D1(y);
b(cid:48) ← LD0,D1(˜y)







≤

1
2

: b = b(cid:48)

+ negl(n)

Proof. This proof is identical to the proof of security of Aleknovich’s public key encryption scheme
[Ale03].

Observe that D0, D1 are completely speciﬁed by the matrix H. So, the learner gets H instead

of sample access. Consider the following random perturbation algorithm P:

P(y) : Output ˜y = x + (cid:15), where (cid:15) ← Zm

2,Ham=t

where Zm
allowable amount of error as t < ε =

√

n.

2,Ham=t is the distribution on vectors of Hamming weight t. This adversary is adding

Suppose an eﬃcient learner L exists that can succeed in this game with high probability, we can
break the learning parity with noise assumption. This is done in two steps. In the ﬁrst step, we
replace the parity check matrix H with a uniformly random matrix H(cid:48) this should not noticeably
change the success probability because the two distributions are indistinguishable. In the second

18

step, now observe that H(cid:48) is a uniformly random parity check matrix hence gives rise to a random
code. Now we can apply the LPN assumption again, this time to replace the error (cid:15) by a uniformly
random vector and not noticably change the success probability. This is a contradiction.

6 Learning with Errors

6.1 Preliminaries

In this section, we deﬁne the learning with errors problem and describe the notion of trapdoor
sampling that it supports. In this section, the norm used is the (cid:96)∞ norm obtained by embedding
q , (cid:107)x − y(cid:107) = maxi |xi − yi| where |z| for z ∈ Zq is obtained
Zq in Z. That is, for vectors x, y ∈ Zn
by embedding z ∈ {−(cid:98)q/2(cid:99), . . . , −1, 0, 1, . . . (cid:98)q/2(cid:99)} and taking the absolute value.
Deﬁnition 6.1 (Learning with Errors Problem). For n, m ∈ N and modulus q ≥ 1, distribution
for error vectors χ ⊂ Zq, a Learning with Errors (LWE) sample is obtained by sampling s ← Zn
q ,
A ← Zn×m

, e ← χm and outputting (A, sT A + eT mod q).

q

We say that an algorithm solves LWEn,m,q,χ if it distinguishes LWE sample from a random

sample distributed as Zn×m

q

× Z1×m
q

.

Assumption 6.2 (Learning with Errors Assumption). The Learning with Errors (LWE) assump-
tion assumes that for m = poly(n), q = Ω(n3) and χ is truncated discrete gaussian over Zq with
standard deviation q/n2 truncated to q/2n, the LWE samples are indistinguishable from random.
That is, for every eﬃcient distinguisher D,

(cid:12)
(cid:12)

P
s←Zn
2

e←Zm

2,Ham=t

(cid:2)D(A, sT A + eT ) = 1(cid:3) − P

r←Zm
2

[D(A, r) = 1](cid:12)

(cid:12) < negl(n)

We have written speciﬁc versions of the LWE assumption. LWE is conjectured to be hard for

a large setting of parameters. For a discussion on parameters, see [Pei16].

Deﬁnition 6.3 (LWE with Preprocessing Problem (LWEP)). We say that a pair of algorithms
(Preprocess, D) where Preprocess is possibly ineﬃcient and D is eﬃcient, solves LWEn,m,t if D can
distinguish an LWE sample from a random sample given the trapdoor trap generated by Preprocess(A).

The Learning Parity with Noise problem is hard even with preprocessing in the constant noise

regime. We state the assumption below formally.

Assumption 6.4 (LWE with Preprocessing (LWEP)). Let m = n log q + 2n, q = n3 and χ is a
discrete Gaussian with standard deviation q/100 truncated to q/10. For every pair of algorithms
(Preprocess, D) with a possibly ineﬃcient algorithm Preprocess and polynomial time D, the following
experiment is performed: Sample A ← Zn×m
and get trap ← Preprocess(A). Then, the distinguisher
D given trap cannot distinguish the LPN samples from random. That is,

2

(cid:12)
(cid:12)
(cid:12)

P
s←Zn
2

e←Zm

2,Ham=t

(cid:2)D(trap, A, sT A + eT ) = 1(cid:3) − P

r←Zm
2

[D(trap, A, r) = 1]

(cid:12)
(cid:12)
(cid:12) < negl(n)

Deﬁnition 6.5 (Lattice Trapdoor). For a matrix A ∈ Zn×m
A composed of all vectors in the kernel of A:

q

, we denote by L⊥ the dual lattice of

A trapdoor for A is a short basis for the lattice L⊥(A).

L⊥ = {x ∈ Zm : Ax = 0 mod q}

19

In the case of LWE, it is known that we can sample matrices A from a distribution statistically
close to uniformly random along with a trapdoor which allows for eﬃcient distinguishing and
recovering the lattice point from a noisy one, for close distances (this is referred to as bounded
distance decoding).

Theorem 6.6 (Trapdoor Sampling [GPV08]). There exists an algorithm TrapSamp such that,
TrapSamp on input (q, m, n) where m ≥ n log q + 2n outputs a pair of matrices (A, T) where
A ∈ Zn×m

, with the following properties:

, T ∈ Zm×n log q
q

q

• AT = 0 mod q.
• The output distribution of A is statistically close to uniform (total variation distance <

2−O(n)).

• T has only zero-one entries.

6.2 No Eﬃcient Robust Classiﬁer Exists

In this section we describe a learning task based on LWE that has no robust classiﬁer. This is
identical to the LPN based task except the noise distribution is set diﬀerently.

Theorem 6.7. For any q = n3 and m = n log q + 2n, and χ is a discrete Gaussian with standard
deviation q/100 truncated to q/10. Consider the following learning task. Let A ← Zm×n
. Deﬁne
D0, D1 as:

q

D(A)

0 = (cid:8)sT A : s ← {0, 1}n(cid:9) and, D(A)

1 =

(cid:26)

sT A +

q

2

: s ← {0, 1}n

(cid:27)

.

The learning task has the following properties.

1. (Learnability) A classiﬁer to distinguish D0 from D1 can be learned from the samples eﬃ-

ciently. Furthermore, it is easy to learn a generator/ evaluator for these distributions.

2. (No Eﬃcient Robust Classiﬁer Exists) There exists a perturbation algorithm P such that there

exists no eﬃcient robust classiﬁer R such that,

P(cid:2)R(˜y) ∈ R−1(b)(cid:3) ≥ 0.5 + negl(n)

where the perturbed sample ˜y is generated by sampling x ← Db for a random b and is then
perturbing ˜x ← PD0,D1(x) such that (cid:107)y − ˜y(cid:107) ≤ q/10.

The proof is identical to the LPN case, with the perturbation adversary P instead adding noise

distributed according to χm.

6.3 An Eﬃcient Robust Classiﬁer Exists but is Hard to Learn

We deﬁne the classiﬁcation task (D0, D1) as follows: Given a matrix A ∈ Zn×m
tions D0 and D1 deﬁned as:

q

consider distribu-

D(A)

0 = (cid:8)sT A : s ∈ Zn

q

(cid:9) and, D(A)

1 =

(cid:110)

sT A +

· 1T : s ∈ Zn
q

(cid:111)

.

q
2

where both are uniform distributions on the sets and 1 is the all ones vector on m dimensions. We
drop A from the notation to avoid clutter and denote the distributions as D0, D1.

20

Hence, the task consists of distinguishing lattice vectors from an aﬃne shift of the lattice.
That is, given a vector x ∈ (D0 ∪ D1), classify weather x ∈ D0 or x ∈ D1. Gaussian elimination
accomplishes this task easily. We want to show that (a) a robust classiﬁer exists, and, (b) it is
diﬃcult to ﬁnd any robust classiﬁer eﬃciently. We argue this based on the learning with errors
assumption.

At the heart of the construction is the idea of lattice trapdoors. For a matrix A ∈ Zn×m
,
the trapdoor is a “short” matrix T such that AT = 0 mod q. There are two key properties of
these trapdoors that we leverage: (1) This short matrix allows us to solve the “bounded distance
decoding (BDD)” problem : that is, given a vector close to the lattice, ﬁnd the closest lattice vector
eﬃciently. Hence, the trapdoor functions as a robust classiﬁer. Also, we can eﬃciently sample a
random matrix A together with such a trapdoor. (2) It is hard to ﬁnd such a trapdoor given the
matrix A, even when it exists, because these trapdoors allow us to solve the Learning with Errors
problem. This allows us to show that the robust classiﬁer is hard to learn.

q

Theorem 6.8. For any q = n3 and m = n log q + 2n, consider the following learning task. Let
(A, T) ← TrapSamp(n, m, q). Given a matrix A ∈ Zn×m

, deﬁne D0, D1 as:

q

D(A)

0 = (cid:8)sT A : s ∈ Zn

q

(cid:9) and, D(A)

1 =

(cid:110)

sT A +

· 1T : s ∈ Zn
q

(cid:111)

.

q
2

The learning task has the following properties.

1. (Learnability) A classiﬁer to distinguish D0 from D1 can be learned eﬃciently.

2. (Existence of Robust Classiﬁer) There exists a robust classiﬁer R such that,

(cid:2)B(y, q/4m) ∈ R−1(b)(cid:3) ≥ 0.99

P
y←Db

where B(y, ε) = {y(cid:48) ∈ Zm

q : (cid:107)y − y(cid:48)(cid:107)∞ ≤ ε}.

3. (Unlearnability of Robust Classiﬁer) There exists a perturbation algorithm such that no eﬃ-

ciently learned classiﬁer can classify better than chance.

Lemma 6.9 (Existence of a Robust Classiﬁer). Consider the following robust classiﬁer R:

Robust Classifer RT(˜y):

1. Compute z = ˜yT T mod q.
2. If z ∈ (cid:8) −q
4 , . . . , q

(cid:9)n

4

output 0 otherwise, output 1.

Then,

P
x←Db

[R(˜x) = b for all ˜x ∈ B(x, q/4m)] ≥ 0.99

Proof. The correctness of the robust classiﬁer follows from the fact that T is a zero-one matrix and
that the errors are bounded in size. Consider the case when y ← D0, the other case is analogous.
Observe that,

˜y = y + e mod q = sT A + eT mod q

where (cid:107)e(cid:107)∞ ≤ q

4m . Hence,

˜yT T mod q = (sT A + eT )T mod q = eT T mod q

As T has only zero-one entries, eT T is bounded over integers with the absolute value of each
coordinate being at most m · (cid:107)e(cid:107)∞ ≤ q
4 . This implies that the robust classiﬁer would correctly
output 0 when given perturbed samples from D0.

21

In order to show that it is diﬃcult to recover the robust classiﬁer, we rely on the learning with
errors assumption. We consider a perturbation adversary that simply adds random noise to the
sample it receives.

Lemma 6.10 (Hardness of Learning a Robust Classiﬁer). There exists a perturbation algorithm
P such that for every polynomial time learner L, the learner L has no advantage over chance in
classifying examples perturbed by P. That is,

P







A, T ← TrapSamp(n, m, p);
x ← D(A)
where b ← {0, 1}
b
˜x ← PD0,D1(x);
b(cid:48) ← LD0,D1(˜x)







≤

1
2

: b = b(cid:48)

+ negl(n)

Proof. Observe that D0, D1 are completely speciﬁed by the matrix A and given A can be sampled
eﬃciently. So, it suﬃces to give the learner A instead of sample access. Consider the following
random perturbation algorithm P:

So, the experiment above is equivalent to the following:

P(x) : Output ˜x = x + e, where e ← χm.



P



A, T ← TrapSamp(n, m, p);
s ← Zn
q , e ← χm, b ← {0, 1}
b(cid:48) ← L(A, sT A + eT + b q

: b = b(cid:48)

 ≤

+ negl(n)



1
2

2 · 1T )
The cruical observation is that the learner’s job is to distinguish LWE samples (A, sT A + eT ) from
shifted LWE samples (A, sT A + eT + q
2 1T ). The LWE assumption implies that this is diﬃcult
because the two distributions are indistinguishable. That is,

(A, sT A + eT ) ≈c (A, rT ) ≈c (A, sT A + eT +

q
2

· 1T )

and hence no eﬃcient adversary L can distinguish between the distribution when b = 0 from when
b = 1. And hence for any eﬃcient adversary, the success probability of classifying these perturbed
instances is negligibly close to a half, as desired.

Hence, we have described a learning task that is learnable, has a robust classiﬁer, but robust

classiﬁers are computationally hard to learn.

7 Using Pseudorandom Functions and Error Correcting Codes

In this section, we formally describe the hard-to-robustly learn task based on one-way functions.
There are two main ingredients that we use to construct the learning task: Error Correcting Codes
(ECCs) and Pseudorandom Functions (PRFs).

An uniquely decodable binary error correcting code allows encoding messages to redundant
codewords such that from any codeword perturbed to some degree, we can recover the encoded
message.

Deﬁnition 7.1 (Uniquely Decodable Error Correcting Code). An uniquely decodable binary error
correcting code, C : {0, 1}n → {0, 1}m consists of two eﬃcient algorithms Encode, Decode. The
code tolerates error fraction e if for all messages x ∈ {0, 1}n,

where B(Encode(x), em) denotes the Hamming ball of radius em.

Decode(˜y) = x for all ˜y ∈ B(Encode(x), em)

22

We know very good error correcting codes.

Theorem 7.2 ([GI01]). For any constant γ > 0, there exists a binary error correcting code C :
{0, 1}n → {0, 1}m where m = O(n/γ3) with a decoding radius of ( 1
4 − γ)m with polynomial time
encoding and decoding.

We will use this coding scheme with γ = 1/8 giving us an error correcting code C : {0, 1}n →

{0, 1}m where m = θ(n) and tolerates m/8 errors for unique decoding.

A pseudorandom function is a keyed function Fk : {0, 1}n−1 → {0, 1} where the secret key is
picked uniformly random such that, for every eﬃcient adversary, the output of the function is
indistinguishable from the output of a random function. A more formal deﬁnition is given below.
It is known that pseudorandom functions can be constructed from one-way functions.

Deﬁnition 7.3 ([GGM86]). A family of polynomial-time computable functions F = {Fn} where
Fn = {Fk : {0, 1}n → {0, 1}} where k ∈ {0, 1}n and n ∈ N is pseudorandom if every polynomial
time computable adversary A cannot distinguish between F and uniformly random function. That
is,

(cid:12)
(cid:12)
P
(cid:12)
(cid:12)
k←{0,1}n

(cid:2)AFk (1n) = 1(cid:3) − P

Un←Un

(cid:12)
(cid:12)
(cid:2)AUn(1n) = 1(cid:3)
(cid:12)
(cid:12)

< negl(n)

where Un is the uniform distribution over all functions from {0, 1}n to {0, 1}.

Theorem 7.4 ([GGM86]). Pseudorandom functions exist if one-way functions exist.

Next, we informally describe the learning task. Consider the following learning task: The two

distributions D0, D1 are parameterized by the PRF key k and deﬁned as follows:

D0 = (0, Encode(x, Fk(x))) and, D1 = (1, Encode(x, 1 − Fk(x))) .

So, the two distributions are tuples where the ﬁrst half is which distribution the sample was taken
from and the second an error correcting code applied to the tuple (x, Fk(x) + b), that is, either the
PRF evaluation at the location x or its complement. Note that without the ﬁrst bit, classifying
the original distributions is computationally infeasible. The pseudorandom function looks random
at every new location. Including the bit in the sample itself makes the unperturbed classiﬁcation
task easy. The error correcting code ensures that we have a robust classiﬁer.

Theorem 7.5. Let {Fk} for Fk : {0, 1}n−1 → {0, 1} be a pseudorandom function family and
C : {0, 1}n → {0, 1}m where m = θ(n) be an eﬃciently decodable error correcting code with decoding
algorithm Decode that tolerates m/8 errors.

Consider the following learning task. For a random pseudorandom function key k, deﬁne:

D(k)

0 = (cid:8)(0, C(x, Fk(x))) : x ← {0, 1}n−1(cid:9) and, D(k)

1 = (cid:8)(1, C(x, 1 − Fk(x))) : x ← {0, 1}n−1(cid:9)

supported on {0, 1}m. The learning task has the following properties.

1. (Easy to Learn) A classiﬁer to distinguish D0 from D1 can be learned from the samples

eﬃciently.

2. (Robust Classiﬁer Exists) There exists a robust classiﬁer R such that,

(cid:2)B(y, m/8) ∈ R−1(b)(cid:3) ≥ 0.99

P
y←Db

where m/8 is the decoding radius and B(y, d) = {y(cid:48) : (cid:107)y − y(cid:48)(cid:107)Ham ≤ d}.

23

3. (A Robust Classiﬁer is hard-to-learn) There exists a perturbation algorithm such that no
eﬃciently learned classiﬁer can classify perturbed adversarial examples better than chance.

Proof. To prove Part (1) consider the classiﬁer that outputs the ﬁrst bit. It works correctly on
instances from the distributions. To prove Part (2), we rely on the decoding algorithm. After
d = m/8 edits to the sample, we can recover the underlying message by ignoring the ﬁrst bit of the
tuple and decoding the rest to get the underlying message of the form (x, c) and then use the PRF
to classify. More formally, consider the following robust classifer:
Robust Classifer Rk(˜y) where ˜y ∈ {0, 1}m+1:

1. Let (x, c) = Decode(˜y2:m+1) where ˜y2:m+1 are all of ˜y but the ﬁrst bit.
2. Output 0 if c = Fk(x) else output 1.
Observe that error correcting code ensures that from every perturbed sample, we eﬃciently
recover the encoded message. And then because the message is of the form (x, Fk(x) + b) for class
b, this allows for correct classiﬁcation.

To show Part (3), we rely on the unlearnability of the PRF. Consider a perturbing adversary
that replaces the ﬁrst bit of the sample by 0. Classiﬁcation is now equivalent to predicting Fk(x)
given x. Because predicting Fk(x) is computationally infeasible to learn, so is a robust classiﬁer.

Note that, compared to the previous counter-examples, this example does not rely on public
key assumptions. The reason for that is that the samples here are “evasive”. In that there is no
way to generate fresh samples from the two distributions. So, we cannot translate this to a public
key encryption scheme because to encrypt, we need a samples from the distributions D0, D1 along
with the perturbing adversary and we do not have access to these samples.

The hardness of this task comes from the hardness of learning the PRF and not from the

perturbations. This is diﬀerent from the schemes based on LPN and LWE.

8 Using Average-Case Hardness and Error Correcting Codes

In this section, we formally state Theorem 2.1 and provide the proof outlined in Section 3.4. We
also give an alternative construction that relies on one-way-permutations, but yields a classiﬁcation
problem with distributions that are eﬃciently samplable.

We ﬁrst need the notion of an average-case hard function.

Deﬁnition 8.1 (Average-Case Hard). A boolean function g : {0, 1}n → {0, 1} is (s, δ)-average-case
hard if for all non-uniform probabilistic algorithms A running in time s,

P
A,x∈{0,1}n

[A(x) (cid:54)= g(x)] ≥ δ

There exists functions g which are (2Θ(n), 1/2 − 2−Θ(n))-average-case hard (a random function

g will suﬃce with constant probability).

Theorem 8.2. Let g : {0, 1}n → {0, 1} be a function that is (2Θ(n), 1/2−2−Θ(n))-average-case hard,
and let Encode : {0, 1}n+1 → {0, 1}m where m = θ(n) be an eﬃciently decodable error correcting
code with decoding algorithm Decode that tolerates m/8 errors.

Consider the following classiﬁcation task. Deﬁne:

D0 = {(0, Encode(x, g(x))) : x ← {0, 1}n} and D1 = {(1, Encode(x, 1 − g(x))) : x ← {0, 1}n}

This classiﬁcation task has the following properties.

24

1. (Easy to Classify) An eﬃcient classiﬁer to distinguish D0 from D1 exists.

2. (Robust Classiﬁer Exists) There exists a ineﬃcient robust classiﬁer R such that,

(cid:2)B(y, m/8) ∈ R−1(b)(cid:3) ≥ 0.99

P
y←Db

where m/8 is the decoding radius and B(y, d) = {y(cid:48) : (cid:107)y − y(cid:48)(cid:107)Ham ≤ d}.

3. (No Eﬃcient Robust Classiﬁer Exists) There exists a perturbation algorithm P such that there

exists no polynomial-time robust classiﬁer R such that,

P(cid:2)R(˜y) ∈ R−1(b)(cid:3) ≥ 0.5 + negl(n)

where the perturbed sample ˜y is generated by sampling y ← Db for a random b and is then
perturbing ˜y ← PD0,D1(y) such that (cid:107)y − ˜y(cid:107) ≤ ε.

Proof. This proof closely follows the proof of Theorem 7.5. For Part (1), the classiﬁer that simply
outputs the ﬁrst bit is always correct. For Part (2), we can robustly classify by using the error
correcting code to recover the message (x, g(x)) or (x, 1 − g(x)), and then we can compute the
function g to distinguish between these cases. Speciﬁcally, the robust classiﬁer is identical to the
one presented in the proof of Theorem 7.5, but computing the function g instead of Fk(x). For Part
(3), we rely on the average-case hardness of g. Consider the perturbation adversary that replaces the
ﬁrst bit of the sample by 0. Now, classifying D0 vs D1 with non-negligible advantage is equivalent
to predicting g(x) given x with non-negligible advantage. This is impossible in polynomial time by
the average-case hardness of g, and thus eﬃcient robust classiﬁcation is impossible.

We now describe how to achieve the above properties with distributions that are eﬃciently
samplable. First, recall the notion of a hard-core bit: Let f : {0, 1}n → {0, 1}n be a one-way
function. A predicate b : {0, 1}n → {0, 1} is a hard-core bit for f if for all probabilistic polynomial-
time algorithms A,

The construction is as follows.

P
x←{0,1}n

[A(f (x)) = b(x)] ≤

1
2

+ negl(n)

Theorem 8.3. Let f : {0, 1}n → {0, 1}n be a one-way permutation, and let b : {0, 1}n → {0, 1} be
a hard-core bit for f . Let Encode : {0, 1}n+1 → {0, 1}m where m = θ(n) be an eﬃciently decodable
error correcting code with decoding algorithm Decode that tolerates m/8 errors.

Consider the following classiﬁcation task. Deﬁne:

D0 = {(0, Encode(f (x), b(x))) : x ← {0, 1}n} and D1 = {(1, Encode(f (x), 1 − b(x))) : x ← {0, 1}n}

This classiﬁcation task has the following properties.

1. (Easy to Classify) An eﬃcient classiﬁer to distinguish D0 from D1 exists.

2. (Robust Classiﬁer Exists) There exists a ineﬃcient robust classiﬁer R such that,

(cid:2)B(y, m/8) ∈ R−1(b)(cid:3) ≥ 0.99

P
y←Db

where m/8 is the decoding radius and B(y, d) = {y(cid:48) : (cid:107)y − y(cid:48)(cid:107)Ham ≤ d}.

25

3. (No Eﬃcient Robust Classiﬁer Exists) There exists a perturbation algorithm P such that there

exists no polynomial-time robust classiﬁer R such that,

P(cid:2)R(˜y) ∈ R−1(b)(cid:3) ≥ 0.5 + negl(n)

where the perturbed sample ˜y is generated by sampling y ← Db for a random b and is then
perturbing ˜y ← PD0,D1(y) such that (cid:107)y − ˜y(cid:107) ≤ ε.

4. (Eﬃciently Samplable) The distributions D0, D1 can be sampled in polynomial time.

Proof. Parts (1)-(3) follow exactly as in the proof of Theorem 8.2. Note that an ineﬃcent distin-
guisher can invert f (x) to ﬁnd x, and compute b(x). For Part (4), both distributions are clearly
eﬃciently samplable, by ﬁrst sampling x and then computing f (x), b(x).

9 Cryptography from Robustly Hard Tasks

In this section, we show that the existence of tasks with a provable gap in classiﬁcation and robust
classiﬁcation implies one-way functions and hence a variety of cryptographic primitives that include
pseudorandom functions, symmetric key encryption among others.

Theorem 9.1. Provably hard-to-learn robust classiﬁers imply one-way functions. Given a learning
task D0, D1 such that,

1. (Robust Classiﬁer Exists) There exists a robust classiﬁer R such that,

(cid:2)B(y, d) ∈ R−1(b)(cid:3) ≥ 0.90

P
y←Db

where d is the decoding radius and B(y, d) = {y(cid:48) : (cid:107)y − y(cid:48)(cid:107)Ham ≤ d}.

2. (A Robust Classiﬁer is hard-to-learn) There exists an eﬃcient perturbing adversary P such
that every eﬃciently learned classiﬁer L is not a robust classiﬁer. That is, for a learning task
D0, D1 ← Samp(n) and classiﬁer L,

P(cid:2)LD0,D1(˜x) = b(cid:3) ≤

1
2

+ 0.1 .

where the perturbed sample ˜x ∈ B(x, d) is generated by sampling x ← Db for a random b ←
{0, 1} and is then perturbing ˜x ← PD0,D1(x). The probability is over the entire experiment
from sampling the learning tasks to the randomness of the perturbation algorithm and the
classiﬁer.

Then one-way functions exist.

The proof of this theorem relies on fact that we can construct one-way functions from any two
distributions that are staistically far and computationally close. The two distributions considered
are the perturbed distributions. That is,

D(cid:48)

0 = {P(x) : x ← D0} and, D(cid:48)

1 = {P(x) : x ← D1}

We show that these two distributions are statistically far and yet computationally indistinguishable
giving one-way functions. They are statistically far because the robust classﬁer can distinguish
between them. Hence, the total variation distance between the two has to be large. And that they
are computationally close because no eﬃcient algorithm can distinguish between the two. Hence
one way functions exist.

26

Proof. We formally state the theorem used below.

Theorem 9.2 (Folklore, see e.g., Chap. 3, Ex. 11 [Gol01]). Given a pair of distributions (X0, X1) ←
F over X that are statistically far,

dT V (X0, X1) = max

A:X →[0,1]

E
x←X0

A(x) − E

x←X1

A(x) > 0.8

and computationally indistinguishable. That is for every polynomial time adversary A that gets
sample access to the distributions,

E
x←X0

A(X0,X1)(x) − E

x←X1

A(X0,X1)(x) < 0.4

Then one-way functions exist.7

We want to show that these two distributions are statiscally far and computationally close.
This relies on the existence of the robust classiﬁer and the diﬃcultly of learning one respectively.
1) ≥ 0.8. To observe this, consider the robust classiﬁer as

We start by showing that, dT V (D(cid:48)

0, D(cid:48)

the distinguisher. This implies that,

dT V ≥ E
x←D(cid:48)
1

[R(x)] − E

x←D(cid:48)
0

[R(x)] ≥ 0.9 − 0.1 ≥ 0.8

On the other hand, any eﬃcient distinguisher cannot distinguish between the samples by the

assumption. Hence we are done.

Another reasonable deﬁnition, from which we don’t know one-way functions is the following:
there exists a perturbation adversary P that given oracle access to the underlying classiﬁer ﬁnds
(cid:2)R(PR,D0,D1(x)) (cid:54)= b(cid:3) ≥ 0.4. For this deﬁnition, using standard
counter examples. That is, Px←Db
min-max arguments [Imp95, FS+99, VZ13], we can construct “time-bounded” universal adversaries.
That is, for time T , there exists a perturbation adversary PT running in time poly(T ) that ﬁnds
adversarial examples for all adversaries running in time T or less. This is insuﬃcient to imply
one-way functions though.

Public Key Encryption. The two distributions described above have the following public-key
encryption ﬂavor: the robust classiﬁer can serve as the decryption algorithm to distinguish between
0, D(cid:48)
samples from the perturbed distributions D(cid:48)
If after seeing enough samples, the learning
1.
algorithm can generate fresh samples from the two unperturbed distributions D0, D1 then we also
have an encryption algorithm: to encrypt a bit b, ﬁrst sample from the distribution Db and run
the perturbation adversary P to generate the encryption of the bit. To decrypt, use the robust
classiﬁer.

There are two key ingredients missing: (1) The encryption algorithm P needs access to fresh
samples from the two distributions to encrypt. There are learning tasks where we do not have access
to these. (2) The ability to sample the robust classiﬁer along with descriptions of the learning tasks.
This might not be feasible, especially when the tasks are not chosen, but supplied by nature.

Acknowledgments. We would like to thank Shaﬁ Goldwasser and Nadia Heninger for discussions
regarding inversion of the (noisy) BBS PRG.

7The constants in the equations are fairly arbitrary. We can replace them by any constants α, β where α2 > β

and the result holds.

27

References

[ACW18] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false
sense of security: Circumventing defenses to adversarial examples. arXiv preprint
arXiv:1802.00420, 2018.

[AG11]

Sanjeev Arora and Rong Ge. New algorithms for learning in presence of errors.
In
International Colloquium on Automata, Languages, and Programming, pages 403–415.
Springer, 2011.

[Ale03]

Michael Alekhnovich. More on Average Case vs Approximation Complexity. In Foun-
dations of Computer Science, pages 298–307. IEEE, 2003.

[BBS86]

Lenore Blum, Manuel Blum, and Mike Shub. A Simple Unpredictable Pseudo-Random
Number Generator. SIAM Journal on computing, 1986.

[BDRV19]

Itay Berman, Akshay Degwekar, Ron D. Rothblum, and Prashant Nalini Vasudevan.
Statistical Diﬀerence Beyond the Polarizing Regime. Electronic Colloquium on Com-
putational Complexity (ECCC), 26:38, 2019.

[BKW03] Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant Learning, the Parity

Problem, and the Statistical Query Model. J. ACM, 2003.

[BLPR18] S´ebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya P. Razenshteyn. Adversarial
examples from cryptographic pseudo-random generators. CoRR, abs/1811.06418, 2018.

[BN90]

Jehoshua Bruck and Moni Naor. The Hardness of Decoding Linear Codes with Prepro-
cessing. IEEE Trans. Information Theory, 36(2):381–385, 1990.

[BPR18]

S´ebastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from com-
putational constraints. CoRR, abs/1805.10204, 2018.

[BR18]

[CW17]

Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial
machine learning. Pattern Recognition, 84:317–331, 2018.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural net-
works. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39–57. IEEE,
2017.

[DDS+04] Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, Deepak Verma, et al. Adversarial classi-
ﬁcation. In Proceedings of the tenth ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 99–108. ACM, 2004.

[DV19]

[FFF18]

[FS+99]

Akshay Degwekar and Vinod Vaikuntanathan. Computational Limitations in Robust
Classiﬁcation and Win-Win Results. arXiv preprint arXiv:1902.01086, 2019.

Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any
classiﬁer. CoRR, abs/1802.08686, 2018.

Yoav Freund, Robert E Schapire, et al. Adaptive game playing using multiplicative
weights. Games and Economic Behavior, 29(1-2):79–103, 1999.

[GGM86] Oded Goldreich, Shaﬁ Goldwasser, and Silvio Micali. How to Construct Random Func-

tions. J. ACM, 1986.

28

[GI01]

Venkatesan Guruswami and Piotr Indyk. Expander-based Constructions of Eﬃciently
Decodable Codes. In IEEE Foundations of Computer Science. IEEE, 2001.

[GMF+18] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra Raghu,

Martin Wattenberg, and Ian Goodfellow. Adversarial spheres, 2018.

[Gol01]

Oded Goldreich. Foundations of Cryptography: Basic Tools, 2001.

[GPV08] Craig Gentry, Chris Peikert, and Vinod Vaikuntanathan. Trapdoors for Hard Lattices
and New Cryptographic Constructions. In Symposium on Theory of computing, pages
197–206. ACM, 2008.

[Gre13]

Matthew Green. A few more notes on NSA random number generators, 2013.

[GSS]

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. corr (2015).

[Hen19]

Nadia Heninger. Personal communication, 2019.

[Imp95]

Russell Impagliazzo. Hard-core distributions for somewhat hard problems. In Foun-
dations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages
538–545. IEEE, 1995.

[JKPT12] Abhishek Jain, Stephan Krenn, Krzysztof Pietrzak, and Aris Tentes. Commitments
and eﬃcient zero-knowledge proofs from learning parity with noise. In ASIACRYPT,
2012.

[KMR+94] Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E Schapire,
and Linda Sellie. On the Learnability of Discrete Distributions. In ACM symposium
on Theory of computing, pages 273–282, 1994.

[KV94]

[LM05]

[Lyu05]

[Mic01]

[MM18]

[MS91]

[Nak19]

Michael Kearns and Leslie Valiant. Cryptographic Limitations on Learning Boolean
Formulae and Finite Automata. J. ACM, 1994.

Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh
ACM SIGKDD international conference on Knowledge discovery in data mining, pages
641–647. ACM, 2005.

Vadim Lyubashevsky. The parity problem in the presence of noise, decoding random
linear codes, and the subset sum problem. In APPROX’05/RANDOM’05, 2005.

Daniele Micciancio. The hardness of the closest vector problem with preprocessing.
IEEE Transactions on Information Theory, 47(3):1212–1215, 2001.

Saeed Mahloujifar and Mohammad Mahmoody. Can adversarially robust learning lever-
age computational hardness? CoRR, abs/1810.01407, 2018.

Silvio Micali and Claus-Peter Schnorr. Eﬃcient, Perfect Polynomial Random Number
Generators. Journal of Cryptology, 3(3):157–172, 1991.

Preetum Nakkiran. Adversarial robustness may be at odds with simplicity. arXiv
preprint arXiv:1901.00532, 2019.

29

[NR06]

[Pei16]

Moni Naor and Guy N Rothblum. Learning to impersonate. In Proceedings of the 23rd
international conference on Machine learning, pages 649–656. ACM, 2006.

Chris Peikert. A Decade of Lattice Cryptography. Foundations and Trends R(cid:13) in The-
oretical Computer Science, 10(4):283–424, 2016.

[Reg04]

Oded Regev. Improved Inapproximability of Lattice and Coding Problems With Pre-
processing. IEEE Trans. Information Theory, 50(9):2031–2037, 2004.

[SST+18] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Alek-
sander Madry. Adversarially robust generalization requires more data. arXiv preprint
arXiv:1804.11285, 2018.

[SZS+13] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.

[VZ13]

Salil Vadhan and Colin Jia Zheng. A uniform min-max theorem with applications
in cryptography. In Advances in Cryptology–CRYPTO 2013, pages 93–110. Springer,
2013.

A A Description of BLPR Example and the Blum-Blum-Shub

PRG.

In this section, we describe the BLPR counter-example and the Blum-Blum-Shub pseudorandom
generator.

We start by deﬁning the notion of a trapdoor pseudorandom generator. A trapdoor pseudoran-
dom generator TrapPRG : {0, 1}n → {0, 1}2n is an expanding function whose outputs are indistin-
(cid:8)y : y ← {0, 1}2n(cid:9).
guishable from truly random strings. That is, {TrapPRG(x) : x ← {0, 1}n} ≈c
Furthermore, the function has a trapdoor trap that allows distinguishing the output of the PRG
from random outputs. That is, there exists a distinguisher D that given the trapdoor,

P
x←{0,1}n

[D(trap, TrapPRG(x)) = 1] −

P
y←{0,1}2n

[D(trap, y) = 1] > 0.99

Given a trapdoor PRG, the BLPR learning task D0, D1 is the following:

D0 = {(0, TrapPRG(x)) : x ← {0, 1}n} and, D1 = (cid:8)(1, y) : y ← {0, 1}2n(cid:9) .

We describe the BBS PRG and its trapdoor property next. The Blum-Blum-Shub pseudoran-

dom generator is deﬁned as follows:

Consider a number N = pq where p, q are primes congruent to 3 (mod 4). The seed to the
PRG is a random element x0 ∈ ZN . Let hcb be a hardcore bit8 of the function x → x2 (mod N )
(eg parity or the most signiﬁcant bit).

BBSPRG(x0, m):
1. For i ∈ [1 : m],

8A function hcb is a hardcore bit of a one-way function f has the following property, that if given y = f (x) for a
random x, hcb(x) is pseudorandom. That is, given any algorithm that given y = f (x) can predict hcb(x), then we
can use this algorithm to invert f with non-negligible probability.

30

(a) Set xi = x2
(b) Set yi = hcb(xi)

i−1 (mod N ).

2. Output y1, y2, . . . , ym−1, xm.
The trapdoor property BLPR refer to construct the robust classiﬁer is the following one: In
the construction of the PRG, the security does not rely on outputting the last entry (xm) in its
entireity. Though doing so enables the following “trapdoor” property:

Lemma A.1. There exists a distinguisher D that given the factorization of N can distinguish
between the output of the BBSPRG from random strings. That is,

P
x0←ZN

[Dp,q(BBSPRG(x0))] −

P
y←{0,1}m

[Dp,q(y)] > 0.99

Proof Sketch. The proof relies on the fact that Rabin’s one way function f (x) = x2 mod N is a
trapdoor function that can be eﬃciently inverted given the factorization of N . Furthermore, the
inverse returned is the only square root of x2 that is a square itself. Hence the distinguisher does
the following:

D(z):

1. Interpret the input as y1, y2, . . . ym−1, xm.
2. If xm is not a square mod N , output 0.
3. Compute x1, x2, . . . xm−1 as xi = f −1(xi+1).
4. If yi = hcb(xi) for all i, return 1, else return 0.
Observe that the distinguisher always outputs 1 on outputs of the PRG. On the other hand,
when fed a random string, xm is not a square with probability 3/4 and even when it is a square,
the probability of each yi = hcb(xi) is exactly 1/2 independently. Hence the probability that the
distinguisher outputs 1 on a random string is 1
4 · ( 1

2 )m−1 which is tiny.

Based on this, the BLPR counterexample is the following:

BLPR Counter-Example. Let N = pq where p, q are random n-bit primes of the form 3 (mod 4).
Let m = n2. Deﬁne D0, D1 as:

D0 = {(0, BBSPRG(x0)) : x0 ← ZN } and, D1 :

(1, z) : z ← {0, 1}m+log N (cid:111)
(cid:110)

Then, the learning task has the following properties: (1) The distributions are easy to classify non-
robustly. (2) There exists an ineﬃcient robust classiﬁer for ε = θ(
n). (3) No eﬃciently learned
classiﬁer can classify better than chance. (4) Given the factorization of N , there exists an eﬃcient
robust classiﬁer for ε = θ(

n).

√

√

Properties 1, 2, 3 are true. To the best of our knowledge, 4 is not known to be true. As we
described earlier, we know of robust classiﬁers for ε = O(1). This leaves us with the following open
questions.

Open Questions.

1. Given factorization of N , prove that there exists an eﬃcient robust classiﬁer for ε = ω(1)-bits.

2. (Perturbation Adversary 1) Consider the perturbation adversary that erases the ﬁrst bit and
n. Given the factorization a N ,

adds random noise to each bit of the PRG with prob 1/
does there exists an eﬃcient robust classiﬁer for this adversary.

√

31

3. (Perturbation Adversary 2) The adversary deletes the last complete entry output by the PRG
(i.e., xm). Given the factorization of N , can we distinguish this PRG from random, when no
other error is added.

Although BBS is a trapdoor PRG, it crucially relies on the fact that xm, the last value is
available completely intact. Without access to this value, BBS is still a PRG but it is not
clear how to do the trapdoor decoding.

As we described earlier, Open Question 3 is a long-standing open question in the computational
number theory community [Hen19, Gre13]. And Open Question 1 is a harder variant of that
question. Finally, Question 2 asks a error correction or decoding question – given the output of a
PRG with random errors, can you recover the original PRG string (even given some trapdoor). We
are not aware of any way in which this factorization actually helps decoding under random noise.

32


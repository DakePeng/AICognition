8
1
0
2

r
a

M
5
1

]

G
L
.
s
c
[

1
v
3
7
5
5
0
.
3
0
8
1
:
v
i
X
r
a

PublishedasaconferencepaperatICLR2018IMPROVINGGANSUSINGOPTIMALTRANSPORTTimSalimans∗OpenAItim@openai.comHanZhang∗†RutgersUniversityhan.zhang@cs.rutgers.eduAlecRadfordOpenAIalec@openai.comDimitrisMetaxasRutgersUniversitydnm@cs.rutgers.eduABSTRACTWepresentOptimalTransportGAN(OT-GAN),avariantofgenerativeadversar-ialnetsminimizinganewmetricmeasuringthedistancebetweenthegeneratordistributionandthedatadistribution.Thismetric,whichwecallmini-batchen-ergydistance,combinesoptimaltransportinprimalformwithanenergydistancedeﬁnedinanadversariallylearnedfeaturespace,resultinginahighlydiscrimi-nativedistancefunctionwithunbiasedmini-batchgradients.ExperimentallyweshowOT-GANtobehighlystablewhentrainedwithlargemini-batches,andwepresentstate-of-the-artresultsonseveralpopularbenchmarkproblemsforimagegeneration.1INTRODUCTIONGenerativemodelingisamajorsub-ﬁeldofMachineLearningthatstudiestheproblemofhowtolearnmodelsthatgenerateimages,audio,video,textorotherdata.Applicationsofgenerativemodelsincludeimagecompression,generatingspeechfromtext,planninginreinforcementlearn-ing,semi-supervisedandunsupervisedrepresentationlearning,andmanyothers.Sincegenerativemodelscanbetrainedonunlabeleddata,whichisalmostendlesslyavailable,theyhaveenormouspotentialinthedevelopmentofartiﬁcialintelligence.Thecentralproblemingenerativemodelingishowtotrainagenerativemodelsuchthatthedistri-butionofitsgenerateddatawillmatchthedistributionofthetrainingdata.Generativeadversarialnets(GANs)representanadvanceinsolvingthisproblem,usinganeuralnetworkdiscriminatororcritictodistinguishbetweengenerateddataandtrainingdata.Thecriticdeﬁnesadistancebetweenthemodeldistributionandthedatadistributionwhichthegenerativemodelcanoptimizetoproducedatathatmorecloselyresemblesthetrainingdata.Acloselyrelatedapproachtomeasuringthedistancebetweenthedistributionsofgenerateddataandtrainingdataisprovidedbyoptimaltransporttheory.Byframingtheproblemasoptimallytransportingonesetofdatapointstoanother,itrepresentsanalternativemethodofspecifyingametricoverprobabilitydistributionsandprovidesanotherobjectivefortraininggenerativemodels.ThedualproblemofoptimaltransportiscloselyrelatedtoGANs,asdiscussedinthenextsection.However,theprimalformulationofoptimaltransporthastheadvantagethatitallowsforclosedformsolutionsandcanthusmoreeasilybeusedtodeﬁnetractabletrainingobjectivesthatcanbeevaluatedinpracticewithoutmakingapproximations.Acomplicationinusingprimalformoptimaltransportisthatitmaygivebiasedgradientswhenusedwithmini-batches(seeBellemareetal.,2017)andmaythereforebeinconsistentasatechniqueforstatisticalestimation.InthispaperwepresentOT-GAN,avariantofgenerativeadversarialnetsincorporatingprimalformoptimaltransportintoitscritic.Wederiveandjustifyourmodelbydeﬁninganewmetricoverprobabilitydistributions,whichwecallMini-batchEnergyDistance,combiningoptimaltransport∗equalcontribution†workperformedduringaninternshipatOpenAI1 
 
 
 
 
 
PublishedasaconferencepaperatICLR2018inprimalformwithanenergydistancedeﬁnedinanadversariallylearnedfeaturespace.Thiscombinationresultsinahighlydiscriminativemetricwithunbiasedmini-batchgradients.InSection2weprovidethepreliminariesrequiredtounderstandourwork,andweputourcon-tributionintocontextbydiscussingtherelevantliterature.Section3presentsourmaintheoreticalcontribution:Minibatchenergydistance.Weapplythisnewdistancemetrictotheproblemoflearn-inggenerativemodelsinSection4,andshowstate-of-the-artresultsinSection5.Finally,Section6concludesbydiscussingthestrengthsandweaknessesoftheproposedmethod,aswellasdirectionsforfuturework.2GANSANDOPTIMALTRANSPORTGenerativeadversarialnets(Goodfellowetal.,2014)wereoriginallymotivatedusinggametheory:Ageneratorgandadiscriminatordplayazero-sumgamewherethegeneratormapsnoiseztosimulatedimagesy=g(z)andwherethediscriminatortriestodistinguishthesimulatedimagesyfromimagesxdrawnfromthedistributionoftrainingdatap.Thediscriminatortakesineachimagexandyandoutputsanestimatedprobabilitythatthegivenimageisrealratherthangenerated.Thediscriminatorisrewardedforputtinghighprobabilityonthecorrectclassiﬁcation,andthegeneratorisrewardedforfoolingthediscriminator.Thegoaloftrainingisthentoﬁndapairof(g,d)forwhichthisgameisataNashequilibrium.Atsuchanequilibrium,thegeneratorminimizesitsloss,ornegativegamevalue,whichcanbedeﬁnedasLg=supdEx∼plog[d(x)]+Ey∼glog[1−d(y)](1)Arjovskyetal.(2017)re-interpretGANsintheframeworkofoptimaltransporttheory.Speciﬁcally,theyproposetheEarth-MoverdistanceorWasserstein-1distanceasagoodobjectiveforgenerativemodeling:DEMD(p,g)=infγ∈Π(p,g)Ex,y∼γc(x,y),(2)whereΠ(p,g)isthesetofalljointdistributionsγ(x,y)withmarginalsp(x),g(y),andwherec(x,y)isacostfunctionthatArjovskyetal.(2017)taketobetheEuclideandistance.Ifthep(x)andg(y)distributionsareinterpretedaspilesofearth,theEarth-MoverdistanceDEMD(p,g)canbeinterpretedastheminimumamountof“mass”thatγhastotransporttoturnthegeneratordistribu-tiong(y)intothedatadistributionp(x).Fortherightchoiceofcostc,thisquantityisametricinthemathematicalsense,meaningthatDEMD(p,g)≥0andDEMD(p,g)=0ifandonlyifp=g.Min-imizingtheEarth-Moverdistanceingisthusavalidmethodforderivingastatisticallyconsistentestimatorofp,providedpisinthemodelclassofourgeneratorg.Unfortunately,theminimizationoverγinEquation2isgenerallyintractable,soArjovskyetal.(2017)turntothedualformulationofthisoptimaltransportproblem:DEMD(p,g)=supkfkL≤1Ex∼pf(x)−Ey∼gf(y),(3)wherewehavereplacedtheminimizationoverγwithamaximizationoverthesetof1-Lipschitzfunctions.Thisoptimizationproblemisgenerallystillintractable,butArjovskyetal.(2017)arguethatitiswellapproximatedbyusingtheclassofneuralnetworkGANdiscriminatorsorcriticsdescribedearlierinplaceoftheclassof1-Lipschitzfunctions,providedweboundthenormoftheirgradientwithrespecttotheimageinput.Makingthissubstitution,theobjectivebecomesquitesimilartothatofouroriginalGANformulationinEquation1.InfollowupworkGulrajanietal.(2017)proposeadifferentmethodofboundingthegradientsintheclassofallowedcritics,andprovidestrongempiricalresultssupportingthisinterpretationofGANs.Inspiteoftheirsuccess,however,weshouldnotethatGANsarestillonlyabletosolvethisoptimaltransportproblemapproximately.Theoptimizationwithrespecttothecriticcannotbeperformedperfectly,andtheclassofobtainablecriticsonlyveryroughlycorrespondstotheclassof1-Lipschitzfunctions.TheconnectionbetweenGANsanddualformoptimaltransportisfurtherexploredbyBousquetetal.(2017)andGenevayetal.(2017a),whoextendtheanalysistodifferentoptimaltransportcostsandtoabroadermodelclassincludinglatentvariables.AnalternativeapproachtogenerativemodelingischosenbyGenevayetal.(2017b)whoinsteadchosetoapproximatetheprimalformulationofoptimaltransport.Theystartbytakinganentrop-icallysmoothedgeneralizationoftheEarthMoverdistance,calledtheSinkhorndistance(Cuturi,2PublishedasaconferencepaperatICLR20182013):DSinkhorn(p,g)=infγ∈Πβ(p,g)Ex,y∼γc(x,y),(4)wherethesetofallowedjointdistributionΠβisnowrestrictedtodistributionswithentropyofatleastsomeconstantβ.Genevayetal.(2017b)thenapproximatethisdistancebyevaluatingitonmini-batchesofdataX,YconsistingofKdatavectorsx,y.ThecostfunctioncthengivesrisetoaK×KtransportcostmatrixC,whereCi,j=c(xi,yj)tellsushowexpensiveitistotransportthei-thdatavectorxiinmini-batchXtothej-thdatavectoryjinmini-batchY.Similarly,thecouplingdistributionγisreplacedbyaK×KmatrixMofsoftmatchingsbetweenthesei,jelements,whichisrestrictedtothesetofmatricesMwithallpositiveentries,withallrowsandcolumnssummingtoone,andwithsufﬁciententropy−Tr[Mlog(MT)]≥α.Theresultingdistance,evaluatedonaminibatch,isthenWc(X,Y)=infM∈MTr[MCT].(5)Inpractice,theminimizationoverthesoftmatchingsMcanbefoundefﬁcientlyontheGPUusingtheSinkhornalgorithm.Consequently,Genevayetal.(2017b)calltheirmethodofusingEquation5ingenerativemodelingSinkhornAutoDiff.Thegreatadvantageofthismini-batchSinkhorndistanceisthatitisfullytractable,eliminatingtheinstabilitiesoftenexperiencedwithGANsduetoimperfectoptimizationofthecritic.However,adisadvantageisthattheexpectationofEquation5overmini-batchesisnolongeravalidmetricoverprobabilitydistributions.Viewedanotherway,thegradientsofEquation5,forﬁxedmini-batchsize,arenotunbiasedestimatorsofthegradientsofouroriginaloptimaltransportprobleminEquation4.Forthisreason,Bellemareetal.(2017)proposetoinsteadusetheEnergyDistance,alsocalledCramerDistance,asthebasisofgenerativemodeling:DED(p,g)=p2E[kx−yk]−E[kx−x0k]−E[ky−y0k],(6)wherex,x0areindependentsamplesfromdatadistributionpandy,y0independentsamplesfromthegeneratordsitributiong.InCramerGANtheyproposetrainingthegeneratorbyminimizingthisdistancemetric,evaluatedinalatentspacewhichislearnedbytheGANcritic.Inthenextsectionweproposeanewmetricforgenerativemodeling,combiningtheinsightsofGANsandoptimaltransport.AlthoughourworkwasperformedconcurrentlytothatbyGenevayetal.(2017b)andBellemareetal.(2017),itcanbeunderstoodmosteasilyasformingasynthesisoftheideasusedinSinkhornAutoDiffandCramerGAN.3MINI-BATCHENERGYDISTANCEAsdiscussedinthelastsection,mostpreviousworkingenerativemodelingcanbeinterpretedasminimizingadistanceD(g,p)betweenageneratordistributiong(x)andthedatadistributionp(x),wherethedistributionsaredeﬁnedoverasinglevectorxwhichweheretaketobeanimage.How-ever,inpracticedeeplearningtypicallyworkswithmini-batchesofimagesXratherthanindividualimages.Forexample,aGANgeneratoristypicallyimplementedasahighdimensionalfunctionG(Z)thatturnsamini-batchofrandomnoiseZintoamini-batchofimagesX,whichtheGANdiscriminatorthencomparestoamini-batchofimagesfromthetrainingdata.ThecentralinsightofMini-batchGAN(Salimansetal.,2016)isthatitisstrictlymorepowerfultoworkwiththedistri-butionsovermini-batchesg(X),p(X)thanwiththedistributionsoverindividualimages.Herewefurtherpursuethisinsightandproposeanewdistanceovermini-batchdistributionsD[g(X),p(X)]whichwecalltheMini-batchEnergyDistance.Thisnewdistancecombinesoptimaltransportinprimalformwithanenergydistancedeﬁnedinanadversariallylearnedfeaturespace,resultinginahighlydiscriminativedistancefunctionwithunbiasedmini-batchgradients.Inordertoderiveournewdistancefunction,westartbygeneralizingtheenergydistancegiveninEquation6togeneralnon-Euclideandistancefunctionsd.Doingsogivesusthegeneralizedenergydistance:DGED(p,g)=q2E[d(X,Y)]−E[d(X,X0)]−E[d(Y,Y0)],(7)whereX,X0areindependentsamplesfromdistributionpandY,Y0independentsamplesfromg.Thisdistanceistypicallydeﬁnedforindividualsamples,butitisvalidforgeneralrandomobjects,includingmini-batcheslikeweassumehere.TheenergydistanceDGED(p,g)isametric,inthe3PublishedasaconferencepaperatICLR2018mathematicalsense,aslongasthedistancefunctiondisametric(Klebanovetal.,2005).Underthiscondition,meaningthatdsatisﬁesthetriangleinequalityandseveralotherconditions,wehavethatD(p,g)≥0,andD(p,g)=0ifandonlyifp=g.Usingindividualsamplesx,yinsteadofminibatchesX,Y,Sejdinovicetal.(2013)showedthatsuchgeneralizationsoftheenergydistancecanequivalentlybeviewedasaformofmaximummeandiscrepancy,wheretheMMDkernelkisrelatedtothedistancefunctiondbyd(x,x0)≡k(x,x)+k(x0,x0)−2k(x,x0).WeﬁndtheenergydistanceperspectivemoreintuitivehereandfollowCramerGANinusingthisperspectiveinstead.WearefreetochooseanymetricdforuseinEquation7,butnotallchoiceswillbeequallydis-criminativewhenusedforgenerativemodeling.Here,wechoosedtobetheentropy-regularizedWassersteindistance,orSinkhorndistance,asdeﬁnedformini-batchesinEquation5.Althoughtheaverageovermini-batchSinkhorndistancesisnotavalidmetricoverprobabilitydistributionsp,g,resultinginthebiasedgradientsproblemdiscussedinSection2,theSinkhorndistanceisavalidmetricbetweenindividualmini-batches,whichisallwerequireforuseinsidethegeneralizedenergydistance.Puttingeverythingtogether,wearriveatourﬁnaldistancefunctionoverdistributions,whichwecalltheMinibatchEnergyDistance.LikewiththeCramerdistance,wetypicallyworkwiththesquareddistance,whichwedeﬁneasD2MED(p,g)=2E[Wc(X,Y)]−E[Wc(X,X0)]−E[Wc(Y,Y0)],(8)whereX,X0areindependentlysampledmini-batchesfromdistributionpandY,Y0areindependentmini-batchesfromg.Weincludethesubscriptctomakeexplicitthatthisdistancedependsonthechoiceoftransportcostfunctionc,whichwewilllearnadversariallyasdiscussedinSection4.IncomparisonwiththeoriginalSinkhorndistance(Equation5thelossfunctionfortraininggim-pliedbythismetricaddsarepulsiveterm−Wc(Y,Y0)totheattractivetermWc(X,Y).LikewiththeenergydistanceusedbyCramerGAN,thisiswhatmakestheresultingmini-batchgradientsunbiasedandtheobjectivestatisticallyconsistent.However,unliketheplainenergydistance,themini-batchenergydistanceD2MED(p,g)stillincorporatestheprimalformoptimaltransportoftheSinkhorndistance,whichinSection5weshowleadstomuchstrongerdiscriminativepowerandmorestablegenerativemodeling.Inconcurrentwork,Genevayetal.(2018)independentlyproposeaverysimilarlossfunctionto(8),butusingasinglesamplefromthedataandgeneratordistributions.Weobtainedbestresultsusingtwoindependentlysampledminibatchesfromeachdistribution.4OPTIMALTRANSPORTGAN(OT-GAN)Inthelastsectionwedeﬁnedthemini-batchenergydistancewhichweproposeusingfortraininggenerativemodels.However,weleftundeﬁnedthetransportcostfunctionc(x,y)onwhichitde-pends.Onepossibilitywouldbetochoosectobesomeﬁxedfunctionovervectors,likeEuclideandistance,butwefoundthistoperformpoorlyinpreliminaryexperiments.Althoughminimizingthemini-batchenergydistanceD2MED(p,g)guaranteesstatisticalconsistencyforsimpleﬁxedcostfunctionsclikeEuclideandistance,theresultingstatisticalefﬁciencyisgenerallypoorinhighdi-mensions.ThismeansthattheretypicallyexistmanybaddistributionsdistributionsgforwhichD2MED(p,g)issoclosetozerothatwecannottellpandgapartwithoutrequiringanenormoussamplesize.Tosolvethisweproposelearningthecostfunctionadversarially,sothatitcanadapttothegeneratordistributiongandtherebybecomemorediscriminative.Inpracticeweimplementthisbydeﬁningctobethecosinedistancebetweenvectorsvη(x)andvη(y),wherevηisadeepneuralnetworkthatmapstheimagesinourmini-batchintoalearnedlatentspace.Thatiswedeﬁnethetransportcosttobecη(x,y)=1−vη(x)·vη(y)kvη(x)k2kvη(y)k2,wherewechooseηtomaximizetheresultingminibatchenergydistance.Inpractice,trainingourgenerativemodelgθandouradversarialtransportcostcηisdonebyalter-natinggradientdescentasisstandardpracticeinGANs(Goodfellowetal.,2014).Herewechoose4PublishedasaconferencepaperatICLR2018toupdatethegeneratormoreoftenthanweupdateourcritic.Thisiscontrarytostandardpractice(e.g.Arjovskyetal.,2017)andensuresourcostfunctioncdoesnotbecomedegenerate.Ifcweretoassignzerotransportcosttotwonon-identicalregionsinimagespace,thegeneratorwouldquicklyadjusttotakeadvantageofthis.SimilartohowaquicklyadaptingcriticcontrolsthegeneratorinstandardGANs,thisworkstheotherwayaroundinourcase.ContrarytostandardGANs,ourgeneratorhasawelldeﬁnedandstatisticallyconsistenttrainingobjectiveevenwhenthecriticisnotupdated,aslongasthecostfunctioncisnotdegenerate.Wealsoinvestigatedforcingvηtobeone-to-onebyparameterizingitusingaRevNetGomezetal.(2017),therebyensuringccannotdegenerate,butthisprovedunnecessaryifthegeneratorisupdatedoftenenough.OurfulltrainingprocedureisdescribedinAlgorithm1,andisvisuallydepictedinFigure1.HerewecomputethematchingmatrixMinWcusingtheSinkhornalgorithm.UnlikeGenevayetal.(2017b)wedonotbackpropagatethroughthisalgorithm.IgnoringthegradientﬂowthroughthematchingsMisjustiﬁedbytheenvelopetheorem(seee.g.Carter,2001):SinceMischosentominimizeWc,thegradientofWcwithrespecttothisvariableiszero(whenprojectedintotheallowedspaceM).Algorithm1assumesweusestandardSGDforoptimization,butwearefreetouseotheroptimizers.InourexperimentsweuseAdam(Kingma&Ba,2014).Ouralgorithmfortraininggenerativemodelscanbegeneralizedtoincludeconditionalgenerationofimagesgivensomesideinformations,suchasatext-descriptionoftheimageoralabel.Whengeneratinganimageywesimplydrawsfromthetrainingdataandconditionthegeneratoronit.TherestofthealgorithmisidenticaltoAlgorithm1butwith(Y,S)inplaceofY,andsimilarsubstitutionsforX,X0,Y0.ThefullalgorithmforconditionalgenerationisdetailedinAlgorithm2intheappendix.Algorithm1OptimalTransportGAN(OT-GAN)trainingalgorithmwithstepsizeα,usingmini-batchSGDforsimplicityRequire:ngen,thenumberofiterationsofthegeneratorpercriticiterationRequire:η0,initialcriticparameters.θ0,initialgeneratorparameters1:fort=1toNdo2:SampleX,X0twoindependentmini-batchesfromrealdata,andY,Y0twoindependentmini-batchesfromthegeneratedsamples3:L=Wc(X,Y)+Wc(X,Y0)+Wc(X0,Y)+Wc(X0,Y0)−2Wc(X,X0)−2Wc(Y,Y0)4:iftmodngen+1=0then5:η←η+α·∇ηL6:else7:θ←θ−α·∇θL8:endif9:endforFigure1:IllustrationofOT-GAN.Mini-batchesfromthegeneratorandtrainingdataareembeddedintoalearnedfeaturespaceviathecritic.Atransportcostmatrixiscalculatedbetweenthetwomini-batchesoffeatures.Softmatchingalignsfeaturesacrossmini-batchesandalignedfeaturesarecompared.Theﬁgureonlyillustratesthedistancecalculationbetweenonepairofmini-batcheswhereasseveralarecomputed.5PublishedasaconferencepaperatICLR20185EXPERIMENTSInthissection,wedemonstratetheimprovedstabilityandconsistencyoftheproposedmethodonﬁvedifferentdatasetswithincreasingcomplexity.5.1MIXTUREOFGAUSSIANDATASETOneadvantageofOT-GANcomparedtoregularGANisthatforanysettingofthetransportcostc,i.e.anyﬁxedcritic,theobjectiveisstatisticallyconsistentfortrainingthegeneratorg.Evenifwestopupdatingthecritic,thegeneratorshouldthusneverdiverge.Withabadﬁxedcostfunctioncthesignalforlearninggmaybeveryweak,butatleastitshouldneverpointinthewrongdirection.Weinvestigatewhetherthistheoreticalpropertyholdsinpracticebyexaminingasimpletoyexample.WetraingenerativemodelsusingdifferenttypesofGANona2Dmixtureof8Gaussians,withmeansarrangedonacircle.Thegoalforthegeneratoristorecoverall8modes.Fortheproposedmethodandallthebaselinemethods,thearchitecturesaresimpleMLPswithReLUactivations.Asimilarexperimentalsettinghasbeenconsideredin(Metzetal.,2017;Lietal.,2017)todemonstratethemodecoveragebehaviorofvariousGANmodels.There,GANsusingmini-batchfeatures,DAN-S(Lietal.,2017),areshowntocaptureallthe8modeswhentrainingconverges.TotesttheconsistencyofGANmodels,westopupdatingthediscriminatorafter15kiterationsandvisualizethegeneratordistributionforanadditional25Kiterations.AsshowninFigure2,modecollapseoccursinamini-batchfeatureGANafterafewthousanditerationstrainingwithaﬁxeddiscriminator.However,usingthemini-batchenergydistance,thegeneratordoesnotdivergeandthegeneratedsamplesstillcoverall8modesofthedata.Figure2:Resultsforconsistencywhenﬁxingthecriticondatageneratedfrom8Gaussianmixtures.Theﬁrstcolumnshowsthedatadistribution.ThetoprowshowsthetrainingresultsofOT-GANusingmini-batchenergydistance.ThebottomrowshowsthetrainingresultwiththeoriginalGANloss(DAN-S).Thelattercollapsesto3outof8modesafterﬁxingthediscriminator,whileOT-GANremainsconsistent.5.2CIFAR-10CIFAR-10isawell-studieddatasetof32×32colorimagesforgenerativemodels(Krizhevsky,2009).WeusethisdatasettoinvestigatetheimportanceofthedifferentdesigndecisionsmadewithOT-GAN,andwecomparethevisualqualityofitsgeneratedsampleswithotherstate-of-the-artGANmodels.Ourmodelandtheotherreportedresultsaretrainedinanunsupervisedmanner.Wechoose“inceptionscore”(Salimansetal.,2016)asnumericalassessmenttocomparethevisualqualityofsamplesgeneratedbydifferentmodels.Ourgeneratorandcriticarestandardconvnets,similartothoseusedbyDCGAN(Radfordetal.,2015),butwithoutanybatchnormalization,layernormalization,orotherstabilizingadditions.AppendixBcontainsadditionalarchitectureandtrain-ingdetails.Weﬁrstinvestigatetheeffectofbatchsizeontrainingstabilityandsamplequality.AsshowninFigure3,trainingisnotverystablewhenthebatchsizeissmall(i.e.200).Asbatchsizeincreases,trainingbecomesmorestableandtheinceptionscoreofsamplesincreases.Unlikepreviousmeth-ods,ourobjective(theminibatchenergydistance,Section3)dependsonthechosenminibatchsize:Largerminibatchesaremorelikelytocovermanymodesofthedatadistribution,therebynotonlyyieldinglowervarianceestimatesbutalsomakingourdistancemetricmorediscriminative.ToreachthelargebatchsizesneededforoptimalperformancewemakeuseofmultiGPUtraining.Inthisworkweonlyuseupto8GPUsperexperiment,butweanticipatemoreGPUstobeusefulwhenusinglargermodels.6PublishedasaconferencepaperatICLR2018InFigure4wepresentthesamplesgeneratedbyourmodeltrainedwithabatchsizeof8000.Inaddition,wealsocomparewiththesamplequalityofotherstate-of-the-artGANmodelsinTable1.OT-GANachievesascoreof8.47±.12,outperformingallbaselinemodels.ToevaluatetheimportanceofusingoptimaltransportinOT-GAN,werepeatourCIFAR-10exper-imentwithrandommatchingofsamples.Ourminibatchenergydistanceobjectiveremainsvalidwhenwematchsamplesrandomlyratherthanusingoptimaltransport.Inthiscasetheminibatchenergydistancereducestotheregular(generalized)energydistance.WerepeatourCIFAR-10ex-perimentandtrainageneratorwiththesamearchitectureandhyperparametersasabove,butwithrandommatchingofsamplesinsteadofoptimaltransport.ThehighestresultingInceptionscoreachievedduringthetrainingprocessis4.64usingthisapproach,ascomparedto8.47withoptimaltransport.Figure5showsarandomsamplefromtheresultingmodel.MethodInceptionscoreRealData11.95±.12DCGAN6.16±.07ImprovedGAN6.86±.06DenoisingFM7.72±.13WGAN-GP7.86±.07OT-GAN8.47±.12Table1:InceptionscoresonCIFAR-10.Allthemodelsaretrainedinanunsupervisedmanner.01000200030004000Number of Trainig Epochs45678Inception ScoreBatch Size 200Batch Size 800Batch Size 3200Batch Size 8000Figure3:CIFAR-10inceptionscoreoverthecourseoftrainingfordifferentbatchsizes.Figure4:SamplesgeneratedbyOT-GANonCIFAR-10,withoutusinglabels.Figure5:Samplesgeneratedwithoutusingoptimaltransport.7PublishedasaconferencepaperatICLR20185.3IMAGENETDOGSToillustratetheabilityofOT-GANingeneratinghighqualityimagesonmorecomplexdatasets,wetrainOT-GANtogenerate128×128imagesonthedogsubsetofImageNet(Russakovskyetal.,2015).Asmallerbatchsizeof2048isusedduetoGPUmemorycontraints.AsshowninFigure6,thesamplesgeneratedbyOT-GANcontainlessnonsensicalimages,andthesamplequalityissig-niﬁcantlybetterthanthatofatunedDCGANvariantwhichstillsuffersfrommodecollapse.ThesuperiorimagequalityisconﬁrmedbytheinceptionscoreachievedbyOT-GAN(8.97±0.09)onthisdataset,whichoutperformsthatofDCGAN(8.19±0.11)Figure6:ImageNetDogsubsetsamplesgeneratedbyOT-GAN(left)andDCGAN(right).5.4CONDITIONALGENERATIONOFBIRDSTofurtherdemonstratetheeffectivenessoftheproposedmethodonconditionalimagesynthesis,wecompareOT-GANwithstate-of-the-artmodelsontext-to-imagegeneration(Reedetal.,2016b;a;Zhangetal.,2017).AsshowninTable2,theimagesgeneratedbyOT-GANwithbatchsize2048alsoachievethebestinceptionscorehere.ExampleimagesgeneratedbyourconditionalgenerativemodelontheCUBtestsetarepresentedinFigure7.MethodGAN-INT-CLSGAWWNStackGANOT-GANInceptionScore2.88±.043.62±.073.70±.043.84±.05Table2:Inceptionscoresbystate-of-the-artmethods(Reedetal.,2016b;a;Zhangetal.,2017)andtheproposedOT-GANontheCUBtestset.Higherinceptionscoresmeanbetterimagequality.Figure7:BirdexampleimagesgeneratedbyconditionalOT-GAN8PublishedasaconferencepaperatICLR20186DISCUSSIONWehavepresentedOT-GAN,anewvariantofGANswherethegeneratoristrainedtominimizeanoveldistancemetricoverprobabilitydistributions.Thismetric,whichwecallmini-batchen-ergydistance,combinesoptimaltransportinprimalformwithanenergydistancedeﬁnedinanadversariallylearnedfeaturespace,resultinginahighlydiscriminativedistancefunctionwithun-biasedmini-batchgradients.OT-GANwasshowntobeuniquelystablewhentrainedwithlargemini-batchesandtoachievestate-of-the-artresultsonseveralcommonbenchmarks.OnedownsideofOT-GAN,ascurrentlyproposed,isthatitrequireslargeamountsofcomputationandmemory.Weachievethebestresultswhenusingverylargemini-batches,whichincreasesthetimerequiredforeachupdateoftheparameters.Allexperimentsinthispaper,exceptforthemixtureofGaussianstoyexample,wereperformedusing8GPUsandtrainedforseveraldays.Infutureworkwehopetomakethemethodmorecomputationallyefﬁcient,aswellastoscaleupourapproachtomulti-machinetrainingtoenablegenerationofevenmorechallengingandhighresolutionimagedatasets.AuniquepropertyofOT-GANisthatthemini-batchenergydistanceremainsavalidtrainingobjec-tiveevenwhenwestoptrainingthecritic.OurimplementationofOT-GANupdatesthegenerativemodelmoreoftenthanthecritic,whereGANstypicallydothistheotherwayaround(seee.g.Gul-rajanietal.,2017).Asaresultwelearnarelativelystabletransportcostfunctionc(x,y),describinghow(dis)similartwoimagesare,aswellasanimageembeddingfunctionvη(x)capturingthege-ometryofthetrainingdata.Preliminaryexperimentssuggesttheselearnedfunctionscanbeusedsuccessfullyforunsupervisedlearningandotherapplications,whichweplantoinvestigatefurtherinfuturework.REFERENCESMartinArjovsky,SoumithChintala,andL´eonBottou.Wassersteingan.arXivpreprintarXiv:1701.07875,2017.MarcGBellemare,IvoDanihelka,WillDabney,ShakirMohamed,BalajiLakshminarayanan,StephanHoyer,andR´emiMunos.Thecramerdistanceasasolutiontobiasedwassersteingradients.arXivpreprintarXiv:1705.10743,2017.OlivierBousquet,SylvainGelly,IlyaTolstikhin,Carl-JohannSimon-Gabriel,andBernhardSchoelkopf.Fromoptimaltransporttogenerativemodeling:thevegancookbook.arXivpreprintarXiv:1705.07642,2017.MichaelCarter.Foundationsofmathematicaleconomics.MITPress,2001.MarcoCuturi.Sinkhorndistances:Lightspeedcomputationofoptimaltransport.InAdvancesinNeuralInformationProcessingSystems,pp.2292–2300,2013.YannNDauphin,AngelaFan,MichaelAuli,andDavidGrangier.Languagemodelingwithgatedconvolutionalnetworks.arXivpreprintarXiv:1612.08083,2016.AudeGenevay,GabrielPeyr´e,andMarcoCuturi.Ganandvaefromanoptimaltransportpointofview.arXivpreprintarXiv:1706.01807,2017a.AudeGenevay,GabrielPeyr´e,andMarcoCuturi.Sinkhorn-autodiff:Tractablewassersteinlearningofgenera-tivemodels.arXivpreprintarXiv:1706.00292,2017b.AudeGenevay,GabrielPeyr´e,andMarcoCuturi.Learninggenerativemodelswithsinkhorndivergences.AISTATSProceedings,2018.AidanNGomez,MengyeRen,RaquelUrtasun,andRogerBGrosse.Thereversibleresidualnetwork:Back-propagationwithoutstoringactivations.InAdvancesinNeuralInformationProcessingSystems,pp.2211–2221,2017.IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronCourville,andYoshuaBengio.Generativeadversarialnets.InAdvancesinneuralinformationprocess-ingsystems,pp.2672–2680,2014.IshaanGulrajani,FarukAhmed,MartinArjovsky,VincentDumoulin,andAaronCourville.Improvedtrainingofwassersteingans.arXivpreprintarXiv:1704.00028,2017.9PublishedasaconferencepaperatICLR2018DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.LevBorisovichKlebanov,ViktorBeneˇs,andIvanSaxl.N-distancesandtheirapplications.CharlesUniversityinPrague,theKarolinumPress,2005.AlexKrizhevsky.Learningmultiplelayersoffeaturesfromtinyimages.Technicalreport,2009.ChengtaoLi,DavidAlvarez-Melis,KeyuluXu,StefanieJegelka,andSuvritSra.Distributionaladversarialnetworks.arXiv:1706.09549,2017.LukeMetz,BenPoole,DavidPfau,andJaschaSohl-Dickstein.Unrolledgenerativeadversarialnetworks.InICLR,2017.AlecRadford,LukeMetz,andSoumithChintala.Unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434,2015.ScottReed,ZeynepAkata,SantoshMohan,SamuelTenka,BerntSchiele,andHonglakLee.Learningwhatandwheretodraw.InNIPS,2016a.ScottReed,ZeynepAkata,XinchenYan,LajanugenLogeswaran,BerntSchiele,andHonglakLee.Generativeadversarialtext-to-imagesynthesis.InICML,2016b.OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei.ImageNetLargeScaleVisualRecognitionChallenge.InternationalJournalofComputerVision(IJCV),115(3):211–252,2015.doi:10.1007/s11263-015-0816-y.TimSalimansandDiederikPKingma.Weightnormalization:Asimplereparameterizationtoacceleratetrain-ingofdeepneuralnetworks.InAdvancesinNeuralInformationProcessingSystems,pp.901–909,2016.TimSalimans,IanJ.Goodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.Improvedtechniquesfortraininggans.InNIPS,2016.DinoSejdinovic,BharathSriperumbudur,ArthurGretton,andKenjiFukumizu.Equivalenceofdistance-basedandrkhs-basedstatisticsinhypothesistesting.TheAnnalsofStatistics,pp.2263–2291,2013.WenlingShang,KihyukSohn,DiogoAlmeida,andHonglakLee.Understandingandimprovingconvolutionalneuralnetworksviaconcatenatedrectiﬁedlinearunits.InInternationalConferenceonMachineLearning,pp.2217–2225,2016.HanZhang,TaoXu,HongshengLi,ShaotingZhang,XiaogangWang,XiaoleiHuang,andDimitrisMetaxas.Stackgan:Texttophoto-realisticimagesynthesiswithstackedgenerativeadversarialnetworks.InICCV,2017.10PublishedasaconferencepaperatICLR2018ACONDITIONALGENERATIONAlgorithm2ConditionalOptimalTransportGAN(OT-GAN)trainingalgorithmwithstepsizeα,usingminibatchSGDforsimplicityRequire:ngen,thenumberofiterationsofthegeneratorpercriticiterationRequire:η0,initialcriticparameters.θ0,initialgeneratorparameters1:fort=1toNdo2:Sample(X,S),(X0,S0)twoindependentmini-batchesfromrealdata,withsideinformation,and(Y,S),(Y0,S0)twoindependentmini-batchesfromthegenerator,re-usingthesamesideinformation3:L=Wc[(X,S),(Y0,S0)]+Wc[(X0,S0),(Y,S)]−Wc[(X,S),(X0,S0)]−Wc[(Y,S),(Y0,S0)]4:iftmodngen+1=0then5:η←η+α·∇ηL6:else7:θ←θ−α·∇θL8:endif9:endforBCIFAR-10ARCHITECTUREANDTRAININGDETAILSThegeneratorandcriticareimplementedasconvolutionalnetworks.TheirarchitecturesarelooselybasedonDCGANwithvariousmodiﬁcations.Weightnormalizationanddata-dependentinitializa-tion(Salimans&Kingma,2016)areusedforboth.Thegeneratormapslatentcodessampledfroma100dimensionaluniformdistributionbetween-1and1to32×32colorimages.Themainmoduleofthegeneratorisa2x2nearest-neighborupsamplingoperationfollowedbyaconvolutionwitha5×5kernelusinggatedlinearunits(Dauphinetal.,2016).Themainmoduleofthecriticisacon-volutionwitha5×5kernelandstride2usingtheconcatenatedReLUactivationfunction(Shangetal.,2016).Notably,thegeneratorandcriticdonotuseanactivationnormalizationtechniquesuchasbatchorlayernormalization.WetrainthemodelusingAdamwithalearningrateof3×10−4,β1=0.5,β2=0.999.Weupdatethegenerator3timesforeverycriticupdate.OT-GANincludestwoadditionalhyperparametersfortheSinkhornalgorithm,thenumberofiterationstorunthealgo-rithmand1λwhichistheentropypenaltyofalignments.Initialtuningfoundavalueof500toworkwellforboth.operationactivationkernelstrideoutputshapez100linearGLU16384reshape1024×4×42xNNupsample1024×8×8convolutionGLU5×51512×8×82xNNupsample512×16×16convolutionGLU5×51256×16×162xNNupsample256×32×32convolutionGLU5×51128×32×32convolutiontanh5×513×32×32Table3:GeneratorarchitectureforCIFAR-10.11PublishedasaconferencepaperatICLR2018operationactivationkernelstrideoutputshapeconvolutionCReLU5×51256×32×32convolutionCReLU5×52512×16×16convolutionCReLU5×521024×8×8convolutionCReLU5×522048×4×4reshape32768l2normalize32768Table4:CriticarchitectureforCIFAR-10.CADVERSARIALLYLEARNINGTHETRANSPORTCOSTFUNCTIONToillustratetheimportanceoflearningthetransportcostfunctionadversarially,werepeatourCIFAR-10experimentusingcosinedistancedeﬁnedintheoriginalfeaturespace:c(x,y)=1−x·ykxk2kyk2,wherex,yareoriginalimagepixelvalues.Inthiscase,onlythetransportcostfunctionisaﬁxeddistancefunction,butalltherestexperimentsettingsarethesameasthoseofOT-GAN.Thehighestinceptionscoreduringthetrainingprocessis4.93,ascomparedto8.47whenlearningcostfunctionadversariallyusinganotherneuralnetwork.ThegeneratedsamplesareshowninFigure8.Figure8:CIFAR-10Samplesgeneratedwithoutadversariallylearningthecostfunction.12PublishedasaconferencepaperatICLR2018DMODELCOLLAPSEANDSAMPLEDIVERSITYTofurtherinvestigatesamplediversityandmodecollapseinGANs,wetrainthesamegeneratorus-ingDCGANandOT-GANontheImagenetdogdatasetforalargenumberofepochs.ForDCGANweobservemodecollapsestartingtooccurafterabout900epochs,asindicatedinﬁgure9.Themodeldoesnotrecoverfromthisifwecontinuetraining.WehaveobservedsimilarbehaviorformanyothertypesofGAN.ForOT-GANwecontinuedtotrainfor13000epochsonthisdatasetbutneverobservedanymodecollapseorreductioninsamplediversity.Figure9:ImagenetdogsamplesgeneratedwithDCGAN(left)after900epochsandOT-GAN(right)after13000epochs.Whentraininglongenough,DCGANsuffersfrommodecollapseasindicatedbythehighlightedsamples.WedidnotobserveanymodecollapseforOT-GAN,evenwhentrainingformanymoreepochs.13
7
1
0
2

g
u
A
8
2

]

G
L
.
s
c
[

2
v
7
4
3
6
0
.
7
0
7
1
:
v
i
X
r
a

ProximalPolicyOptimizationAlgorithmsJohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,OlegKlimovOpenAI{joschu,filip,prafulla,alec,oleg}@openai.comAbstractWeproposeanewfamilyofpolicygradientmethodsforreinforcementlearning,whichal-ternatebetweensamplingdatathroughinteractionwiththeenvironment,andoptimizinga“surrogate”objectivefunctionusingstochasticgradientascent.Whereasstandardpolicygra-dientmethodsperformonegradientupdateperdatasample,weproposeanovelobjectivefunctionthatenablesmultipleepochsofminibatchupdates.Thenewmethods,whichwecallproximalpolicyoptimization(PPO),havesomeofthebeneﬁtsoftrustregionpolicyoptimiza-tion(TRPO),buttheyaremuchsimplertoimplement,moregeneral,andhavebettersamplecomplexity(empirically).OurexperimentstestPPOonacollectionofbenchmarktasks,includ-ingsimulatedroboticlocomotionandAtarigameplaying,andweshowthatPPOoutperformsotheronlinepolicygradientmethods,andoverallstrikesafavorablebalancebetweensamplecomplexity,simplicity,andwall-time.1IntroductionInrecentyears,severaldiﬀerentapproacheshavebeenproposedforreinforcementlearningwithneuralnetworkfunctionapproximators.TheleadingcontendersaredeepQ-learning[Mni+15],“vanilla”policygradientmethods[Mni+16],andtrustregion/naturalpolicygradientmethods[Sch+15b].However,thereisroomforimprovementindevelopingamethodthatisscalable(tolargemodelsandparallelimplementations),dataeﬃcient,androbust(i.e.,successfulonavarietyofproblemswithouthyperparametertuning).Q-learning(withfunctionapproximation)failsonmanysimpleproblems1andispoorlyunderstood,vanillapolicygradientmethodshavepoordataeﬃencyandrobustness;andtrustregionpolicyoptimization(TRPO)isrelativelycomplicated,andisnotcompatiblewitharchitecturesthatincludenoise(suchasdropout)orparametersharing(betweenthepolicyandvaluefunction,orwithauxiliarytasks).ThispaperseekstoimprovethecurrentstateofaﬀairsbyintroducinganalgorithmthatattainsthedataeﬃciencyandreliableperformanceofTRPO,whileusingonlyﬁrst-orderoptimization.Weproposeanovelobjectivewithclippedprobabilityratios,whichformsapessimisticestimate(i.e.,lowerbound)oftheperformanceofthepolicy.Tooptimizepolicies,wealternatebetweensamplingdatafromthepolicyandperformingseveralepochsofoptimizationonthesampleddata.Ourexperimentscomparetheperformanceofvariousdiﬀerentversionsofthesurrogateobjec-tive,andﬁndthattheversionwiththeclippedprobabilityratiosperformsbest.WealsocomparePPOtoseveralpreviousalgorithmsfromtheliterature.Oncontinuouscontroltasks,itperformsbetterthanthealgorithmswecompareagainst.OnAtari,itperformssigniﬁcantlybetter(intermsofsamplecomplexity)thanA2CandsimilarlytoACERthoughitismuchsimpler.1WhileDQNworkswellongameenvironmentsliketheArcadeLearningEnvironment[Bel+15]withdiscreteactionspaces,ithasnotbeendemonstratedtoperformwelloncontinuouscontrolbenchmarkssuchasthoseinOpenAIGym[Bro+16]anddescribedbyDuanetal.[Dua+16].1 
 
 
 
 
 
2Background:PolicyOptimization2.1PolicyGradientMethodsPolicygradientmethodsworkbycomputinganestimatorofthepolicygradientandpluggingitintoastochasticgradientascentalgorithm.Themostcommonlyusedgradientestimatorhastheformˆg=ˆEth∇θlogπθ(at|st)ˆAti(1)whereπθisastochasticpolicyandˆAtisanestimatoroftheadvantagefunctionattimestept.Here,theexpectationˆEt[...]indicatestheempiricalaverageoveraﬁnitebatchofsamples,inanalgorithmthatalternatesbetweensamplingandoptimization.Implementationsthatuseautomaticdiﬀerentiationsoftwareworkbyconstructinganobjectivefunctionwhosegradientisthepolicygradientestimator;theestimatorˆgisobtainedbydiﬀerentiatingtheobjectiveLPG(θ)=ˆEthlogπθ(at|st)ˆAti.(2)WhileitisappealingtoperformmultiplestepsofoptimizationonthislossLPGusingthesametrajectory,doingsoisnotwell-justiﬁed,andempiricallyitoftenleadstodestructivelylargepolicyupdates(seeSection6.1;resultsarenotshownbutweresimilarorworsethanthe“noclippingorpenalty”setting).2.2TrustRegionMethodsInTRPO[Sch+15b],anobjectivefunction(the“surrogate”objective)ismaximizedsubjecttoaconstraintonthesizeofthepolicyupdate.Speciﬁcally,maximizeθˆEt(cid:20)πθ(at|st)πθold(at|st)ˆAt(cid:21)(3)subjecttoˆEt[KL[πθold(·|st),πθ(·|st)]]≤δ.(4)Here,θoldisthevectorofpolicyparametersbeforetheupdate.Thisproblemcaneﬃcientlybeapproximatelysolvedusingtheconjugategradientalgorithm,aftermakingalinearapproximationtotheobjectiveandaquadraticapproximationtotheconstraint.ThetheoryjustifyingTRPOactuallysuggestsusingapenaltyinsteadofaconstraint,i.e.,solvingtheunconstrainedoptimizationproblemmaximizeθˆEt(cid:20)πθ(at|st)πθold(at|st)ˆAt−βKL[πθold(·|st),πθ(·|st)](cid:21)(5)forsomecoeﬃcientβ.Thisfollowsfromthefactthatacertainsurrogateobjective(whichcomputesthemaxKLoverstatesinsteadofthemean)formsalowerbound(i.e.,apessimisticbound)ontheperformanceofthepolicyπ.TRPOusesahardconstraintratherthanapenaltybecauseitishardtochooseasinglevalueofβthatperformswellacrossdiﬀerentproblems—orevenwithinasingleproblem,wherethethecharacteristicschangeoverthecourseoflearning.Hence,toachieveourgoalofaﬁrst-orderalgorithmthatemulatesthemonotonicimprovementofTRPO,experimentsshowthatitisnotsuﬃcienttosimplychooseaﬁxedpenaltycoeﬃcientβandoptimizethepenalizedobjectiveEquation(5)withSGD;additionalmodiﬁcationsarerequired.23ClippedSurrogateObjectiveLetrt(θ)denotetheprobabilityratiort(θ)=πθ(at|st)πθold(at|st),sor(θold)=1.TRPOmaximizesa“surrogate”objectiveLCPI(θ)=ˆEt(cid:20)πθ(at|st)πθold(at|st)ˆAt(cid:21)=ˆEthrt(θ)ˆAti.(6)ThesuperscriptCPIreferstoconservativepolicyiteration[KL02],wherethisobjectivewaspro-posed.Withoutaconstraint,maximizationofLCPIwouldleadtoanexcessivelylargepolicyupdate;hence,wenowconsiderhowtomodifytheobjective,topenalizechangestothepolicythatmovert(θ)awayfrom1.Themainobjectiveweproposeisthefollowing:LCLIP(θ)=ˆEthmin(rt(θ)ˆAt,clip(rt(θ),1−(cid:15),1+(cid:15))ˆAt)i(7)whereepsilonisahyperparameter,say,(cid:15)=0.2.Themotivationforthisobjectiveisasfollows.TheﬁrstterminsidetheminisLCPI.Thesecondterm,clip(rt(θ),1−(cid:15),1+(cid:15))ˆAt,modiﬁesthesurrogateobjectivebyclippingtheprobabilityratio,whichremovestheincentiveformovingrtoutsideoftheinterval[1−(cid:15),1+(cid:15)].Finally,wetaketheminimumoftheclippedandunclippedobjective,sotheﬁnalobjectiveisalowerbound(i.e.,apessimisticbound)ontheunclippedobjective.Withthisscheme,weonlyignorethechangeinprobabilityratiowhenitwouldmaketheobjectiveimprove,andweincludeitwhenitmakestheobjectiveworse.NotethatLCLIP(θ)=LCPI(θ)toﬁrstorderaroundθold(i.e.,wherer=1),however,theybecomediﬀerentasθmovesawayfromθold.Figure1plotsasingleterm(i.e.,asinglet)inLCLIP;notethattheprobabilityratiorisclippedat1−(cid:15)or1+(cid:15)dependingonwhethertheadvantageispositiveornegative.rLCLIP011+(cid:15)A>0rLCLIP011−(cid:15)A<0Figure1:Plotsshowingoneterm(i.e.,asingletimestep)ofthesurrogatefunctionLCLIPasafunctionoftheprobabilityratior,forpositiveadvantages(left)andnegativeadvantages(right).Theredcircleoneachplotshowsthestartingpointfortheoptimization,i.e.,r=1.NotethatLCLIPsumsmanyoftheseterms.Figure2providesanothersourceofintuitionaboutthesurrogateobjectiveLCLIP.Itshowshowseveralobjectivesvaryasweinterpolatealongthepolicyupdatedirection,obtainedbyproximalpolicyoptimization(thealgorithmwewillintroduceshortly)onacontinuouscontrolproblem.WecanseethatLCLIPisalowerboundonLCPI,withapenaltyforhavingtoolargeofapolicyupdate.301Linear interpolation factor0.020.000.020.040.060.080.100.12Et[KLt]LCPI=Et[rtAt]Et[clip(rt,1,1+)At]LCLIP=Et[min(rtAt,clip(rt,1,1+)At)]Figure2:Surrogateobjectives,asweinterpolatebetweentheinitialpolicyparameterθold,andtheupdatedpolicyparameter,whichwecomputeafteroneiterationofPPO.TheupdatedpolicyhasaKLdivergenceofabout0.02fromtheinitialpolicy,andthisisthepointatwhichLCLIPismaximal.ThisplotcorrespondstotheﬁrstpolicyupdateontheHopper-v1problem,usinghyperparametersprovidedinSection6.1.4AdaptiveKLPenaltyCoeﬃcientAnotherapproach,whichcanbeusedasanalternativetotheclippedsurrogateobjective,orinadditiontoit,istouseapenaltyonKLdivergence,andtoadaptthepenaltycoeﬃcientsothatweachievesometargetvalueoftheKLdivergencedtargeachpolicyupdate.Inourexperiments,wefoundthattheKLpenaltyperformedworsethantheclippedsurrogateobjective,however,we’veincludeditherebecauseit’sanimportantbaseline.Inthesimplestinstantiationofthisalgorithm,weperformthefollowingstepsineachpolicyupdate:•UsingseveralepochsofminibatchSGD,optimizetheKL-penalizedobjectiveLKLPEN(θ)=ˆEt(cid:20)πθ(at|st)πθold(at|st)ˆAt−βKL[πθold(·|st),πθ(·|st)](cid:21)(8)•Computed=ˆEt[KL[πθold(·|st),πθ(·|st)]]–Ifd<dtarg/1.5,β←β/2–Ifd>dtarg×1.5,β←β×2Theupdatedβisusedforthenextpolicyupdate.Withthisscheme,weoccasionallyseepolicyupdateswheretheKLdivergenceissigniﬁcantlydiﬀerentfromdtarg,however,thesearerare,andβquicklyadjusts.Theparameters1.5and2abovearechosenheuristically,butthealgorithmisnotverysensitivetothem.Theinitialvalueofβisaanotherhyperparameterbutisnotimportantinpracticebecausethealgorithmquicklyadjustsit.5AlgorithmThesurrogatelossesfromtheprevioussectionscanbecomputedanddiﬀerentiatedwithaminorchangetoatypicalpolicygradientimplementation.Forimplementationsthatuseautomaticdif-ferentation,onesimplyconstructsthelossLCLIPorLKLPENinsteadofLPG,andoneperformsmultiplestepsofstochasticgradientascentonthisobjective.Mosttechniquesforcomputingvariance-reducedadvantage-functionestimatorsmakeusealearnedstate-valuefunctionV(s);forexample,generalizedadvantageestimation[Sch+15a],orthe4ﬁnite-horizonestimatorsin[Mni+16].Ifusinganeuralnetworkarchitecturethatsharesparametersbetweenthepolicyandvaluefunction,wemustusealossfunctionthatcombinesthepolicysurrogateandavaluefunctionerrorterm.Thisobjectivecanfurtherbeaugmentedbyaddinganentropybonustoensuresuﬃcientexploration,assuggestedinpastwork[Wil92;Mni+16].Combiningtheseterms,weobtainthefollowingobjective,whichis(approximately)maximizedeachiteration:LCLIP+VF+St(θ)=ˆEt(cid:2)LCLIPt(θ)−c1LVFt(θ)+c2S[πθ](st)(cid:3),(9)wherec1,c2arecoeﬃcients,andSdenotesanentropybonus,andLVFtisasquared-errorloss(Vθ(st)−Vtargt)2.Onestyleofpolicygradientimplementation,popularizedin[Mni+16]andwell-suitedforusewithrecurrentneuralnetworks,runsthepolicyforTtimesteps(whereTismuchlessthantheepisodelength),andusesthecollectedsamplesforanupdate.ThisstylerequiresanadvantageestimatorthatdoesnotlookbeyondtimestepT.Theestimatorusedby[Mni+16]isˆAt=−V(st)+rt+γrt+1+···+γT−t+1rT−1+γT−tV(sT)(10)wheretspeciﬁesthetimeindexin[0,T],withinagivenlength-Ttrajectorysegment.Generalizingthischoice,wecanuseatruncatedversionofgeneralizedadvantageestimation,whichreducestoEquation(10)whenλ=1:ˆAt=δt+(γλ)δt+1+···+···+(γλ)T−t+1δT−1,(11)whereδt=rt+γV(st+1)−V(st)(12)Aproximalpolicyoptimization(PPO)algorithmthatusesﬁxed-lengthtrajectorysegmentsisshownbelow.Eachiteration,eachofN(parallel)actorscollectTtimestepsofdata.ThenweconstructthesurrogatelossontheseNTtimestepsofdata,andoptimizeitwithminibatchSGD(orusuallyforbetterperformance,Adam[KB14]),forKepochs.Algorithm1PPO,Actor-CriticStyleforiteration=1,2,...doforactor=1,2,...,NdoRunpolicyπθoldinenvironmentforTtimestepsComputeadvantageestimatesˆA1,...,ˆATendforOptimizesurrogateLwrtθ,withKepochsandminibatchsizeM≤NTθold←θendfor6Experiments6.1ComparisonofSurrogateObjectivesFirst,wecompareseveraldiﬀerentsurrogateobjectivesunderdiﬀerenthyperparameters.Here,wecomparethesurrogateobjectiveLCLIPtoseveralnaturalvariationsandablatedversions.Noclippingorpenalty:Lt(θ)=rt(θ)ˆAtClipping:Lt(θ)=min(rt(θ)ˆAt,clip(rt(θ)),1−(cid:15),1+(cid:15))ˆAtKLpenalty(ﬁxedoradaptive)Lt(θ)=rt(θ)ˆAt−βKL[πθold,πθ]5FortheKLpenalty,onecaneitheruseaﬁxedpenaltycoeﬃcientβoranadaptivecoeﬃcientasdescribedinSection4usingtargetKLvaluedtarg.Notethatwealsotriedclippinginlogspace,butfoundtheperformancetobenobetter.Becausewearesearchingoverhyperparametersforeachalgorithmvariant,wechoseacompu-tationallycheapbenchmarktotestthealgorithmson.Namely,weused7simulatedroboticstasks2implementedinOpenAIGym[Bro+16],whichusetheMuJoCo[TET12]physicsengine.Wedoonemilliontimestepsoftrainingoneachone.Besidesthehyperparametersusedforclipping((cid:15))andtheKLpenalty(β,dtarg),whichwesearchover,theotherhyperparametersareprovidedininTable3.Torepresentthepolicy,weusedafully-connectedMLPwithtwohiddenlayersof64units,andtanhnonlinearities,outputtingthemeanofaGaussiandistribution,withvariablestandarddeviations,following[Sch+15b;Dua+16].Wedon’tshareparametersbetweenthepolicyandvaluefunction(socoeﬃcientc1isirrelevant),andwedon’tuseanentropybonus.Eachalgorithmwasrunonall7environments,with3randomseedsoneach.Wescoredeachrunofthealgorithmbycomputingtheaveragetotalrewardofthelast100episodes.Weshiftedandscaledthescoresforeachenvironmentsothattherandompolicygaveascoreof0andthebestresultwassetto1,andaveragedover21runstoproduceasinglescalarforeachalgorithmsetting.TheresultsareshowninTable1.Notethatthescoreisnegativeforthesettingwithoutclippingorpenalties,becauseforoneenvironment(halfcheetah)itleadstoaverynegativescore,whichisworsethantheinitialrandompolicy.algorithmavg.normalizedscoreNoclippingorpenalty-0.39Clipping,(cid:15)=0.10.76Clipping,(cid:15)=0.20.82Clipping,(cid:15)=0.30.70AdaptiveKLdtarg=0.0030.68AdaptiveKLdtarg=0.010.74AdaptiveKLdtarg=0.030.71FixedKL,β=0.30.62FixedKL,β=1.0.71FixedKL,β=3.0.72FixedKL,β=10.0.69Table1:Resultsfromcontinuouscontrolbenchmark.Averagenormalizedscores(over21runsofthealgorithm,on7environments)foreachalgorithm/hyperparametersetting.βwasinitializedat1.6.2ComparisontoOtherAlgorithmsintheContinuousDomainNext,wecomparePPO(withthe“clipped”surrogateobjectivefromSection3)toseveralothermethodsfromtheliterature,whichareconsideredtobeeﬀectiveforcontinuousproblems.Wecom-paredagainsttunedimplementationsofthefollowingalgorithms:trustregionpolicyoptimization[Sch+15b],cross-entropymethod(CEM)[SL06],vanillapolicygradientwithadaptivestepsize3,2HalfCheetah,Hopper,InvertedDoublePendulum,InvertedPendulum,Reacher,Swimmer,andWalker2d,all“-v1”3Aftereachbatchofdata,theAdamstepsizeisadjustedbasedontheKLdivergenceoftheoriginalandupdatedpolicy,usingarulesimilartotheoneshowninSection4.Animplementationisavailableathttps://github.com/berkeleydeeprlcourse/homework/tree/master/hw4.6A2C[Mni+16],A2Cwithtrustregion[Wan+16].A2Cstandsforadvantageactorcritic,andisasynchronousversionofA3C,whichwefoundtohavethesameorbetterperformancethantheasynchronousversion.ForPPO,weusedthehyperparametersfromtheprevioussection,with(cid:15)=0.2.WeseethatPPOoutperformsthepreviousmethodsonalmostallthecontinuouscontrolenvironments.010000005000500100015002000HalfCheetah-v10100000005001000150020002500Hopper-v10100000002000400060008000InvertedDoublePendulum-v10100000002004006008001000InvertedPendulum-v10100000012010080604020Reacher-v101000000020406080100120Swimmer-v1010000000100020003000Walker2d-v1A2CA2C + Trust RegionCEMPPO (Clip)Vanilla PG, AdaptiveTRPOFigure3:ComparisonofseveralalgorithmsonseveralMuJoCoenvironments,trainingforonemilliontimesteps.6.3ShowcaseintheContinuousDomain:HumanoidRunningandSteeringToshowcasetheperformanceofPPOonhigh-dimensionalcontinuouscontrolproblems,wetrainonasetofproblemsinvolvinga3Dhumanoid,wheretherobotmustrun,steer,andgetupoﬀtheground,possiblywhilebeingpeltedbycubes.Thethreetaskswetestonare(1)Ro-boschoolHumanoid:forwardlocomotiononly,(2)RoboschoolHumanoidFlagrun:positionoftargetisrandomlyvariedevery200timestepsorwheneverthegoalisreached,(3)RoboschoolHumanoid-FlagrunHarder,wheretherobotispeltedbycubesandneedstogetupoﬀtheground.SeeFigure5forstillframesofalearnedpolicy,andFigure4forlearningcurvesonthethreetasks.Hyperpa-rametersareprovidedinTable4.Inconcurrentwork,Heessetal.[Hee+17]usedtheadaptiveKLvariantofPPO(Section4)tolearnlocomotionpoliciesfor3Drobots.050MTimestep01000200030004000RoboschoolHumanoid-v00100MTimestep05001000150020002500RoboschoolHumanoidFlagrun-v00100MTimestep0100020003000RoboschoolHumanoidFlagrunHarder-v0Figure4:LearningcurvesfromPPOon3Dhumanoidcontroltasks,usingRoboschool.7Figure5:StillframesofthepolicylearnedfromRoboschoolHumanoidFlagrun.Intheﬁrstsixframes,therobotrunstowardsatarget.Thenthepositionisrandomlychanged,andtherobotturnsandrunstowardthenewtarget.6.4ComparisontoOtherAlgorithmsontheAtariDomainWealsoranPPOontheArcadeLearningEnvironment[Bel+15]benchmarkandcomparedagainstwell-tunedimplementationsofA2C[Mni+16]andACER[Wan+16].Forallthreealgorithms,weusedthesamepolicynetworkarchitechtureasusedin[Mni+16].ThehyperparametersforPPOareprovidedinTable5.Fortheothertwoalgorithms,weusedhyperparametersthatweretunedtomaximizeperformanceonthisbenchmark.Atableofresultsandlearningcurvesforall49gamesisprovidedinAppendixB.Weconsiderthefollowingtwoscoringmetrics:(1)averagerewardperepisodeoverentiretrainingperiod(whichfavorsfastlearning),and(2)averagerewardperepisodeoverlast100episodesoftraining(whichfavorsﬁnalperformance).Table2showsthenumberofgames“won”byeachalgorithm,wherewecomputethevictorbyaveragingthescoringmetricacrossthreetrials.A2CACERPPOTie(1)avg.episoderewardoveralloftraining118300(2)avg.episoderewardoverlast100episodes128191Table2:Numberofgames“won”byeachalgorithm,wherethescoringmetricisaveragedacrossthreetrials.7ConclusionWehaveintroducedproximalpolicyoptimization,afamilyofpolicyoptimizationmethodsthatusemultipleepochsofstochasticgradientascenttoperformeachpolicyupdate.Thesemethodshavethestabilityandreliabilityoftrust-regionmethodsbutaremuchsimplertoimplement,requiringonlyfewlinesofcodechangetoavanillapolicygradientimplementation,applicableinmoregeneralsettings(forexample,whenusingajointarchitectureforthepolicyandvaluefunction),andhavebetteroverallperformance.8AcknowledgementsThankstoRockyDuan,PeterChen,andothersatOpenAIforinsightfulcomments.8References[Bel+15]M.Bellemare,Y.Naddaf,J.Veness,andM.Bowling.“Thearcadelearningenviron-ment:Anevaluationplatformforgeneralagents”.In:Twenty-FourthInternationalJointConferenceonArtiﬁcialIntelligence.2015.[Bro+16]G.Brockman,V.Cheung,L.Pettersson,J.Schneider,J.Schulman,J.Tang,andW.Zaremba.“OpenAIGym”.In:arXivpreprintarXiv:1606.01540(2016).[Dua+16]Y.Duan,X.Chen,R.Houthooft,J.Schulman,andP.Abbeel.“BenchmarkingDeepReinforcementLearningforContinuousControl”.In:arXivpreprintarXiv:1604.06778(2016).[Hee+17]N.Heess,S.Sriram,J.Lemmon,J.Merel,G.Wayne,Y.Tassa,T.Erez,Z.Wang,A.Eslami,M.Riedmiller,etal.“EmergenceofLocomotionBehavioursinRichEnvi-ronments”.In:arXivpreprintarXiv:1707.02286(2017).[KL02]S.KakadeandJ.Langford.“Approximatelyoptimalapproximatereinforcementlearn-ing”.In:ICML.Vol.2.2002,pp.267–274.[KB14]D.KingmaandJ.Ba.“Adam:Amethodforstochasticoptimization”.In:arXivpreprintarXiv:1412.6980(2014).[Mni+15]V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,etal.“Human-levelcontrolthroughdeepreinforcementlearning”.In:Nature518.7540(2015),pp.529–533.[Mni+16]V.Mnih,A.P.Badia,M.Mirza,A.Graves,T.P.Lillicrap,T.Harley,D.Silver,andK.Kavukcuoglu.“Asynchronousmethodsfordeepreinforcementlearning”.In:arXivpreprintarXiv:1602.01783(2016).[Sch+15a]J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel.“High-dimensionalcontin-uouscontrolusinggeneralizedadvantageestimation”.In:arXivpreprintarXiv:1506.02438(2015).[Sch+15b]J.Schulman,S.Levine,P.Moritz,M.I.Jordan,andP.Abbeel.“Trustregionpolicyoptimization”.In:CoRR,abs/1502.05477(2015).[SL06]I.SzitaandA.L¨orincz.“LearningTetrisusingthenoisycross-entropymethod”.In:Neuralcomputation18.12(2006),pp.2936–2941.[TET12]E.Todorov,T.Erez,andY.Tassa.“MuJoCo:Aphysicsengineformodel-basedcon-trol”.In:IntelligentRobotsandSystems(IROS),2012IEEE/RSJInternationalCon-ferenceon.IEEE.2012,pp.5026–5033.[Wan+16]Z.Wang,V.Bapst,N.Heess,V.Mnih,R.Munos,K.Kavukcuoglu,andN.deFreitas.“SampleEﬃcientActor-CriticwithExperienceReplay”.In:arXivpreprintarXiv:1611.01224(2016).[Wil92]R.J.Williams.“Simplestatisticalgradient-followingalgorithmsforconnectionistre-inforcementlearning”.In:Machinelearning8.3-4(1992),pp.229–256.9AHyperparametersHyperparameterValueHorizon(T)2048Adamstepsize3×10−4Num.epochs10Minibatchsize64Discount(γ)0.99GAEparameter(λ)0.95Table3:PPOhyperparametersusedfortheMujoco1milliontimestepbenchmark.HyperparameterValueHorizon(T)512Adamstepsize∗Num.epochs15Minibatchsize4096Discount(γ)0.99GAEparameter(λ)0.95Numberofactors32(locomotion),128(ﬂagrun)Logstdev.ofactiondistributionLinearAnneal(−0.7,−1.6)Table4:PPOhyperparametersusedfortheRoboschoolexperiments.AdamstepsizewasadjustedbasedonthetargetvalueoftheKLdivergence.HyperparameterValueHorizon(T)128Adamstepsize2.5×10−4×αNum.epochs3Minibatchsize32×8Discount(γ)0.99GAEparameter(λ)0.95Numberofactors8Clippingparameter(cid:15)0.1×αVFcoeﬀ.c1(9)1Entropycoeﬀ.c2(9)0.01Table5:PPOhyperparametersusedinAtariexperiments.αislinearlyannealedfrom1to0overthecourseoflearning.BPerformanceonMoreAtariGamesHereweincludeacomparisonofPPOagainstA2Conalargercollectionof49Atarigames.Figure6showsthelearningcurvesofeachofthreerandomseeds,whileTable6showsthemeanperformance.1010002000Alien0250500750Amidar020004000Assault0250050007500Asterix150020002500Asteroids0100000020000003000000Atlantis05001000BankHeist5000100001500020000BattleZone1000200030004000BeamRider304050Bowling050100Boxing0200400Breakout500010000Centipede200040006000ChopperCommand50000100000CrazyClimber02000040000DemonAttack17.515.012.510.0DoubleDunk0250500750Enduro100500FishingDerby0102030Freeway100200300Frostbite02000040000Gopher250500750Gravitar10864IceHockey0200400600Jamesbond0500010000Kangaroo2000400060008000Krull02000040000KungFuMaster050100MontezumaRevenge100020003000MsPacman25005000750010000NameThisGame1000Pitfall20020Pong0500PrivateEye050001000015000Qbert25005000750010000Riverraid02000040000RoadRunner246Robotank050010001500Seaquest5001000SpaceInvaders02000040000StarGunner201510Tennis30004000TimePilot0100200300Tutankham0100000200000UpNDown040MFrames0510Venture040MFrames50000100000150000VideoPinball040MFrames20004000WizardOfWor040MFrames0200040006000ZaxxonA2CACERPPOFigure6:ComparisonofPPOandA2Conall49ATARIgamesincludedinOpenAIGymatthetimeofpublication.11A2CACERPPOAlien1141.71655.41850.3Amidar380.8827.6674.6Assault1562.94653.84971.9Asterix3176.36801.24532.5Asteroids1653.32389.32097.5Atlantis729265.31841376.02311815.0BankHeist1095.31177.51280.6BattleZone3080.08983.317366.7BeamRider3031.73863.31590.0Bowling30.133.340.1Boxing17.798.994.6Breakout303.0456.4274.8Centipede3496.58904.84386.4ChopperCommand1171.75287.73516.3CrazyClimber107770.0132461.0110202.0DemonAttack6639.138808.311378.4DoubleDunk-16.2-13.2-14.9Enduro0.00.0758.3FishingDerby20.634.717.8Freeway0.00.032.5Frostbite261.8285.6314.2Gopher1500.937802.32932.9Gravitar194.0225.3737.2IceHockey-6.4-5.9-4.2Jamesbond52.3261.8560.7Kangaroo45.350.09928.7Krull8367.47268.47942.3KungFuMaster24900.327599.323310.3MontezumaRevenge0.00.342.0MsPacman1626.92718.52096.5NameThisGame5961.28488.06254.9Pitfall-55.0-16.9-32.9Pong19.720.720.7PrivateEye91.3182.069.5Qbert10065.715316.614293.3Riverraid7653.59125.18393.6RoadRunner32810.035466.025076.0Robotank2.22.55.5Seaquest1714.31739.51204.5SpaceInvaders744.51213.9942.5StarGunner26204.049817.732689.0Tennis-22.2-17.6-14.8TimePilot2898.04175.74342.0Tutankham206.8280.8254.4UpNDown17369.8145051.495445.0Venture0.00.00.0VideoPinball19735.9156225.637389.0WizardOfWor859.02308.34185.3Zaxxon16.329.05008.7Table6:Meanﬁnalscores(last100episodes)ofPPOandA2ConAtarigamesafter40Mgameframes(10Mtimesteps).12
8
1
0
2

b
e
F
3
2

]

G
L
.
s
c
[

3
v
5
9
4
1
0
.
7
0
7
1
:
v
i
X
r
a

HindsightExperienceReplayMarcinAndrychowicz∗,FilipWolski,AlexRay,JonasSchneider,RachelFong,PeterWelinder,BobMcGrew,JoshTobin,PieterAbbeel†,WojciechZaremba†OpenAIAbstractDealingwithsparserewardsisoneofthebiggestchallengesinReinforcementLearning(RL).WepresentanoveltechniquecalledHindsightExperienceReplaywhichallowssample-efﬁcientlearningfromrewardswhicharesparseandbinaryandthereforeavoidtheneedforcomplicatedrewardengineering.Itcanbecom-binedwithanarbitraryoff-policyRLalgorithmandmaybeseenasaformofimplicitcurriculum.Wedemonstrateourapproachonthetaskofmanipulatingobjectswitharoboticarm.Inparticular,werunexperimentsonthreedifferenttasks:pushing,sliding,andpick-and-place,ineachcaseusingonlybinaryrewardsindicatingwhetherornotthetaskiscompleted.OurablationstudiesshowthatHindsightExperienceReplayisacrucialingredientwhichmakestrainingpossibleinthesechallengingenvironments.Weshowthatourpoliciestrainedonaphysicssimulationcanbedeployedonaphysicalrobotandsuccessfullycompletethetask.Thevideopresentingourexperimentsisavailableathttps://goo.gl/SMrQnI.1IntroductionReinforcementlearning(RL)combinedwithneuralnetworkshasrecentlyledtoawiderangeofsuccessesinlearningpoliciesforsequentialdecision-makingproblems.Thisincludessimulatedenvironments,suchasplayingAtarigames(Mnihetal.,2015),anddefeatingthebesthumanplayeratthegameofGo(Silveretal.,2016),aswellasrobotictaskssuchashelicoptercontrol(Ngetal.,2006),hittingabaseball(PetersandSchaal,2008),screwingacapontoabottle(Levineetal.,2015),ordooropening(Chebotaretal.,2016).However,acommonchallenge,especiallyforrobotics,istheneedtoengineerarewardfunctionthatnotonlyreﬂectsthetaskathandbutisalsocarefullyshaped(Ngetal.,1999)toguidethepolicyoptimization.Forexample,Popovetal.(2017)useacostfunctionconsistingofﬁverelativelycomplicatedtermswhichneedtobecarefullyweightedinordertotrainapolicyforstackingabrickontopofanotherone.ThenecessityofcostengineeringlimitstheapplicabilityofRLintherealworldbecauseitrequiresbothRLexpertiseanddomain-speciﬁcknowledge.Moreover,itisnotapplicableinsituationswherewedonotknowwhatadmissiblebehaviourmaylooklike.Itisthereforeofgreatpracticalrelevancetodevelopalgorithmswhichcanlearnfromunshapedrewardsignals,e.g.abinarysignalindicatingsuccessfultaskcompletion.Oneabilityhumanshave,unlikethecurrentgenerationofmodel-freeRLalgorithms,istolearnalmostasmuchfromachievinganundesiredoutcomeasfromthedesiredone.Imaginethatyouarelearninghowtoplayhockeyandaretryingtoshootapuckintoanet.Youhitthepuckbutitmissesthenetontherightside.TheconclusiondrawnbyastandardRLalgorithminsuchasituationwould∗marcin@openai.com†Equaladvising.31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. 
 
 
 
 
 
bethattheperformedsequenceofactionsdoesnotleadtoasuccessfulshot,andlittle(ifanything)wouldbelearned.Itishoweverpossibletodrawanotherconclusion,namelythatthissequenceofactionswouldbesuccessfulifthenethadbeenplacedfurthertotheright.InthispaperweintroduceatechniquecalledHindsightExperienceReplay(HER)whichallowsthealgorithmtoperformexactlythiskindofreasoningandcanbecombinedwithanyoff-policyRLalgorithm.Itisapplicablewhenevertherearemultiplegoalswhichcanbeachieved,e.g.achievingeachstateofthesystemmaybetreatedasaseparategoal.NotonlydoesHERimprovethesampleefﬁciencyinthissetting,butmoreimportantly,itmakeslearningpossibleeveniftherewardsignalissparseandbinary.Ourapproachisbasedontraininguniversalpolicies(Schauletal.,2015a)whichtakeasinputnotonlythecurrentstate,butalsoagoalstate.ThepivotalideabehindHERistoreplayeachepisodewithadifferentgoalthantheonetheagentwastryingtoachieve,e.g.oneofthegoalswhichwasachievedintheepisode.2BackgroundInthissectionweintroducereinforcementlearningformalismusedinthepaperaswellasRLalgorithmsweuseinourexperiments.2.1ReinforcementLearningWeconsiderthestandardreinforcementlearningformalismconsistingofanagentinteractingwithanenvironment.Tosimplifytheexpositionweassumethattheenvironmentisfullyobservable.AnenvironmentisdescribedbyasetofstatesS,asetofactionsA,adistributionofinitialstatesp(s0),arewardfunctionr:S×A→R,transitionprobabilitiesp(st+1|st,at),andadiscountfactorγ∈[0,1].Adeterministicpolicyisamappingfromstatestoactions:π:S→A.Everyepisodestartswithsamplinganinitialstates0.Ateverytimestepttheagentproducesanactionbasedonthecurrentstate:at=π(st).Thenitgetstherewardrt=r(st,at)andtheenvironment’snewstateissampledfromthedistributionp(·|st,at).Adiscountedsumoffuturerewardsiscalledareturn:Rt=P∞i=tγi−tri.Theagent’sgoalistomaximizeitsexpectedreturnEs0[R0|s0].TheQ-functionoraction-valuefunctionisdeﬁnedasQπ(st,at)=E[Rt|st,at].Letπ∗denoteanoptimalpolicyi.e.anypolicyπ∗s.t.Qπ∗(s,a)≥Qπ(s,a)foreverys∈S,a∈Aandanypolicyπ.AlloptimalpolicieshavethesameQ-functionwhichiscalledoptimalQ-functionanddenotedQ∗.ItiseasytoshowthatitsatisﬁesthefollowingequationcalledtheBellmanequation:Q∗(s,a)=Es0∼p(·|s,a)(cid:20)r(s,a)+γmaxa0∈AQ∗(s0,a0)(cid:21).2.2DeepQ-Networks(DQN)DeepQ-Networks(DQN)(Mnihetal.,2015)isamodel-freeRLalgorithmfordiscreteactionspaces.Herewesketchitonlyinformally,seeMnihetal.(2015)formoredetails.InDQNwemaintainaneuralnetworkQwhichapproximatesQ∗.Agreedypolicyw.r.t.QisdeﬁnedasπQ(s)=argmaxa∈AQ(s,a).An(cid:15)-greedypolicyw.r.t.Qisapolicywhichwithprobability(cid:15)takesarandomaction(sampleduniformlyfromA)andtakestheactionπQ(s)withprobability1−(cid:15).Duringtrainingwegenerateepisodesusing(cid:15)-greedypolicyw.r.t.thecurrentapproximationoftheaction-valuefunctionQ.Thetransitiontuples(st,at,rt,st+1)encounteredduringtrainingarestoredintheso-calledreplaybuffer.Thegenerationofnewepisodesisinterleavedwithneuralnetworktraining.Thenetworkistrainedusingmini-batchgradientdescentonthelossLwhichencouragestheapproximatedQ-functiontosatisfytheBellmanequation:L=E(Q(st,at)−yt)2,whereyt=rt+γmaxa0∈AQ(st+1,a0)andthetuples(st,at,rt,st+1)aresampledfromthereplaybuffer1.Inordertomakethisoptimizationproceduremorestablethetargetsytareusuallycomputedusingaseparatetargetnetworkwhichchangesataslowerpacethanthemainnetwork.Acommonpractice1Thetargetsytdependonthenetworkparametersbutthisdependencyisignoredduringbackpropagation.2istoperiodicallysettheweightsofthetargetnetworktothecurrentweightsofthemainnetwork(e.g.Mnihetal.(2015))ortouseapolyak-averaged2(PolyakandJuditsky,1992)versionofthemainnetworkinstead(Lillicrapetal.,2015).2.3DeepDeterministicPolicyGradients(DDPG)DeepDeterministicPolicyGradients(DDPG)(Lillicrapetal.,2015)isamodel-freeRLalgorithmforcontinuousactionspaces.Herewesketchitonlyinformally,seeLillicrapetal.(2015)formoredetails.InDDPGwemaintaintwoneuralnetworks:atargetpolicy(alsocalledanactor)π:S→Aandanaction-valuefunctionapproximator(calledthecritic)Q:S×A→R.Thecritic’sjobistoapproximatetheactor’saction-valuefunctionQπ.Episodesaregeneratedusingabehavioralpolicywhichisanoisyversionofthetargetpolicy,e.g.πb(s)=π(s)+N(0,1).ThecriticistrainedinasimilarwayastheQ-functioninDQNbutthetargetsytarecomputedusingactionsoutputtedbytheactor,i.e.yt=rt+γQ(st+1,π(st+1)).Theactoristrainedwithmini-batchgradientdescentonthelossLa=−EsQ(s,π(s)),wheresissampledfromthereplaybuffer.ThegradientofLaw.r.t.actorparameterscanbecomputedbybackpropagationthroughthecombinedcriticandactornetworks.2.4UniversalValueFunctionApproximators(UVFA)UniversalValueFunctionApproximators(UVFA)(Schauletal.,2015a)isanextensionofDQNtothesetupwherethereismorethanonegoalwemaytrytoachieve.LetGbethespaceofpossiblegoals.Everygoalg∈Gcorrespondstosomerewardfunctionrg:S×A→R.Everyepisodestartswithsamplingastate-goalpairfromsomedistributionp(s0,g).Thegoalstaysﬁxedforthewholeepisode.Ateverytimesteptheagentgetsasinputnotonlythecurrentstatebutalsothecurrentgoalπ:S×G→Aandgetstherewardrt=rg(st,at).TheQ-functionnowdependsnotonlyonastate-actionpairbutalsoonagoalQπ(st,at,g)=E[Rt|st,at,g].Schauletal.(2015a)showthatinthissetupitispossibletotrainanapproximatortotheQ-functionusingdirectbootstrappingfromtheBellmanequation(justlikeincaseofDQN)andthatagreedypolicyderivedfromitcangeneralizetopreviouslyunseenstate-actionpairs.TheextensionofthisapproachtoDDPGisstraightforward.3HindsightExperienceReplay3.1AmotivatingexampleConsiderabit-ﬂippingenvironmentwiththestatespaceS={0,1}nandtheactionspaceA={0,1,...,n−1}forsomeintegerninwhichexecutingthei-thactionﬂipsthei-thbitofthestate.Foreveryepisodewesampleuniformlyaninitialstateaswellasatargetstateandthepolicygetsarewardof−1aslongasitisnotinthetargetstate,i.e.rg(s,a)=−[s6=g].Figure1:Bit-ﬂippingexperi-ment.01020304050number of bits n0.00.20.40.60.81.0success rateDQNDQN+HERStandardRLalgorithmsareboundtofailinthisenvironmentforn>40becausetheywillneverexperienceanyrewardotherthan−1.Noticethatusingtechniquesforimprovingexploration(e.g.VIME(Houthooftetal.,2016),count-basedexploration(Ostrovskietal.,2017)orbootstrappedDQN(Osbandetal.,2016))doesnothelpherebecausetherealproblemisnotinlackofdiversityofstatesbeingvisited,ratheritissimplyimpracticaltoexploresuchalargestatespace.Thestandardsolutiontothisproblemwouldbetouseashapedrewardfunctionwhichismoreinformativeandguidestheagenttowardsthegoal,e.g.rg(s,a)=−||s−g||2.Whileusingashapedrewardsolvestheprobleminourtoyenvironment,itmaybedifﬁculttoapplytomorecomplicatedproblems.WeinvestigatetheresultsofrewardshapingexperimentallyinSec.4.4.Insteadofshapingtherewardweproposeadifferentsolutionwhichdoesnotrequireanydomainknowledge.Consideranepisodewith2Apolyak-averagedversionofaparametricmodelMwhichisbeingtrainedisamodelwhoseparametersarecomputedasanexponentialmovingaverageoftheparametersofMovertime.3astatesequences1,...,sTandagoalg6=s1,...,sTwhichimpliesthattheagentreceivedarewardof−1ateverytimestep.Thepivotalideabehindourapproachistore-examinethistrajectorywithadifferentgoal—whilethistrajectorymaynothelpuslearnhowtoachievethestateg,itdeﬁnitelytellsussomethingabouthowtoachievethestatesT.Thisinformationcanbeharvestedbyusinganoff-policyRLalgorithmandexperiencereplaywherewereplaceginthereplaybufferbysT.Inadditionwecanstillreplaywiththeoriginalgoalgleftintactinthereplaybuffer.Withthismodiﬁcationatleasthalfofthereplayedtrajectoriescontainrewardsdifferentfrom−1andlearningbecomesmuchsimpler.Fig.1comparestheﬁnalperformanceofDQNwithandwithoutthisadditionalreplaytechniquewhichwecallHindsightExperienceReplay(HER).DQNwithoutHERcanonlysolvethetaskforn≤13whileDQNwithHEReasilysolvesthetaskfornupto50.SeeAppendixAforthedetailsoftheexperimentalsetup.Notethatthisapproachcombinedwithpowerfulfunctionapproximators(e.g.,deepneuralnetworks)allowstheagenttolearnhowtoachievethegoalgevenifithasneverobserveditduringtraining.Wemoreformallydescribeourapproachinthefollowingsections.3.2Multi-goalRLWeareinterestedintrainingagentswhichlearntoachievemultipledifferentgoals.WefollowtheapproachfromUniversalValueFunctionApproximators(Schauletal.,2015a),i.e.wetrainpoliciesandvaluefunctionswhichtakeasinputnotonlyastates∈Sbutalsoagoalg∈G.Moreover,weshowthattraininganagenttoperformmultipletaskscanbeeasierthantrainingittoperformonlyonetask(seeSec.4.3fordetails)andthereforeourapproachmaybeapplicableevenifthereisonlyonetaskwewouldliketheagenttoperform(asimilarsituationwasrecentlyobservedbyPintoandGupta(2016)).Weassumethateverygoalg∈Gcorrespondstosomepredicatefg:S→{0,1}andthattheagent’sgoalistoachieveanystatesthatsatisﬁesfg(s)=1.InthecasewhenwewanttoexactlyspecifythedesiredstateofthesystemwemayuseS=Gandfg(s)=[s=g].Thegoalscanalsospecifyonlysomepropertiesofthestate,e.g.supposethatS=R2andwewanttobeabletoachieveanarbitrarystatewiththegivenvalueofxcoordinate.InthiscaseG=Randfg((x,y))=[x=g].Moreover,weassumethatgivenastateswecaneasilyﬁndagoalgwhichissatisﬁedinthisstate.Moreformally,weassumethatthereisgivenamappingm:S→Gs.t.∀s∈Sfm(s)(s)=1.Noticethatthisassumptionisnotveryrestrictiveandcanusuallybesatisﬁed.Inthecasewhereeachgoalcorrespondstoastatewewanttoachieve,i.e.G=Sandfg(s)=[s=g],themappingmisjustanidentity.Forthecaseof2-dimensionalstateand1-dimensionalgoalsfromthepreviousparagraphthismappingisalsoverysimplem((x,y))=x.AuniversalpolicycanbetrainedusinganarbitraryRLalgorithmbysamplinggoalsandinitialstatesfromsomedistributions,runningtheagentforsomenumberoftimestepsandgivingitanegativerewardateverytimestepwhenthegoalisnotachieved,i.e.rg(s,a)=−[fg(s)=0].Thisdoesnothoweverworkverywellinpracticebecausethisrewardfunctionissparseandnotveryinformative.InordertosolvethisproblemweintroducethetechniqueofHindsightExperienceReplaywhichisthecruxofourapproach.3.3AlgorithmTheideabehindHindsightExperienceReplay(HER)isverysimple:afterexperiencingsomeepisodes0,s1,...,sTwestoreinthereplaybuffereverytransitionst→st+1notonlywiththeoriginalgoalusedforthisepisodebutalsowithasubsetofothergoals.Noticethatthegoalbeingpursuedinﬂuencestheagent’sactionsbutnottheenvironmentdynamicsandthereforewecanreplayeachtrajectorywithanarbitrarygoalassumingthatweuseanoff-policyRLalgorithmlikeDQN(Mnihetal.,2015),DDPG(Lillicrapetal.,2015),NAF(Guetal.,2016)orSDQN(Metzetal.,2017).OnechoicewhichhastobemadeinordertouseHERisthesetofadditionalgoalsusedforreplay.Inthesimplestversionofouralgorithmwereplayeachtrajectorywiththegoalm(sT),i.e.thegoalwhichisachievedintheﬁnalstateoftheepisode.WeexperimentallycomparedifferenttypesandquantitiesofadditionalgoalsforreplayinSec.4.5.Inallcaseswealsoreplayeachtrajectorywiththeoriginalgoalpursuedintheepisode.SeeAlg.1foramoreformaldescriptionofthealgorithm.4Algorithm1HindsightExperienceReplay(HER)Given:•anoff-policyRLalgorithmA,.e.g.DQN,DDPG,NAF,SDQN•astrategySforsamplinggoalsforreplay,.e.g.S(s0,...,sT)=m(sT)•arewardfunctionr:S×A×G→R..e.g.r(s,a,g)=−[fg(s)=0]InitializeA.e.g.initializeneuralnetworksInitializereplaybufferRforepisode=1,MdoSampleagoalgandaninitialstates0.fort=0,T−1doSampleanactionatusingthebehavioralpolicyfromA:at←πb(st||g).||denotesconcatenationExecutetheactionatandobserveanewstatest+1endforfort=0,T−1dort:=r(st,at,g)Storethetransition(st||g,at,rt,st+1||g)inR.standardexperiencereplaySampleasetofadditionalgoalsforreplayG:=S(currentepisode)forg0∈Gdor0:=r(st,at,g0)Storethetransition(st||g0,at,r0,st+1||g0)inR.HERendforendforfort=1,NdoSampleaminibatchBfromthereplaybufferRPerformonestepofoptimizationusingAandminibatchBendforendforHERmaybeseenasaformofimplicitcurriculumasthegoalsusedforreplaynaturallyshiftfromoneswhicharesimpletoachieveevenbyarandomagenttomoredifﬁcultones.However,incontrasttoexplicitcurriculum,HERdoesnotrequirehavinganycontroloverthedistributionofinitialenvironmentstates.NotonlydoesHERlearnwithextremelysparserewards,inourexperimentsitalsoperformsbetterwithsparserewardsthanwithshapedones(SeeSec.4.4).Theseresultsareindicativeofthepracticalchallengeswithrewardshaping,andthatshapedrewardswouldoftenconstituteacompromiseonthemetricwetrulycareabout(suchasbinarysuccess/failure).4ExperimentsThevideopresentingourexperimentsisavailableathttps://goo.gl/SMrQnI.Thissectionisorganizedasfollows.InSec.4.1weintroducemulti-goalRLenvironmentsweusefortheexperimentsaswellasourtrainingprocedure.InSec.4.2wecomparetheperformanceofDDPGwithandwithoutHER.InSec.4.3wecheckifHERimprovesperformanceinthesingle-goalsetup.InSec.4.4weanalyzetheeffectsofusingshapedrewardfunctions.InSec.4.5wecomparedifferentstrategiesforsamplingadditionalgoalsforHER.InSec.4.6weshowtheresultsoftheexperimentsonthephysicalrobot.4.1EnvironmentsThearenostandardenvironmentsformulti-goalRLandthereforewecreatedourownenvironments.Wedecidedtousemanipulationenvironmentsbasedonanexistinghardwarerobottoensurethatthechallengeswefacecorrespondascloselyaspossibletotherealworld.Inallexperimentsweusea7-DOFFetchRoboticsarmwhichhasatwo-ﬁngeredparallelgripper.TherobotissimulatedusingtheMuJoCo(Todorovetal.,2012)physicsengine.ThewholetrainingprocedureisperformedinthesimulationbutweshowinSec.4.6thatthetrainedpoliciesperformwellonthephysicalrobotwithoutanyﬁnetuning.5Figure2:Differenttasks:pushing(toprow),sliding(middlerow)andpick-and-place(bottomrow).Theredballdenotesthegoalposition.PoliciesarerepresentedasMulti-LayerPerceptrons(MLPs)withRectiﬁedLinearUnit(ReLU)activationfunctions.TrainingisperformedusingtheDDPGalgorithm(Lillicrapetal.,2015)withAdam(KingmaandBa,2014)astheoptimizer.Forimprovedefﬁciencyweuse8workerswhichaveragetheparametersaftereveryupdate.SeeAppendixAformoredetailsandthevaluesofallhyperparameters.Weconsider3differenttasks:1.Pushing.Inthistaskaboxisplacedonatableinfrontoftherobotandthetaskistomoveittothetargetlocationonthetable.Therobotﬁngersarelockedtopreventgrasping.Thelearnedbehaviourisamixtureofpushingandrolling.2.Sliding.Inthistaskapuckisplacedonalongslipperytableandthetargetpositionisoutsideoftherobot’sreachsothatithastohitthepuckwithsuchaforcethatitslidesandthenstopsintheappropriateplaceduetofriction.3.Pick-and-place.Thistaskissimilartopushingbutthetargetpositionisintheairandtheﬁngersarenotlocked.Tomakeexplorationinthistaskeasierwerecordedasinglestateinwhichtheboxisgraspedandstarthalfofthetrainingepisodesfromthisstate3.States:ThestateofthesystemisrepresentedintheMuJoCophysicsengineandconsistsofanglesandvelocitiesofallrobotjointsaswellaspositions,rotationsandvelocities(linearandangular)ofallobjects.Goals:Goalsdescribethedesiredpositionoftheobject(aboxorapuckdependingonthetask)withsomeﬁxedtoleranceof(cid:15)i.e.G=R3andfg(s)=[|g−sobject|≤(cid:15)],wheresobjectisthepositionoftheobjectinthestates.ThemappingfromstatestogoalsusedinHERissimplym(s)=sobject.Rewards:Unlessstatedotherwiseweusebinaryandsparserewardsr(s,a,g)=−[fg(s0)=0]wheres0ifthestateaftertheexecutionoftheactionainthestates.WecomparesparseandshapedrewardfunctionsinSec.4.4.State-goaldistributions:Foralltaskstheinitialpositionofthegripperisﬁxed,whiletheinitialpositionoftheobjectandthetargetarerandomized.SeeAppendixAfordetails.3Thiswasnecessarybecausewecouldnotsuccessfullytrainanypoliciesforthistaskwithoutusingthedemonstrationstate.Wehavelaterdiscoveredthattrainingispossiblewithoutthistrickifonlythegoalpositionissometimesonthetableandsometimesintheair.6Observations:Inthisparagraphrelativemeansrelativetothecurrentgripperposition.Thepolicyisgivenasinputtheabsolutepositionofthegripper,therelativepositionoftheobjectandthetarget4,aswellasthedistancebetweentheﬁngers.TheQ-functionisadditionallygiventhelinearvelocityofthegripperandﬁngersaswellasrelativelinearandangularvelocityoftheobject.Wedecidedtorestricttheinputtothepolicyinordertomakedeploymentonthephysicalroboteasier.Actions:Noneoftheproblemsweconsiderrequiregripperrotationandthereforewekeepitﬁxed.Actionspaceis4-dimensional.Threedimensionsspecifythedesiredrelativegripperpositionatthenexttimestep.WeuseMuJoCoconstraintstomovethegrippertowardsthedesiredpositionbutJacobian-basedcontrolcouldbeusedinstead5.Thelastdimensionspeciﬁesthedesireddistancebetweenthe2ﬁngerswhicharepositioncontrolled.StrategySforsamplinggoalsforreplay:UnlessstatedotherwiseHERusesreplaywiththegoalcorrespondingtotheﬁnalstateineachepisode,i.e.S(s0,...,sT)=m(sT).WecomparedifferentstrategiesforchoosingwhichgoalstoreplaywithinSec.4.5.4.2DoesHERimproveperformance?InordertoverifyifHERimprovesperformanceweevaluateDDPGwithandwithoutHERonall3tasks.Moreover,wecompareagainstDDPGwithcount-basedexploration6(StrehlandLittman,2005;KolterandNg,2009;Tangetal.,2016;Bellemareetal.,2016;Ostrovskietal.,2017).ForHERwestoreeachtransitioninthereplaybuffertwice:oncewiththegoalusedforthegenerationoftheepisodeandoncewiththegoalcorrespondingtotheﬁnalstatefromtheepisode(wecallthisstrategyfinal).InSec.4.5weperformablationstudiesofdifferentstrategiesSforchoosinggoalsforreplay,hereweincludethebestversionfromSec.4.5intheplotforcomparison.0501001502000%20%40%60%80%100%success ratepushingDDPGDDPG+count-based explorationDDPG+HERDDPG+HER (version from Sec. 4.5)050100150200epoch number (every epoch = 800 episodes = 800x50 timesteps)0%20%40%60%80%100%sliding0501001502000%20%40%60%80%100%pick-and-placeFigure3:Learningcurvesformulti-goalsetup.Anepisodeisconsideredsuccessfulifthedistancebetweentheobjectandthegoalattheendoftheepisodeislessthan7cmforpushingandpick-and-placeandlessthan20cmforsliding.Theresultsareaveragedacross5randomseedsandshadedareasrepresentonestandarddeviation.Theredcurvescorrespondtothefuturestrategywithk=4fromSec.4.5whiletheblueonecorrespondstothefinalstrategy.FromFig.3itisclearthatDDPGwithoutHERisunabletosolveanyofthetasks7andDDPGwithcount-basedexplorationisonlyabletomakesomeprogressontheslidingtask.Ontheotherhand,DDPGwithHERsolvesalltasksalmostperfectly.ItconﬁrmsthatHERisacrucialelementwhichmakeslearningfromsparse,binaryrewardspossible.4Thetargetpositionisrelativetothecurrentobjectposition.5Thesuccessfuldeploymentonaphysicalrobot(Sec.4.6)conﬁrmsthatourcontrolmodelproducesmovementswhicharereproducibleonthephysicalrobotdespitenotbeingfullyphysicallyplausible.6Wediscretizethestatespaceanduseanintrinsicrewardoftheformα/√N,whereαisahyper-parameterandNisthenumberoftimesthegivenstatewasvisited.Thediscretizationworksasfol-lows.Wetaketherelativepositionoftheboxandthetargetandthendiscretizeeverycoordinateusingagridwithastepsizeβwhichisahyperparameter.Wehaveperformedahyperparametersearchoverα∈{0.032,0.064,0.125,0.25,0.5,1,2,4,8,16,32},β∈{1cm,2cm,4cm,8cm}.Thebestresultswereobtainedusingα=1andβ=1cmandthesearetheresultswereport.7WealsoevaluatedDQN(withoutHER)onourtasksanditwasnotabletosolveanyofthem.70501001502000%20%40%60%80%100%success ratepushingDDPGDDPG+count-based explorationDDPG+HER050100150200epoch number (every epoch = 800 episodes = 800x50 timesteps)0%20%40%60%80%100%sliding0501001502000%20%40%60%80%100%pick-and-placeFigure4:Learningcurvesforthesingle-goalcase.4.3DoesHERimproveperformanceevenifthereisonlyonegoalwecareabout?InthissectionweevaluatewhetherHERimprovesperformanceinthecasewherethereisonlyonegoalwecareabout.Tothisend,werepeattheexperimentsfromtheprevioussectionbutthegoalstateisidenticalinallepisodes.FromFig.4itisclearthatDDPG+HERperformsmuchbetterthanpureDDPGevenifthegoalstateisidenticalinallepisodes.Moreimportantly,comparingFig.3andFig.4wecanalsonoticethatHERlearnsfasteriftrainingepisodescontainmultiplegoals,soinpracticeitisadvisabletotrainonmultiplegoalsevenifwecareonlyaboutoneofthem.4.4HowdoesHERinteractwithrewardshaping?Sofarweonlyconsideredbinaryrewardsoftheformr(s,a,g)=−[|g−sobject|>(cid:15)].InthissectionwecheckhowtheperformanceofDDPGwithandwithoutHERchangesifwereplacethisrewardwithonewhichisshaped.Weconsideredrewardfunctionsoftheformr(s,a,g)=λ|g−sobject|p−|g−s0object|p,wheres0isthestateoftheenvironmentaftertheexecutionoftheactionainthestatesandλ∈{0,1},p∈{1,2}arehyperparameters.Fig.5showstheresults.SurprisinglyneitherDDPG,norDDPG+HERwasabletosuccessfullysolveanyofthetaskswithanyoftheserewardfunctions8.OurresultsareconsistentwiththefactthatsuccessfulapplicationsofRLtodifﬁcultmanipulationtaskswhichdoesnotusedemonstrationsusuallyhavemorecomplicatedrewardfunctionsthantheoneswetried(e.g.Popovetal.(2017)).Thefollowingtworeasonscancauseshapedrewardstoperformsopoorly:(1)Thereisahugediscrepancybetweenwhatweoptimize(i.e.ashapedrewardfunction)andthesuccesscondition(i.e.:istheobjectwithinsomeradiusfromthegoalattheendoftheepisode);(2)Shapedrewardspenalizeforinappropriatebehaviour(e.g.movingtheboxinawrongdirection)whichmayhinderexploration.Itcancausetheagenttolearnnottotouchtheboxatallifitcannotmanipulateitpreciselyandwenoticedsuchbehaviourinsomeofourexperiments.Ourresultssuggestthatdomain-agnosticrewardshapingdoesnotworkwell(atleastinthesimpleformswehavetried).Ofcourseforeveryproblemthereexistsarewardwhichmakesiteasy(Ngetal.,1999)butdesigningsuchshapedrewardsrequiresalotofdomainknowledgeandmayinsomecasesnotbemucheasierthandirectlyscriptingthepolicy.Thisstrengthensourbeliefthatlearningfromsparse,binaryrewardsisanimportantproblem.4.5Howmanygoalsshouldwereplayeachtrajectorywithandhowtochoosethem?Inthissectionweexperimentallyevaluatedifferentstrategies(i.e.SinAlg.1)forchoosinggoalstousewithHER.Sofartheonlyadditionalgoalsweusedforreplayweretheonescorrespondingto8Wealsotriedtorescalethedistances,sothattherangeofrewardsissimilarasinthecaseofbinaryrewards,clippingbigdistancesandaddingasimple(linearorquadratic)termencouragingthegrippertomovetowardstheobjectbutnoneofthesetechniqueshaveledtosuccessfultraining.80501001502000%20%40%60%80%100%success ratepushingDDPGDDPG+HER050100150200epoch number (every epoch = 800 episodes = 800x50 timesteps)0%20%40%60%80%100%sliding0501001502000%20%40%60%80%100%pick-and-placeFigure5:Learningcurvesfortheshapedrewardr(s,a,g)=−|g−s0object|2(itperformedbestamongtheshapedrewardswehavetried).Bothalgorithmsfailonalltasks.124816all0.00.20.40.60.81.0highest success ratepushingno HERfinalrandomepisodefuture124816all0.00.20.40.60.81.0sliding124816all0.00.20.40.60.81.0pick-and-place124816all0.00.20.40.60.81.0average success ratepushing124816allnumber of additional goals used to replay each transition with0.00.20.40.60.81.0sliding124816all0.00.20.40.60.81.0pick-and-placeFigure6:Ablationstudyofdifferentstrategiesforchoosingadditionalgoalsforreplay.Thetoprowshowsthehighest(acrossthetrainingepochs)testperformanceandthebottomrowshowstheaveragetestperformanceacrossalltrainingepochs.Ontherighttopplotthecurvesforfinal,episodeandfuturecoincideasallthesestrategiesachieveperfectperformanceonthistask.theﬁnalstateoftheenvironmentandwewillcallthisstrategyfinal.Apartfromitweconsiderthefollowingstrategies:•future—replaywithkrandomstateswhichcomefromthesameepisodeasthetransitionbeingreplayedandwereobservedafterit,•episode—replaywithkrandomstatescomingfromthesameepisodeasthetransitionbeingreplayed,•random—replaywithkrandomstatesencounteredsofarinthewholetrainingprocedure.AllofthesestrategieshaveahyperparameterkwhichcontrolstheratioofHERdatatodatacomingfromnormalexperiencereplayinthereplaybuffer.TheplotscomparingdifferentstrategiesanddifferentvaluesofkcanbefoundinFig.6.Wecanseefromtheplotsthatallstrategiesapartfromrandomsolvepushingandpick-and-placealmostperfectlyregardlessofthevaluesofk.Inallcasesfuturewithkequal4or8performsbestanditistheonlystrategywhichisabletosolvetheslidingtaskalmostperfectly.Thelearningcurvesfor9Figure7:Thepick-and-placepolicydeployedonthephysicalrobot.futurewithk=4canbefoundinFig.3.Itconﬁrmsthatthemostvaluablegoalsforreplayaretheoneswhicharegoingtobeachievedinthenearfuture9.Noticethatincreasingthevaluesofkabove8degradesperformancebecausethefractionofnormalreplaydatainthebufferbecomesverylow.4.6DeploymentonaphysicalrobotWetookapolicyforthepick-and-placetasktrainedinthesimulator(versionwiththefuturestrategyandk=4fromSec.4.5)anddeployeditonaphysicalfetchrobotwithoutanyﬁnetuning.TheboxpositionwaspredictedusingaseparatelytrainedCNNusingrawfetchheadcameraimages.SeeAppendixBfordetails.Initiallythepolicysucceededin2outof5trials.Itwasnotrobusttosmallerrorsintheboxpositionestimationbecauseitwastrainedonperfectstatecomingfromthesimulation.Afterretrainingthepolicywithgaussiannoise(std=1cm)addedtoobservations10thesuccessrateincreasedto5/5.Thevideoshowingsomeofthetrialsisavailableathttps://goo.gl/SMrQnI.5RelatedworkThetechniqueofexperiencereplayhasbeenintroducedinLin(1992)andbecameverypopularafteritwasusedintheDQNagentplayingAtari(Mnihetal.,2015).Prioritizedexperiencereplay(Schauletal.,2015b)isanimprovementtoexperiencereplaywhichprioritizestransitionsinthereplaybufferinordertospeeduptraining.Ititorthogonaltoourworkandbothapproachescanbeeasilycombined.Learningsimultaneouslypoliciesformultipletaskshavebeenheavilyexploredinthecontextofpolicysearch,e.g.SchmidhuberandHuber(1990);Caruana(1998);DaSilvaetal.(2012);Koberetal.(2012);Devinetal.(2016);PintoandGupta(2016).Learningoff-policyvaluefunctionsformultipletaskswasinvestigatedbyFosterandDayan(2002)andSuttonetal.(2011).OurworkismostheavilybasedonSchauletal.(2015a)whoconsiderstrainingasingleneuralnetworkapproximatingmultiplevaluefunctions.LearningsimultaneouslytoperformmultipletaskshasbeenalsoinvestigatedforalongtimeinthecontextofHierarchicalReinforcementLearning,e.g.BakkerandSchmidhuber(2004);Vezhnevetsetal.(2017).Ourapproachmaybeseenasaformofimplicitcurriculumlearning(Elman,1993;Bengioetal.,2009).Whilecurriculumisnowoftenusedfortrainingneuralnetworks(e.g.ZarembaandSutskever(2014);Gravesetal.(2016)),thecurriculumisalmostalwayshand-crafted.TheproblemofautomaticcurriculumgenerationwasapproachedbySchmidhuber(2004)whoconstructedanasymptoticallyoptimalalgorithmforthisproblemusingprogramsearch.AnotherinterestingapproachisPowerPlay(Schmidhuber,2013;Srivastavaetal.,2013)whichisageneralframeworkforautomatictaskselection.Gravesetal.(2017)considerasetupwherethereisaﬁxeddiscretesetoftasksandempiricallyevaluatedifferentstrategiesforautomaticcurriculumgenerationinthissettings.AnotherapproachinvestigatedbySukhbaataretal.(2017)andHeldetal.(2017)usesself-playbetweenthepolicyandatask-setterinordertoautomaticallygenerategoalstateswhichareontheborderofwhatthecurrentpolicycanachieve.Ourapproachisorthogonaltothesetechniquesandcanbecombinedwiththem.9Wehavealsotriedreplayingthegoalswhichareclosetotheonesachievedinthenearfuturebutithasnotperformedbetterthanthefuturestrategy10TheQ-functionapproximatorwastrainedusingexactobservations.Itdoesnothavetoberobusttonoisyobservationsbecauseitisnotusedduringthedeploymentonthephysicalrobot.106ConclusionsWeintroducedanoveltechniquecalledHindsightExperienceReplaywhichmakespossibleapplyingRLalgorithmstoproblemswithsparseandbinaryrewards.Ourtechniquecanbecombinedwithanarbitraryoff-policyRLalgorithmandweexperimentallydemonstratedthatwithDQNandDDPG.WeshowedthatHERallowstrainingpolicieswhichpush,slideandpick-and-placeobjectswitharoboticarmtothespeciﬁedpositionswhilethevanillaRLalgorithmfailstosolvethesetasks.Wealsoshowedthatthepolicyforthepick-and-placetaskperformswellonthephysicalrobotwithoutanyﬁnetuning.Asfarasweknow,itistheﬁrsttimesocomplicatedbehaviourswerelearnedusingonlysparse,binaryrewards.AcknowledgmentsWewouldliketothankAnkurHanda,JonathanHo,JohnSchulman,MatthiasPlappert,TimSalimans,andVikashKumarforprovidingfeedbackonthepreviousversionsofthismanuscript.WewouldalsoliketothankReinHouthooftandthewholeOpenAIteamforfruitfuldiscussionsaswellasBowenBakerforperformingsomeadditionalexperiments.ReferencesAbadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,A.,Dean,J.,Devin,M.,etal.(2016).Tensorﬂow:Large-scalemachinelearningonheterogeneousdistributedsystems.arXivpreprintarXiv:1603.04467.Bakker,B.andSchmidhuber,J.(2004).Hierarchicalreinforcementlearningbasedonsubgoaldiscoveryandsubpolicyspecialization.InProc.ofthe8-thConf.onIntelligentAutonomousSystems,pages438–445.Bellemare,M.,Srinivasan,S.,Ostrovski,G.,Schaul,T.,Saxton,D.,andMunos,R.(2016).Unifyingcount-basedexplorationandintrinsicmotivation.InAdvancesinNeuralInformationProcessingSystems,pages1471–1479.Bengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.InProceedingsofthe26thannualinternationalconferenceonmachinelearning,pages41–48.ACM.Caruana,R.(1998).Multitasklearning.InLearningtolearn,pages95–133.Springer.Chebotar,Y.,Kalakrishnan,M.,Yahya,A.,Li,A.,Schaal,S.,andLevine,S.(2016).Pathintegralguidedpolicysearch.arXivpreprintarXiv:1610.00529.DaSilva,B.,Konidaris,G.,andBarto,A.(2012).Learningparameterizedskills.arXivpreprintarXiv:1206.6398.Devin,C.,Gupta,A.,Darrell,T.,Abbeel,P.,andLevine,S.(2016).Learningmodularneuralnetworkpoliciesformulti-taskandmulti-robottransfer.arXivpreprintarXiv:1609.07088.Elman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceofstartingsmall.Cognition,48(1):71–99.Foster,D.andDayan,P.(2002).Structureinthespaceofvaluefunctions.MachineLearning,49(2):325–346.Graves,A.,Bellemare,M.G.,Menick,J.,Munos,R.,andKavukcuoglu,K.(2017).Automatedcurriculumlearningforneuralnetworks.arXivpreprintarXiv:1704.03003.Graves,A.,Wayne,G.,Reynolds,M.,Harley,T.,Danihelka,I.,Grabska-Barwi´nska,A.,Colmenarejo,S.G.,Grefenstette,E.,Ramalho,T.,Agapiou,J.,etal.(2016).Hybridcomputingusinganeuralnetworkwithdynamicexternalmemory.Nature,538(7626):471–476.Gu,S.,Lillicrap,T.,Sutskever,I.,andLevine,S.(2016).Continuousdeepq-learningwithmodel-basedacceleration.arXivpreprintarXiv:1603.00748.Held,D.,Geng,X.,Florensa,C.,andAbbeel,P.(2017).Automaticgoalgenerationforreinforcementlearningagents.arXivpreprintarXiv:1705.06366.Houthooft,R.,Chen,X.,Duan,Y.,Schulman,J.,DeTurck,F.,andAbbeel,P.(2016).Vime:Variationalinformationmaximizingexploration.InAdvancesinNeuralInformationProcessingSystems,pages1109–1117.11Kingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980.Kober,J.,Wilhelm,A.,Oztop,E.,andPeters,J.(2012).Reinforcementlearningtoadjustparametrizedmotorprimitivestonewsituations.AutonomousRobots,33(4):361–379.Kolter,J.Z.andNg,A.Y.(2009).Near-bayesianexplorationinpolynomialtime.InProceedingsofthe26thAnnualInternationalConferenceonMachineLearning,pages513–520.ACM.Levine,S.,Finn,C.,Darrell,T.,andAbbeel,P.(2015).End-to-endtrainingofdeepvisuomotorpolicies.arXivpreprintarXiv:1504.00702.Lillicrap,T.P.,Hunt,J.J.,Pritzel,A.,Heess,N.,Erez,T.,Tassa,Y.,Silver,D.,andWierstra,D.(2015).Continuouscontrolwithdeepreinforcementlearning.arXivpreprintarXiv:1509.02971.Lin,L.-J.(1992).Self-improvingreactiveagentsbasedonreinforcementlearning,planningandteaching.Machinelearning,8(3-4):293–321.Metz,L.,Ibarz,J.,Jaitly,N.,andDavidson,J.(2017).Discretesequentialpredictionofcontinuousactionsfordeeprl.arXivpreprintarXiv:1705.05035.Mnih,V.,Kavukcuoglu,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,A.,Riedmiller,M.,Fidjeland,A.K.,Ostrovski,G.,etal.(2015).Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533.Ng,A.,Coates,A.,Diel,M.,Ganapathi,V.,Schulte,J.,Tse,B.,Berger,E.,andLiang,E.(2006).Autonomousinvertedhelicopterﬂightviareinforcementlearning.ExperimentalRoboticsIX,pages363–372.Ng,A.Y.,Harada,D.,andRussell,S.(1999).Policyinvarianceunderrewardtransformations:Theoryandapplicationtorewardshaping.InICML,volume99,pages278–287.Osband,I.,Blundell,C.,Pritzel,A.,andVanRoy,B.(2016).Deepexplorationviabootstrappeddqn.InAdvancesInNeuralInformationProcessingSystems,pages4026–4034.Ostrovski,G.,Bellemare,M.G.,Oord,A.v.d.,andMunos,R.(2017).Count-basedexplorationwithneuraldensitymodels.arXivpreprintarXiv:1703.01310.Peters,J.andSchaal,S.(2008).Reinforcementlearningofmotorskillswithpolicygradients.Neuralnetworks,21(4):682–697.Pinto,L.andGupta,A.(2016).Learningtopushbygrasping:Usingmultipletasksforeffectivelearning.arXivpreprintarXiv:1609.09025.Polyak,B.T.andJuditsky,A.B.(1992).Accelerationofstochasticapproximationbyaveraging.SIAMJournalonControlandOptimization,30(4):838–855.Popov,I.,Heess,N.,Lillicrap,T.,Hafner,R.,Barth-Maron,G.,Vecerik,M.,Lampe,T.,Tassa,Y.,Erez,T.,andRiedmiller,M.(2017).Data-efﬁcientdeepreinforcementlearningfordexterousmanipulation.arXivpreprintarXiv:1704.03073.Schaul,T.,Horgan,D.,Gregor,K.,andSilver,D.(2015a).Universalvaluefunctionapproximators.InProceedingsofthe32ndInternationalConferenceonMachineLearning(ICML-15),pages1312–1320.Schaul,T.,Quan,J.,Antonoglou,I.,andSilver,D.(2015b).Prioritizedexperiencereplay.arXivpreprintarXiv:1511.05952.Schmidhuber,J.(2004).Optimalorderedproblemsolver.MachineLearning,54(3):211–254.Schmidhuber,J.(2013).Powerplay:Traininganincreasinglygeneralproblemsolverbycontinuallysearchingforthesimpleststillunsolvableproblem.Frontiersinpsychology,4.Schmidhuber,J.andHuber,R.(1990).Learningtogeneratefocustrajectoriesforattentivevision.InstitutfürInformatik.Silver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L.,VanDenDriessche,G.,Schrittwieser,J.,Antonoglou,I.,Panneershelvam,V.,Lanctot,M.,etal.(2016).Masteringthegameofgowithdeepneuralnetworksandtreesearch.Nature,529(7587):484–489.Srivastava,R.K.,Steunebrink,B.R.,andSchmidhuber,J.(2013).Firstexperimentswithpowerplay.NeuralNetworks,41:130–136.12Strehl,A.L.andLittman,M.L.(2005).Atheoreticalanalysisofmodel-basedintervalestimation.InProceedingsofthe22ndinternationalconferenceonMachinelearning,pages856–863.ACM.Sukhbaatar,S.,Kostrikov,I.,Szlam,A.,andFergus,R.(2017).Intrinsicmotivationandautomaticcurriculaviaasymmetricself-play.arXivpreprintarXiv:1703.05407.Sutton,R.S.,Modayil,J.,Delp,M.,Degris,T.,Pilarski,P.M.,White,A.,andPrecup,D.(2011).Horde:Ascalablereal-timearchitectureforlearningknowledgefromunsupervisedsensorimotorinteraction.InThe10thInternationalConferenceonAutonomousAgentsandMultiagentSystems-Volume2,pages761–768.InternationalFoundationforAutonomousAgentsandMultiagentSystems.Tang,H.,Houthooft,R.,Foote,D.,Stooke,A.,Chen,X.,Duan,Y.,Schulman,J.,DeTurck,F.,andAbbeel,P.(2016).#exploration:Astudyofcount-basedexplorationfordeepreinforcementlearning.arXivpreprintarXiv:1611.04717.Tobin,J.,Fong,R.,Ray,A.,Schneider,J.,Zaremba,W.,andAbbeel,P.(2017).Domainrandomizationfortransferringdeepneuralnetworksfromsimulationtotherealworld.arXivpreprintarXiv:1703.06907.Todorov,E.,Erez,T.,andTassa,Y.(2012).Mujoco:Aphysicsengineformodel-basedcontrol.InIntelligentRobotsandSystems(IROS),2012IEEE/RSJInternationalConferenceon,pages5026–5033.IEEE.Vezhnevets,A.S.,Osindero,S.,Schaul,T.,Heess,N.,Jaderberg,M.,Silver,D.,andKavukcuoglu,K.(2017).Feudalnetworksforhierarchicalreinforcementlearning.arXivpreprintarXiv:1703.01161.Zaremba,W.andSutskever,I.(2014).Learningtoexecute.arXivpreprintarXiv:1410.4615.13AExperimentdetailsInthissectionweprovidemoredetailsonourexperimentalsetupandhyperparametersused.Bit-ﬂippingexperiment:Weusedanetworkwith1hiddenlayerwith256neurons.Thelengthofeachepisodewasequaltothenumberofbitsandtheepisodewasconsideredsuccessfulifthegoalstatewasachievedatanarbitrarytimestepduringtheepisode.AllotherhyperparametersusedwerethesameasinthecaseofDDPGexperiments.State-goaldistributions:Foralltaskstheinitialpositionofthegripperisﬁxed,forthepushingandslidingtasksitislocatedjustabovethetablesurfaceandforpushingitislocated20cmabovethetable.Theobjectisplacedrandomlyonthetableinthe30cmx30cm(20cx20cmforsliding)squarewiththecenterdirectlyunderthegripper(bothobjectsare5cmwide).Forpushing,thegoalstateissampleduniformlyfromthesamesquareastheboxposition.Inthepick-and-placetaskthetargetislocatedintheairinordertoforcetherobottograsp(andnotjustpush).Thexandycoordinatesofthegoalpositionaresampleduniformlyfromthementionedsquareandtheheightissampleduniformlybetween10cmand45cm.Forslidingthegoalpositionissampledfroma60cmx60cmsquarecentered40cmawayfromtheinitialgripperposition.Foralltaskswediscardinitialstate-goalpairsinwhichthegoalisalreadysatisﬁed.Networkarchitecture:Bothactorandcriticnetworkshave3hiddenlayerswith64hiddenunitsineachlayer.HiddenlayersuseReLuactivationfunctionandtheactoroutputlayerusestanh.Theoutputofthetanhisthenrescaledsothatitliesintherange[−5cm,5cm].Inordertopreventtanhsaturationandvanishinggradientsweaddthesquareofthetheirpreactivationstotheactor’scostfunction.Trainingprocedure:Wetrainfor200epochs.Eachepochconsistsof50cycleswhereeachcycleconsistsofrunningthepolicyfor16episodesandthenperforming40optimizationstepsonminibatchesofsize128sampleduniformlyfromareplaybufferconsistingof106transitions.Weupdatethetargetnetworksaftereverycycleusingthedecaycoefﬁcientof0.95.ApartfromusingthetargetnetworkforcomputingQ-targetsforthecriticwealsouseitintestingepisodesasitismorestablethanthemainnetwork.Thewholetrainingprocedureisdistributedover8threads.FortheAdamoptimizationalgorithmweusethelearningrateof0.001andthedefaultvaluesfromTensorﬂowframework(Abadietal.,2016)fortheotherhyperparameters.Weusethediscountfactorofγ=0.98foralltransitionsincludingtheonesendinganepisode.Moreover,weclipthetargetsusedtotrainthecritictotherangeofpossiblevalues,i.e.[−11−γ,0].Inputscaling:Neuralnetworkshaveproblemsdealingwithinputsofdifferentmagnitudesandthereforeitiscrucialtoscalethemproperly.Tothisend,werescaleinputstoneuralnetworkssothattheyhavemeanzeroandstandarddeviationequaltooneandthenclipthemtotherange[−5,5].Meansandstandarddeviationsusedforrescalingarecomputedusingalltheobservationsencounteredsofarinthetraining.Exploration:Thebehavioralpolicyweuseforexplorationworksasfollows.Withprobability20%wesample(uniformly)arandomactionfromthehypercubeofvalidactions.Otherwise,wetaketheoutputofthepolicynetworkandaddindependentlytoeverycoordinatenormalnoisewithstandarddeviationequalto5%ofthetotalrangeofallowedvaluesonthiscoordinate.Simulation:Everyepisodeconsistsof50environmenttimesteps,eachofwhichconsistsof10MuJoCostepswith∆t=0.002s.MuJoCousessoftconstraintsforcontactsandthereforeobjectpenetrationispossible.Itcanbeminimizedbyusingasmalltimestepandmoreconstraintsolverepochsbutitwouldslowdownthesimulation.Weencounteredsomepenetrationinthepushingtask(theagentlearnttopushtheboxintothetableinawaythatitispushedoutbycontactforcesontothetarget).Inordertovoidthisbehaviourweaddedtotherewardatermpenalizingthesquareddepthofpenetrationforeverycontactpair.14Trainingtime:Trainingfor200epochstookusapproximately2.5hforpushingandthepick-and-placetasksand6hforsliding(becausephysicssimulationwasslowerforthistask)using8cpucores.BDeploymentonthephysicalrobotWehavetrainedaconvolutionalneuralnetwork(CNN)whichpredictstheboxpositiongiventherawimagefromthefetchheadcamera.TheCNNwastrainedusingonlyimagescomingfromtheMujocorenderer.Despitethefactthattrainingimageswerenotphotorealistic,thetrainednetworkperformswellonrealworlddatathankstoahighdegreeofrandomizationoftextures,lightningandothervisualparametersintraining.ThisapproachcalleddomainrandomizationisdescribedinmoredetailinTobinetal.(2017).AtthebeginningofeachepisodeweinitializeasimulatedenvironmentusingtheboxpositionpredictedbytheCNNandrobotstatecomingfromthephysicalrobot.Fromthispointwerunthepolicyinthesimulator.Aftereachtimestepwesendthesimulatedrobotjointanglestotherealonewhichisposition-controlledandusesthesimulateddataastargets.15
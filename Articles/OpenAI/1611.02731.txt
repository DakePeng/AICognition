7
1
0
2

r
a

M
4

]

G
L
.
s
c
[

2
v
1
3
7
2
0
.
1
1
6
1
:
v
i
X
r
a

PublishedasaconferencepaperatICLR2017VARIATIONALLOSSYAUTOENCODERXiChen†‡,DiederikP.Kingma‡,TimSalimans‡,YanDuan†‡,PrafullaDhariwal‡,JohnSchulman†‡,IlyaSutskever‡,PieterAbbeel†‡†UCBerkeley,DepartmentofElectricalEngineeringandComputerScience‡OpenAI{peter,dpkingma,tim,rocky,prafulla,joschu,ilyasu,pieter}@openai.comABSTRACTRepresentationlearningseekstoexposecertainaspectsofobserveddatainalearnedrepresentationthat’samenabletodownstreamtaskslikeclassiﬁcation.Forinstance,agoodrepresentationfor2Dimagesmightbeonethatdescribesonlyglobalstructureanddiscardsinformationaboutdetailedtexture.Inthispaper,wepresentasimplebutprincipledmethodtolearnsuchglobalrepresentationsbycombiningVariationalAutoencoder(VAE)withneuralautoregressivemodelssuchasRNN,MADEandPixelRNN/CNN.OurproposedVAEmodelallowsustohavecontroloverwhatthegloballatentcodecanlearnandbydesigningthearchitectureaccordingly,wecanforcethegloballatentcodetodiscardirrelevantinformationsuchastexturein2Dimages,andhencetheVAEonly“autoencodes”datainalossyfashion.Inaddition,byleveragingautoregressivemodelsasbothpriordistributionp(z)anddecodingdistributionp(x|z),wecangreatlyimprovegenerativemodelingperformanceofVAEs,achievingnewstate-of-the-artresultsonMNIST,OMNIGLOTandCaltech-101SilhouettesdensityestimationtasksaswellascompetitiveresultsonCIFAR10.1INTRODUCTIONAkeygoalofrepresentationlearningistoidentifyanddisentangletheunderlyingcausalfactorsofthedata,sothatitbecomeseasiertounderstandthedata,toclassifyit,ortoperformothertasks(Bengioetal.,2013).Forimagedatathisoftenmeansthatweareinterestedinuncoveringthe“globalstructure”thatcapturesthecontentofanimage(forexample,theidentityofobjectspresentintheimage)andits“style”,butthatwearetypicallylessinterestedinthelocalandhighfrequencysourcesofvariationsuchasthespeciﬁctexturesorwhitenoisepatterns.Apopularapproachforlearningrepresentationsistoﬁtaprobabilisticlatentvariablemodel,anap-proachalsoknownasanalysis-by-synthesis(Yuille&Kersten,2006;Nairetal.,2008).Bylearningagenerativemodelofthedatawiththeappropriatehierarchicalstructureoflatentvariables,itishopedthatthemodelwillsomehowuncoveranduntanglethosecausalsourcesofvariationsthatwehappentobeinterestedin.However,withoutfurtherassumptions,representationlearningviagenerativemodelingisill-posed:therearemanydifferentpossiblegenerativemodelswithdifferent(orno)kindsoflatentvariablesthatallencodethesameprobabilitydensityfunctiononourob-serveddata.Thus,theresultsweempiricallygetusingthisapproacharehighlydependentonthespeciﬁcarchitecturalandmodelingchoicesthataremade.Moreover,theobjectivethatweoptimizeisoftencompletelydisconnectedfromthegoaloflearningagoodrepresentation:Anautoregressivemodelofthedatamayachievethesamelog-likelihoodasavariationalautoencoder(VAE)(Kingma&Welling,2013),butthestructurelearnedbythetwomodelsiscompletelydifferent:thelattertypicallyhasaclearhierarchyoflatentvariables,whiletheautoregressivemodelhasnostochasticlatentvariablesatall(althoughitisconceivablethatthedeterministichiddenunitsoftheautore-gressivemodelswillhavemeaningfulandusefulrepresentations).Forthisreason,autoregressivemodelshavethusfarnotbeenpopularforthepurposeoflearningrepresentations,eventhoughtheyareextremelypowerfulasgenerativemodels(seee.g.vandenOordetal.,2016a).Anaturalquestionbecomes:isitpossibletohaveamodelthatisapowerfuldensityestimatorandatthesametimehastherighthierarchicalstructureforrepresentationlearning?ApotentialsolutionwouldbetouseahybridmodelthathasboththelatentvariablestructureofaVAE,as1 
 
 
 
 
 
PublishedasaconferencepaperatICLR2017wellasthepowerfulrecurrenceofanautoregressivemodel.However,earlierattemptsatcombiningthesetwokindsofmodelshaverunintotheproblemthattheautoregressivepartofthemodelendsupexplainingallstructureinthedata,whilethelatentvariablesarenotused(Fabius&vanAmersfoort,2014;Chungetal.,2015;Bowmanetal.,2015;Serbanetal.,2016;Fraccaroetal.,2016;Xu&Sun,2016).Bowmanetal.(2015)notedthatweakeningtheautoregressivepartofthemodelby,forexample,dropoutcanencouragethelatentvariablestobeused.Weanalyzewhyweakeningisnecessary,andweproposeaprincipledsolutionthattakesadvantageofthispropertytocontrolwhatkindofinformationgoesintolatentvariables.Themodelweproposeperformswellasadensityestimator,asevidencedbystate-of-the-artlog-likelihoodresultsonMNIST,OMNIGLOTandCaltech-101,andalsohasastructurethatisuniquelysuitedforlearninginterestingglobalrepresentationsofdata.2VAESDONOTAUTOENCODEINGENERALAVAEisfrequentlyinterpretedasaregularizedautoencoder(Kingma&Welling,2013;Zhangetal.,2016),buttheconditionsunderwhichitisguaranteedtoautoencode(reconstructionbeingclosetooriginaldatapoint)arenotdiscussed.Inthissection,wediscusstheoften-neglectedfactthatVAEsdonotalwaysautoencodeandgiveexplicitreasonswhypreviousattemptstoapplyVAEinsequencemodelingfoundthatthelatentcodeisgenerallynotusedunlessthedecoderisweakened(Bowmanetal.,2015;Serbanetal.,2016;Fraccaroetal.,2016).TheunderstandingofwhenVAEdoesautoencodewillbeanessentialbuildingpieceforVLAE.2.1TECHNICALBACKGROUNDLetxbeobservedvariables,zlatentvariablesandletp(x,z)betheparametricmodeloftheirjointdistribution,calledthegenerativemodeldeﬁnedoverthevariables.GivenadatasetX={x1,...,xN}wewishtoperformmaximumlikelihoodlearningofitsparameters:logp(X)=NXi=1logp(x(i)),(1)butingeneralthismarginallikelihoodisintractabletocomputeordifferentiatedirectlyforﬂexiblegenerativemodelsthathavehigh-dimensionallatentvariablesandﬂexiblepriorsandlikelihoods.Asolutionistointroduceq(z|x),aparametricinferencemodeldeﬁnedoverthelatentvariables,andoptimizethevariationallowerboundonthemarginallog-likelihoodofeachobservationx:logp(x)≥Eq(z|x)[logp(x,z)−logq(z|x)]=L(x;θ)(2)whereθindicatestheparametersofpandqmodels.TherearevariouswaystooptimizethelowerboundL(x;θ);forcontinuouszitcanbedoneefﬁ-cientlythroughare-parameterizationofq(z|x)(Kingma&Welling,2013;Rezendeetal.,2014).Thiswayofoptimizingthevariationallowerboundwithaparametricinferencenetworkandre-parameterizationofcontinuouslatentvariablesisusuallycalledVAE.The“autoencoding”termi-nologycomesfromthefactthatthelowerboundL(x;θ)canbere-arranged:L(x;θ)=Eq(z|x)[logp(x,z)−logq(z|x)](3)=Eq(z|x)[logp(x|z)]−DKL(q(z|x)||p(z))(4)wheretheﬁrsttermcanbeseenastheexpectationofnegativereconstructionerrorandtheKLdivergencetermcanbeseenasaregularizer,whichasawholecouldbeseenasaregularizedautoencoderlosswithq(z|x)beingtheencoderandp(x|z)beingthedecoder.Inthecontextof2Dimagesmodeling,thedecodingdistributionp(x|z)isusuallychosentobeasimplefactorizeddistribution,i.e.p(x|z)=Qip(xi|z),andthissetupoftenyieldsasharpdecodingdistributionp(x|z)thattendstoreconstructoriginaldatapointxexactly.2.2BITS-BACKCODINGANDINFORMATIONPREFERENCEIt’sstraightforwardtoseethathavingamorepowerfulp(x|z)willmakeVAE’smarginalgenerativedistributionp(x)=Rzp(z)p(x|z)dzmoreexpressive.Thisideahasbeenexploredextensively2PublishedasaconferencepaperatICLR2017inpreviousworkapplyingVAEtosequencemodeling(Fabius&vanAmersfoort,2014;Chungetal.,2015;Bowmanetal.,2015;Serbanetal.,2016;Fraccaroetal.,2016;Xu&Sun,2016),wherethedecodingdistributionisapowerfulRNNwithautoregressivedependency,i.e.,p(x|z)=Qip(xi|z,x<i).SinceRNNsareuniversalfunctionapproximatorsandanyjointdistributionoverxadmitsanautoregressivefactorization,theRNNautoregressivedecodingdistributioncanintheoryrepresentanyprobabilitydistributionevenwithoutdependenceonz.However,previousattemptshavefoundithardtobeneﬁtfromVAEwhenusinganexpressivede-codingdistributionp(x|z).Indeedit’sdocumentedindetailbyBowmanetal.(2015)thatinmostcaseswhenanRNNautoregressivedecodingdistributionisused,thelatentcodeziscompletelyignoredandthemodelregressestobeastandardunconditionalRNNautoregressivedistributionthatdoesn’tdependonthelatentcode.Thisphenomenoniscommonlyattributedto“optimizationchal-lenges”ofVAEintheliterature(Bowmanetal.,2015;Serbanetal.,2016;KaaeSønderbyetal.,2016)becauseearlyinthetrainingtheapproximateposteriorq(z|x)carrieslittleinformationaboutdatapointxandhenceit’seasyforthemodeltojustsettheapproximateposteriortobethepriortoavoidpayinganyregularizationcostDKL(q(z|x)||p(z)).Herewepresentasimplebutoften-neglectedobservationthatthisphenomenonarisesnotjustduetooptimizationchallengesandinsteadevenifwecansolvetheoptimizationproblemsexactly,thelatentcodeshouldstillbeignoredatoptimumformostpracticalinstancesofVAEthathavein-tractabletrueposteriordistributionsandsufﬁcientlypowerfuldecoders.ItiseasiesttounderstandthisobservationfromaBits-BackCodingperspectiveofVAE.Itiswell-knownthatBits-BackCodingisaninformation-theoreticviewofVariationalInference(Hinton&VanCamp,1993;Honkela&Valpola,2004)andspeciﬁclinkshavebeenestablishedbetweenBits-BackCodingandtheHelmholtzMachine/VAE(Hinton&Zemel,1994;Gregoretal.,2013).HerewebrieﬂyrelateVAEtoBits-BackCodingforself-containedness:Firstrecallthatthegoalofdesigninganefﬁcientcodingprotocolistominimizetheexpectedcodelengthofcommunicatingx.ToexplainBits-BackCoding,let’sﬁrstconsideramorenaivecodingscheme.VAEcanbeseenasawaytoencodedatainatwo-partcode:p(z)andp(x|z),wherezcanbeseenastheessence/structureofadatumandisencodedﬁrstandthenthemodelingerror(deviationfromz’sstructure)isencodednext.Theexpectedcodelengthunderthisnaivecodingschemeforagivendatadistributionishence:Cnaive(x)=Ex∼data,z∼q(z|x)[−logp(z)−logp(x|z)](5)Thiscodingschemeis,however,inefﬁcient.Bits-BackCodingimprovesonitbynoticingthattheencoderdistributionq(z|x)canbeusedtotransmitadditionalinformation,uptoH(q(z|x))expectednats,aslongasthereceiveralsohasaccesstoq(z|x).Thedecodingschemeworksasfollows:areceiverﬁrstdecodeszfromp(z),thendecodesxfromp(x|z)and,byrunningthesameapproximateposteriorthatthesenderisusing,decodesasecondarymessagefromq(z|x).Hence,toproperlymeasurethecodelengthofVAE’stwo-partcode,weneedtosubtracttheextrainformationfromq(z|x).UsingBit-BackCoding,theexpectedcodelengthequatestothenegativevariationallowerboundortheso-calledHelmholtzvariationalfreeenergy,whichmeansminimizingcodelengthisequivalenttomaximizingthevariationallowerbound:CBitsBack(x)=Ex∼data,z∼q(z|x)[logq(z|x)−logp(z)−logp(x|z)](6)=Ex∼data[−L(x)](7)CastingtheproblemofoptimizingVAEintodesigninganefﬁcientcodingschemeeasilyallowsustoreasonwhenthelatentcodezwillbeused:thelatentcodezwillbeusedwhenthetwo-partcodeisanefﬁcientcode.Recallingthatthelower-boundofexpectedcodelengthfordataisgivenbytheShannonentropyofdatagenerationdistribution:H(data)=Ex∼data[−logpdata(x)],wecananalyzeVAE’scodingefﬁciency:CBitsBack(x)=Ex∼data,z∼q(z|x)[logq(z|x)−logp(z)−logp(x|z)](8)=Ex∼data[−logp(x)+DKL(q(z|x)||p(z|x))](9)≥Ex∼data[−logpdata(x)+DKL(q(z|x)||p(z|x))](10)=H(data)+Ex∼data[DKL(q(z|x)||p(z|x))](11)3PublishedasaconferencepaperatICLR2017SinceKullbackLeiblerdivergenceisalwaysnon-negative,weknowthatusingthetwo-partcodederivedfromVAEsuffersatleastanextracodelengthofDKL(q(z|x)||p(z|x))natsforusingaposteriorthat’snotprecise.ManypreviousworksinVariationalInferencehavedesignedﬂexibleapproximateposteriorstobetterapproximatetrueposterior(Salimansetal.,2014;Rezende&Mo-hamed,2015;Tranetal.,2015;Kingmaetal.,2016).Improvedposteriorapproximationshaveshowntobeeffectiveinimprovingvariationalinferencebutnoneoftheexistingmethodsareabletocompletelyclosethegapbetweenapproximateposteriorandtrueposterior.Thisleadsustobelievethatformostpracticalmodels,atleastinthenearfuture,theextracodingcostDKL(q(z|x)||p(z|x))willexistandwillnotbenegligible.OnceweunderstandtheinefﬁciencyoftheBits-BackCodingmechanism,it’ssimpletorealizewhysometimesthelatentcodezisnotused:ifthep(x|z)couldmodelpdata(x)withoutusinginforma-tionfromz,thenitwillnotusez,inwhichcasethetrueposteriorp(z|x)issimplythepriorp(z)andit’susuallyeasytosetq(z|x)tobep(z)toavoidincurringanextracostDKL(q(z|x)||p(z|x)).Andit’sexactlythecasewhenapowerfuldecodingdistributionisusedlikeanRNNautoregressivedistribution,whichgivenenoughcapacityisabletomodelarbitrarilycomplexdistributions.HencethereexistsapreferenceofinformationwhenaVAEisoptimized:informationthatcanbemodeledlocallybydecodingdistributionp(x|z)withoutaccesstozwillbeencodedlocallyandonlytheremainderwillbeencodedinz.Wenotethatonecommonwaytoencourageputtinginformationintothecodeistouseafactorizeddecoderp(x|z)=Qip(xi|z)butsolongasthereisonedimensionxjthat’sindependentofallotherdimensionsfortruedatadistribution,pdata(x)=pdata(xj)pdata(x6=j),thenthelatentcodedoesn’tcontainalltheinformationaboutxsinceatleastxjwillbemodeledlocallybyfactorizedp(x|z).ThiskindofindependencestructurerarelyexistsinimagessocommonVAEsthathavefactorizeddecoderautoencodealmostexactly.OthertechniquestoencouragetheusageofthelatentcodeincludeannealingtherelativeweightofofDKL(q(z|x)||p(z))inthevariationallowerbound(Bowmanetal.,2015;KaaeSønderbyetal.,2016)ortheuseoffreebits(Kingmaetal.,2016),whichcanservethedualpurposeofsmoothingtheoptimizationlandscapeandcancelingoutpartoftheBits-BackCodeinefﬁciencyDKL(q(z|x)||p(z|x)).3VARIATIONALLOSSYAUTOENCODERThediscussioninSection2.2suggeststhatautoregressivemodelscannotbecombinedwithVAEsinceinformationwillbepreferredtobemodeledbyautoregressivemodels.Nevertheless,inthissection,wepresenttwocomplementaryclassesofimprovementstoVAEthatutilizeautoregressivemodelsfruitfullytoexplicitlycontrolrepresentationlearningandimprovedensityestimation.3.1LOSSYCODEVIAEXPLICITINFORMATIONPLACEMENTEventhoughtheinformationpreferencepropertyofVAEmightsuggestthatoneshouldalwaysusethefullautoregressivemodelstoachieveabettercodelength/log-likelihood,especiallywhenslowdatagenerationisnotaconcern,wearguethatthisinformationpreferencepropertycanbeexploitedtoturntheVAEintoapowerfulrepresentationlearningmethodthatgivesusﬁne-grainedcontroloverthekindofinformationthatgetsincludedinthelearnedrepresentation.Whenwetrytolearnalossycompression/representationofdata,wecansimplyconstructade-codingdistributionthat’scapableofmodelingthepartofinformationthatwedon’twantthelossyrepresentationtocapture,but,critically,that’sincapableofmodellingtheinformationthatwedowantthelossyrepresentationtocapture.Forinstance,ifweareinterestedinlearningaglobalrepresentationfor2Dimagesthatdoesn’tencodeinformationaboutdetailedtexture,wecanconstructaspeciﬁcfactorizationoftheautore-gressivedistributionsuchthatithasasmalllocalreceptiveﬁeldasdecodingdistribution,e.g.,plocal(x|z)=Qip(xi|z,xWindowAround(i)).Noticethat,aslongasxWindowAround(i)issmallerthanx<i,plocal(x|z)won’tbeabletorepresentarbitrarilycomplexdistributionoverxwithoutde-pendenceonzsincethereceptiveﬁeldislimitedsuchthatnotalldistributionsoverxadmitsuchfactorizations.Inparticular,thereceptiveﬁeldwindowcanbeasmallrectangleadjacenttoapixelxiandinthiscaselong-rangedependencywillbeencodedinthelatentcodez.Ontheotherhand,ifthetruedatadistributionadmitssuchfactorizationforagivendatumxanddimensioni,i.e.4PublishedasaconferencepaperatICLR2017pdata(xi|xWindowAround(i))=pdata(xi|x<i),thentheinformationpreferencepropertydiscussedinSection2.2willapplyhere,whichmeansthatalltheinformationwillbeencodedinlocalau-toregressivedistributionforxi.Localstatisticsof2Dimagesliketexturewilllikelybemodeledcompletelybyasmalllocalwindow,whereasglobalstructuralinformationofanimageslikeshapesofobjectsislong-rangedependencythatcanonlybecommunicatedthroughlatentcodez.There-forewehavegivenanexampleVAEthatwillproducealossycompressionof2Dimagescarryingexclusivelyglobalinformationthatcan’tbemodeledlocally.Noticethataglobalrepresentationisonlyoneofmanypossiblelossyrepresentationsthatwecanconstructusingthisinformationpreferenceproperty.Forinstance,theconditionalofanautoregres-sivedistributionmightdependonaheavilydown-sampledreceptiveﬁeldsothatitcanonlymodellong-rangepatternwhereaslocalhigh-frequencystatisticsneedtobeencodedintothelatentcode.Hencewehavedemonstratedthatwecanachieveexplicitplacementofinformationbyconstrainingthereceptiveﬁeld/factorizationofanautoregressivedistributionthat’susedasdecodingdistribution.Wewanttoadditionallyemphasizetheinformationpreferencepropertyisanasymptoticviewinasensethatitonlyholdswhenthevariationallowerboundcanbeoptimizedwell.Thus,wearenotproposinganalternativetotechniqueslikefreebitsKingmaetal.(2016)orKLannealing,andindeedtheyarestillusefulmethodstosmooththeoptimizationproblemandusedinthispaper’sexperiments.3.2LEARNEDPRIORWITHAUTOREGRESSIVEFLOWInefﬁciencyinBits-BackCoding,i.e.,themismatchbetweenapproximateposteriorandtrueposte-rior,canbeexploitedtoconstructalossycodebutit’sstillimportanttominimizesuchinefﬁciencytoimproveoverallmodelingperformance/codingefﬁciency.Weproposetoparametrizethepriordistributionp(z;θ)withanautoregressivemodelandshowthatatypeofautoregressivelatentcodecanintheoryreduceinefﬁciencyinBits-Backcoding.Itiswell-knownthatlimitedapproximateposteriorsimpedelearningandthereforevariousexpres-siveposteriorapproximationshavebeenproposedtoimproveVAE’sdensityestimationperformance(Turneretal.,2008;Mnih&Gregor,2014;Salimansetal.,2014;Rezende&Mohamed,2015;Kingmaetal.,2016).OnesuchclassofapproximateposteriorsthathasbeenshowntoattaingoodempiricalperformanceisbasedontheideaofNormalizingFlow,whichistoapplyaninvertiblemappingtoasimplerandomvariable,forexampleafactorizedGaussianascommonlyusedforq(z|x),inordertoobtainacomplicatedrandomvariable.Foraninvertibletransformationbetweenasimpledistributionyandamoreﬂexiblez,weknowfromthechange-of-variabletechniquethatlogq(z|x)=logq(y|x)−logdetdzdyandusingq(z|x)asapproximateposteriorwilldecreasethecodingefﬁciencygapDKL(q(z|x)||p(z|x))providedthetransformationissufﬁcientlyexpressive.Kingmaetal.(2016)introducedInverseAutoregressiveFlow,whichisapowerfulclassofsuchinvertiblemappingsthathavesimpledeterminant:zi=yi−µi(y1:i−1)σi(y1:i−1),whereµi(.)∈R,σi(.)∈R+aregeneralfunctionsthatcanbeparametrizedbyexpressiveneuralnetworks,suchasMADEandPixelCNNvariants(Germainetal.,2015;vandenOordetal.,2016a).Inverseautoregressiveﬂowistheinverse/whiteningofautoregressiveﬂow:yi=ziσi(y1:i−1)+µi(y1:i−1).Wereferinterestedreadersto(Rezende&Mohamed,2015;Kingmaetal.,2016)forin-depthdiscussionsonrelatedtopics.Inthispaper,weproposetoparametrizeourlearnablepriorasanautoregressiveﬂowfromsomesimplenoisesourcelikesphericalGaussian.Next,weshowthatusinglatentcodetransformedbyautoregressiveﬂow(AF)isequivalenttousinginverseautoregressiveﬂow(IAF)approximateposterior,whichexplainswhyitcansimilarlyimproveBits-BackCodingefﬁciency.Moreover,comparedwithanIAFposterior,anAFpriorhasamoreexpressivegenerativemodelthatessentially“comesforfree”.Foranautoregressiveﬂowf,somecontinuousnoisesource(cid:15)istransformedintolatentcodez:z=f((cid:15)).Assumingthedensityfunctionfornoisesourceisu((cid:15)),wesimilarlyknowthatlogp(z)=logu((cid:15))+logdetd(cid:15)dz.5PublishedasaconferencepaperatICLR2017Simplyre-arrangingthevariationallowerboundforusingAFpriorrevealsthathavinganAFlatentcodezisequivalenttousinganIAFposteriorfor(cid:15)thatwecaninterpretasthenewlatentcode:L(x;θ)=Ez∼q(z|x)[logp(x|z)+logp(z)−logq(z|x)](12)=Ez∼q(z|x),(cid:15)=f−1(z)(cid:20)logp(x|f((cid:15)))+logu((cid:15))+logdetd(cid:15)dz−logq(z|x)(cid:21)(13)=Ez∼q(z|x),(cid:15)=f−1(z)logp(x|f((cid:15)))+logu((cid:15))−(logq(z|x)−logdetd(cid:15)dz)|{z}IAFPosterior(14)AFprioristhesameasIAFposterioralongtheencoderpath,f−1(q(z|x)),butdiffersalongthedecoder/generatorpath:IAFposteriorhasashorterdecoderpathp(x|z)whereasAFpriorhasadeeperdecoderpathp(x|f((cid:15))).ThecrucialobservationisthatAFpriorandIAFposteriorhavethesamecomputationcostundertheexpectationofz∼q(z|x),sousingAFpriormakesthemodelmoreexpressiveatnotrainingtimecost.4EXPERIMENTSInthispaper,weevaluateVLAEon2Dimagesandleaveextensionstootherformsofdatatofuturework.Fortherestofthesection,wedeﬁneaVLAEmodelasaVAEthatusesAFpriorandautoregressivedecoder.Wechoosetoimplementconditionaldistributionp(x|z)withasmall-receptive-ﬁeldPixelCNN(vandenOordetal.,2016a),whichhasbeenprovedtobeascalableautoregressivemodel.Forevaluation,weusebinaryimagedatasetsthatarecommonlyusedfordensityestimationtasks:MNIST(LeCunetal.,1998)(bothstaticallybinarized1anddynamicallybinarizedversion(Burdaetal.,2015a)),OMNIGLOT(Lakeetal.,2013;Burdaetal.,2015a)andCaltech-101Silhouettes(Marlinetal.,2010).Alldatasetsuniformlyconsistof28x28binaryimages,whichallowustouseauniﬁedarchitecture.VAEnetworksusedinbinaryimagedatasetsaresimplevariantsofResNetVAEsdescribedin(Salimansetal.,2014;Kingmaetal.,2016).Forthedecoder,weuseavariantofPixelCNNthathas6layersofmaskedconvolutionwithﬁltersize3,whichmeansthewindowofdependency,xWindowAround(i),islimitedtoasmalllocalpatch.Duringtraining,”freebits”(Kingmaetal.,2016)isusedimproveoptimizationstability.Experimentalsetupandhyperparametersaredetailedintheappendix.ReportedmarginalNLLisestimatedusingImportanceSamplingwith4096samples.Wedesignedexperimentstoanswerthefollowingquestions:•CanVLAElearnlossycodesthatencodeglobalstatistics?•DoesusingAFpriorsimprovesuponusingIAFposteriorsaspredictedbytheory?•Doesusingautoregressivedecodingdistributionsimprovedensityestimationperformance?4.1LOSSYCOMPRESSIONFirstweareinterestedinwhetherVLAEcanlearnalossyrepresentation/compressionofdatabyusingthePixelCNNdecodertomodellocalstatistics.WetrainedVLAEmodelonStaticallyBina-rizedMNISTandtheconvergedmodelhasE[DKL(q(z|x)||p(z))]=13.3nats=19.2bits,whichisthenumberofbitsitusesonaveragetoencode/compressoneMNISTimage.Bycomparison,anidenticalVAEmodelwithfactorizeddecodingdistributionwillusesonaverage37.3bitsinlatentcode,andthisthusindicatesthatVLAEcanlearnalossiercompressionthanaVAEwithregularfactorizedconditionaldistribution.ThenextquestioniswhetherVLAE’slossycompressionencodesglobalstatisticsanddiscardslocalstatistics.InFig1a,wevisualizeoriginalimagesxdataandonerandom“decompression”xdecompressedfromVLAE:z∼q(z|xdata),xdecompressed∼p(x|z).Weobservethatnoneofthe1WeusetheversionprovidedbyHugoLarochelle.6PublishedasaconferencepaperatICLR2017(a)Originaltest-setimages(left)and“decompressioned”versionsfromVLAE’slossycode(right)(b)SamplesfromVLAEFigure1:StaticallyBinarizedMNISTdecompressionsisanexactreconstructionoftheoriginalimagebutinsteadtheglobalstructureoftheimagewasencodedinthelossycodezandregenerated.Alsoworthnotingisthatlocalstatisticsarenotpreservedbutanewsetoflikelylocalstatisticsaregeneratedinthedecompressedimages:thebinarymasksareusuallydifferentandlocalstyleslikestrokewidtharesometimesslightlydif-ferent.However,weremarkthatthelossycodezdoesn’talwayscapturethekindofglobalinformationthatwecareaboutandit’sdependentonthetypeofconstraintweputonthedecoder.Forinstance,inFig4b,weshowdecompressionsforOMNIGLOTdataset,whichhasmoremeaningfulvariationsinsmallpatchesthanMNIST,andwecanobservethatsemanticsarenotpreservedinsomecases.Thishighlightstheneedtospecifythetypeofstatisticswecareaboutinarepresentation,whichwillbedifferentacrosstasksanddatasets,anddesigndecodingdistributionaccordingly.(a)Originaltest-setimages(left)and“decompressioned”versionsfromVLAE’slossycode(right)(b)SamplesfromVLAEFigure2:OMNIGLOT4.2DENSITYESTIMATIONNextweinvestigatewhetherleveragingautoregressivemodelsaslatentdistributionp(z)andasdecodingdistributionp(x|z)wouldimprovedensityestimationperformance.ToverifywhetherAFpriorisabletoimproveuponIAFposterioralone,it’sdesirabletotestthismodelwithoutusingautoregressivedecoderbutinsteadusingtheconventionalindependentBernoullidistributionforp(x|z).HenceweusethebestperformingmodelfromKingmaetal.7PublishedasaconferencepaperatICLR2017Table1:StaticallyBinarizedMNISTModelNLLTestNormalizingﬂows(Rezende&Mohamed,2015)85.10DRAW(Gregoretal.,2015)<80.97DiscreteVAE(Rolfe,2016)81.01PixelRNN(vandenOordetal.,2016a)79.20IAFVAE(Kingmaetal.,2016)79.88AFVAE79.30VLAE79.03(2016)onstaticallybinarizedMNISTandmakethesinglemodiﬁcationofreplacingtheoriginalIAFposteriorwithanequivalentAFprior,removingthecontext.AsseeninTable1,VAEwithAFpriorisoutperformingVAEwithanequivalentIAFposterior,indicatingthatthedeepergenerativemodelfromAFpriorisbeneﬁcial.Asimilargaincarriesoverwhenanautoregressivedecoderisused:onstaticallybinarizedMNIST,usingAFpriorinsteadofIAFposteriorreducestrainNLLby0.8natandtestNLLby0.6nat.NextweevaluatewhetherusingautoregressivedecodingdistributioncanimproveperformanceandweshowinTable1thataVLAEmodel,withAFpriorandPixelCNNconditional,isabletoout-performaVAEwithjustAFpriorandachievesnewstate-of-the-artresultsonstaticallybinarizedMNIST.Inaddition,wehypothesizethattheseparationofdifferenttypesofinformation,themodelingglobalstructureinlatentcodeandlocalstatisticsinPixelCNN,likelyhassomeformofgoodinductivebi-asesfor2Dimages.InordertoevaluateifVLAEisanexpressivedensityestimatorwithgoodinductivebiases,wewilltestasingleVLAEmodel,withthesamenetworkarchitecture,onallbinarydatasets.WechoosehyperparametersmanuallyonstaticallybinarizedMNISTandusethesamehyperparameterstoevaluateondynamicallybinarizedMNIST,OMNIGLOTandCaltech-101Silhouettes.Wealsonotethatbetterperformancecanbeobtainedifweindividuallytunehyperpa-rametersforeachdataset.Asaconcretedemonstration,wereporttheperformanceofaﬁne-tunedVLAEonOMNIGLOTdatasetinTable3.Table2:DynamicallybinarizedMNISTModelNLLTestConvolutionalVAE+HVI(Salimansetal.,2014)81.94DLGM2hl+IWAE(Burdaetal.,2015a)82.90DiscreteVAE(Rolfe,2016)80.04LVAE(KaaeSønderbyetal.,2016)81.74DRAW+VGP(Tranetal.,2015)<79.88IAFVAE(Kingmaetal.,2016)79.10UnconditionalDecoder87.55VLAE78.53Table3:OMNIGLOT.[1](Burdaetal.,2015a),[2](Burdaetal.,2015b),[3](Gregoretal.,2015),[4](Gregoretal.,2016),ModelNLLTestVAE[1]106.31IWAE[1]103.38RBM(500hidden)[2]100.46DRAW[3]<96.50ConvDRAW[4]<91.00UnconditionalDecoder95.02VLAE90.98VLAE(ﬁne-tuned)89.83Table4:Caltech-101Silhouettes.[1](Born-schein&Bengio,2014),[2](Choetal.,2011),[3](Duetal.,2015),[4](Rolfe,2016),[5](Goessling&Amit,2015),ModelNLLTestRWSSBN[1]113.3RBM[2]107.8NAISNADE[3]100.0DiscreteVAE[4]97.6SpARN[5]88.48UnconditionalDecoder89.26VLAE77.368PublishedasaconferencepaperatICLR2017AsseeninTable2,3,4,withthesamesetofhyperparameterstunedonstaticallybinarizedMNIST,VLAEisabletoperformwellontherestofdatasets,signiﬁcantlyexceedingpreviousstate-of-the-artresultsondynamicallybinarizedMNISTandCaltech-101SilhouettesandtyingstatisticallywithbestpreviousresultonOMNIGLOT.InordertoisolatetheeffectofexpressivePixelCNNasdecoder,wealsoreportperformanceofthesamePixelCNNtrainedwithoutVAEpartunderthename“UnconditionalDecoder”.4.3NATURALIMAGES:CIFAR10Inadditiontobinaryimagedatasets,wehaveappliedVLAEtotheCIFAR10datasetofnaturalimages.DensityestimationofCIFAR10imageshasbeenachallengingbenchmarkproblemusedbymanyrecentgenerativemodelsandhenceisgreattasktopositionVLAEamongexistingmethods.WeinvestigatedusingResNet(Heetal.,2016)andDenseNet(Huangetal.,2016)asbuildingblocksforVAEnetworksandobservedthatDenseNetreducesoverﬁtting.WealsoproposeanewoptimizationtechniquethatblendstheadvantagesofKLannealing(Serbanetal.,2016)and”freebits”(Kingmaetal.,2016)tostabilizelearningonthischallengingdataset.DetailedexperimentalsetupisdescribedinAppendix.VLAEiscomparedtoothermethodsonCIFAR10inTable5.WeshowthatVLAEmodelsattainnewstate-of-the-artperformanceamongothervariationallytrainedlatent-variablemodels.DenseNetVLAEmodelalsooutperformsmostothertractablelikelihoodmodelsincludingGatedPixelCNNandPixelRNNandhasresultsonlyslightlyworsethancurrentlyunarchivedstate-of-the-artPixel-CNN++.Table5:CIFAR10.LikelihoodforVLAEisapproximatedwith512importancesamples.[1](vandenOordetal.,2016a),[2](Dinhetal.,2014),[3](vandenOord&Schrauwen,2014),[4](Dinhetal.,2016),[5](vandenOordetal.,2016b),[6](Salimansetal.,2017),[7](Sohl-Dicksteinetal.,2015),[8](Gregoretal.,2016),[9](Kingmaetal.,2016)Methodbits/dim≤Resultswithtractablelikelihoodmodels:Uniformdistribution[1]8.00MultivariateGaussian[1]4.70NICE[2]4.48DeepGMMs[3]4.00RealNVP[4]3.49PixelCNN[1]3.14GatedPixelCNN[5]3.03PixelRNN[1]3.00PixelCNN++[6]2.92Resultswithvariationallytrainedlatent-variablemodels:DeepDiffusion[7]5.40ConvolutionalDRAW[8]3.58ResNetVAEwithIAF[9]3.11ResNetVLAE3.04DenseNetVLAE2.95WealsoinvestigatelearninglossycodesonCIFAR10images.ToillustratehowdoesthereceptiveﬁeldsizeofPixelCNNdecoderinﬂuencepropertiesoflearnedlatentcodes,weshowvisualizationsofsimilarVLAEmodelswithreceptiveﬁeldsofdifferentsizes.Speciﬁcallywesayareceptiveﬁeld,xWindowAround(i),hassizeAxBwhenapixelxicandependontherectangleblockofsizeAxBimmediatelyontopofxiaswellasthe(cid:6)A−12(cid:7)pixelsimmediatelytotheleftofxi.WeusethisnotationtorefertodifferenttypesofPixelCNNdecodersinFigure3.From(a)-(c)inFigure3,wecanseethatlargerreceptiveﬁeldsprogressivelymakeautoregressivedecoderscapturemorestructuralinformation.In(a),asmallerreceptiveﬁeldtendstopreserveratherdetailedshapeinformationinthelossycodewhereasthelatentcodeonlyretainsroughshapein(c)withalargerreceptiveﬁeld.9PublishedasaconferencepaperatICLR2017(a)4x2(b)5x3(c)7x4(d)7x4GrayscaleFigure3:CIFAR10:Originaltest-setimages(left)and“decompressioned”versionsfromVLAE’slossycode(right)withdifferenttypesofreceptiveﬁeldsIt’sinterestingtoalsonotethatin(a)-(c),oftentimescolorinformationispartiallyomittedfromlatentcodesandoneexplanationcanbethatcolorisverypredictablelocally.However,colorinformationcanbeimportanttopreserveifourtaskis,forexample,objectclassiﬁcation.Todemonstratehowwecanencodecolorinformationinthelossycode,wecanchoosetomakePixelCNNdecoderdependonlyonimages’grayscaleversions.Inotherwords,insteadofchoos-ingthedecodertobeplocal(x|z)=Qip(xi|z,xWindowAround(i)),weuseadecoderoftheformplocal(x|z)=Qip(xi|z,Grayscale(xWindowAround(i))).In(d)ofFigure3,wevisualizelossycodesforaVLAEthathasthesamereceptiveﬁeldsizeas(c)butusesa“grayscalereceptiveﬁeld”.Wenotethatthelossycodesin(d)encoderoughlythesamestructuralinformationasthosein(c)butgenerallygenerateobjectsthataremorerecognizableduetothepreservationofcolorinformation.Thisservesasoneexampleofhowwecandesignthelossylatentcodecarefullytoencodewhat’simportantandwhat’snot.5RELATEDWORKWeinvestigateafusionbetweenvariationalautoencoderswithcontinuouslatentvariables(Kingma&Welling,2013;Rezendeetal.,2014)andneuralautoregressivemodels.Forautoregression,wespeciﬁcallyapplyanoveltypeofarchitecturewhereautoregressionisrealisedthroughacarefully10PublishedasaconferencepaperatICLR2017constructeddeepconvolutionalnetwork,introducedinthePixelCNNmodelforimages(vandenOordetal.,2016a,b).Thesefamilyofconvolutionalautoregressivemodelswasfurtherexplored,andextended,foraudioinWaveNet(Oordetal.,2016),videoinVideoPixelNetworks(Kalchbrenneretal.,2016b)andlanguageinByteNet(Kalchbrenneretal.,2016a).Thecombinationoflatentvariableswithexpressivedecoderwaspreviouslyexploredusingrecurrentnetworksmainlyinthecontextoflanguagemodeling(Chungetal.,2015;Bowmanetal.,2015;Serbanetal.,2016;Fraccaroetal.,2016;Xu&Sun,2016).Bowmanetal.(2015)hasalsoproposedtoweakenanotherwisetooexpressivedecoderbydropouttoforcesomeinformationintolatentcodes.Concurrentwithourwork,PixelVAE(Gulrajanietal.,2016)alsoexploredusingconditionalPixel-CNNasaVAE’sdecoderandhasobtainedimpressivedensitymodelingresultsthroughtheuseofmultiplelevelsofstochasticunits.UsingautoregressivemodelonlatentcodewasexploredinthecontextofdiscretelatentvariablesinDARN(Gregoretal.,2013).Kingmaetal.(2016),KaaeSønderbyetal.(2016),Gregoretal.(2016)andSalimans(2016)exploredVAEarchitecturewithanexplicitlydeepautoregressivepriorforcontinuouslatentvariables,buttheautoregressivedatalikelihoodisintractableinthosearchitecturesandneedstoinferredvariationally.Incontrast,weusemultiplestepsofautoregressiveﬂowsthathasexactlikelihoodandanalyzetheeffectofusingexpressivelatentcode.Optimizationchallengesforusing(alllevelsof)continuouslatentcodewerediscussedbeforeandpracticalsolutionswereproposed(Bowmanetal.,2015;KaaeSønderbyetal.,2016;Kingmaetal.,2016).Inthispaper,wepresentacomplementaryperspectiveonwhen/howshouldthelatentcodebeusedbyappealingtoaBits-BackinterpretationofVAE.LearningalossycompressorwithlatentvariablemodelhasbeeninvestigatedwithCon-vDRAW(Gregoretal.,2016).Itlearnsahierarchyoflatentvariablesandjustusinghigh-levellatentvariableswillresultinalossycompressionthatperformssimilarlytoJPEG.Ourmodelsimi-larlylearnsalossycompressorbutitusesanautoregressivemodeltoexplicitlycontrolwhatkindofinformationshouldbelostincompression.6CONCLUSIONInthispaper,weanalyzetheconditionunderwhichthelatentcodeinVAEshouldbeused,i.e.whendoesVAEautoencode,andusethisobservationtodesignaVAEmodelthat’salossycompressorofobserveddata.Atmodelinglevel,weproposetwocomplementaryimprovementstoVAEthatareshowntohavegoodempiricalperformance.VLAEhastheappealingpropertiesofcontrollablerepresentationlearningandimproveddensityestimationperformancebutthesepropertiescomeatacost:comparedwithVAEmodelsthathavesimpleprioranddecoder,VLAEissloweratgenerationduetothesequentialnatureofautoregres-sivemodel.Movingforward,webelieveit’sexcitingtoextendthisprincipleoflearninglossycodestootherformsofdata,inparticularthosethathaveatemporalaspectlikeaudioandvideo.Anotherpromis-ingdirectionistodesignrepresentationsthatcontainonlyinformationfordownstreamtasksandutilizethoserepresentationstoimprovesemi-supervisedlearning.REFERENCESYoshuaBengio,AaronCourville,andPascalVincent.Representationlearning:Areviewandnewperspectives.IEEEtransactionsonpatternanalysisandmachineintelligence,35(8):1798–1828,2013.J¨orgBornscheinandYoshuaBengio.Reweightedwake-sleep.arXivpreprintarXiv:1406.2751,2014.SamuelRBowman,LukeVilnis,OriolVinyals,AndrewMDai,RafalJozefowicz,andSamyBen-gio.Generatingsentencesfromacontinuousspace.arXivpreprintarXiv:1511.06349,2015.11PublishedasaconferencepaperatICLR2017YuriBurda,RogerGrosse,andRuslanSalakhutdinov.Importanceweightedautoencoders.arXivpreprintarXiv:1509.00519,2015a.YuriBurda,RogerBGrosse,andRuslanSalakhutdinov.Accurateandconservativeestimatesofmrflog-likelihoodusingreverseannealing.InAISTATS,2015b.KyungHyunCho,TapaniRaiko,andAlexanderTIhler.Enhancedgradientandadaptivelearningratefortrainingrestrictedboltzmannmachines.InProceedingsofthe28thInternationalConfer-enceonMachineLearning(ICML-11),pp.105–112,2011.JunyoungChung,KyleKastner,LaurentDinh,KratarthGoel,AaronCCourville,andYoshuaBen-gio.Arecurrentlatentvariablemodelforsequentialdata.InAdvancesinneuralinformationprocessingsystems,pp.2980–2988,2015.LaurentDinh,DavidKrueger,andYoshuaBengio.Nice:non-linearindependentcomponentsesti-mation.arXivpreprintarXiv:1410.8516,2014.LaurentDinh,JaschaSohl-Dickstein,andSamyBengio.DensityestimationusingRealNVP.arXivpreprintarXiv:1605.08803,2016.ChaoDu,JunZhu,andBoZhang.Learningdeepgenerativemodelswithdoublystochasticmcmc.arXivpreprintarXiv:1506.04557,2015.OttoFabiusandJoostRvanAmersfoort.Variationalrecurrentauto-encoders.arXivpreprintarXiv:1412.6581,2014.MarcoFraccaro,SørenKaaeSønderby,UlrichPaquet,andOleWinther.Sequentialneuralmodelswithstochasticlayers.arXivpreprintarXiv:1605.07571,2016.MathieuGermain,KarolGregor,IainMurray,andHugoLarochelle.Made:Maskedautoencoderfordistributionestimation.arXivpreprintarXiv:1502.03509,2015.MarcGoesslingandYaliAmit.Sparseautoregressivenetworks.arXivpreprintarXiv:1511.04776,2015.KarolGregor,AndriyMnih,andDaanWierstra.DeepAutoRegressiveNetworks.arXivpreprintarXiv:1310.8499,2013.KarolGregor,IvoDanihelka,AlexGraves,andDaanWierstra.DRAW:Arecurrentneuralnetworkforimagegeneration.arXivpreprintarXiv:1502.04623,2015.KarolGregor,FredericBesse,DaniloJimenezRezende,IvoDanihelka,andDaanWierstra.Towardsconceptualcompression.arXivpreprintarXiv:1604.08772,2016.IshaanGulrajani,KundanKumar,FarukAhmed,AdrienAliTaiga,FrancescoVisin,DavidVazquez,andAaronCourville.Pixelvae:Alatentvariablemodelfornaturalimages.arXivpreprintarXiv:1611.05013,2016.KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Identitymappingsindeepresidualnetworks.arXivpreprintarXiv:1603.05027,2016.GeoffreyEHintonandDrewVanCamp.Keepingtheneuralnetworkssimplebyminimizingthedescriptionlengthoftheweights.InProceedingsofthesixthannualconferenceonComputationallearningtheory,pp.5–13.ACM,1993.GeoffreyEHintonandRichardSZemel.Autoencoders,minimumdescriptionlength,andHelmholtzfreeenergy.Advancesinneuralinformationprocessingsystems,pp.3–3,1994.AnttiHonkelaandHarriValpola.Variationallearningandbits-backcoding:aninformation-theoreticviewtobayesianlearning.IEEETransactionsonNeuralNetworks,15(4):800–810,2004.GaoHuang,ZhuangLiu,KilianQWeinberger,andLaurensvanderMaaten.Denselyconnectedconvolutionalnetworks.arXivpreprintarXiv:1608.06993,2016.12PublishedasaconferencepaperatICLR2017CasperKaaeSønderby,TapaniRaiko,LarsMaaløe,SørenKaaeSønderby,andOleWinther.Howtotraindeepvariationalautoencodersandprobabilisticladdernetworks.arXivpreprintarXiv:1602.02282,2016.NalKalchbrenner,LasseEspheholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKorayKavukcuoglu.euralmachinetranslationinlineartime.arXivpreprintarXiv:1610.00527,2016a.NalKalchbrenner,AaronvandenOord,KarenSimonyan,IvoDanihelka,OriolVinyals,AlexGraves,andKorayKavukcuoglu.Videopixelnetworks.arXivpreprintarXiv:1610.00527,2016b.DiederikPKingmaandMaxWelling.Auto-encodingvariationalBayes.Proceedingsofthe2ndInternationalConferenceonLearningRepresentations,2013.DiederikPKingma,TimSalimans,andMaxWelling.Improvingvariationalinferencewithinverseautoregressiveﬂow.arXivpreprintarXiv:1606.04934,2016.BrendenMLake,RuslanRSalakhutdinov,andJoshTenenbaum.One-shotlearningbyinvertingacompositionalcausalprocess.InAdvancesinneuralinformationprocessingsystems,pp.2526–2534,2013.YannLeCun,L´eonBottou,YoshuaBengio,andPatrickHaffner.Gradient-basedlearningappliedtodocumentrecognition.ProceedingsoftheIEEE,86(11):2278–2324,1998.BenjaminMMarlin,KevinSwersky,BoChen,andNandodeFreitas.Inductiveprinciplesforrestrictedboltzmannmachinelearning.InAISTATS,pp.509–516,2010.AndriyMnihandKarolGregor.Neuralvariationalinferenceandlearninginbeliefnetworks.arXivpreprintarXiv:1402.0030,2014.VinodNair,JoshSusskind,andGeoffreyEHinton.Analysis-by-synthesisbylearningtoinvertgenerativeblackboxes.InInternationalConferenceonArtiﬁcialNeuralNetworks,pp.971–981.Springer,2008.AaronvandenOord,SanderDieleman,HeigaZen,KarenSimonyan,OriolVinyals,AlexGraves,NalKalchbrenner,AndrewSenior,andKorayKavukcuoglu.Wavenet:Agenerativemodelforrawaudio.arXivpreprintarXiv:1609.03499,2016.DaniloRezendeandShakirMohamed.Variationalinferencewithnormalizingﬂows.InProceedingsofThe32ndInternationalConferenceonMachineLearning,pp.1530–1538,2015.DaniloJRezende,ShakirMohamed,andDaanWierstra.Stochasticbackpropagationandapprox-imateinferenceindeepgenerativemodels.InProceedingsofthe31stInternationalConferenceonMachineLearning(ICML-14),pp.1278–1286,2014.JasonTylerRolfe.Discretevariationalautoencoders.arXivpreprintarXiv:1609.02200,2016.TimSalimans.Astructuredvariationalauto-encoderforlearningdeephierarchiesofsparsefeatures.arXivpreprintarXiv:1602.08734,2016.TimSalimans,DiederipP.Kingma,andMaxWelling.MarkovchainMonteCarloandvariationalinference:Bridgingthegap.arXivpreprintarXiv:1410.6460,2014.TimSalimans,AndrejKarpathy,XiChen,andDiederikPKingma.Pixelcnn++:Improvingthepixelcnnwithdiscretizedlogisticmixturelikelihoodandothermodiﬁcations.arXivpreprintarXiv:1701.05517,2017.IulianVladSerban,AlessandroSordoni,RyanLowe,LaurentCharlin,JoellePineau,AaronCourville,andYoshuaBengio.Ahierarchicallatentvariableencoder-decodermodelforgen-eratingdialogues.arXivpreprintarXiv:1605.06069,2016.JaschaSohl-Dickstein,EricAWeiss,NiruMaheswaranathan,andSuryaGanguli.Deepunsuper-visedlearningusingnonequilibriumthermodynamics.arXivpreprintarXiv:1503.03585,2015.DustinTran,RajeshRanganath,andDavidMBlei.Variationalgaussianprocess.arXivpreprintarXiv:1511.06499,2015.13PublishedasaconferencepaperatICLR2017RichardETurner,PietroBerkes,andManeeshSahani.Twoproblemswithvariationalexpectationmaximisationfortime-seriesmodels.InProceedingsoftheWorkshoponInferenceandEstima-tioninProbabilisticTime-SeriesModels,pp.107–115,2008.AaronvandenOordandBenjaminSchrauwen.Factoringvariationsinnaturalimageswithdeepgaussianmixturemodels.InAdvancesinNeuralInformationProcessingSystems,pp.3518–3526,2014.AaronvandenOord,NalKalchbrenner,andKorayKavukcuoglu.Pixelrecurrentneuralnetworks.arXivpreprintarXiv:1601.06759,2016a.AaronvandenOord,NalKalchbrenner,OriolVinyals,LasseEspeholt,AlexGraves,andKo-rayKavukcuoglu.Conditionalimagegenerationwithpixelcnndecoders.arXivpreprintarXiv:1606.05328,2016b.WeidiXuandHaozeSun.Semi-supervisedvariationalautoencodersforsequenceclassiﬁcation.arXivpreprintarXiv:1603.02514,2016.AlanYuilleandDanielKersten.Visionasbayesianinference:analysisbysynthesis?Trendsincognitivesciences,10(7):301–308,2006.BiaoZhang,DeyiXiong,andJinsongSu.Variationalneuralmachinetranslation.arXivpreprintarXiv:1605.07869,2016.14PublishedasaconferencepaperatICLR2017APPENDIXADETAILEDEXPERIMENTSETUPFORBINARYIMAGESForVAE’sencoderanddecoder,weusethesameResNet(Heetal.,2015)VAEarchitectureastheoneusedinIAFMNISTexperiment(Kingmaetal.,2016).Theonlydifferenceisthatthedecodernetworknow,insteadofoutputinga28x28x1spatialfeaturemaptospecifythemeanofafactorizedbernoullidistribution,outputsa28x28x4spatialfeaturemapthat’sconcatenatedwiththeoriginalbinaryimagechannel-wise,forminga28x28x5featuremapthat’sthenfedthroughatypicalmaskedPixelCNN(vandenOordetal.,2016a).AssucheventhoughthePixelCNNconditionsonthelatentcode,wedon’tcallitaConditionalPixelCNNbecauseitdoesn’tusethespeciﬁcarchitecturethatwasproposedinvandenOordetal.(2016b).ForthePixelCNN,ithas6maskedconvolutionlayerswith123x3ﬁltersorganizedinResNetblocks,andithas4additional1x1convolutionResNetblockbetweeneveryothermaskedconvolutionlayertoincreaseprocessingcapacitysinceitemploysfewermaskedconvolutionsthanusual.AllthemaskedconvolutionlayerhavetheirweightstiedtoreduceoverﬁttingonstaticallybinarizedMNIST,anduntyingtheweightswillincreaseperformanceforotherdatasets.Experimentsaretunedonthevalidationsetandthenﬁnalexperimentwasrunwithtrainandvalidationset,withperformanceevaluatedwithtestset.ExponentialLinearUnits(Clevertetal.,2015)areusedasactivationfunctionsinbothVAEnetworkandPixelCNNnetwork.Weightnormalizationiseverywherewithdata-dependentinitialization(Salimans&Kingma,2016).Alatentcodeofdimension64wasused.ForAFprior,it’simplementedwithMADE(Germainetal.,2015)asdetailedinKingmaetal.(2016).Weused4stepsofautoregressiveﬂowandeachﬂowisimplementedbya3-layerMADEthathas640hiddenunitsandusesRelu(Nair&Hinton,2010)asactivationfunctions.DifferingfromthepracticeofKingmaetal.(2016),weusemean-onlyautoregressiveﬂow,whichwefoundtobemorenumericallystable.Intermsoftraining,Adamax(Kingma&Ba,2014)wasusedwithalearningrateof0.002.0.01nats/data-dimfreebits(Kingmaetal.,2016)wasfoundtobeeffectiveindealingwiththeproblemofallthelatentcodebeingignoredearlyintraining.Polyakaveraging(Polyak&Juditsky,1992)wasusedtocomputetheﬁnalparameters,withα=0.998.AllexperimentsareimplementedusingTensorFlow(Abadietal.,2016).BADDITIONALEXPERIMENTSETUPFORCIFAR10Latentcodesarerepresentedby16featuremapsofsize8x8,andthischoiceofspatialstochas-ticunitsareinspiredbyResNetIAFVAE(Kingmaetal.,2016).PriordistributionisfactorizedGaussiannoisetransformedby6autoregressiveﬂows,eachofwhichisimplementedbyaPixel-CNN(vandenOordetal.,2016a)with2hiddenlayersand128featuremaps.Betweeneveryotherautoregressiveﬂow,theorderingofstochasticunitsisreversed.ResNetVLAEhasthefollowingstructureforencoder:2ResNetblocks,Convw/stride=2,2ResNetblocks,Convw/stride=2,3ResNetblocks,1x1convolutionandhasasymmetricdecoder.Channelsize=48for32x32featuremapsand96forotherfeaturemaps.DenseNetVLAEfollowsasimilarstructure:replacing2ResNetblockswithoneDenseNetblockof3stepsandeachstepproducesacertainnumberoffeaturemapssuchthatattheendofablock,theconcatenatedfeaturemapsisslightlymorethantheResNetVLAEatthesamestage.ConditionalPixelCNN++(Salimansetal.,2017)isusedasthedecoder.Speciﬁcallythechannel-autoregressivevariantisusedtoensurethereissufﬁcientcapacityevenwhenthereceptiveﬁeldissmall.Speciﬁcally,thedecoderPixelCNNhas4blocksof64featuremapswhereeachblockisconditionedonpreviousblockswithGatedResNetconnectionsandhencethePixelCNNdecodersweuseareshallowbutverywide.For4x2receptiveﬁeldexperiment,weuse1layerofverticalstackconvolutionsand2layersofhorizontalstackconvolutions;for5x3receptiveﬁeldexperiment,weuse2layersofverticalstackconvolutionsand2layersofhorizontalstackconvolutions;For5x3receptiveﬁeldexperiment,weuse2layersofverticalstackconvolutionsand2layersofhorizontalstackconvolutions;For7x4receptiveﬁeldexperiment,weuse3layersofverticalstackconvolutionsand3layersofhorizontalstackconvolutions;for7x4Grayscaleexperiment,wetransformRGB15PublishedasaconferencepaperatICLR2017imagesintogray-scaleimagesviathisspeciﬁctransformation:(0.299∗R)+(0.587G)+(0.114B).Bestdensityestimationresultisobtainedwith7x4receptiveﬁeldexperiments.CSOFTFREEBITS”Freebits”wasatechniqueproposedin(Kingmaetal.,2016)whereKgroupsofstochasticunitsareencouragedtobeusedthroughthefollowingsurrogateobjective:eLλ=Ex∼M(cid:2)Eq(z|x)[logp(x|z)](cid:3)−KXj=1maximum(λ,Ex∼M[DKL(q(zj|x)||p(zj))])Thistechniqueiseasytousesinceit’susuallyeasytodeterminetheminimumnumberofbits/nats,λ,stochasticunitsneedtoencode.ChoosingλishenceeasierthansettingaﬁxedKLannealingschedule(Serbanetal.,2016).Ontheotherhand,Klannealinghasthebeneﬁtofthesurrogateobjectivewillsmoothlybecomethetrueobjective,thevariationallowerboundwhereas”freebits”hasasharptransitionattheboundary.Therefore,weproposetostilluseλashyperparametertospecifyatleastλnatsshouldbeusedbuttrytochangetheoptimizationobjectiveasslowlyaspossible:LSoftFreeBits(x;θ)=Eq(z|x)[logp(x|z)]−γDKL(q(z|x)||p(z))where0<γ≤1.Andwemaketheoptimizationsmootherbychangingγslowlyonlinetomakesureatleastλnatsareused:whenKlistoomuchhigherthanλ(weexperimentedwiderangeofthresholdsfrom3%to30%,allofwhichyieldimprovedresults,andwetendtouse5%usathreshold),γisincreased,andwhenKllowerthanλ,γisdecreasedtoencourageinformationﬂow.Wefounditsufﬁcienttoincrease/decreaseat10%incrementanddidn’tfurthertunethisparameter.DAUTOREGRESSIVEDECODERWITHOUTAUTOREGRESSIVEPRIORInthissection,weinvestigatethescenarioofjustusinganautoregressivedecoderwithoutusinganautoregressiveprior.Wecomparetheexactsamemodelinthreeconﬁgurations:1)usingsmall-receptive-ﬁeldPixelCNNasanunconditionaldensityestimator;2)usingsmall-receptive-ﬁeldasadecoderinaVAEwithGaussianlatentvariables;3)replacingGaussianlatentvariableswithautoregressiveﬂowlatentvariablesin2).Table1:AblationonDynamicallybinarizedMNISTModelNLLTestKLUnconditionalPixelCNN87.550PixelCNNDecoder+GaussianPrior79.4810.60PixelCNNDecoder+AFPrior78.9411.73InTable1,wecanobservethateachstepofmodiﬁcationimprovesdensityestimationperformance.Inaddition,usinganautoregressivelatentcodemakesthelatentcodetransmitmoreinformationasshowninthedifferenceofE[DKL(q(z|x)||p(z))].ECIFAR10GENERATEDSAMPLESREFERENCESMartınAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,ZhifengChen,CraigCitro,GregSCorrado,AndyDavis,JeffreyDean,MatthieuDevin,etal.Tensorﬂow:Large-scalemachinelearningonheterogeneousdistributedsystems.arXivpreprintarXiv:1603.04467,2016.16PublishedasaconferencepaperatICLR2017(a)4x2@3.12bits/dim(b)7x4@2.95bits/dimFigure4:CIFAR10:GeneratedsamplesfordifferentmodelsDjork-Arn´eClevert,ThomasUnterthiner,andSeppHochreiter.FastandaccuratedeepnetworklearningbyExponentialLinearUnits(ELUs).arXivpreprintarXiv:1511.07289,2015.MathieuGermain,KarolGregor,IainMurray,andHugoLarochelle.Made:Maskedautoencoderfordistributionestimation.arXivpreprintarXiv:1502.03509,2015.KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimagerecog-nition.arXivpreprintarXiv:1512.03385,2015.DiederikKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.DiederikPKingma,TimSalimans,andMaxWelling.Improvingvariationalinferencewithinverseautoregressiveﬂow.arXivpreprintarXiv:1606.04934,2016.VinodNairandGeoffreyEHinton.Rectiﬁedlinearunitsimproverestrictedboltzmannmachines.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10),pp.807–814,2010.BorisTPolyakandAnatoliBJuditsky.Accelerationofstochasticapproximationbyaveraging.SIAMJournalonControlandOptimization,30(4):838–855,1992.TimSalimansandDiederikPKingma.Weightnormalization:Asimplereparameterizationtoac-celeratetrainingofdeepneuralnetworks.arXivpreprintarXiv:1602.07868,2016.TimSalimans,AndrejKarpathy,XiChen,andDiederikPKingma.Pixelcnn++:Improvingthepixelcnnwithdiscretizedlogisticmixturelikelihoodandothermodiﬁcations.arXivpreprintarXiv:1701.05517,2017.IulianVladSerban,AlessandroSordoni,RyanLowe,LaurentCharlin,JoellePineau,AaronCourville,andYoshuaBengio.Ahierarchicallatentvariableencoder-decodermodelforgen-eratingdialogues.arXivpreprintarXiv:1605.06069,2016.AaronvandenOord,NalKalchbrenner,andKorayKavukcuoglu.Pixelrecurrentneuralnetworks.arXivpreprintarXiv:1601.06759,2016a.AaronvandenOord,NalKalchbrenner,OriolVinyals,LasseEspeholt,AlexGraves,andKo-rayKavukcuoglu.Conditionalimagegenerationwithpixelcnndecoders.arXivpreprintarXiv:1606.05328,2016b.17
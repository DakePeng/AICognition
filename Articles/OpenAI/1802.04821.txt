8
1
0
2

r
p
A
9
2

]

G
L
.
s
c
[

2
v
1
2
8
4
0
.
2
0
8
1
:
v
i
X
r
a

EvolvedPolicyGradientsReinHouthooft1RichardY.Chen1PhillipIsola123BradlyC.Stadie2FilipWolski1JonathanHo12PieterAbbeel12AbstractWeproposeametalearningapproachforlearninggradient-basedreinforcementlearning(RL)algo-rithms.Theideaistoevolveadifferentiablelossfunction,suchthatanagent,whichoptimizesitspolicytominimizethisloss,willachievehighre-wards.Thelossisparametrizedviatemporalcon-volutionsovertheagent’sexperience.Becausethislossishighlyﬂexibleinitsabilitytotakeintoaccounttheagent’shistory,itenablesfasttasklearning.Empiricalresultsshowthatourevolvedpolicygradientalgorithm(EPG)achievesfasterlearningonseveralrandomizedenviron-mentscomparedtoanoff-the-shelfpolicygra-dientmethod.WealsodemonstratethatEPG’slearnedlosscangeneralizetoout-of-distributiontesttimetasks,andexhibitsqualitativelydifferentbehaviorfromotherpopularmetalearningalgo-rithms.1.IntroductionWhenahumanlearnstosolveanewcontroltask,suchasplayingtheviolin,theyimmediatelyhaveafeelforwhattotry.Atﬁrst,theymaytryaquick,roughstroke,and,produc-ingascreech,willintuitivelyknowthiswasthewrongthingtodo.Justbylisteningtothesoundstheyproduce,theywillhaveasenseofwhetherornottheyaremakingprogresstowardthegoal.Effectively,humanshaveaccesstoverywellshapedinternalrewardfunctions,derivedfrompriorexperienceonothermotortasks,orperhapsfromlisteningtoandplayingothermusicalinstruments(36;49).Incontrast,mostcurrentreinforcementlearning(RL)agentsapproacheachnewtaskdenovo.Initially,theyhavenonotionofwhatactionstotryout,norwhichoutcomesaredesirable.Instead,theyrelyentirelyonexternalrewardsignalstoguidetheirinitialbehavior.Comingfromsuchablankslate,itisnosurprisethatRLagentstakefarlongerthanhumanstolearnsimpleskills(21).1OpenAI2UCBerkeley3MIT.Correspondenceto:ReinHouthooft<rein.houthooft@openai.com>.Figure1:High-leveloverviewofourapproach.Themethodconsistsofaninnerandouteroptimizationloop.Theinnerloop(boxed)optimizestheagent’spolicyagainstalossprovidedbytheouterloop,usinggradientdescent.Theouterloopoptimizestheparametersofthelossfunction,suchthattheoptimizedinner-looppolicyachieveshighperformanceonanarbitrarytask,suchassolvingacontroltaskofinterest.TheevolvedlossLcanbeviewedasasurrogatewhosegradientisusedtoupdatethepolicy,whichissimilarinspirittopolicygradients,lendingthename“evolvedpolicygradients".Ouraiminthispaperistodeviseagentsthathaveapriornotionofwhatconstitutesmakingprogressonanoveltask.Ratherthanencodingthisknowledgeexplicitlythroughalearnedbehavioralpolicy,weencodeitimplicitlythroughalearnedlossfunction.Theendgoalisagentsthatcanusethislossfunctiontolearnquicklyonanoveltask.Thisapproachcanbeseenasaformofmetalearning,inwhichwelearnalearningalgorithm.Ratherthanminingrulesthatgeneralizeacrossdatapoints,asintraditionalma-chinelearning,metalearningconcernsitselfwithdevisingalgorithmsthatgeneralizeacrosstasks,byinfusingpriorknowledgeofthetaskdistribution(12).Ourmethodconsistsoftwooptimizationloops.Intheinnerloop,anagentlearnstosolveatask,sampledfromapar-ticulardistributionoverafamilyoftasks.Theagentlearnstosolvethistaskbyminimizingalossfunctionprovidedbytheouterloop.Intheouterloop,theparametersofthelossfunctionareadjustedsoastomaximizetheﬁnalre-turnsachievedafterinnerlooplearning.Figure1providesahigh-leveloverviewofthisapproach.Althoughtheinnerloopcanbeoptimizedwithstochasticgradientdescent(SGD),optimizingtheouterlooppresents 
 
 
 
 
 
EvolvedPolicyGradientssubstantialdifﬁculty.Eachevaluationoftheouterobjectiverequirestrainingacompleteinner-loopagent,andthisob-jectivecannotbewrittenasanexplicitfunctionofthelossparametersweareoptimizingover.Duetothelackofeasilyexploitablestructureinthisoptimizationproblem,weturntoevolutionstrategies(ES)(35;46;16;37)asablackboxoptimizer.TheevolvedlossLcanbeviewedasasurrogateloss(43;44)whosegradientisusedtoupdatethepolicy,whichissimilarinspirittopolicygradients,lendingthename“evolvedpolicygradients".Inadditiontoencodingpriorknowledge,thelearnedlossoffersseveraladvantagescomparedtocurrentRLmethods.SinceRLmethodsoptimizeforshort-termreturnsinsteadofaccountingforthecompletelearningprocess,theymaygetstuckinlocalminimaandfailtoexplorethefullsearchspace.Priorworksaddauxiliaryrewardtermsthatempha-sizeexploration(8;19;32;56;6;33)andentropylossterms(31;42;15;26).Thesetermsareoftentradedoffusingaseparatehyperparameterthatisnotonlytask-dependent,butalsodependentonwhichpartofthestatespacetheagentisvisiting.Assuch,itisunclearhowtoincludethesetermsintotheRLalgorithminaprincipledway.UsingEStoevolvethelossfunctionallowsustooptimizethetrueobjective,namelytheﬁnaltrainedpolicyperfor-mance,ratherthanshort-termreturns.OurmethodalsoimprovesonstandardRLalgorithmsbyallowingthelossfunctiontobeadaptivetotheenvironmentandagenthis-tory,leadingtofasterlearningandthepotentialforlearningwithoutexternalrewards.EPGcanintheorybecombinedwithpolicyinitializationmetalearningalgorithms,suchasMAML(11),sinceEPGimposesnorestrictiononthepolicyitoptimizes.Therehasbeenaﬂurryofrecentworkonmetalearningpolicies,e.g.,(10;59;11;25),anditisworthaskingwhymetalearnthelossasopposedtodirectlymetalearningthepolicy?Ourmotivationisthatweexpectlossfunctionstobethekindofobjectthatmaygeneralizeverywellacrosssubstantiallydifferenttasks.Thisiscertainlytrueofhand-engineeredlossfunctions:awell-designedRLlossfunction,suchasthatin(45),canbeverygenericallyapplicable,ﬁndinguseinproblemsrangingfromplayingAtarigamestocontrollingrobots(45).InSection4,weﬁndevidencethatalosslearnedbyEPGcantrainanagenttosolveataskoutsidethedistributionoftasksonwhichEPGwastrained.ThisgeneralizationbehaviordiffersqualitativelyfromMAML(11)andRL2(10),methodsthatdirectlymetalearnpolicies,providinginitialindicationofthegeneralizationpotentialoflosslearning.Ourcontributionsincludethefollowing:•Formulatingametalearningapproachthatlearnsadif-ferentiablelossfunctionforRLagents,calledEPG.•OptimizingtheparametersofthislossfunctionviaES,overcomingthechallengethatﬁnalreturnsarenotexplicitfunctionsofthelossparameters.•Designingalossarchitecturethattakesintoaccountagenthistoryviatemporalconvolutions.•DemonstratingthatEPGproducesalearnedlossthatcantrainagentsfasterthananoff-the-shelfpolicygra-dientmethod.•ShowingthatEPG’slearnedlosscangeneralizetoout-of-distributiontesttimetasks,exhibitingqualitativelydifferentbehaviorfromotherpopularmetalearningalgorithms.WesetforththenotationinSection2.Section3explainsthemainalgorithmandSection4showsitsresultsonseveralrandomizedcontinuouscontrolenvironments.InSection5,wecompareourmethodswiththemostrelatedideasinliterature.WeconcludethispaperwithadiscussioninSection6.AnimplementationofEPGisavailableathttp://github.com/openai/EPG.2.NotationandBackgroundWemodelreinforcementlearning(54)asaMarkovdecisionprocess(MDP),deﬁnedasthetupleM=(S,A,T,R,p0,γ),whereSandAarethestateandac-tionspace.ThetransitiondynamicT:S×A×S7→R+determinesthedistributionofthenextstatest+1giventhecurrentstatestandtheactionat.R:S×A7→Ristherewardfunctionandγ∈(0,1)isadiscountfactor.p0isthedistributionoftheinitialstates0.Anagent’spolicyπ:S7→Ageneratesanactionafterobservingastate.Anepisodeτ∼MwithhorizonHisasequence(s0,a0,r0,...,sH,aH,rH)ofstate,action,andrewardateachtimestept.ThediscountedepisodicreturnofτisdeﬁnedasRτ=PHt=0γtrt,whichdependsontheinitialstatedistributionp0,theagent’spolicyπ,andthetransitiondistributionT.Theexpectedepisodicreturngivenagent’spolicyπisEπ[Rτ].Theoptimalpolicyπ∗maximizestheexpectedepisodicreturnπ∗=argmaxπEτ∼M,π[Rτ].Inhigh-dimensionalreinforcementlearningsettings,thepolicyπisoftenparametrizedusingadeepneuralnetworkπθwithparametersθ.Thegoalistosolveforθ∗thatattainsthehighestexpectedepisodicreturnθ∗=argmaxθEτ∼M,πθ[Rτ].(1)EvolvedPolicyGradientsThisobjectivecanbeoptimizedviapolicygradientmethods(60;55)bysteppinginthedirectionofE[Rτ∇logπ(τ)].Thisgradientcanbetransformedintoasurrogatelossfunc-tion(43;44)Lpg=E[Rτlogπ(τ)]=E"RτHXt=0logπ(at|st)#,(2)suchthatthegradientofLpgequalsthepolicygradient.Throughvariancereductiontechniquesincludingactor-criticalgorithms(20),thelossfunctionLpgisoftenchangedintoLac=E(cid:20)XHt=0A(st,at)logπ(at|st)(cid:21),(3)thatis,thelog-probabilityoftakingactionatatstatestismultipliedbyanadvantagefunctionA(st,at)(4).However,thisprocedureremainslimitedsinceitreliesonaparticularformofdiscountingthereturns,andtakingaﬁxedgradientstepwithrespecttothepolicy.Ourapproachlearnsalossratherthanusingahand-deﬁnedfunctionsuchasLac.Thus,itmaybeabletodiscovermoreeffectivesurrogatesformakingfastprogresstowardtheultimateobjectiveofmaximizingﬁnalreturns.3.MethodologyOurmetalearningapproachaimstolearnalossfunctionLφthatoutperformstheusualpolicygradientsurrogateloss(43).Thislossfunctionconsistsoftemporalconvolutionsovertheagent’srecenthistory.Inadditiontointernalizingenvironmentrewards,thislosscould,inprinciple,havesev-eralotherpositiveeffects.Forexample,byexaminingtheagent’shistory,thelosscouldincentivizedesirableextendedbehaviors,suchasexploration.Further,thelosscouldper-formaformofsystemidentiﬁcation,inferringenvironmentparametersandadaptinghowitguidestheagentasafunc-tionoftheseparameters(e.g.,byadjustingtheeffectivelearningrateoftheagent).ThelossfunctionparametersφareevolvedthroughESandthelosstrainsanagent’spolicyπθinanon-policyfashionviastochasticgradientdescent.3.1.MetalearningObjectiveInourmetalearningsetup,weassumeaccesstoadistribu-tionp(M)overMDPs.GivenasampledMDPM,theinnerloopoptimizationproblemistominimizethelossLφwithrespecttotheagent’spolicyπθ:θ∗=argminθEτ∼M,πθ[Lφ(πθ,τ)].(4)NotethatthisissimilartotheusualRLobjectives(Eqs.(1)(2)(3)),exceptthatweareoptimizingalearnedlossAlgorithm1:EvolvedPolicyGradients(EPG)1[OuterLoop]forepoche=1,...,Edo2Sample(cid:15)v∼N(0,I)andcalculatethelossparameterφ+σ(cid:15)vforv=1,...,V3Eachworkerw=1,...,WgetsassignednoisevectordwV/Weas(cid:15)w4foreachworkerw=1,...,Wdo5SampleMDPMw∼p(M)6InitializebufferwithNzerotuples7Initializepolicyparameterθrandomly8[InnerLoop]forstept=1,...,Udo9Sampleinitialstatest∼p0ifMwneedstobereset10Sampleactionat∼πθ(·|st)11TakeactionatinMwandreceivert,st+1,andterminationﬂagdt12Addtuple(st,at,rt,dt)tobuffer13iftmodM=0then14Withlossparameterφ+σ(cid:15)w,calculatelossesLiforstepsi=t−M,...,tusingbuffertuplesi−N,...,i15SampleminibatchesmbfromlastMstepsshufﬂed,computeLmb=Pj∈mbLj,andupdatethepolicyparameterθandmemoryparameter(Eq.(6))16InMw,usingthetrainedpolicyπθ,sampleseveraltrajectoriesandcomputethemeanreturnRw17Updatethelossparameterφ(Eq.(7))18Output:LossLφthattrainsπfromscratchaccordingtotheinnerloopscheme,onMDPsfromp(M)LφratherthandirectlyoptimizingtheexpectedepisodicreturnEM,πθ[Rτ]orothersurrogatelosses.TheouterloopobjectiveistolearnLφsuchthatanagent’spolicyπθ∗trainedwiththelossfunctionachieveshighexpectedreturnsintheMDPdistribution:φ∗=argmaxφEM∼p(M)Eτ∼M,πθ∗[Rτ].(5)3.2.AlgorithmTheﬁnalepisodicreturnRτofatrainedpolicyπθ∗cannotberepresentedasanexplicitfunctionofthelossfunctionLφ.Thuswecannotusegradient-basedmethodstodirectlysolveEq.(5).Ourapproach,summarizedinAlgorithm1,reliesonevolutionstrategies(ES)tooptimizethelossfunctionintheouterloop.AsdescribedbySalimansetal.(37),EScomputesthegra-EvolvedPolicyGradientsAlgorithm2:EPGtest-timetraining1[Input]:learnedlossfunctionLφfromEPG,MDPM2InitializebufferwithNzerotuples3Initializepolicyparameterθrandomly4forstept=1,...,Udo5Sampleinitialstatest∼p0ifMneedstobereset6Sampleactionat∼πθ(·|st)7TakeactionatinM,receivert,st+1,andterminationﬂagdt8Addtuple(st,at,rt,dt)tobuffer9iftmodM=0then10CalculatelossesLiforstepsi=t−M,...,tusingbuffertuplesi−N,...,i11SampleminibatchesmbfromlastMstepsshufﬂed,computeLmb=Pj∈mbLj,andupdatethepolicyparameterθandmemoryparameter(Eq.(6))12[Output]:AtrainedpolicyπθforMDPMdientofafunctionF(φ)accordingto∇φE(cid:15)∼N(0,I)F(φ+σ(cid:15))=1σE(cid:15)∼N(0,I)F(φ+σ(cid:15))(cid:15).Similarformulationsalsoappearinpriorworksin-cluding(52;47;27).Inourcase,F(φ)=EM∼p(M)Eτ∼M,πθ∗[Rτ](Eq.(5)).Notethatthedepen-denceonφcomesthroughθ∗(Eq.(4)).Stepbystep,thealgorithmworksasfollows.Atthestartofeachepochintheouterloop,forWinner-loopwork-ers,wegenerateVstandardmultivariatenormalvectors(cid:15)v∈N(0,I)withthesamedimensionasthelossfunctionparameterφ,assignedtoVsetsofW/Vworkers.Assuch,forthew-thworker,theouterloopassignsthedwV/We-thperturbedlossfunctionLw=Lφ+σ(cid:15)vwherev=dwV/Wewithperturbedparametersφ+σ(cid:15)vandσasthestandarddeviation.GivenalossfunctionLw,w∈{1,...,W},fromtheouterloop,eachinner-loopworkerwsamplesarandomMDPfromthetaskdistribution,Mw∼p(M).TheworkerthentrainsapolicyπθinMwoverUstepsofexperience.Wheneveraterminationsignalisreached,theenvironmentresetswithstates0sampledfromtheinitialstatedistributionp0(Mw).EveryMstepsthepolicyisupdatedthroughSGDonthelossfunctionLw,usingminibatchessampledfromthestepst−M,...,t:θ←θ−δin·∇θLw(cid:0)πθ,τt−M,...,t(cid:1).(6)Attheendoftheinner-looptraining,eachworkerreturnstheﬁnalreturnRw1totheouterloop.Theouter-loopaggregatestheﬁnalreturns{Rw}Ww=1fromallworkersandupdatesthelossfunctionparameterφasfollows:φ←φ+δout·1VσXVv=1F(φ+σ(cid:15)v)(cid:15)v,(7)whereF(φ+σ(cid:15)v)=R(v−1)∗W/V+1+···+Rv∗W/VW/V.Asaresult,eachperturbedlossfunctionLvisevaluatedonW/VrandomlysampledMDPsfromthetaskdistributionusingtheﬁnalreturns.Thisachievesvariancereductionbypreventingtheouter-loopESupdatefrompromotinglossfunctionsthatareassignedtoMDPsthatconsistentlygen-eratehigherreturns.Notethattheactualimplementationcalculateseachlossfunction’srelativerankfortheESup-date.Algorithm1outputsalearnedlossfunctionLφafterEepochsofESupdates.Attesttime,weevaluatethelearnedlossfunctionLφpro-ducedbyAlgorithm1onatestMDPMbytrainingapolicyfromscratch.Thetest-timetrainingscheduleisthesameastheinnerloopofAlgorithm1andwesummarizeitinAlgorithm2.3.3.ArchitectureTheagentisparametrizedusinganMLPpolicywithobser-vationspaceSandactionspaceA.Thelosshasamemoryunittoassistlearningintheinnerloop.Thismemoryunitisasingle-layerneuralnetworktowhichaninvariableinputvectorofonesisfed.Assuch,itisessentiallyalayerofbiasterms.Sincethisnetworkhasaconstantinputvector,wecanviewitsweightsasaverysimpleformofmemorytowhichthelosscanwriteviaemittingtherightgradientsignals.Anexperiencebufferstorestheagent’sNmostrecentexpe-riencesteps,intheformofalistoftuples(st,at,rt,dt),withdtthetrajectoryterminationﬂag.Sincethisbufferislimitedinthenumberofstepsitstores,thememoryunitmightallowthelossfunctiontostoreinformationoveralongerperiodoftime.ThelossfunctionLφconsistsoftemporalconvolutionallayerswhichgenerateacontextvectorfcontext,anddenselayers,whichoutputtheloss.ThearchitectureisdepictedinFigure2.Atstept,thedenselayersoutputthelossLtbytakingabatchofMsequentialsamples{si,ai,di,mem,fcontext,πθ(·|si)}ti=t−M,(8)whereM<Nandweaugmenteachtransitionwiththememoryoutputmem,acontextvectorfcontextgenerated1Morespeciﬁcally,theaveragereturnover3sampledtrajecto-riesusingtheﬁnalpolicyforworkerw.EvolvedPolicyGradientsFigure2:ArchitectureofalosscomputedfortimesteptwithinabatchofMsequentialsamples(fromt−Mtot),usingtemporalconvolutionsoverabufferofsizeN(fromt−Ntot),withM≤N:densenetonthebottomisthepolicyπ(s),takingasinputtheobservations(orange),whileoutputtingactionprobabilities(green).Thegreenblockonthetoprepresentsthelossoutput.Grayblocksareevolved,yellowblocksareupdatedthroughSGD.fromtheloss’stemporalconvolutionallayers,andthepolicydistributionπθ(·|si).Incontinuousactionspace,πθisaGaussianpolicy,i.e.,πθ(·|si)=N(·;µ(si;θ0),Σ),withµ(si;θ0)theMLPoutputandΣalearnableparametervec-tor.Thepolicyparametervectorisdeﬁnedasθ=[θ0,Σ].Togeneratethecontextvector,weﬁrstaugmenteachtransi-tioninthebufferwiththeoutputofthememoryunitmemandthepolicydistributionπθ(·|si)toobtainaset{si,ai,di,mem,πθ(·|si)}ti=t−N.(9)Westacktheseitemssequentiallyintoamatrixandthetemporalconvolutionallayerstakeitasinputandoutputthecontextvectorfcontext.Thememoryunit’sparametersareupdatedviagradientdescentateachinner-loopupdate(Eq.(6)).Notethatboththetemporalconvolutionlayersandthedenselayersdonotobservetheenvironmentrewardsdi-rectly.However,incaseswheretherewardcannotbefullyinferredfromtheenvironment,suchastheDirectionalHop-perenvironmentwewillexamineinSection4.2,weaddrewardsritothesetofinputsinEqs.(8)and(9).Infact,anyinformationthatcanbeobtainedfromtheenvironmentcouldbeaddedasaninputtothelossfunction,e.g.,ex-plorationsignals,thecurrenttimestepnumber,etc,andweleavefurthersuchextensionsasfuturework.Inpractice,tobootstrapthelearningprocess,weaddtoLφaguidancepolicygradientsurrogatelosssignalLpg,suchastheREINFORCE(60)orPPO(45)surrogatelossfunction,makingthetotallossˆLφ=(1−α)Lφ+αLpg,(10)andannealαfrom1to0overaﬁnitenumberofouter-loopepochs.Assuch,learningisﬁrstderivedmostlyfromthewell-structuredLpg,whileovertimeLφtakesoveranddriveslearningcompletelyafterαhasbeenannealedto0.4.ExperimentsWeapplyourmethodtoseveralrandomizedcontinuouscon-trolMuJoCoenvironments(5;34;9),namelyRandomHop-perandRandomWalker(withrandomizedgravity,friction,bodymass,andlinkthickness),RandomReacher(withran-domizedlinklengths),DirectionalHopperandDirectional-HalfCheetah(withrandomizedforward/backwardrewardfunction),GoalAnt(rewardfunctionbasedontherandom-izedtargetlocation),andFetch(randomizedtargetlocation).WedescribetheseenvironmentsindetailinAppendixA.Theseenvironmentsarechosenbecausetheyrequiretheagenttoidentifyarandomlysampledenvironmentattesttimeviaexploratorybehavior.ExamplesoftherandomizedHopperenvironmentsareshowninFigure4andtheFetchenvironmentinFigure3.Theplotsinthissectionshowthemeanvalueof20test-timetrainingcurvesasasolidline,whiletheshadedarearepresentstheinterquartilerange.Thedottedlinesplot5randomlysampledcurves.Figure3:ExamplesoflearningtoreachrandomtargetsintheFetchenvironmentImplementationdetailsInourexperiments,thetemporalconvolutionallayersofthelossfunctionhas3layers.Theﬁrstlayerhasakernelsizeof8,strideof7,andoutputs10channels.Thesecondlayerhasakernelof4,strideof2,andoutputs10channels.Thethirdlayerisfully-connectedwith32outputunits.LeakyReLUactivationisappliedtoeachconvolutionallayer.Thefully-connectedcomponentEvolvedPolicyGradientsFigure4:Exampleoflearningtohopforwardfromaran-domlyinitializedpolicyinRandomHopperenvironmentswithrandomizedmorphologyandphysicsparameters.Eachrowisadifferentenvironmentrandomization,whilefromlefttoright,trajectoriesarerecordedaslearningprogresses.takesasinputthetrajectoryfeaturesfromtheconvolutionalcomponentconcatenatedwithstate,action,terminationsig-nal,andpolicyoutput,aswellasrewardinexperimentsinwhichrewardisobserved.Ithas1hiddenlayerwith16hiddenunitsandleakyReLUactivation,followedbyanoutputlayer.ThebuffersizeisN∈{512,1024}.Theagent’sMLPpolicyhas2hiddenlayersof64unitswithtanhactivation.Thememoryunitisa32-unitsinglelayerwithtanhactivation.WeuseW=256inner-loopworkersinAlgorithm1,com-binedwithV=64ESnoisevectors.Thelossfunctionisevolvedover5000epochs,withα,asinEq.(10),an-nealedlinearlyfrom1to0overtheﬁrst500epochs.Theoff-the-shelfPGalgorithm(PPO)wasmoderatelytunedtoperformwellonthesetasks,however,itisimportanttokeepinmindthatthesemethodsinherentlyhavetroubleopti-mizingwhenthenumberofsamplesdrawnforeachpolicyupdatebatchislow.EPG’sinnerloopupdatefrequencyissettoM∈{32,64,128}andtheinnerlooplengthisU∈{64×M,128×M,256×M,512×M}.AteveryEPGinnerloopupdate,thepolicyandmemoryparame-tersareupdatedbythelearnedlossfunctionusingshufﬂedminibatchesofsize32withineachsetofMmostrecenttransitionstepsinthereplaybuffer,goingovereachstepexactlyonce.WetabulatethehyperparametersforeachrandomizedenvironmentinTable1inAppendixC.Normalizationaccordingtoarunningmeanandstandarddeviationwereappliedtotheobservations,actions,andrewardsforeachEPGinnerloopworkerindependently(Al-gorithm1)andfortest-timetraining(Algorithm2).Adam(18)isusedfortheEPGinnerloopoptimizationandtest-timetrainingwithβ1=0.9andβ2=0.999,whiletheouterloopESgradientsaremodiﬁedbyAdamwithβ1=0andβ2=0.999(whichmeansmomentumhasbeenturnedoff)beforeupdatingthelossfunction.Furthermore,L2-regularizationoverthelossfunctionparameterswithcoef-ﬁcient0.001isaddedtoouterloopobjective.Theinnerloopstepsizeisﬁxedto10−3,whiletheouterloopstepsizeisannealedlinearlyfrom10−2to10−3overtheﬁrst2000epochs.4.1.PerformanceWecomparetest-timetrainingperformanceusingtheEPGlossfunction,Algorithm2,againstanoff-the-shelfpolicygradientmethod,PPO(45).Figures5,6,7,and11showlearningcurvesforthesetwomethodsontheRandomHop-per,RandomWalker,RandomReacher,andFetchenviron-mentsrespectivelyattesttime.Thetopplotshowstheepisodicreturnw.r.t.thenumberofenvironmentstepstakensofar.ThebottomplotshowshowmuchthepolicychangesateveryupdatebyplottingtheKL-divergencebetweenthepolicydistributionsbeforeandaftereveryupdate,w.r.t.thenumberofupdatessofar.Inalloftheseenvironments,thePPOagentlearnsbyobserv-ingrewardsignalswhereasattesttime,theEPGagentdoesnotobserverewards(notethatattesttime,αinEq.(10)equals0).ObservingrewardsisnotneededinEPG,sinceanypieceofinformationtheagentencountersformsaninputtotheEPGlossfunction.Aslongastheagentcanidentifywhichtasktosolvewithinthedistribution,itdoesnotmatterwhetherthisidentiﬁcationisdonethroughobservationsorrewards.Keepinmind,however,thattherewardswereusedintheESobjectivefunctionduringtheEPGevolutionphase.Inallexperiments,EPGagentslearnmorequicklyandobtainhigherreturnscomparedtoPPOagents,asexpected,sincetheEPGlossfunctionisabletotailoritselftotheenvironmentdistributionitismetatrainedon.Thisindicatesthatourmethodgeneratesanobjectivethatismoreeffectiveattrainingagents,withinthesetaskdistributions,thananoff-the-shelfon-policypolicygradientmethod.Thisistrueeventhoughthelearnedlossdoesnotobserverewardsattesttime.ThisdemonstratesthepotentialtouseEPGwhenrewardsareonlyavailableattrainingtime,forexample,ifasystemweretrainedinsimulationbutdeployedintherealworldwhererewardsignalsarehardtomeasure.ThecorrelationbetweenthegradientsofourlearnedlossandthePPOobjectiveisaroundρ=0.5(Spearman’srankcorrelationcoefﬁcient)fortheenvironmentstested.Thisindicatesthatthegradientsproducedbythelearnedlossarerelatedto,butdifferentfrom,thoseproducedbythePPOobjective.EvolvedPolicyGradientsFigure5:RandomHoppertest-timetrainingover128(policyupdates)×64(updatefrequency)=8196timesteps:PPOvsno-rewardEPGFigure6:RandomWalkertest-timetrainingover256(policyupdates)×128(updatefrequency)=32768timesteps:PPOvsno-rewardEPGFigure7:RandomReachertest-timetrainingover512(policyupdates)×128(updatefrequency)=65536timesteps:PGvsno-rewardEPG.Figure8:DirectionalHopperenvironment:eachHopperenvironmentrandomlydecideswhethertorewardforwardorbackwardhopping.Theagentneedstoidentifywhethertojumpforwardorbackwards:PPOvsEPG.Herewecanclearlyseeexploratorybehavior,indicatedbythenegativespikesintherewardcurve,thelossforcesthepolicytotryoutbackwardsbehavior.Eachsubplotcolumncorrespondstoadifferentrandomizationoftheenvironment.Figure9:ComparisonwithMAML(singlegradientstepaftermetalearn-ingapolicyinitialization)ontheDirec-tionalHalfCheetahenvironmentfromFinnetal.(11)(Fig.5)EvolvedPolicyGradientsFigure10:GoalAnttest-timetrainingover512(policyupdates)×32(updatefrequency)=16384timesteps:PPOvsEPGFigure11:Fetchreachingenviron-mentlearningover256(policyup-dates)×32(updatefrequency)=8192timesteps:PPOvsno-rewardEPGFigure12:TransferringEPG(met-alearnedusing128policyupdatesonRandomHopper)to1536updatesattesttime:randompolicyinitialization(A),initializationbysampledpreviouspolicies(B)Figures8,9,and10showexperimentsinwhichasignalingﬂagisrequiredtoidentifytheenvironment.Generally,thisisdonethrougharewardfunctionoranobservationﬂag,whichiswhyEPGtakestherewardasinputincasethestatespaceispartially-observed.Similartothepreviousex-periments,EPGsigniﬁcantlyoutperformsPPOonthetaskdistributionitismetatrainedon.Speciﬁcally,inFigure9,wecompareEPGwithbothMAML(datafrom(11))andRL2(10).Thisexperimentshowsthat,atleastinthisexper-imentalsetup,startingfromarandompolicyinitializationcanbringasmuchbeneﬁtaslearningagoodpolicyinitial-ization(MAML).InSection4.2,wewillinvestigatewhattheeffectofevolvingthepolicyinitializationtogetherwiththelossparametersis.WhencomparingEPGtoRL2(amethodthatlearnsarecurrentpolicythatdoesnotresettheinternalstateupontrajectoryresets),weseethatRL2solvestheDirectionalHalfCheetahtaskalmostinstantlythroughsystemidentiﬁcation.Bylearningboththealgorithmandthepolicyinitializationsimultaneously,itisabletosignif-icantlyoutperformbothMAMLandEPG.However,thiscomesatthecostofgeneralizationpower,aswewilldiscussinSection4.3.4.2.AnalysisInthissection,weﬁrstanalyzewhetherEPGproducesalossfunctionthatencouragesexplorationandadaptivepolicyupdatesduringtest-timetraining.Next,weevaluatetheeffectofevolvingthepolicyinitialization.LearningexploratorybehaviorWithoutadditionalex-ploratoryincentives,PGmethodsleadtosuboptimalpoli-cies.TounderstandwhetherEPGisabletotrainagentsthatexplore,wetestourmethodandPPOontheDirectional-HopperandGoalAntenvironments.InDirectionalHopper,eachsampledHopperenvironmenteitherrewardstheagentforforwardorbackwardhopping.Notethatwithoutobserv-ingthereward,theagentcannotinferwhethertheHopperenvironmentdesiresforwardorbackwardhopping.Thusweaugmenttheenvironmentrewardtotheinputbatchesofthelossfunctioninthissetting.Figure8showslearningcurvesofbothPPOagentsandagentstrainedwiththelearnedlossintheDirectionalHopperenvironment.Thelearningcurvesgiveindicationthatthelearnedlossisabletotrainagentsthatexhibitexploratorybehavior.Weseethatinmostinstances,PPOagentsstag-nateinlearning,whileagentstrainedwithourlearnedlossmanagetoexplorebothforwardandbackwardhoppingandeventuallyhopinthecorrectdirection.Figure8(right)demonstratesthequalitativebehaviorofouragentduringlearningandFigure14visualizestheexploratorybehavior.Weseethatthehopperﬁrstexploresonehoppingdirectionbeforelearningtohopbackwards.EvolvedPolicyGradients(a)RandomWalker(b)DirectionalHalfCheetah(c)GoalAntFigure13:Effectofevolvingthepolicyinitialization(+I)onvariousrandomizedenvironments.test-timetrainingcurveswithevolvedpolicyinitializationstartatthesamereturnvalueasthosewithoutevolvedinitialization.ThisisconsistentwithMAMLtrainedonawidetaskdistribution(Figure5of(11)).Figure14:ExampleoflearningtohopbackwardfromarandomlyinitializedpolicyinaDirectionalHopperenviron-ment.Fromlefttoright,trajectoriesarerecordedaslearningprogresses.Figure15:Trajectoriessampledfromtest-timetrainingontwosampledGoalAntenvironments:theAntlearnshowtoexplorevariousdirectionsbeforegoingtothecorrecttarget.Lightercolorsrepresentinitialtrajectories,darkercolorsarelatertrajectories,accordingtotheagent’slearningprocess.TheGoalAntenvironmentrandomizesthelocationofthegoal.Weaugmentthegoallocationtotheinputbatchesofthelossfunction.Figure15demonstratestheexploratorybehaviorofalearninganttrainedbyEPG.Weseethattheantﬁrstlearnstowalkandexplorevariousdirections,beforeﬁnallyconvergingonthecorrectgoallocation.Theantﬁrstexploresinvariousdirections,includingtheoppositedirectionofthetargetlocation.However,itquicklyﬁguresoutinwhichquadranttoexplore,beforeitfullylearnsthecorrectdirectiontowalkin.LearningadaptivepolicyupdatesPGmethodssuchasREINFORCE(60)sufferfromunstablelearning,suchthatalargelearningstepsizeleadstopolicycrashingduringlearning.Toencouragesmoothpolicyupdates,methodssuchasTRPO(44)andPPO(45)wereproposedtolimitthedistributionalchangefromeachpolicyupdate,throughahyperparameterconstrainingtheKL-divergencebetweenthepolicydistributionsbeforeandaftereachupdate.WedemonstratethatEPGproduceslearnedlossthatadaptivelyscalesthegradientupdates.Figure16:EPGontheRandomHopperenvironment:theKL-divergencebetweenthepolicybeforeandafteranup-dateattheﬁrstepoch(left)vstheﬁnalepoch(right),w.r.tthenumberofupdatessofar,forasingleinnerlooprun.Thesecurvesaregeneratedwithα=0inEq.(10).Figure16showstheKL-divergencebetweenpoliciesfromoneupdatetothenextduringthecourseoftraininginRan-domHopper,usingarandomlyinitializedloss(left)versusalearnedlossproducedbyAlgorithm1(right).Withalearnedlossfunction,thepolicyupdatestendtoshiftthepolicydis-tributionlessoneachstep,butsometimesproducesuddenchanges,indicatedbythespikes.ThesespikesarehighlynoticeableinFigure22ofAppendixB,inwhichweplotindividualtest-timetrainingcurvesforseveralrandomizedenvironments.Thelossfunctionhasevolvedinsuchawaytoadaptitsgradientmagnitudetothecurrentagentstate:forexampleintheDirectionalHalfCheetahexperiments,theagentﬁrstrampsupitsvelocityinonedirection(visiblebyaincreasingKL-divergence)untilitrealizeswhetheritisEvolvedPolicyGradients(a)2layersof256tanhunits(b)2layersof64ReLUunits(c)4layersof64tanhunitsFigure17:TransferringEPG(metalearnedusing2-layer64tanh-unitpoliciesonRandomWalkerasinFigure6)topoliciesofunseenconﬁgurationsattesttimegoingintheright/wrongdirection.Theniteitherfurtherrampsupthevelocitythroughstrongergradients,oremitsaturningsignalviaastronggradientspike(e.g.,visiblebythespikesinFigure22(a)incolumnthree).Inotherexperiments,suchasFigures8,9,and10,weseeasimilarpattern.Basedontheagent’slearninghistory,thegradientmagnitudesarescaledaccordingly.Oftenthegradientwillbesmallinitially,anditgetsincreasinglylargerthemoreenvironmentinformationithasencountered.EffectofevolvingpolicyinitializationPriorworkssuchasMAML(11)metalearnthepolicyinitializationoverataskdistribution.Whileourproposedmethod,EPG,evolvesthelossfunctionparameters,wecanalsoaugmentAlgorithm1withsimultaneouslyevolvingthepolicyinitializationintheESouterloop.WeinvestigatethebeneﬁtsofevolvingthepolicyinitializationontopofEPGandPPOonourrandomizedenvironments.Figure13showsthecompari-sonbetweenEPG,EPGwithevolvedpolicyinitialization(EPG+I),PPO,andPPOwithevolvedpolicyinitialization(PPO+I).Evolvingthepolicyinitializationseemstohelpthemostwhentheenvironmentsrequirelittleexploration,suchasRandomWalker.However,theinitializationplaysafarlessimportantroleinDirectionalHalfCheetahandespeciallytheGoalAntenvironment.HencethesmallerperformancedifferencebetweenEPGandEPG+I.Anotherinterestingobservationisthatevolvingthepolicyinitialization,togetherwiththeEPGlossfunction(EPG+I),leadstoqualitativelydifferentbehaviorthanPPO+I.InPPO+I,theinitializationenablesfastlearninginitially,be-forethereturncurvessaturate.Obtainingapolicyinitializa-tionthatperformswellwithoutlearningupdateswasimpos-sible,sincethereisnosingleinitializationthatperformswellforalltasksMsampledfromthetaskdistributionp(M).IntheEPGcasehowever,weseethatthereturncurvesareoftenlowerinitially,buthigherattheendoflearning.ByfeedingtheﬁnalreturnvalueastheobjectivefunctiontotheESouterloop,thealgorithmisabletoavoidmyopicreturnoptimization.EPG+Isetsthepolicyupforinitialexploratorybehaviorwhich,althoughnotbeneﬁcialintheshortterm,improvesultimateagentbehavior.4.3.GeneralizationKeycomponentsofAlgorithm1includeinner-looptraininghorizonU,theagent’spolicyarchitectureπθ,andthetaskdistributionp(M).Inthissection,weinvestigatethetest-timegeneralizationpropertiesofEPG:generalizationtolongertraininghorizons,todifferentpolicyarchitectures,andtoout-of-distributiontasks.LongertraininghorizonsWeevaluatetheeffectoftrans-ferringtolongeragenttrainingperiodsattesttimeontheRandomHopperenvironmentbyincreasingthetest-timetrainingstepsUinAlgorithm2beyondtheinner-looptrain-ingstepsUofAlgorithm1.Figure12(A)showsthatthelearningcurvedeclinesandeventuallycrashespasttheEvolvedPolicyGradients(a)Taskillustration(b)Rightdirection(asmetatrained)(c)Leftdirection(generalization)Figure18:GeneralizationintheGoalAntexperiment:theanthasonlybeenmetatrainedtoreachtargetonthepositivex-axis(itsrightside).Canitgeneralizetotargetsonthenegativex-axis(itsleftside)?train-timehorizon,whichdemonstratesthatAlgorithm1haslimitedgeneralizationbeyondEPG’sinner-looptrain-ingsteps.However,wecanovercomethislimitationbyinitializingeachinner-looppolicywithrandomlysampledpoliciesthathavebeenobtainedbyinner-looptraininginpastepochs.Figure12(B)illustratescontinuedlearningpastthetrain-timehorizon,validatingthatthismodiﬁcationeffectivelymakesthelearnedlossfunctionrobusttolongertraininglengthattesttime.DifferentpolicyarchitecturesWeevaluateEPG’strans-fertodifferentpolicyarchitecturesbyvaryingthenumberofhiddenlayers,theactivationfunction,andhiddenunitsoftheagent’spolicyattesttime(Algorithm2),whilekeep-ingtheagent’spolicyﬁxedat2-layerwith64tanhunitsduringtrainingtime(Algorithm1)ontheRandomWalkerenvironment.Thetest-timetrainingcurvesonvariedpol-icyarchitecturesareshowninFigure17.ComparedtothelearningcurveFigure6withthesametrain-timeandtest-timepolicyarchitecture,thetransferperformanceisinferior.However,westillseethatEPGproducesalearnedlossfunc-tionthatgeneralizestopoliciesotherthanitwastrainedon,achievingnon-trivialwalkingbehavior.Out-of-distributiontasklearningWeevaluategeneral-izationtoout-of-distributiontasklearningontheGoalAntenvironment.Duringmetatraining,goalsarerandomlysam-pledonthepositivex-axis(antwalkingtotheright)andattesttime,wesamplegoalsfromthenegativex-axis(antwalkingtotheleft).Achievinggeneralizationtotheleftsideisnottrivial,sinceitmaybeeasyforametalearnertooverﬁttothetaskmetatrainingdistribution.Figure18(a)illustratesthisgeneralizationtask.WecomparetheperformanceofEPGagainstMAML(11)andRL2(10).SincePPOisnotmetatrained,thereisnodifferencebetweenbothdirections.Therefore,theperformanceofPPOisthesameasshowninFigure10.First,weevaluateallmetalearningmethods’performancewhenthetest-timetaskissampledfromthetraining-timetaskdistribution.Figure18(b)showsthetest-timetrainingcurveofbothRL2andEPGwhenthetest-timegoalsaresampledfromthepositivex-axis.Asexpected,RL2solvesthistaskextremelyfast,sinceitcouplesboththelearningal-gorithmandthepolicy.EPGperformsverywellonthistaskaswell,learninganeffectivepolicyfromscratch(randominitialization)in8192steps,withﬁnalperformancematch-ingthatofRL2.MAMLachievesapproximatelythesameﬁnalperformanceaftertakingasingleSGDstep(basedon8192sampledsteps).Next,welookatthegeneralizationsettingwithtest-timegoalssampledfromthenegativex-axis.Figure18(c)dis-playsthetest-timetrainingcurvesofbothmethods.RL2seemstohavecompletelyoverﬁttothetaskdistribution,ithasnotsucceededinlearningagenerallearningalgorithm.Notethat,althoughtheRL2agentstillwalksinthewrongdirection,itdoessoatalowerspeed,indicatingthatitno-ticesadeviationfromtheexpectedrewardsignal.WhenlookingatMAML,weseethatMAMLhasalsooverﬁttothemetatrainingdistribution,resultinginawalkingspeedinthewrongdirectionsimilartothenon-generalizationsetting.Theplotalsodepictstheresultofperforming10gradientupdatesfromtheMAMLinit,denotedMAML10(notethateachgradientupdateusesabatchof8192steps).Withmultiplegradientsteps,MAMLisabletooutperformRL2,consistentwith(12),butstilllearnsatafarslowerratethanEPG(intermsofnumberoftimestepsofexperiencere-quired).MAMLcanachievethisbecauseitusesastandardPGlearningalgorithmtomakeprogressbeyonditsinit,andthereforeenjoysthegeneralizationpropertyofgenericPGmethods.Incontrast,EPGevolvesalossfunctionthattrainsagentstoquicklyreachgoalssampledfromnegativex-axis,neverseenduringmetatraining.Thisdemonstratesrudimentarygeneralizationproperties,asmaybeexpectedfromlearningalossfunctionthatisdecoupledfromthepolicy.Figure15alsoshowstrajectoriessampledduringtheEPGlearningEvolvedPolicyGradientsprocessforthisexactexperimentalsetup.25.RelationtoExistingLiteratureTheconceptoflearninganalgorithmforlearningisquitegeneral,andhencethereexistsalargebodyofsomewhatdisconnectedliteratureonthetopic.Tobeginwith,thereareseveralrelevantandrecentpublica-tionsinthemetalearningliterature(11;10;59;25).In(11),analgorithmnamedMAMLisintroduced.MAMLtreatsthemetalearningproblemasininitializationproblem.Morespeciﬁcally,MAMLattemptstoﬁndapolicyinitializationfromwhichonlyaminimalnumberofpolicygradientstepsarerequiredtosolvenewtasks.Thisisaccomplishedbyper-forminggradientdescentontheoriginalpolicyparameterswithrespecttothepostpolicyupdaterewards.InSection4.1ofFinnetal.(13),learningtheMAMLlossviagradientdescentisproposed.Theirlosshasamorerestrictedfor-mulationthanEPGandreliesonlossdifferentiabilitywithrespecttotheobjectivefunction.Inaworkconcurrentwithours,Yuetal.(62)extendedthemodelfrom(13)toincorporateamoreelaboratelearnedlossfunction.Theproposedlossinvolvestemporalconvolu-tionsovertrajectoriesofexperience,similartothemethodproposedinthispaper.However,unlikeourwork,(62)primarilyconsiderstheproblemofbehavioralcloning.Typ-ically,thismeanstheirmethodwillrequiredemonstrations,incontrasttoourmethodwhichdoesnot.Further,theirouterobjectivedoesnotrequiresequentialreasoningandmustbedifferentiableandtheirinnerloopisasingleSGDstep.Wehavenosuchrestrictions.Ourouterobjectiveislonghorizonandnon-differentiableandconsequentlyourinnerloopcanrunovertensofthousandsoftimesteps.AnotherrecentmetalearningalgorithmisRL2(10)(andrelatedmethodssuchas(59)and(25)).RL2isessentiallyarecurrentpolicylearningoverataskdistribution.Thepolicyreceivesﬂagsfromtheenvironmentmarkingtheendofepisodes.Usingtheseﬂagsandsimultaneouslyingestingdataforseveraldifferenttasks,itlearnshowtocomputegradientupdatesthroughitsinternallogic.RL2islimitedbyitsdecisiontocouplethepolicyandlearningalgorithm(usingrecurrencyforboth),whereaswedecouplethesecom-ponents.DuetoRL2’spolicy-gradient-basedoptimizationprocedure,weseethatitdoesnotdirectlyoptimizeﬁnalpolicyperformancenorexhibitexploration.Hence,exten-sionshavebeenproposedsuchasE-RL2(53)inwhichtherewardsofepisodessampledearlyinthelearningprocessaredeliberatelysettozerotodriveexploratorybehavior.Furtherresearchonmetareinforcementlearningcomprises2Ademonstrationcanbeviewedathttp://blog.openai.com/evolved-policy-gradients/.avastselection.Theliterature’svastnessisfurthercompli-catedbythefactthattheresearchappearsundermanydif-ferentheadings.Speciﬁcally,thereexistrelevantliteratureon:life-longlearning,learningtolearn,continuallearning,andmulti-tasklearning.Forexample,(41;40)considerself-modifyinglearningmachines(geneticprograms).Ifweconsiderageneticprogramthatitselfmodiﬁesthelearnedgeneticprogram,wecansubsequentlyderiveameta-GPap-proach(See(53),forfurtherdiscussiononhowthismethodrelatestothemorerecentmetalearningliteraturediscussedabove).Themethoddescribedaboveissufﬁcientlygeneralthatitencompassmostmodernmetalearningapproaches.Forafurtherreviewofothermetalearningapproaches,seethereviewarticles(48;57;58)andcitationgraphtheygen-erate.Thereareseveralotheravenuesofrelatedworkthattackleslightlydifferentproblems.Forinstance,severalmethodsattempttolearnarewardfunctiontodrivelearning.See(7)(whichsuggestslearningfromhumanfeedback)andtheﬁeldofInverseReinforcementLearning(28)(whichrecoverstherewardfromdemonstrations).Bothoftheseﬁeldsrelatetoourideasonlossfunctionlearning.Similarly,(29;30)applypopulation-basedevolutionaryalgorithmstorewardfunctionlearningingridworldenvironments.Thisal-gorithmisencompassedbythealgorithmswepresentinthispaper.However,itistypicallymucheasiersincelearningjusttherewardfunctionisinmanycasesatrivialtask(e.g.,inlearningtowalk,mappingtheobservationofdistancetoarewardfunction).Seealso(49;50)and(1)foradditionalevolutionaryperspectivesonrewardlearning.OtherrewardlearningmethodsincludetheworkofGuoetal.(14),whichfocusesonlearningrewardbonuses,andtheworkofSorgetal.(51),whichfocusesonlearningrewardfunctionsthroughgradientdescent.Thesebonusesaretypicallydesignedtoaugmentbutnotreplacethelearnedrewardandhavenotbeenshowntoeasilygeneralizeacrossbroadtaskdistri-butions.Rewardbonusesarecloselylinkedtotheideaofcuriosity,inwhichanagentattemptstolearnaninternalrewardsignaltodrivefutureexploration.Schmidhuber(39)wasperhapstheﬁrsttoexaminetheproblemofintrinsicmo-tivationinametalearningcontext.Theproposedalgorithmsmakeuseofdynamicprogrammingtoexplicitlypartitionexperienceintocheckpoints.Further,thereisusuallylittlefocusonmetalearningthecuriositysignalacrossseveraldifferenttasks.Finally,theworkof(17;61;2;22;24)stud-iesmetalearningovertheoptimizationprocessinwhichmetalearnermakesexplicitupdatestoaparametrizedmodelinsupervisedsettings.AlsoworthmentioningisthatapproachessuchasUVFA(38)andHER(3),whichlearnauniversalgoal-directedvaluefunction,somewhatresembleEPGinthesensethattheircriticcouldbeinterpretedasasortoflossfunctionthatislearnedaccordingtoaspeciﬁcsetofrules.Furthermore,inDDPG(23),thecriticcanbeinterpretedinasimilarEvolvedPolicyGradientswaysinceitalsomakesuseofback-propagationthroughalearnedfunctionintoapolicynetwork.6.DiscussionInthispaper,weintroducedanewmetalearningapproachcapableoflearningadifferentiablelossfunctionoverthou-sandsofsequentialenvironmentalactions.Crucially,thislearnedlossisbothadaptive(allowingforquickerlearningofnewtasks)andinstructive(sometimeseliminatingtheneedforenvironmentalrewardsattesttime),whileexhibit-ingstrongergeneralizationpropertiesthancontemporarymetalearningmethods.Incertaincases,theadaptabilityofourlearnedlossisap-preciated.Forexample,considertheDirectionalHopperexperimentsfromSection4.Here,therewardsattesttimeareimpossibletoinferfromobservationsoftheenvironmentalone.Therefore,theycannotbecompletelyinternalized.However,whenwedogettoobservearewardsignalontheseenvironments,thenEPGdoesimprovelearningspeed.Meanwhile,inmostothercases,ourloss’instructivenature–whichallowsittooperateattesttimewithoutenvironmentalrewards–isinterestinganddesirable.Thisinstructivena-turecanbeunderstoodasthelossfunction’sinternalizationoftherewardstructuresithaspreviouslyencounteredunderthetrainingtaskdistribution.Weseethisinternalizationasasteptowardlearningintrinsicmotivation.Agoodin-trinsicallymotivatedagentwouldsuccessfullyinferusefulactionsinnewsituationsbyusingheuristicsitdevelopedoveritsentirelifetime.Thisabilityislikelyrequiredtoachievetrulyintelligentagents(39).Furthermore,throughdecouplingofthepolicyandlearningalgorithm,EPGshowsrudimentarygeneralizationproper-tiesthatgobeyondcurrentmetalearningmethodssuchasRL2.ImprovingthegeneralizationabilityofEPG,aswellotherothermetalearningalgorithms,willbeanimportantcomponentoffuturework.Rightnow,wecantrainanEPGlosstobeeffectiveforonesmallfamilyoftasksatatime,e.g.,gettingananttowalkleftandright.However,theEPGlossforthisfamilyoftasksisunlikelytobeatalleffectiveonawildlydifferentkindoftask,likeplayingSpaceInvaders.Incontrast,standardRLlossesdohavethislevelofgenerality–thesamelossfunctioncanbeusedtolearnahugevarietyofskills.EPGgainsonperformancebylosingongenerality.TheremaybealongroadaheadtowardmetalearningmethodsthatbothoutperformstandardRLmethodsandhavethesamelevelofgenerality.Improvingcomputationalefﬁciencyisanotherimportantdirectionforfuturework.EPGdemandssequentiallearning.Thatistosay,onemustﬁrstperformouterloopupdateibeforelearningaboutupdatei+1.Thiscanbottleneckthemetalearningcycleandcreatelargecomputationalde-mands.Indeed,thenumberofsequentialstepsforeachinner-loopworkerinouralgorithmisE×U,usingnota-tionfromAlgorithm1.Inpractice,thisvaluemaybeveryhigh,forexample,eachinner-loopworkertakesapproxi-mately196millionstepstoevolvethelossfunctionusedintheRandomReacherexperiments(Figure7).Findingwaystoparallelizepartsofthisprocess,orincreasesampleefﬁciency,couldgreatlyimprovethepracticalapplicabilityofouralgorithm.Improvementsincomputationalefﬁciencywouldalsoallowtheinvestigationofmorechallengingtasks.Nevertheless,wefeelthesuccessontheenvironmentswetestedisnon-trivialandprovidesaproofofconceptofourmethod’spower.AcknowledgmentsWethankIgorMordatch,IlyaSutskever,JohnSchulman,andKarthikNarasimhanforhelpfulcommentsandconver-sations.WethankMaruanAl-ShedivatforassistingwiththerandomMuJoCoenvironments.References[1]DavidAckleyandMichaelLittman.Interactionsbe-tweenlearningandevolution.ArtiﬁciallifeII,10:487–509,1991.[2]MarcinAndrychowicz,MishaDenil,SergioGomez,MatthewWHoffman,DavidPfau,TomSchaul,andNandodeFreitas.Learningtolearnbygra-dientdescentbygradientdescent.arXivpreprintarXiv:1606.04474,2016.[3]MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,RachelFong,PeterWelinder,BobMc-Grew,JoshTobin,OpenAIPieterAbbeel,andWo-jciechZaremba.Hindsightexperiencereplay.InAdvancesinNeuralInformationProcessingSystems,pages5048–5058,2017.[4]LeemonCBairdIII.Advantageupdating.Technicalreport,WRIGHTLABWRIGHT-PATTERSONAFBOH,1993.[5]GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,andWo-jciechZaremba.OpenAIGym.arXivpreprintarXiv:1606.01540,2016.[6]RichardYChen,JohnSchulman,PieterAbbeel,andSzymonSidor.UCBexplorationviaQ-ensembles.arXivpreprintarXiv:1706.01502,2017.[7]PaulFChristiano,JanLeike,TomBrown,MiljanMar-tic,ShaneLegg,andDarioAmodei.Deepreinforce-mentlearningfromhumanpreferences.InAdvancesinEvolvedPolicyGradientsNeuralInformationProcessingSystems,pages4302–4310,2017.[8]RichardDearden,NirFriedman,andDavidAndre.Modelbasedbayesianexploration.InProceedingsoftheFifteenthconferenceonUncertaintyinartiﬁ-cialintelligence,pages150–159.MorganKaufmannPublishersInc.,1999.[9]YanDuan,XiChen,ReinHouthooft,JohnSchulman,andPieterAbbeel.Benchmarkingdeepreinforce-mentlearningforcontinuouscontrol.InInternationalConferenceonMachineLearning,pages1329–1338,2016.[10]YanDuan,JohnSchulman,XiChen,PeterLBartlett,IlyaSutskever,andPieterAbbeel.RL2:Fastreinforce-mentlearningviaslowreinforcementlearning.arXivpreprintarXiv:1611.02779,2016.[11]ChelseaFinn,PieterAbbeel,andSergeyLevine.Model-agnosticmeta-learningforfastadaptationofdeepnetworks.arXivpreprintarXiv:1703.03400,2017.[12]ChelseaFinnandSergeyLevine.Meta-learninganduniversality:Deeprepresentationsandgradientde-scentcanapproximateanylearningalgorithm.arXivpreprintarXiv:1710.11622,2017.[13]ChelseaFinn,TianheYu,TianhaoZhang,PieterAbbeel,andSergeyLevine.One-shotvisualim-itationlearningviameta-learning.arXivpreprintarXiv:1709.04905,2017.[14]XiaoxiaoGuo,SatinderSingh,RichardLewis,andHonglakLee.Deeplearningforrewarddesigntoimprovemontecarlotreesearchinatarigames.arXivpreprintarXiv:1604.07095,2016.[15]TuomasHaarnoja,HaoranTang,PieterAbbeel,andSergeyLevine.Reinforcementlearningwithdeepenergy-basedpolicies.arXivpreprintarXiv:1702.08165,2017.[16]NikolausHansenandAndreasOstermeier.Completelyderandomizedself-adaptationinevolutionstrategies.Evolutionarycomputation,9(2):159–195,2001.[17]SeppHochreiter,AStevenYounger,andPeterRCon-well.Learningtolearnusinggradientdescent.InIn-ternationalConferenceonArtiﬁcialNeuralNetworks,pages87–94.Springer,2001.[18]DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.[19]JZicoKolterandAndrewYNg.Near-bayesianexplo-rationinpolynomialtime.InProceedingsofthe26thAnnualInternationalConferenceonMachineLearn-ing,pages513–520.ACM,2009.[20]VijayRKondaandJohnNTsitsiklis.Actor-criticalgo-rithms.InAdvancesinNeuralInformationProcessingSystems,pages1008–1014,2000.[21]BrendenMLake,TomerDUllman,JoshuaBTenen-baum,andSamuelJGershman.Buildingmachinesthatlearnandthinklikepeople.BehavioralandBrainSciences,40,2017.[22]KeLiandJitendraMalik.Learningtooptimize.arXivpreprintarXiv:1606.01885,2016.[23]TimothyPLillicrap,JonathanJHunt,AlexanderPritzel,NicolasHeess,TomErez,YuvalTassa,DavidSilver,andDaanWierstra.Continuouscontrolwithdeepreinforcementlearning.arXivpreprintarXiv:1509.02971,2015.[24]LukeMetz,NiruMaheswaranathan,BrianCheung,andJaschaSohl-Dickstein.Learningunsupervisedlearningrules.arXivpreprintarXiv:1804.00222,2018.[25]NikhilMishra,MostafaRohaninejad,XiChen,andPieterAbbeel.Meta-learningwithtemporalconvolu-tions.arXivpreprintarXiv:1707.03141,2017.[26]OﬁrNachum,MohammadNorouzi,KelvinXu,andDaleSchuurmans.Bridgingthegapbetweenvalueandpolicybasedreinforcementlearning.InAdvancesinNeuralInformationProcessingSystems,pages2772–2782,2017.[27]YuriiNesterovandVladimirSpokoiny.Randomgradient-freeminimizationofconvexfunctions.Foun-dationsofComputationalMathematics,17(2):527–566,2017.[28]AndrewYNgandStuartRussell.Algorithmsforinversereinforcementlearning.IninProc.17thInter-nationalConf.onMachineLearning.Citeseer,2000.[29]ScottNiekum,AndrewGBarto,andLeeSpector.Ge-neticprogrammingforrewardfunctionsearch.IEEETransactionsonAutonomousMentalDevelopment,2(2):83–90,2010.[30]ScottNiekum,LeeSpector,andAndrewBarto.Evo-lutionofrewardfunctionsforreinforcementlearning.InProceedingsofthe13thannualconferencecompan-iononGeneticandevolutionarycomputation,pages177–178.ACM,2011.EvolvedPolicyGradients[31]BrendanO’Donoghue,RemiMunos,KorayKavukcuoglu,andVolodymyrMnih.Pgq:Combiningpolicygradientandq-learning.arXivpreprintarXiv:1611.01626,2016.[32]GeorgOstrovski,MarcGBellemare,AaronvandenOord,andRémiMunos.Count-basedexplo-rationwithneuraldensitymodels.arXivpreprintarXiv:1703.01310,2017.[33]DeepakPathak,PulkitAgrawal,AlexeiAEfros,andTrevorDarrell.Curiosity-drivenexplorationbyself-supervisedprediction.InInternationalConferenceonMachineLearning(ICML),volume2017,2017.[34]MatthiasPlappert,MarcinAndrychowicz,AlexRay,BobMcGrew,BowenBaker,GlennPowell,JonasSchneider,JoshTobin,MaciekChociej,PeterWelin-der,etal.Multi-goalreinforcementlearning:Chal-lengingroboticsenvironmentsandrequestforresearch.arXivpreprintarXiv:1802.09464,2018.[35]I.RechenbergandM.Eigen.Evolutionsstrategie:Op-timierungTechnischerSystemenachPrinzipienderBiologischenEvolution.1973.[36]RichardMRyanandEdwardLDeci.Intrinsicandextrinsicmotivations:Classicdeﬁnitionsandnewdirections.Contemporaryeducationalpsychology,25(1):54–67,2000.[37]TimSalimans,JonathanHo,XiChen,andIlyaSutskever.Evolutionstrategiesasascalablealter-nativetoreinforcementlearning.arXivpreprintarXiv:1703.03864,2017.[38]TomSchaul,DanielHorgan,KarolGregor,andDavidSilver.Universalvaluefunctionapproximators.InInternationalConferenceonMachineLearning,pages1312–1320,2015.[39]JuergenSchmidhuber.Exploringthepredictable.InAdvancesinevolutionarycomputing,pages579–612.Springer,2003.[40]JürgenSchmidhuber.Evolutionaryprinciplesinself-referentiallearning,oronlearninghowtolearn:Themeta-meta-...hook.Diplomathesis,TUM,1987.[41]JürgenSchmidhuber.Gödelmachines:Fullyself-referentialoptimaluniversalself-improvers.InArti-ﬁcialgeneralintelligence,pages199–226.Springer,2007.[42]JohnSchulman,PieterAbbeel,andXiChen.Equiv-alencebetweenpolicygradientsandsoftq-learning.arXivpreprintarXiv:1704.06440,2017.[43]JohnSchulman,NicolasHeess,TheophaneWeber,andPieterAbbeel.Gradientestimationusingstochasticcomputationgraphs.InAdvancesinNeuralInforma-tionProcessingSystems,pages3528–3536,2015.[44]JohnSchulman,SergeyLevine,PieterAbbeel,MichaelJordan,andPhilippMoritz.Trustregionpolicyoptimization.InInternationalConferenceonMachineLearning,pages1889–1897,2015.[45]JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov.Proximalpolicyoptimiza-tionalgorithms.arXivpreprintarXiv:1707.06347,2017.[46]Hans-PaulSchwefel.NumerischeOptimierungvonComputer-ModellenmittelsderEvolutionsstrategie:miteinervergleichendenEinführungindieHill-Climbing-undZufallsstrategie.Birkhäuser,1977.[47]FrankSehnke,ChristianOsendorfer,ThomasRück-stieß,AlexGraves,JanPeters,andJürgenSchmid-huber.Parameter-exploringpolicygradients.NeuralNetworks,23(4):551–559,2010.[48]DanielLSilver,QiangYang,andLianghaoLi.Life-longmachinelearningsystems:Beyondlearningalgo-rithms.InAAAISpringSymposium:LifelongMachineLearning,volume13,page05,2013.[49]SatinderSingh,RichardLLewis,andAndrewGBarto.Wheredorewardscomefrom.[50]SatinderSingh,RichardLLewis,AndrewGBarto,andJonathanSorg.Intrinsicallymotivatedreinforce-mentlearning:Anevolutionaryperspective.IEEETransactionsonAutonomousMentalDevelopment,2(2):70–82,2010.[51]JonathanSorg,RichardLLewis,andSatinderPSingh.Rewarddesignviaonlinegradientascent.InAd-vancesinNeuralInformationProcessingSystems,pages2190–2198,2010.[52]JamesCSpall.Multivariatestochasticapproxima-tionusingasimultaneousperturbationgradientap-proximation.IEEEtransactionsonautomaticcontrol,37(3):332–341,1992.[53]B.C.Stadie,G.Yang,R.Houthooft,X.Chen,Y.Duan,W.Yuhuai,P.Abbeel,andI.Sutskever.Someconsid-erationsonlearningtoexploreviameta-reinforcementlearning.InInternationalConferenceonLearningRepresentations(ICLR),WorkshopTrack,2018.[54]RichardSSuttonandAndrewGBarto.Reinforce-mentlearning:Anintroduction,volume1.MITpressCambridge,1998.EvolvedPolicyGradients[55]RichardSSutton,DavidAMcAllester,SatinderPSingh,andYishayMansour.Policygradientmethodsforreinforcementlearningwithfunctionapproxima-tion.InAdvancesinNeuralInformationProcessingSystems,pages1057–1063,2000.[56]HaoranTang,ReinHouthooft,DavisFoote,AdamStooke,XiChen,YanDuan,JohnSchulman,FilipDeTurck,andPieterAbbeel.#Exploration:Astudyofcount-basedexplorationfordeepreinforcementlearn-ing.AdvancesinNeuralInformationProcessingSys-tems(NIPS),2017.[57]MatthewETaylorandPeterStone.Transferlearningforreinforcementlearningdomains:Asurvey.JournalofMachineLearningResearch,10(Jul):1633–1685,2009.[58]SebastianThrun.Islearningthen-ththinganyeasierthanlearningtheﬁrst?InAdvancesinNeuralInfor-mationProcessingSystems,pages640–646,1996.[59]JaneXWang,ZebKurth-Nelson,DhruvaTirumala,HubertSoyer,JoelZLeibo,RemiMunos,CharlesBlundell,DharshanKumaran,andMattBotvinick.Learningtoreinforcementlearn.arXivpreprintarXiv:1611.05763,2016.[60]RonaldJWilliams.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.InReinforcementLearning,pages5–32.Springer,1992.[61]AStevenYounger,SeppHochreiter,andPeterRCon-well.Meta-learningwithbackpropagation.InNeu-ralNetworks,2001.Proceedings.IJCNN’01.Interna-tionalJointConferenceon,volume3.IEEE,2001.[62]TianheYu,ChelseaFinn,AnnieXie,SudeepDasari,TianhaoZhang,PieterAbbeel,andSergeyLevine.One-shotimitationfromobservinghumansviadomain-adaptivemeta-learning.arXivpreprintarXiv:1802.01557,2018.A.EnvironmentDescriptionWedescribetherandomizedenvironmentsusedinourex-perimentsinthefollowing:•RandomHopperandRandomWalker:randomizedgrav-ity,friction,bodymass,andlinkthicknessatmetatrain-ingtime,usingaforward-velocityreward.Attest-time,therewardisnotfedasaninputtoEPG.•RandomReacher:randomizedlinklengths,usingthenegativedistanceasarewardatmetatrainingtime.Attest-time,therewardisnotfedasaninputtoEPG,how-ever,thetargetlocationisfedasaninputobservation.•DirectionalHopperandDirectionalHalfCheetah3:ran-domizedvelocityrewardfunction.•GoalAnt:antenvironmentwithrandomizedtargetlo-cationandrandomizedinitialrotationoftheant.Thevelocitytothetargetisfedinasareward.Thetargetlocationisnotobserved.•Fetch:randomizedtargetlocation,therewardfunctionisthenegativedistancetothetarget.Therewardfunc-tionisnotaninputtotheEPGlossfunction,butthetargetlocationis.B.AdditionalExperimentsLearningwithoutenvironmentresetsWeshowthatitisstraightforwardtoevolvealossthatisabletoperformwellonno-resetlearning,suchthattheagentisneverre-settoaﬁxedstartinglocationandconﬁgurationaftereachepisode.Figure19showstheaveragereturnw.r.t.theepochontheGoalAntenvironmentwithoutreset.Theantcon-tinueslearningfromthelocationandconﬁgurationaftereachepisodeﬁnishesandisresettothestartingpointonlywhenthetargetisreached.Qualitativeinspectionofthelearnedbehaviorshowsthattheagentlearnshowtoreachthetargetmultipletimesduringitslifetime.Incomparison,runningPPOinano-resetenvironmentishighlydifﬁcult,sincetheagent’spolicytendstogetstuckinapositionitcannotescapefrom(leadingtoanalmostﬂatzero-returnlearningcurve).Insomeway,thisdemonstratesthatEPG’slearnedlossguidestheagenttoavoidstatesfromwhichitcannotescape.Figure19:Theaveragereturnw.r.t.theepochontheGoalAntenvironmentwithnoreset.Trainingperformancew.r.t.evolutionepochFigure20showsthemetatraining-timeperformance(calculatedbasedonthenoise-perturbedlossfunctions)w.r.t.thenumberESepochssofar,averagedacross256differentinner-loopworkersforvariousrandomseeds,onseveralofouren-vironments.Thisexperimenthighlightsthestabilityofﬁndingwell-performinglossfunctionviaevolution.All3Environmentsourcedfromhttp://github.com/cbfinn/maml_rl.EvolvedPolicyGradients(a)RandomHopper(b)DirectionalHopper(c)GoalAnt(d)FetchFigure20:Finalreturnsaveragedacross256inner-loopworkersw.r.t.thenumberouter-loopESepochssofarinEPGtraining(Algorithm1).WerunEPGtrainingoneachenvironmentacross5differentrandomseedsandplotthemeanandstandarddeviationasasolidlineandashadedarearespectively.experimentsuse256workersover64noisevectorsand256updatesevery32steps(8196-stepinnerloop).EPGlossinputsensitivityInthereward-freecase(e.g.,RandomHopper,RandomWalker,RandomReacher,andFetch),theEPGlossfunctiontakesfourkindsofinputs:ob-servations,actions,terminationsignals,andpolicyoutputs,andevaluatesentirebufferwithNtransitionsteps.Whichtypesofinputandwhichtimepointsinthebuffermatterthemost?InFigure21,weplotthesensitivityofthelearnedlossfunctiontoeachofthesekindsofinputsbycomputing||∂Lt=25∂xt||2fordifferentkindsofinputxtatdifferenttimepointstintheinputbuffer.Thisanalysisdemonstratesthatthelossisespeciallysensitivetoexperienceatthecurrenttimestepwhereitisbeingevaluated,butalsodependsontheentiretemporalcontextintheinputbuffer.Thissuggeststhatthetemporalconvolutionsareindeedmakinguseoftheagent’shistory(andfutureexperience)toscorethebehavior.Individualtest-timetrainingcurvesFigures5,9,and10showthetest-timetrainingtrajectoriesoftheEPGagentonRandomHopper,DirectionalHalfCheetah,andGoalAnt.AdetailedplotofhowindividuallearnersbehaveineachenvironmentisshowninFigure22.LookingatboththereturnandKLplotsfortheDirectionalHalfCheetahandGoalAntenvironments,weseethattheagentrampsupitsvelocity,afterwhichiteitherﬁndsoutitisgoingintherightdirectionornot.Ifitisgoinginthewrongdirectioninitially,itprovidesacountersignal,turns,andthenrampsupitsvelocityintheappropriatedirection,increasingitsreturn.Thisdemonstratestheexploratorybehaviorthatoccursintheseenvironments.IntheRandomHoppercase,onlyaslightperiodofsystemidentiﬁcationexists,afterwhichtheFigure21:Lossinputsensitivity:gradientmagnitudeofLt=25w.r.t.itsinputsatdifferenttimestepswithintheinputbuffer.Noticenotonlythestrongdependenceoncurrenttimepoint(t=25),butalsothedependenceontheentirebufferwindow.velocityofthehopperisquicklyrampedup(visiblebytheincreasingKLdivergences).C.ExperimentHyperparametersTheexperimenthyperparametersusedinSection4arelistedinTable1.EvolvedPolicyGradientsEnvironmentworkersWnoisevectorsVupdatefrequencyMupdatesinnerlooplengthRandomHopper25664641288196RandomWalker2566412825632768RandomReacher2566412851265536DirectionalHopper25664641288196DirectionalHalfCheetah25664322568196GoalAnt256643251216384Fetch25664322568192Table1:EPGhyperparametersfordifferentenvironments(a)DifferentrunsofthelearningagentinFigure5(RandomHopper)(b)DifferentrunsofthelearningagentinFigure9(DirectionalHalfCheetah)(c)DifferentrunsofthelearningagentinFigure10(GoalAnt,butlimitedtoforward/backwardgoals)Figure22:Moretest-timetrainingcurvesinrandomizedenvironments.Eachcolumnrepresentsadifferentsampledenvironment.Theredcurvesplotsthereturnw.r.t.thenumberofsampledtrajectoriesduringtraining,whilethebluecurvesrepresenttheKLdivergenceofthepolicyupdatesw.r.t.thenumberofpolicyupdates.Thenumbershownineachﬁrstrowplotrepresentstheﬁnalreturnaveragedovertheﬁnal3trajectories.Thereturncurve(red)x-axisrepresentthenumberoftrajectoriessampledsofar,whiletheKL-divergence(blue)x-axisrepresentsthenumberofupdatesperformedsofar.
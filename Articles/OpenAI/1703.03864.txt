7
1
0
2

p
e
S
7

]
L
M

.
t
a
t
s
[

2
v
4
6
8
3
0
.
3
0
7
1
:
v
i
X
r
a

EvolutionStrategiesasaScalableAlternativetoReinforcementLearningTimSalimansJonathanHoXiChenSzymonSidorIlyaSutskeverOpenAIAbstractWeexploretheuseofEvolutionStrategies(ES),aclassofblackboxoptimizationalgorithms,asanalternativetopopularMDP-basedRLtechniquessuchasQ-learningandPolicyGradients.ExperimentsonMuJoCoandAtarishowthatESisaviablesolutionstrategythatscalesextremelywellwiththenumberofCPUsavailable:Byusinganovelcommunicationstrategybasedoncommonrandomnumbers,ourESimplementationonlyneedstocommunicatescalars,makingitpossibletoscaletooverathousandparallelworkers.Thisallowsustosolve3Dhumanoidwalkingin10minutesandobtaincompetitiveresultsonmostAtarigamesafteronehouroftraining.Inaddition,wehighlightseveraladvantagesofESasablackboxoptimizationtechnique:itisinvarianttoactionfrequencyanddelayedrewards,tolerantofextremelylonghorizons,anddoesnotneedtemporaldiscountingorvaluefunctionapproximation.1IntroductionDevelopingagentsthatcanaccomplishchallengingtasksincomplex,uncertainenvironmentsisakeygoalofartiﬁcialintelligence.Recently,themostpopularparadigmforanalyzingsuchproblemshasbeenusingaclassofreinforcementlearning(RL)algorithmsbasedontheMarkovDecisionProcess(MDP)formalismandtheconceptofvaluefunctions.SuccessesofthisapproachincludesystemsthatlearntoplayAtarifrompixels[Mnihetal.,2015],performhelicopteraerobaticsNgetal.[2006],orplayexpert-levelGo[Silveretal.,2016].AnalternativeapproachtosolvingRLproblemsisusingblack-boxoptimization.Thisapproachisknownasdirectpolicysearch[SchmidhuberandZhao,1998],orneuro-evolution[RisiandTogelius,2015],whenappliedtoneuralnetworks.Inthispaper,westudyEvolutionStrategies(ES)[RechenbergandEigen,1973],aparticularsetofoptimizationalgorithmsinthisclass.WeshowthatEScanreliablytrainneuralnetworkpolicies,inafashionwellsuitedtobescaleduptomoderndistributedcomputersystems,forcontrollingrobotsintheMuJoCophysicssimulator[Todorovetal.,2012]andplayingAtarigameswithpixelinputs[Mnihetal.,2015].Ourkeyﬁndingsareasfollows:1.Wefoundthattheuseofvirtualbatchnormalization[Salimansetal.,2016]andotherreparameterizationsoftheneuralnetworkpolicy(section2.2)greatlyimprovethereliabilityofevolutionstrategies.WithoutthesemethodsESprovedbrittleinourexperiments,butwiththesereparameterizationsweachievedstrongresultsoverawidevarietyofenvironments.2.Wefoundtheevolutionstrategiesmethodtobehighlyparallelizable:byintroducinganovelcommunicationstrategybasedoncommonrandomnumbers,weareabletoachievelinearspeedupsinruntimeevenwhenusingoverathousandworkers.Inparticular,using1,440workers,wehavebeenabletosolvetheMuJoCo3Dhumanoidtaskinunder10minutes.3.Thedataefﬁciencyofevolutionstrategieswassurprisinglygood:wewereabletomatchtheﬁnalperformanceofA3C[Mnihetal.,2016]onmostAtarienvironmentswhileusingbetween3xand10xasmuchdata.Theslightdecreaseindataefﬁciencyispartlyoffsetbya 
 
 
 
 
 
reductioninrequiredcomputationofroughly3xduetonotperformingbackpropagationandnothavingavaluefunction.Our1-hourESresultsrequireaboutthesameamountofcomputationasthepublished1-dayresultsforA3C,whileperformingbetteron23gamestested,andworseon28.OnMuJoCotasks,wewereabletomatchthelearnedpolicyperformanceofTrustRegionPolicyOptimization[TRPO;Schulmanetal.,2015],usingnomorethan10xasmuchdata.4.WefoundthatESexhibitedbetterexplorationbehaviourthanpolicygradientmethodslikeTRPO:ontheMuJoCohumanoidtask,EShasbeenabletolearnaverywidevarietyofgaits(suchaswalkingsidewaysorwalkingbackwards).TheseunusualgaitsareneverobservedwithTRPO,whichsuggestsaqualitativelydifferentexplorationbehavior.5.Wefoundtheevolutionstrategiesmethodtoberobust:weachievedtheaforementionedresultsusingﬁxedhyperparametersforalltheAtarienvironments,andadifferentsetofﬁxedhyperparametersforallMuJoCoenvironments(withtheexceptionofonebinaryhyper-parameter,whichhasnotbeenheldconstantbetweenthedifferentMuJoCoenvironments).Black-boxoptimizationmethodshaveseveralhighlyattractiveproperties:indifferencetothedistribu-tionofrewards(sparseordense),noneedforbackpropagatinggradients,andtoleranceofpotentiallyarbitrarilylongtimehorizons.However,theyareperceivedaslesseffectiveatsolvinghardRLproblemscomparedtotechniqueslikeQ-learningandpolicygradients.Thecontributionofthiswork,whichwehopewillrenewinterestinthisclassofmethodsandleadtonewusefulapplications,isademonstrationthatevolutionstrategiescanbecompetitivewithcompetingRLalgorithmsonthehardestenvironmentsstudiedbythedeepRLcommunitytoday,andthatthisapproachcanscaletomanymoreparallelworkers.2EvolutionStrategiesEvolutionStrategies(ES)isaclassofblackboxoptimizationalgorithms[RechenbergandEigen,1973,Schwefel,1977]thatareheuristicsearchproceduresinspiredbynaturalevolution:Ateveryiteration(“generation”),apopulationofparametervectors(“genotypes”)isperturbed(“mutated”)andtheirobjectivefunctionvalue(“ﬁtness”)isevaluated.Thehighestscoringparametervectorsarethenrecombinedtoformthepopulationforthenextgeneration,andthisprocedureisiterateduntiltheobjectiveisfullyoptimized.Algorithmsinthisclassdifferinhowtheyrepresentthepopulationandhowtheyperformmutationandrecombination.ThemostwidelyknownmemberoftheESclassisthecovariancematrixadaptationevolutionstrategy[CMA-ES;HansenandOstermeier,2001],whichrepresentsthepopulationbyafull-covariancemultivariateGaussian.CMA-EShasbeenextremelysuccessfulinsolvingoptimizationproblemsinlowtomediumdimension.TheversionofESweuseinthisworkbelongstotheclassofnaturalevolutionstrategies(NES)[Wierstraetal.,2008,2014,Yietal.,2009,Sunetal.,2009,Glasmachersetal.,2010a,b,Schauletal.,2011]andiscloselyrelatedtotheworkofSehnkeetal.[2010].LetFdenotetheobjectivefunctionactingonparametersθ.NESalgorithmsrepresentthepopulationwithadistributionoverparameterspψ(θ)—itselfparameterizedbyψ—andproceedtomaximizetheaverageobjectivevalueEθ∼pψF(θ)overthepopulationbysearchingforψwithstochasticgradientascent.Speciﬁcally,usingthescorefunctionestimatorfor∇ψEθ∼pψF(θ)inafashionsimilartoREINFORCE[Williams,1992],NESalgorithmstakegradientstepsonψwiththefollowingestimator:∇ψEθ∼pψF(θ)=Eθ∼pψ{F(θ)∇ψlogpψ(θ)}ForthespecialcasewherepψisfactoredGaussian(asinthiswork),theresultinggradientestimatorisalsoknownassimultaneousperturbationstochasticapproximation[Spall,1992],parameter-exploringpolicygradients[Sehnkeetal.,2010],orzero-ordergradientestimation[NesterovandSpokoiny,2011].Inthiswork,wefocusonRLproblems,soF(·)willbethestochasticreturnprovidedbyanenvironment,andθwillbetheparametersofadeterministicorstochasticpolicyπθdescribinganagentactinginthatenvironment,controlledbyeitherdiscreteorcontinuousactions.MuchoftheinnovationinRLalgorithmsisfocusedoncopingwiththelackofaccesstoorexistenceofderivativesoftheenvironmentorpolicy.Suchnon-smoothnesscanbeaddressedwithESasfollows.WeinstantiatethepopulationdistributionpψasanisotropicmultivariateGaussianwithmeanψandﬁxedcovarianceσ2I,allowingustowriteEθ∼pψF(θ)intermsofameanparametervectorθdirectly:we2setEθ∼pψF(θ)=E(cid:15)∼N(0,I)F(θ+σ(cid:15)).Withthissetup,ourstochasticobjectivecanbeviewedasaGaussian-blurredversionoftheoriginalobjectiveF,freeofnon-smoothnessintroducedbytheenvironmentorpotentiallydiscreteactionstakenbythepolicy.FurtherdiscussiononhowESandpolicygradientmethodscopewithnon-smoothnesscanbefoundinsection3.Withourobjectivedeﬁnedintermsofθ,weoptimizeoverθdirectlyusingstochasticgradientascentwiththescorefunctionestimator:∇θE(cid:15)∼N(0,I)F(θ+σ(cid:15))=1σE(cid:15)∼N(0,I){F(θ+σ(cid:15))(cid:15)}whichcanbeapproximatedwithsamples.Theresultingalgorithm(1)repeatedlyexecutestwophases:1)Stochasticallyperturbingtheparametersofthepolicyandevaluatingtheresultingparametersbyrunninganepisodeintheenvironment,and2)Combiningtheresultsoftheseepisodes,calculatingastochasticgradientestimate,andupdatingtheparameters.Algorithm1EvolutionStrategies1:Input:Learningrateα,noisestandarddeviationσ,initialpolicyparametersθ02:fort=0,1,2,...do3:Sample(cid:15)1,...(cid:15)n∼N(0,I)4:ComputereturnsFi=F(θt+σ(cid:15)i)fori=1,...,n5:Setθt+1←θt+α1nσPni=1Fi(cid:15)i6:endfor2.1ScalingandparallelizingESESiswellsuitedtobescaleduptomanyparallelworkers:1)Itoperatesoncompleteepisodes,therebyrequiringonlyinfrequentcommunicationbetweenworkers.2)Theonlyinformationobtainedbyeachworkeristhescalarreturnofanepisode:ifwesynchronizerandomseedsbetweenworkersbeforeoptimization,eachworkerknowswhatperturbationstheotherworkersused,soeachworkeronlyneedstocommunicateasinglescalartoandfromeachotherworkertoagreeonaparameterupdate.ESthusrequiresextremelylowbandwidth,insharpcontrasttopolicygradientmethods,whichrequireworkerstocommunicateentiregradients.3)Itdoesnotrequirevaluefunctionapproximations.RLwithvaluefunctionestimationisinherentlysequential:Toimproveuponagivenpolicy,multipleupdatestothevaluefunctionaretypicallyneededtogetenoughsignal.Eachtimethepolicyissigniﬁcantlychanged,multipleiterationsarenecessaryforthevaluefunctionestimatetocatchup.AsimpleparallelversionofESisgiveninAlgorithm2.Themainnoveltyhereisthatthealgo-rithmmakesuseofsharedrandomseeds,whichdrasticallyreducesthebandwidthrequiredforcommunicationbetweentheworkers.Algorithm2ParallelizedEvolutionStrategies1:Input:Learningrateα,noisestandarddeviationσ,initialpolicyparametersθ02:Initialize:nworkerswithknownrandomseeds,andinitialparametersθ03:fort=0,1,2,...do4:foreachworkeri=1,...,ndo5:Sample(cid:15)i∼N(0,I)6:ComputereturnsFi=F(θt+σ(cid:15)i)7:endfor8:SendallscalarreturnsFifromeachworkertoeveryotherworker9:foreachworkeri=1,...,ndo10:Reconstructallperturbations(cid:15)jforj=1,...,nusingknownrandomseeds11:Setθt+1←θt+α1nσPnj=1Fj(cid:15)j12:endfor13:endforInpractice,weimplementsamplingbyhavingeachworkerinstantiatealargeblockofGaussiannoiseatthestartoftraining,andthenperturbingitsparametersbyaddingarandomlyindexedsubsetofthesenoisevariablesateachiteration.Althoughthismeansthattheperturbationsarenotstrictly3independentacrossiterations,wedidnotﬁndthistobeaprobleminpractice.Usingthisstrategy,weﬁndthatthesecondpartofAlgorithm2(lines9-12)onlytakesupasmallfractionoftotaltimespendforallourexperiments,evenwhenusingupto1,440parallelworkers.Whenusingmanymoreworkersstill,orwhenusingverylargeneuralnetworks,wecanreducethecomputationrequiredforthispartofthealgorithmbyhavingworkersonlyperturbasubsetoftheparametersθratherthanallofthem:InthiscasetheperturbationdistributionpψcorrespondstoamixtureofGaussians,forwhichtheupdateequationsremainunchanged.Attheveryextreme,everyworkerwouldperturbonlyasinglecoordinateoftheparametervector,whichmeansthatwewouldbeusingpureﬁnitedifferences.Toreducevariance,weuseantitheticsamplingGeweke[1988],alsoknownasmirroredsamplingBrockhoffetal.[2010]intheESliterature:thatis,wealwaysevaluatepairsofperturbations(cid:15),−(cid:15),forGaussiannoisevector(cid:15).WealsoﬁnditusefultoperformﬁtnessshapingWierstraetal.[2014]byapplyingaranktransformationtothereturnsbeforecomputingeachparameterupdate.DoingsoremovestheinﬂuenceofoutlierindividualsineachpopulationanddecreasesthetendencyforEStofallintolocaloptimaearlyintraining.Inaddition,weapplyweightdecaytotheparametersofourpolicynetwork:thispreventstheparametersfromgrowingverylargecomparedtotheperturbations.UnlikeWierstraetal.[2014]wedidnotseebeneﬁtfromadaptingσduringtraining,andwethereforetreatitasaﬁxedhyperparameterinstead.Weperformtheoptimizationdirectlyinparameterspace;exploringindirectencodingsStanleyetal.[2009],vanSteenkisteetal.[2016]isleftforfuturework.EvolutionStrategies,aspresentedabove,workswithfull-lengthepisodes.InsomerarecasesthiscanleadtolowCPUutilization,assomeepisodesrunformanymorestepsthanothers.Forthisreason,wecapepisodelengthataconstantmstepsforallworkers,whichwedynamicallyadjustastrainingprogresses.Forexample,bysettingmtobeequaltotwicethemeannumberofstepstakenperepisode,wecanguaranteethatCPUutilizationstaysabove50%intheworstcase.2.2TheimpactofnetworkparameterizationWhereasRLalgorithmslikeQ-learningandpolicygradientsexplorebysamplingactionsfromastochasticpolicy,EvolutionStrategiesderiveslearningsignalfromsamplinginstantiationsofpolicyparameters.ExplorationinESisthusdrivenbyparameterperturbation.ForEStoimproveuponparametersθ,somemembersofthepopulationmustachievebetterreturnthanothers:i.e.itiscrucialthatGaussianperturbationvectors(cid:15)occasionallyleadtonewindividualsθ+σ(cid:15)withbetterreturn.FortheAtarienvironments,wefoundthatGaussianparameterperturbationsonDeepMind’scon-volutionalarchitectures[Mnihetal.,2015]didnotalwaysleadtoadequateexploration:Forsomeenvironments,randomlyperturbedparameterstendedtoencodepoliciesthatalwaystookonespeciﬁcactionregardlessofthestatethatwasgivenasinput.However,wediscoveredthatwecouldmatchtheperformanceofpolicygradientmethodsformostgamesbyusingvirtualbatchnormalization[Sali-mansetal.,2016]inthepolicyspeciﬁcation.Virtualbatchnormalizationispreciselyequivalenttobatchnormalization[IoffeandSzegedy,2015]wheretheminibatchusedforcalculatingnormalizingstatisticsischosenatthestartoftrainingandisﬁxed.Thischangeinparameterizationmakesthepolicymoresensitivetoverysmallchangesintheinputimageattheearlystagesoftrainingwhentheweightsofthepolicyarerandom,ensuringthatthepolicytakesawide-enoughvarietyofactionstogatheroccasionalrewards.Formostapplications,adownsideofvirtualbatchnormalizationisthatitmakestrainingmoreexpensive.Forourapplication,however,theminibatchusedtocalculatethenormalizingstatisticsismuchsmallerthanthenumberofstepstakenduringatypicalepisode,meaningthattheoverheadisnegligible.FortheMuJoCotasks,weachievedgoodperformanceonnearlyalltheenvironmentswiththestandardmultilayerperceptronsmappingtocontinuousactions.However,weobservedthatforsomeenvironments,wecouldencouragemoreexplorationbydiscretizingtheactions.Thisforcedtheactionstobenon-smoothwithrespecttoinputobservationsandparameterperturbations,andtherebyencouragedawidevarietyofbehaviorstobeplayedoutoverthecourseofrollouts.3SmoothinginparameterspaceversussmoothinginactionspaceAsmentionedinsection2,alargesourceofdifﬁcultyinRLstemsfromthelackofinformativegradientsofpolicyperformance:suchgradientsmaynotexistduetonon-smoothnessoftheenviron-4mentorpolicy,ormayonlybeavailableashigh-varianceestimatesbecausetheenvironmentusuallycanonlybeaccessedviasampling.Explicitly,supposewewishtosolvegeneraldecisionproblemsthatgiveareturnR(a)afterwetakeasequenceofactionsa={a1,...,aT},wheretheactionsaredeterminedbyaeitheradeterministicorastochasticpolicyfunctionat=π(s;θ).TheobjectivewewouldliketooptimizeisthusF(θ)=R(a(θ)).Sincetheactionsareallowedtobediscreteandthepolicyisallowedtobedeterministic,F(θ)canbenon-smoothinθ.Moreimportantly,becausewedonothaveexplicitaccesstotheunder-lyingstatetransitionfunctionofourdecisionproblems,thegradientscannotbecomputedwithabackpropagation-likealgorithm.Thismeanswecannotdirectlyusestandardgradient-basedoptimizationmethodstoﬁndagoodsolutionforθ.Inordertobothmaketheproblemsmoothandtohaveameansoftoestimateitsgradients,weneedtoaddnoise.Policygradientmethodsaddthenoiseinactionspace,whichisdonebysamplingtheactionsfromanappropriatedistribution.Forexample,iftheactionsarediscreteandπ(s;θ)calculatesascoreforeachactionbeforeselectingthebestone,thenwewouldsampleanactiona((cid:15),θ)(here(cid:15)isthenoisesource)fromacategoricaldistributionoveractionsateachtimeperiod,applyingasoftmaxtothescoresofeachaction.DoingsoyieldstheobjectiveFPG(θ)=E(cid:15)R(a((cid:15),θ)),withgradients∇θFPG(θ)=E(cid:15){R(a((cid:15),θ))∇θlogp(a((cid:15),θ);θ)}.Evolutionstrategies,ontheotherhand,addthenoiseinparameterspace.Thatis,theyperturbtheparametersas˜θ=θ+ξ,withξfromamultivariateGaussiandistribution,andthenpickactionsasat=a(ξ,θ)=π(s;˜θ).ItcanbeinterpretedasaddingaGaussianblurtotheoriginalobjective,whichresultsinasmooth,differentiablecostFES(θ)=EξR(a(ξ,θ)),thistimewithgradients∇θFES(θ)=EξnR(a(ξ,θ))∇θlogp(˜θ(ξ,θ);θ)o.Thetwomethodsforsmoothingthedecisionproblemarethusquitesimilar,andcanbemadeevenmoresobyaddingnoisetoboththeparametersandtheactions.3.1WhenisESbetterthanpolicygradients?Giventhesetwomethodsofsmoothingthedecisionproblem,whichshouldweuse?TheanswerdependsstronglyonthestructureofthedecisionproblemandonwhichtypeofMonteCarloestimatorisusedtoestimatethegradients∇θFPG(θ)and∇θFES(θ).Supposethecorrelationbetweenthereturnandtheindividualactionsislow(asistrueforanyhardRLproblem).AssumingweapproximatethesegradientsusingsimpleMonteCarlo(REINFORCE)withagoodbaselineonthereturn,wehaveVar[∇θFPG(θ)]≈Var[R(a)]Var[∇θlogp(a;θ)],Var[∇θFES(θ)]≈Var[R(a)]Var[∇θlogp(˜θ;θ)].Ifbothmethodsperformasimilaramountofexploration,Var[R(a)]willbesimilarforbothex-pressions.Thedifferencewillthusbeinthesecondterm.Herewehavethat∇θlogp(a;θ)=PTt=1∇θlogp(at;θ)isasumofTuncorrelatedterms,sothatthevarianceofthepolicygradi-entestimatorwillgrownearlylinearlywithT.Thecorrespondingtermforevolutionstrategies,∇θlogp(˜θ;θ),isindependentofT.Evolutionstrategieswillthushaveanadvantagecomparedtopolicygradientsforlongepisodeswithverymanytimesteps.Inpractice,theeffectivenumberofstepsTisoftenreducedinpolicygradientmethodsbydiscountingrewards.Iftheeffectsofactionsareshort-lasting,thisallowsustodramaticallyreducethevarianceinourgradientestimate,andthishasbeencriticaltothesuccessofapplicationssuchasAtarigames.However,thisdiscountingwillbiasourgradientestimateifactionshavelonglastingeffects.AnotherstrategyforreducingtheeffectivevalueofTistousevaluefunctionapproximation.Thishasalsobeeneffective,butonceagainrunstheriskofbiasingourgradientestimates.EvolutionstrategiesisthusanattractivechoiceiftheeffectivenumberoftimestepsTislong,actionshavelong-lastingeffects,andifnogoodvaluefunctionestimatesareavailable.53.2ProblemdimensionalityThegradientestimateofEScanbeinterpretedasamethodforrandomizedﬁnitedifferencesinhigh-dimensionalspace.Indeed,usingthefactthatE(cid:15)∼N(0,I){F(θ)(cid:15)/σ}=0,weget∇θη(θ)=E(cid:15)∼N(0,I){F(θ+σ(cid:15))(cid:15)/σ}=E(cid:15)∼N(0,I){(F(θ+σ(cid:15))−F(θ))(cid:15)/σ}ItisnowapparentthatEScanbeseenascomputingaﬁnitedifferencederivativeestimateinarandomlychosendirection,especiallyasσbecomessmall.TheresemblanceofEStoﬁnitedifferencessuggeststhemethodwillscalepoorlywiththedimensionoftheparametersθ.Theoreticalanalysisindeedshowsthatforgeneralnon-smoothoptimizationproblems,therequirednumberofoptimizationstepsscaleslinearlywiththedimension[NesterovandSpokoiny,2011].However,itisimportanttonotethatthisdoesnotmeanthatlargerneuralnetworkswillperformworsethansmallernetworkswhenoptimizedusingES:whatmattersisthedifﬁculty,orintrinsicdimension,oftheoptimizationproblem.Toseethatthedimensionalityofourmodelcanbecompletelyseparatefromtheeffectivedimensionoftheoptimizationproblem,consideraregressionproblemwhereweapproximateaunivariatevariableywithalinearmodelˆy=x·w:ifwedoublethenumberoffeaturesandparametersinthismodelbyconcatenatingxwithitself(i.e.usingfeaturesx0=(x,x)),theproblemdoesnotbecomemoredifﬁcult.TheESalgorithmwilldoexactlythesamethingwhenappliedtothishigherdimensionalproblem,aslongaswedividethestandarddeviationofthenoisebytwo,aswellasthelearningrate.Inpractice,weobserveslightlybetterresultswhenusinglargernetworkswithES.Forexample,wetriedboththelargernetworkandsmallernetworkusedinA3C[Mnihetal.,2016]forlearningAtari2600games,andonaverageobtainedbetterresultsusingthelargernetwork.Wehypothesizethatthisisduetothesameeffectthatmakesstandardgradient-basedoptimizationoflargeneuralnetworkseasierthanforsmallones:largenetworkshavefewerlocalminima[Kawaguchi,2016].3.3AdvantagesofnotcalculatinggradientsInadditiontobeingeasytoparallelize,andtohavinganadvantageincaseswithlongactionsequencesanddelayedrewards,blackboxoptimizationalgorithmslikeEShaveotheradvantagesoverRLtechniquesthatcalculategradients.ThecommunicationoverheadofimplementingESinadistributedsettingislowerthanforcompetingRLmethodssuchaspolicygradientsandQ-learning,astheonlyinformationthatneedstobecommunicatedacrossprocessesarethescalarreturnandtherandomseedthatwasusedtogeneratetheperturbations(cid:15),ratherthanafullgradient.Also,EScandealwithmaximallysparseanddelayedrewards;onlythetotalreturnofanepisodeisused,whereasothermethodsuseindividualrewardsandtheirexacttiming.Bynotrequiringbackpropagation,blackboxoptimizersreducetheamountofcomputationperepisodebyabouttwothirds,andmemorybypotentiallymuchmore.Inaddition,notexplicitlycalculatingananalyticalgradientprotectsagainstproblemswithexplodinggradientsthatarecommonwhenworkingwithrecurrentneuralnetworks.Bysmoothingthecostfunctioninparameterspace,wereducethepathologicalcurvaturethatcausestheseproblems:boundedcostfunctionsthataresmoothenoughcan’thaveexplodinggradients.Attheextreme,ESallowsustoincorporatenon-differentiableelementsintoourarchitecture,suchasmodulesthatusehardattention[Xuetal.,2015].Blackboxoptimizationmethodsareuniquelysuitedtolowprecisionhardwarefordeeplearning.Lowprecisionarithmetic,suchasinbinaryneuralnetworks,canbeperformedmuchcheaperthanathighprecision.Whenoptimizingsuchlowprecisionarchitectures,biasedlowprecisiongradientestimatescanbeaproblemwhenusinggradient-basedmethods.Similarly,specializedhardwareforneuralnetworkinference,suchasTPUs[Jouppietal.,2017],canbeuseddirectlywhenperformingoptimizationusingES,whiletheirlimitedmemoryusuallymakesbackpropagationimpossible.Byperturbinginparameterspaceinsteadofactionspace,blackboxoptimizersarenaturallyinvarianttothefrequencyatwhichouragentactsintheenvironment.ForMDP-basedreinforcementlearningalgorithms,ontheotherhand,itiswellknownthatframeskipisacrucialparametertogetrightfortheoptimizationtosucceed[Braylanetal.,2005].Whilethisisusuallyasolvableproblemforgamesthatonlyrequireshort-termplanningandaction,itisaproblemforlearninglongertermstrategicbehavior.Fortheseproblems,RLneedshierarchytosucceed[ParrandRussell,1998],whichisnotasnecessarywhenusingblackboxoptimization.64Experiments4.1MuJoCoWeevaluatedESonabenchmarkofcontinuousroboticcontrolproblemsintheOpenAIGym[Brockmanetal.,2016]againstahighlytunedimplementationofTrustRegionPolicyOptimiza-tion[Schulmanetal.,2015],apolicygradientalgorithmdesignedtoefﬁcientlyoptimizeneuralnetworkpolicies.Wetestedonbothclassicproblems,likebalancinganinvertedpendulum,andmoredifﬁcultonesfoundinrecentliterature,likelearning2Dhoppingandwalkinggaits.TheenvironmentsweresimulatedbyMuJoCo[Todorovetal.,2012].WeusedbothESandTRPOtotrainpolicieswithidenticalarchitectures:multilayerperceptronswithtwo64-unithiddenlayersseparatedbytanhnonlinearities.WefoundthatESoccasionallybeneﬁtedfromdiscreteactions,sincecontinuousactionscouldbetoosmoothwithrespecttoparameterperturbationandcouldhamperexploration(seesection2.2).Forthehoppingandswimmingtasks,wediscretizedtheactionsforESinto10binsforeachactioncomponent.WefoundthatESwasabletosolvethesetasksuptoTRPO’sﬁnalperformanceafter5milliontimestepsofenvironmentinteraction.Toobtainthisresult,weranESover6randomseedsandcomparedthemeanlearningcurvestosimilarlycomputedcurvesforTRPO.TheexactsamplecomplexitytradeoffsoverthecourseoflearningarelistedinTable1,anddetailedresultsarelistedinTable3ofthesupplement.Generally,wewereabletosolvetheenvironmentsinlessthan10xpenaltyinsamplecomplexityonthehardenvironments(HopperandWalker2d)comparedtoTRPO.Onsimpleenvironments,weachievedupto3xbettersamplecomplexitythanTRPO.Table1:MuJoCotasks:RatioofEStimestepstoTRPOtimestepsneededtoreachvariouspercentagesofTRPO’slearningprogressat5milliontimesteps.Environment25%50%75%100%HalfCheetah0.150.490.420.58Hopper0.533.646.056.94InvertedDoublePendulum0.460.480.491.23InvertedPendulum0.280.520.780.88Swimmer0.560.470.530.30Walker2d0.415.698.027.884.2AtariWeranourparallelimplementationofEvolutionStrategies,describedinAlgorithm2,on51Atari2600gamesavailableinOpenAIGym[Brockmanetal.,2016].WeusedthesamepreprocessingandfeedforwardCNNarchitectureusedbyMnihetal.[2016].Allgamesweretrainedfor1billionframes,whichrequiresaboutthesameamountofneuralnetworkcomputationasthepublished1-dayresultsforA3C[Mnihetal.,2016]whichuses320millionframes.ThedifferenceisduetothefactthatESdoesnotperformbackpropagationanddoesnotuseavaluefunction.Byparallelizingtheevaluationofperturbedparametersacross720CPUsonAmazonEC2,wecanbringdownthetimerequiredforthetrainingprocesstoaboutonehourpergame.Aftertraining,wecomparedﬁnalperformanceagainstthepublishedA3CresultsandfoundthatESperformedbetterin23gamestested,whileitperformedworsein28.ThefullresultsareinTable2inthesupplementarymaterial.4.3ParallelizationESisparticularlyamenabletoparallelizationbecauseofitslowcommunicationbandwidthrequire-ment(Section2.1).WeimplementedadistributedversionofAlgorithm2toinvestigatehowESscaleswiththenumberofworkers.OurdistributedimplementationdidnotrelyonspecialnetworkingsetupandwastestedonpubliccloudcomputingserviceAmazonEC2.Wepickedthe3DHumanoidwalkingtaskfromOpenAIGym[Brockmanetal.,2016]asthetestproblemforourscalingexperiment,becauseitisoneofthemostchallengingcontinuouscontrolproblemssolvablebystate-of-the-artRLtechniques,whichrequireaboutadaytolearnonmodernhardware[Schulmanetal.,2015,Duanetal.,2016a].Solving3DHumanoidwithESonone18-coremachinetakesabout11hours,whichisonparwithRL.However,whendistributedacross80710210310110218cores,657minutes1440cores,10minutesNumberofCPUcoresMediantimetosolve(minutes)Figure1:Timetoreachascoreof6000on3DHumanoidwithdifferentnumberofCPUcores.Experimentsarerepeated7timesandmediantimeisreported.Figure2:LearningcurvesforPongusingvaryingframe-skipparameters.Althoughper-formanceisstochastic,eachsettingleadstoaboutequallyfastlearning,witheachruncon-verginginaround100weightupdates.machinesand1,440CPUcores,EScansolve3DHumanoidinjust10minutes,reducingexperimentturnaroundtimebytwoordersofmagnitude.Figure1showsthat,forthistask,ESisabletoachievelinearspeedupinthenumberofCPUcores.4.4InvariancetotemporalresolutionItiscommonpracticeinRLtohavetheagentdecideonitsactionsinalowerfrequencythanisusedinthesimulatorthatrunstheenvironment.Thisactionfrequency,orframe-skip,isacrucialparameterinmanyRLalgorithms[Braylanetal.,2005].Iftheframe-skipissettoohigh,theagentcannotmakeitsdecisionsataﬁneenoughtimeframetoperformwellintheenvironment.If,ontheotherhand,theframeskipissettoolow,theeffectivetimelengthoftheepisodeincreasestoomuch,whichdeterioratesoptimizationperformanceasanalyzedinsection3.1.AnadvantageofESisthatitsgradientestimateisinvarianttothelengthoftheepisode,whichmakesitmuchmorerobusttotheactionfrequency.WedemonstratethisbyrunningtheAtarigamePongusingaframeskipparameterin{1,2,3,4}.AscanbeseeninFigure2,thelearningcurvesforeachsettingindeedlookverysimilar.5RelatedworkTherehavebeenmanyattemptsatapplyingmethodsrelatedtoEStotrainneuralnetworksRisiandTogelius[2015].ForAtari,Hausknechtetal.[2014]obtainimpressiveresults.Sehnkeetal.[2010]proposedamethodcloselyrelatedtheoneinvestigatedinourwork.Koutníketal.[2013,2010]andSrivastavaetal.[2012]havesimilarlyappliedananESmethodtoRLproblemswithvisualinputs,butwherethepolicywascompressedinanumberofdifferentways.NaturalevolutionstrategieshasbeensuccessfullyappliedtoblackboxoptimizationWierstraetal.[2008,2014],aswellasforthetrainingoftherecurrentweightsinrecurrentneuralnetworksSchmidhuberetal.[2007].StulpandSigaud[2012]exploredsimilarapproachestoblackboxoptimization.Aninterestinghybridofblack-boxoptimizationandpolicygradientmethodswasrecentlyexploredbyUsunieretal.[2016].Hyper-NeatStanleyetal.[2009]isanalternativeapproachtoevolvingboththeweightsoftheneuralnetworksandtheirparameters.DerivativefreeoptimizationmethodshavealsobeenanalyzedintheconvexsettingDuchietal.[2015],Nesterov[2012].Themaincontributioninourworkisinshowingthatthisclassofalgorithmsisextremelyscalableandefﬁcienttouseondistributedhardware.WehaveshownthatES,whencarefullyimplemented,iscompetitivewithcompetingRLalgorithmsintermsofperformanceonthehardestproblemssolvabletoday,andissurprisinglycloseintermsofdataefﬁciency,whiletakinglesswallclocktimetotrain.86ConclusionWehaveexploredEvolutionStrategies,aclassofblack-boxoptimizationalgorithms,asanalternativetopopularMDP-basedRLtechniquessuchasQ-learningandpolicygradients.ExperimentsonAtariandMuJoCoshowthatitisaviableoptionwithsomeattractivefeatures:itisinvarianttoactionfrequencyanddelayedrewards,anditdoesnotneedtemporaldiscountingorvaluefunctionapproximation.Mostimportantly,ESishighlyparallelizable,whichallowsustomakeupforadecreaseddataefﬁciencybyscalingtomoreparallelworkers.Infuturework,weplantoapplyevolutionstrategiestothoseproblemsforwhichMDP-basedreinforcementlearningislesswell-suited:problemswithlongtimehorizonsandcomplicatedrewardstructure.Weareparticularlyinterestedinmeta-learning,orlearning-to-learn.Aproofofconceptformeta-learninginanRLsettingwasgivenbyDuanetal.[2016b]:Usingblack-boxoptimizationwehopetobeabletoextendtheseresults.WealsoplantoexaminecombiningESwithfastlowprecisionneuralnetworkimplementationstofullymakeuseofthegradient-freenatureofES.ReferencesAlexBraylan,MarkHollenbeck,ElliotMeyerson,andRistoMiikkulainen.Frameskipisapowerfulparameterforlearningtoplayatari.Space,1600:1800,2005.DimoBrockhoff,AnneAuger,NikolausHansen,DirkVArnold,andTimHohm.Mirroredsamplingandsequentialselectionforevolutionstrategies.InInternationalConferenceonParallelProblemSolvingfromNature,pages11–21.Springer,2010.GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,andWojciechZaremba.OpenAIGym.arXivpreprintarXiv:1606.01540,2016.YanDuan,XiChen,ReinHouthooft,JohnSchulman,andPieterAbbeel.Benchmarkingdeepreinforcementlearningforcontinuouscontrol.InProceedingsofthe33rdInternationalConferenceonMachineLearning(ICML),2016a.YanDuan,JohnSchulman,XiChen,PeterLBartlett,IlyaSutskever,andPieterAbbeel.RL2:Fastreinforcementlearningviaslowreinforcementlearning.arXivpreprintarXiv:1611.02779,2016b.JohnCDuchi,MichaelIJordan,MartinJWainwright,andAndreWibisono.Optimalratesforzero-orderconvexoptimization:Thepoweroftwofunctionevaluations.IEEETransactionsonInformationTheory,61(5):2788–2806,2015.JohnGeweke.Antitheticaccelerationofmontecarlointegrationinbayesianinference.JournalofEconometrics,38(1-2):73–89,1988.TobiasGlasmachers,TomSchaul,andJürgenSchmidhuber.Anaturalevolutionstrategyformulti-objectiveoptimization.InInternationalConferenceonParallelProblemSolvingfromNature,pages627–636.Springer,2010a.TobiasGlasmachers,TomSchaul,SunYi,DaanWierstra,andJürgenSchmidhuber.Exponentialnatu-ralevolutionstrategies.InProceedingsofthe12thannualconferenceonGeneticandevolutionarycomputation,pages393–400.ACM,2010b.NikolausHansenandAndreasOstermeier.Completelyderandomizedself-adaptationinevolutionstrategies.Evolutionarycomputation,9(2):159–195,2001.MatthewHausknecht,JoelLehman,RistoMiikkulainen,andPeterStone.Aneuroevolutionapproachtogeneralatarigameplaying.IEEETransactionsonComputationalIntelligenceandAIinGames,6(4):355–366,2014.SergeyIoffeandChristianSzegedy.Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.arXivpreprintarXiv:1502.03167,2015.NormanPJouppi,CliffYoung,NishantPatil,DavidPatterson,GauravAgrawal,RaminderBajwa,SarahBates,SureshBhatia,NanBoden,AlBorchers,etal.In-datacenterperformanceanalysisofatensorprocessingunit.arXivpreprintarXiv:1704.04760,2017.9KenjiKawaguchi.Deeplearningwithoutpoorlocalminima.InAdvancesInNeuralInformationProcessingSystems,pages586–594,2016.JanKoutník,FaustinoGomez,andJürgenSchmidhuber.Evolvingneuralnetworksincompressedweightspace.InProceedingsofthe12thannualconferenceonGeneticandevolutionarycomputa-tion,pages619–626.ACM,2010.JanKoutník,GiuseppeCuccu,JürgenSchmidhuber,andFaustinoGomez.Evolvinglarge-scaleneuralnetworksforvision-basedreinforcementlearning.InProceedingsofthe15thannualconferenceonGeneticandevolutionarycomputation,pages1061–1068.ACM,2013.VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-levelcontrolthroughdeepreinforcementlearning.Nature,518(7540):529–533,2015.VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyPLillicrap,TimHarley,DavidSilver,andKorayKavukcuoglu.Asynchronousmethodsfordeepreinforcementlearning.InInternationalConferenceonMachineLearning,2016.YuriiNesterov.Efﬁciencyofcoordinatedescentmethodsonhuge-scaleoptimizationproblems.SIAMJournalonOptimization,22(2):341–362,2012.YuriiNesterovandVladimirSpokoiny.Randomgradient-freeminimizationofconvexfunctions.FoundationsofComputationalMathematics,pages1–40,2011.AndrewNg,AdamCoates,MarkDiel,VarunGanapathi,JamieSchulte,BenTse,EricBerger,andEricLiang.Autonomousinvertedhelicopterﬂightviareinforcementlearning.ExperimentalRoboticsIX,pages363–372,2006.RonaldParrandStuartRussell.Reinforcementlearningwithhierarchiesofmachines.Advancesinneuralinformationprocessingsystems,pages1043–1049,1998.I.RechenbergandM.Eigen.Evolutionsstrategie:OptimierungTechnischerSystemenachPrinzipienderBiologischenEvolution.Frommann-HolzboogStuttgart,1973.SebastianRisiandJulianTogelius.Neuroevolutioningames:Stateoftheartandopenchallenges.IEEETransactionsonComputationalIntelligenceandAIinGames,2015.TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.Improvedtechniquesfortraininggans.InAdvancesinNeuralInformationProcessingSystems,pages2226–2234,2016.TomSchaul,TobiasGlasmachers,andJürgenSchmidhuber.Highdimensionsandheavytailsfornaturalevolutionstrategies.InProceedingsofthe13thannualconferenceonGeneticandevolutionarycomputation,pages845–852.ACM,2011.JuergenSchmidhuberandJieyuZhao.Directpolicysearchanduncertainpolicyevaluation.InAaaispringsymposiumonsearchunderuncertainandincompleteinformation,stanforduniv,pages119–124,1998.JürgenSchmidhuber,DaanWierstra,MatteoGagliolo,andFaustinoGomez.Trainingrecurrentnetworksbyevolino.Neuralcomputation,19(3):757–779,2007.JohnSchulman,SergeyLevine,PieterAbbeel,MichaelIJordan,andPhilippMoritz.Trustregionpolicyoptimization.InICML,pages1889–1897,2015.H.-P.Schwefel.Numerischeoptimierungvoncomputer-modellenmittelsderevolutionsstrategie.1977.FrankSehnke,ChristianOsendorfer,ThomasRückstieß,AlexGraves,JanPeters,andJürgenSchmid-huber.Parameter-exploringpolicygradients.NeuralNetworks,23(4):551–559,2010.DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche,JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,etal.Masteringthegameofgowithdeepneuralnetworksandtreesearch.Nature,529(7587):484–489,2016.10JamesCSpall.Multivariatestochasticapproximationusingasimultaneousperturbationgradientapproximation.IEEEtransactionsonautomaticcontrol,37(3):332–341,1992.RupeshKumarSrivastava,JürgenSchmidhuber,andFaustinoGomez.Generalizedcompressednetworksearch.InInternationalConferenceonParallelProblemSolvingfromNature,pages337–346.Springer,2012.KennethOStanley,DavidBD’Ambrosio,andJasonGauci.Ahypercube-basedencodingforevolvinglarge-scaleneuralnetworks.Artiﬁciallife,15(2):185–212,2009.FreekStulpandOlivierSigaud.Policyimprovementmethods:Betweenblack-boxoptimizationandepisodicreinforcementlearning.2012.YiSun,DaanWierstra,TomSchaul,andJuergenSchmidhuber.Efﬁcientnaturalevolutionstrategies.InProceedingsofthe11thAnnualconferenceonGeneticandevolutionarycomputation,pages539–546.ACM,2009.EmanuelTodorov,TomErez,andYuvalTassa.Mujoco:Aphysicsengineformodel-basedcontrol.InIntelligentRobotsandSystems(IROS),2012IEEE/RSJInternationalConferenceon,pages5026–5033.IEEE,2012.NicolasUsunier,GabrielSynnaeve,ZemingLin,andSoumithChintala.Episodicexplorationfordeepdeterministicpolicies:Anapplicationtostarcraftmicromanagementtasks.arXivpreprintarXiv:1609.02993,2016.SjoerdvanSteenkiste,JanKoutník,KurtDriessens,andJürgenSchmidhuber.Awavelet-basedencodingforneuroevolution.InProceedingsofthe2016onGeneticandEvolutionaryComputationConference,pages517–524.ACM,2016.DaanWierstra,TomSchaul,JanPeters,andJuergenSchmidhuber.Naturalevolutionstrategies.InEvolutionaryComputation,2008.CEC2008.(IEEEWorldCongressonComputationalIntelli-gence).IEEECongresson,pages3381–3387.IEEE,2008.DaanWierstra,TomSchaul,TobiasGlasmachers,YiSun,JanPeters,andJürgenSchmidhuber.Naturalevolutionstrategies.JournalofMachineLearningResearch,15(1):949–980,2014.RonaldJWilliams.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcementlearning.Machinelearning,8(3-4):229–256,1992.KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCCourville,RuslanSalakhutdinov,RichardSZemel,andYoshuaBengio.Show,attendandtell:Neuralimagecaptiongenerationwithvisualattention.InICML,volume14,pages77–81,2015.SunYi,DaanWierstra,TomSchaul,andJürgenSchmidhuber.Stochasticsearchusingthenaturalgradient.InProceedingsofthe26thAnnualInternationalConferenceonMachineLearning,pages1161–1168.ACM,2009.11GameDQNA3CFF,1dayHyperNEATESFF,1hourA2CFFAmidar133.4283.9184.4112.0548.2Assault3332.33746.1912.61673.92026.6Asterix124.56723.02340.01440.03779.7Asteroids697.13009.41694.01562.01733.4Atlantis76108.0772392.061260.01267410.02872644.8BankHeist176.3946.0214.0225.0724.1BattleZone17560.011340.036200.016600.08406.2BeamRider8672.413235.91412.8744.04438.9Berzerk1433.41394.0686.0720.6Bowling41.236.2135.830.028.9Boxing25.833.716.449.895.8Breakout303.9551.62.89.5368.5Centipede3773.13306.525275.27783.92773.3ChopperCommand3046.04669.03960.03710.01700.0CrazyClimber50992.0101624.00.026430.0100034.4DemonAttack12835.284997.514620.01166.523657.7DoubleDunk21.60.12.00.23.2Enduro475.682.293.695.00.0FishingDerby2.313.649.849.033.9Freeway25.80.129.031.00.0Frostbite157.4180.12260.0370.0266.6Gopher2731.88442.8364.0582.06266.2Gravitar216.5269.5370.0805.0256.2IceHockey3.84.710.64.14.9Kangaroo2696.0106.0800.011200.01357.6Krull3864.08066.612601.48647.26411.5Montezuma’sRevenge50.053.00.00.00.0NameThisGame5439.95614.06742.04503.05532.8Phoenix28181.81762.04041.014104.7PitFall123.00.00.08.2Pong16.211.417.421.020.8PrivateEye298.2194.410747.4100.0100.0Q*Bert4589.813752.3695.0147.515758.6RiverRaid4065.310001.22616.05009.09856.9RoadRunner9264.031769.03220.016590.033846.9Robotank58.52.343.811.92.2Seaquest2793.92300.2716.01390.01763.7Skiing13700.07983.615442.515245.8Solaris1884.8160.02090.02265.0SpaceInvaders1449.72214.71251.0678.5951.9StarGunner34081.064393.02720.01470.040065.6Tennis2.310.20.04.511.2TimePilot5640.05825.07340.04970.04637.5Tutankham32.426.123.6130.3194.3UpandDown3311.354525.443734.067974.075785.9Venture54.019.00.0760.00.0VideoPinball20228.1185852.60.022834.846470.1WizardofWor246.05278.03360.03480.01587.5YarsRevenge7270.824096.416401.78963.5Zaxxon831.02659.03000.06380.05.6Table2:FinalresultsobtainedusingEvolutionStrategiesonAtari2600games(feedforwardCNNpolicy,deterministicpolicyevaluation,averagedover10re-runswithupto30randominitialno-ops),andcomparedtoresultsforDQNandA3CfromMnihetal.[2016]andHyperNEATfromHausknechtetal.[2014].A2CisoursynchronousvariantofA3C,anditsreportedscoresareobtainedwith320MtrainingframeswiththesameevaluationsetupasfortheESresults.Allmethodsweretrainedonrawpixelinput.12Table3:MuJoCotasks:RatioofEStimestepstoTRPOtimestepsneededtoreachvariouspercentagesofTRPO’slearningprogressat5milliontimesteps.TheseresultswerecomputedfromESlearningcurvesaveragedover6reruns.Environment%TRPOﬁnalscoreTRPOscoreTRPOtimestepsEStimestepsEStimesteps/TRPOtimestepsHalfCheetah25%-1.359.05e+051.36e+050.1550%793.551.70e+068.28e+050.4975%1589.833.34e+061.42e+060.42100%2385.795.00e+062.88e+060.58Hopper25%877.457.29e+053.83e+050.5350%1718.161.03e+063.73e+063.6475%2561.111.59e+069.63e+066.05100%3403.464.56e+063.16e+076.94InvertedDoublePendulum25%2358.988.73e+053.98e+050.4650%4609.689.65e+054.66e+050.4875%6874.031.07e+065.30e+050.49100%9104.074.39e+065.39e+061.23InvertedPendulum25%276.592.21e+056.25e+040.2850%519.152.73e+051.43e+050.5275%753.173.25e+052.55e+050.78100%1000.005.17e+054.55e+050.88Swimmer25%41.971.04e+065.88e+050.5650%70.731.82e+068.52e+050.4775%99.682.33e+061.23e+060.53100%128.254.59e+061.39e+060.30Walker2d25%957.681.55e+066.43e+050.4150%1916.482.27e+061.29e+075.6975%2872.812.89e+062.31e+078.02100%3830.034.81e+063.79e+077.8813
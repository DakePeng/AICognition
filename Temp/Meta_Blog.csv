affiliation,postLink,researchLink,title,text
Meta_Blog,https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/,,Our next generation Meta Training and Inference Accelerator,"The next generation of Meta’s large-scale infrastructure is being built with AI in mind, including supporting new generative AI (GenAI) products and services, recommendation systems, and advanced AI research. It’s an investment we expect will grow in the years ahead as the compute requirements to support AI models increase alongside the models’ sophistication.

Last year, we unveiled the Meta Training and Inference Accelerator (MTIA) v1, our first-generation AI inference accelerator that we designed in-house with Meta’s AI workloads in mind – specifically our deep learning recommendation models that are improving a variety of experiences across our products.

MTIA is a long-term venture to provide the most efficient architecture for Meta’s unique workloads. As AI workloads become increasingly important to our products and services, this efficiency will improve our ability to provide the best experiences for our users around the world. MTIA v1 was an important step in improving the compute efficiency of our infrastructure and better supporting our software developers as they build AI models that will facilitate new and better user experiences.

Now, we’re sharing details about the next generation of MTIA."
Meta_Blog,https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/,,Our next generation Meta Training and Inference Accelerator,"The next generation of Meta’s large-scale infrastructure is being built with AI in mind, including supporting new generative AI (GenAI) products and services, recommendation systems, and advanced AI research. It’s an investment we expect will grow in the years ahead as the compute requirements to support AI models increase alongside the models’ sophistication.

Last year, we unveiled the Meta Training and Inference Accelerator (MTIA) v1, our first-generation AI inference accelerator that we designed in-house with Meta’s AI workloads in mind – specifically our deep learning recommendation models that are improving a variety of experiences across our products.

MTIA is a long-term venture to provide the most efficient architecture for Meta’s unique workloads. As AI workloads become increasingly important to our products and services, this efficiency will improve our ability to provide the best experiences for our users around the world. MTIA v1 was an important step in improving the compute efficiency of our infrastructure and better supporting our software developers as they build AI models that will facilitate new and better user experiences.

Now, we’re sharing details about the next generation of MTIA."
Meta_Blog,https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/,,"Introducing SceneScript, a novel approach for 3D scene reconstruction","However, building these 3D scene representations is a complex task. Current MR headsets like Meta Quest 3 create a virtual representation of physical spaces based on raw visual data from cameras or 3D sensors. This raw data is converted into a series of shapes that describe distinct features of the environment, like walls, ceilings, and doors. Typically, these systems rely on pre-defined rules to convert the raw data into shapes. Yet that heuristic approach can often lead to errors, especially in spaces with unique or irregular geometries.

Imagine a pair of stylish, lightweight glasses that combined contextualized AI with a display that could seamlessly give you access to real-time information when you need it and proactively help you as you go about your day. In order for such a pair of augmented reality (AR) glasses to become reality, the system must be able to understand the layout of your physical environment and how the world is shaped in 3D. That understanding would let AR glasses tailor content to you and your individual context, like seamlessly blending a digital overlay with your physical space or giving you turn-by-turn directions to help you navigate unfamiliar locations.

Large language models (LLMs) like Llama operate using a technique called next token prediction, in which the AI model predicts the next word in a sentence based on the words that came before it. For example, if you typed the words, “The cat sat on the...,” the model would predict that the next word is likely to be “mat” or “floor.”

This results in a representation of physical scenes which is compact , reducing memory requirements to only a few bytes; complete , resulting in crisp geometry, similar to scalable vector graphics; and importantly, interpretable , meaning that we can easily read and edit those representations.

Rather than using hard-coded rules to convert raw visual data into an approximation of a room’s architectural elements, SceneScript is trained to directly infer a room’s geometry using end-to-end machine learning.



SceneScript leverages the same concept of next token prediction used by LLMs. However, instead of predicting a general language token, the SceneScript model predicts the next architectural token, such as ‘wall’ or ‘door.’ By giving the network a large amount of training data, the SceneScript model learns how to encode visual data into a fundamental representation of the scene, which it can then decode into language that describes the room layout. This allows SceneScript to interpret and reconstruct complex environments from visual data and create text descriptions that effectively describe the structure of the scenes that it analyzes. However, the team required a substantial amount of data to train the network and teach it how physical spaces are typically laid out—and they needed to ensure they were preserving privacy. This presented a unique challenge.



Training SceneScript in simulation



While LLMs rely on vast amounts of training data that typically comes from a range of publicly available text sources on the web, no such repository of information yet exists for physical spaces at the scale needed for training an end-to-end model. So the Reality Labs Research team had to find another solution. Instead of relying on data from physical environments, the SceneScript team created a synthetic dataset of indoor environments, called Aria Synthetic Environments . This dataset comprises 100,000 completely unique interior environments, each described using the SceneScript language and paired with a simulated video walking through each scene. The video rendered through each scene is simulated using the same sensor characteristics as Project Aria , Reality Labs Research’s glasses for accelerating AI and ML research. This approach allows the SceneScript model to be completely trained in simulation, under privacy-preserving conditions. The model can then be validated using physical-world footage from Project Aria glasses, confirming the model’s ability to generalize to actual environments.



Last year, we made the Aria Synthetic Environments dataset available to academic researchers, which we hope will help accelerate public research within this exciting area of study.



Extending SceneScript to describe objects, states, and complex geometry

Another of SceneScript’s strengths is its extensibility. Simply by adding a few additional parameters to scene language that describes doors in the Aria Synthetic Environments dataset, the network can be trained to accurately predict the degree to which doors are open or closed in physical environments. Additionally, by adding new features to the architectural language, it’s possible to accurately predict the location of objects and—further still—decompose those objects into their constituent parts. For example, a sofa could be represented within the SceneScript language as a set of geometric shapes including the cushions, legs, and arms. This level of detail could eventually be used by designers to create AR content that is truly customized to a wide range of physical environments.



Accelerating AR, pushing LLMs forward, and advancing the state of the art in AI and ML research"
Meta_Blog,https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/,,"Introducing SceneScript, a novel approach for 3D scene reconstruction","However, building these 3D scene representations is a complex task. Current MR headsets like Meta Quest 3 create a virtual representation of physical spaces based on raw visual data from cameras or 3D sensors. This raw data is converted into a series of shapes that describe distinct features of the environment, like walls, ceilings, and doors. Typically, these systems rely on pre-defined rules to convert the raw data into shapes. Yet that heuristic approach can often lead to errors, especially in spaces with unique or irregular geometries.

Imagine a pair of stylish, lightweight glasses that combined contextualized AI with a display that could seamlessly give you access to real-time information when you need it and proactively help you as you go about your day. In order for such a pair of augmented reality (AR) glasses to become reality, the system must be able to understand the layout of your physical environment and how the world is shaped in 3D. That understanding would let AR glasses tailor content to you and your individual context, like seamlessly blending a digital overlay with your physical space or giving you turn-by-turn directions to help you navigate unfamiliar locations.

Large language models (LLMs) like Llama operate using a technique called next token prediction, in which the AI model predicts the next word in a sentence based on the words that came before it. For example, if you typed the words, “The cat sat on the...,” the model would predict that the next word is likely to be “mat” or “floor.”

This results in a representation of physical scenes which is compact , reducing memory requirements to only a few bytes; complete , resulting in crisp geometry, similar to scalable vector graphics; and importantly, interpretable , meaning that we can easily read and edit those representations.

Rather than using hard-coded rules to convert raw visual data into an approximation of a room’s architectural elements, SceneScript is trained to directly infer a room’s geometry using end-to-end machine learning.



SceneScript leverages the same concept of next token prediction used by LLMs. However, instead of predicting a general language token, the SceneScript model predicts the next architectural token, such as ‘wall’ or ‘door.’ By giving the network a large amount of training data, the SceneScript model learns how to encode visual data into a fundamental representation of the scene, which it can then decode into language that describes the room layout. This allows SceneScript to interpret and reconstruct complex environments from visual data and create text descriptions that effectively describe the structure of the scenes that it analyzes. However, the team required a substantial amount of data to train the network and teach it how physical spaces are typically laid out—and they needed to ensure they were preserving privacy. This presented a unique challenge.



Training SceneScript in simulation



While LLMs rely on vast amounts of training data that typically comes from a range of publicly available text sources on the web, no such repository of information yet exists for physical spaces at the scale needed for training an end-to-end model. So the Reality Labs Research team had to find another solution. Instead of relying on data from physical environments, the SceneScript team created a synthetic dataset of indoor environments, called Aria Synthetic Environments . This dataset comprises 100,000 completely unique interior environments, each described using the SceneScript language and paired with a simulated video walking through each scene. The video rendered through each scene is simulated using the same sensor characteristics as Project Aria , Reality Labs Research’s glasses for accelerating AI and ML research. This approach allows the SceneScript model to be completely trained in simulation, under privacy-preserving conditions. The model can then be validated using physical-world footage from Project Aria glasses, confirming the model’s ability to generalize to actual environments.



Last year, we made the Aria Synthetic Environments dataset available to academic researchers, which we hope will help accelerate public research within this exciting area of study.



Extending SceneScript to describe objects, states, and complex geometry

Another of SceneScript’s strengths is its extensibility. Simply by adding a few additional parameters to scene language that describes doors in the Aria Synthetic Environments dataset, the network can be trained to accurately predict the degree to which doors are open or closed in physical environments. Additionally, by adding new features to the architectural language, it’s possible to accurately predict the location of objects and—further still—decompose those objects into their constituent parts. For example, a sofa could be represented within the SceneScript language as a set of geometric shapes including the cushions, legs, and arms. This level of detail could eventually be used by designers to create AR content that is truly customized to a wide range of physical environments.



Accelerating AR, pushing LLMs forward, and advancing the state of the art in AI and ML research"
Meta_Blog,https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/,,V-JEPA: The next step toward advanced machine intelligence,"Takeaways:

“V-JEPA is a step toward a more grounded understanding of the world so machines can achieve more generalized reasoning and planning,” says Meta’s VP & Chief AI Scientist Yann LeCun, who proposed the original Joint Embedding Predictive Architectures (JEPA) in 2022. “Our goal is to build advanced machine intelligence that can learn more like humans do, forming internal models of the world around them to learn, adapt, and forge plans efficiently in the service of completing complex tasks.”

As humans, much of what we learn about the world around us—particularly in our early stages of life—is gleaned through observation. Take Newton’s third law of motion: Even an infant (or a cat) can intuit, after knocking several items off a table and observing the results, that what goes up must come down. You don’t need hours of instruction or to read thousands of books to arrive at that result. Your internal world model—a contextual understanding based on a mental model of the world—predicts these consequences for you, and it’s highly efficient.

V-JEPA is a non-generative model that learns by predicting missing or masked parts of a video in an abstract representation space. This is similar to how our Image Joint Embedding Predictive Architecture (I-JEPA) compares abstract representations of images (rather than comparing the pixels themselves). Unlike generative approaches that try to fill in every missing pixel, V-JEPA has the flexibility to discard unpredictable information, which leads to improved training and sample efficiency by a factor between 1.5x and 6x.

Because it takes a self-supervised learning approach, V-JEPA is pre-trained entirely with unlabeled data. Labels are only used to adapt the model to a particular task after pre-training. This type of architecture proves more efficient than previous models, both in terms of the number of labeled examples needed and the total amount of effort put into learning even the unlabeled data. With V-JEPA, we’ve seen efficiency boosts on both of these fronts.

With V-JEPA, we mask out a large portion of a video so the model is only shown a little bit of the context. We then ask the predictor to fill in the blanks of what’s missing—not in terms of the actual pixels, but rather as a more abstract description in this representation space."
Meta_Blog,https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/,,V-JEPA: The next step toward advanced machine intelligence,"Takeaways:

“V-JEPA is a step toward a more grounded understanding of the world so machines can achieve more generalized reasoning and planning,” says Meta’s VP & Chief AI Scientist Yann LeCun, who proposed the original Joint Embedding Predictive Architectures (JEPA) in 2022. “Our goal is to build advanced machine intelligence that can learn more like humans do, forming internal models of the world around them to learn, adapt, and forge plans efficiently in the service of completing complex tasks.”

As humans, much of what we learn about the world around us—particularly in our early stages of life—is gleaned through observation. Take Newton’s third law of motion: Even an infant (or a cat) can intuit, after knocking several items off a table and observing the results, that what goes up must come down. You don’t need hours of instruction or to read thousands of books to arrive at that result. Your internal world model—a contextual understanding based on a mental model of the world—predicts these consequences for you, and it’s highly efficient.

V-JEPA is a non-generative model that learns by predicting missing or masked parts of a video in an abstract representation space. This is similar to how our Image Joint Embedding Predictive Architecture (I-JEPA) compares abstract representations of images (rather than comparing the pixels themselves). Unlike generative approaches that try to fill in every missing pixel, V-JEPA has the flexibility to discard unpredictable information, which leads to improved training and sample efficiency by a factor between 1.5x and 6x.

Because it takes a self-supervised learning approach, V-JEPA is pre-trained entirely with unlabeled data. Labels are only used to adapt the model to a particular task after pre-training. This type of architecture proves more efficient than previous models, both in terms of the number of labeled examples needed and the total amount of effort put into learning even the unlabeled data. With V-JEPA, we’ve seen efficiency boosts on both of these fronts.

With V-JEPA, we mask out a large portion of a video so the model is only shown a little bit of the context. We then ask the predictor to fill in the blanks of what’s missing—not in terms of the actual pixels, but rather as a more abstract description in this representation space."
Meta_Blog,https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/,,Announcing Purple Llama: Towards open trust and safety in the new world of generative AI,"Components within the Purple Llama project will be licensed permissively, enabling both research and commercial usage. We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.

Today, we are announcing the launch of Purple Llama — an umbrella project that, over time, will bring together tools and evaluations to help the community build responsibly with open generative AI models. The initial release will include tools and evaluations for cybersecurity and input/output safeguards, with more tools to come in the near future.

Collaboration on safety will build trust in the developers driving this new wave of innovation, and requires additional research and contributions on responsible AI. The people building AI systems can’t address the challenges of AI in a vacuum, which is why we want to level the playing field and create a center of mass for open trust and safety.

Generative AI has brought about a new wave of innovation unlike we’ve ever seen before. With it, we have the ability to converse with conversational AIs, generate realistic imagery, and accurately summarize large corpora of documents, all from simple prompts. With over 100 million downloads of Llama models to date, a lot of this innovation is being fueled by open models.

Cybersecurity and LLM prompt safety are important areas for generative AI safety today. We have prioritized these considerations in our first party products and highlighted them as best practice in the Llama 2 Responsible Use Guide.

Cybersecurity

We are sharing what we believe is the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&CK) and built in collaboration with our security subject matter experts. With this initial release, we aim to provide tools that will help address some risks outlined in the White House commitments on developing responsible AI, including: Metrics for quantifying LLM cybersecurity risks.

Tools to evaluate the frequency of insecure code suggestions.

Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks. We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our Cybersec Eval paper for more details.



Input/Output Safeguards

As we outlined in Llama 2’s Responsible Use Guide, we recommend that all inputs and outputs to the LLM be checked and filtered in accordance with content guidelines appropriate to the application. To support this, and empower the community, we are releasing Llama Guard, an openly-available model that performs competitively on common open benchmarks and provides developers with a pretrained model to help defend against generating potentially risky outputs. As part of our ongoing commitment to open and transparent science, we are releasing our methodology and an extended discussion of model performance in our Llama Guard paper. This model has been trained on a mix of publicly-available datasets to enable detection of common types of potentially risky or violating content that may be relevant to a number of developer use cases. Ultimately, our vision is to enable developers to customize this model to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem.



Why purple?

We believe that to truly mitigate the challenges that generative AI presents we need to take both attack (red team) and defensive (blue team) postures. Purple teaming, composed of both red and blue team responsibilities, is a collaborative approach to evaluating and mitigating potential risks. The same ethos applies to generative AI. Hence, our investment in Purple Llama will be comprehensive.

An open ecosystem

Taking an open approach to AI is not new for Meta. Exploratory research, open science, and cross-collaboration are foundational to our AI efforts, and we believe there’s an important opportunity to create an open ecosystem. This collaborative mindset was at the forefront when Llama 2 launched in July with over 100 partners, and we’re excited to share that many of those same partners will be partnering with us on open trust and safety, including: AI Alliance, AMD, Anyscale, AWS, Bain, Cloudflare, Databricks, Dell Technologies, Dropbox, Google Cloud, Hugging Face, IBM, Intel, Microsoft, MLCommons, Nvidia, Oracle, Orange, Scale AI, Together.AI, and many more to come. We’ve also worked with our partners at Papers With Code and HELM to incorporate these evals into their benchmarks, alongside our collaborators within the MLCommons AI Safety Working Group. We’re excited to collaborate with each and every one of our partners as well as others who share the same vision of an open ecosystem of responsibly-developed generative AI.

The path forward

We are hosting a workshop at NeurIPS 2023, where we plan to share these tools and provide a technical deep dive to help people get started. We hope you’ll join us. We expect safety guidelines and best practices to be an ongoing conversation in the field, and we want your input. We are excited to continue the conversation, find ways to partner, and learn more about what areas matter to you.

Dive deeper and learn more"
Meta_Blog,https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/,,Announcing Purple Llama: Towards open trust and safety in the new world of generative AI,"Components within the Purple Llama project will be licensed permissively, enabling both research and commercial usage. We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.

Today, we are announcing the launch of Purple Llama — an umbrella project that, over time, will bring together tools and evaluations to help the community build responsibly with open generative AI models. The initial release will include tools and evaluations for cybersecurity and input/output safeguards, with more tools to come in the near future.

Collaboration on safety will build trust in the developers driving this new wave of innovation, and requires additional research and contributions on responsible AI. The people building AI systems can’t address the challenges of AI in a vacuum, which is why we want to level the playing field and create a center of mass for open trust and safety.

Generative AI has brought about a new wave of innovation unlike we’ve ever seen before. With it, we have the ability to converse with conversational AIs, generate realistic imagery, and accurately summarize large corpora of documents, all from simple prompts. With over 100 million downloads of Llama models to date, a lot of this innovation is being fueled by open models.

Cybersecurity and LLM prompt safety are important areas for generative AI safety today. We have prioritized these considerations in our first party products and highlighted them as best practice in the Llama 2 Responsible Use Guide.

Cybersecurity

We are sharing what we believe is the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&CK) and built in collaboration with our security subject matter experts. With this initial release, we aim to provide tools that will help address some risks outlined in the White House commitments on developing responsible AI, including: Metrics for quantifying LLM cybersecurity risks.

Tools to evaluate the frequency of insecure code suggestions.

Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks. We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our Cybersec Eval paper for more details.



Input/Output Safeguards

As we outlined in Llama 2’s Responsible Use Guide, we recommend that all inputs and outputs to the LLM be checked and filtered in accordance with content guidelines appropriate to the application. To support this, and empower the community, we are releasing Llama Guard, an openly-available model that performs competitively on common open benchmarks and provides developers with a pretrained model to help defend against generating potentially risky outputs. As part of our ongoing commitment to open and transparent science, we are releasing our methodology and an extended discussion of model performance in our Llama Guard paper. This model has been trained on a mix of publicly-available datasets to enable detection of common types of potentially risky or violating content that may be relevant to a number of developer use cases. Ultimately, our vision is to enable developers to customize this model to support relevant use cases and to make it easier to adopt best practices and improve the open ecosystem.



Why purple?

We believe that to truly mitigate the challenges that generative AI presents we need to take both attack (red team) and defensive (blue team) postures. Purple teaming, composed of both red and blue team responsibilities, is a collaborative approach to evaluating and mitigating potential risks. The same ethos applies to generative AI. Hence, our investment in Purple Llama will be comprehensive.

An open ecosystem

Taking an open approach to AI is not new for Meta. Exploratory research, open science, and cross-collaboration are foundational to our AI efforts, and we believe there’s an important opportunity to create an open ecosystem. This collaborative mindset was at the forefront when Llama 2 launched in July with over 100 partners, and we’re excited to share that many of those same partners will be partnering with us on open trust and safety, including: AI Alliance, AMD, Anyscale, AWS, Bain, Cloudflare, Databricks, Dell Technologies, Dropbox, Google Cloud, Hugging Face, IBM, Intel, Microsoft, MLCommons, Nvidia, Oracle, Orange, Scale AI, Together.AI, and many more to come. We’ve also worked with our partners at Papers With Code and HELM to incorporate these evals into their benchmarks, alongside our collaborators within the MLCommons AI Safety Working Group. We’re excited to collaborate with each and every one of our partners as well as others who share the same vision of an open ecosystem of responsibly-developed generative AI.

The path forward

We are hosting a workshop at NeurIPS 2023, where we plan to share these tools and provide a technical deep dive to help people get started. We hope you’ll join us. We expect safety guidelines and best practices to be an ongoing conversation in the field, and we want your input. We are excited to continue the conversation, find ways to partner, and learn more about what areas matter to you.

Dive deeper and learn more"
Meta_Blog,https://ai.meta.com/blog/ai-alliance/,,Introducing the AI Alliance,"To ensure open innovation in AI benefits everyone and that it is built responsibly, the AI Alliance consists of a broad range of organizations that are working across aspects of AI education, research, development and deployment, and governance.

How the AI Alliance Will Work

In addition to bringing together leading developers, scientists, academics, students, and business leaders in the field of artificial intelligence, the AI Alliance will plan to partner with important existing initiatives from governments, non-profit and civil society organizations who are doing valuable and aligned work in the AI space.

The AI Alliance will begin its work with the formation of member-driven working groups across all major topical areas listed above. The Alliance will also establish a governing board and technical oversight committee dedicated to advancing the above project areas, as well as establishing overall project standards and guidelines.

Member Quotes

Arvind Krishna, IBM Chairman and CEO: ""The progress we continue to witness in AI is a testament to open innovation and collaboration across communities of creators, scientists, academics and business leaders. This is a pivotal moment in defining the future of AI. IBM is proud to partner with like-minded organizations through the AI Alliance to ensure this open ecosystem drives an innovative AI agenda underpinned by safety, accountability and scientific rigor.""

Nick Clegg, President, Global Affairs of Meta: “We believe it’s better when AI is developed openly — more people can access the benefits, build innovative products and work on safety. The AI Alliance brings together researchers, developers and companies to share tools and knowledge that can help us all make progress whether models are shared openly or not. We’re looking forward to working with partners to advance the state-of-the-art in AI and help everyone build responsibly.”

Lisa Su, AMD CEO and Chair: “At AMD, we are committed to advancing technology through collaboration. The history of our industry highlights how open, standards-based development leveraging the capabilities of the entire industry both accelerate innovation and ensure technology advances have the largest positive impact. By embracing open standards and transparency across all aspects of the rapidly developing AI ecosystem, we can help ensure the transformational benefits of responsible AI are broadly available. We are proud to join with other industry leaders as a founding member of the AI Alliance and look forward to working together to ensure the rapid advances in AI are a force for positive change.”

Dr. Sun Sumei, Acting Executive Director, A*STAR’s Institute for Infocomm Research (I²R):“As a research institute under A*STAR, I²R spearheads research initiatives and actively strives to enhance the tangible impact of our innovations to address the national needs of Singapore and its people. We are proud to be a founding member of this AI Alliance, which will allow us to play a pivotal role in the ecosystem, contribute to the development of ethical and trustworthy AI innovations with well-regulated AI models for impactful real-world applications. Through the collective efforts with the rest of the members, we believe that the evolution, application and adoption of AI will reach new heights.”

Christopher Nguyen, CEO, Aitomatic & Industrial AI Leader: “AI independence is vital for industrial companies, focusing on specialized domain expertise as key to competitiveness. Open-source foundations, driven by the AI Alliance, are essential for seamless model deployment at the computing edge. Aitomatic and our partners are eager to both contribute to and benefit from the transformative impact of the global AI Alliance.”

Robert Nishihara, CEO of Anyscale: “AI will have a positive impact on our daily lives and address some of the world’s most pressing challenges, but like with any new technology or innovation, we need to consider the risks. To ensure that open source communities can continue to flourish, innovate, deliver rich technological progress and advance the broader AI ecosystem, it’s imperative that we advance AI ethics, governance and safety. The AI Alliance is an important step to ensuring that our society can benefit from AI responsibly and equitably.”

Yannis Paschalidis, Distinguished Professor of ECE and Director of the Hariri Institute for Computing and Computational Science & Engineering at Boston University: “The Hariri Institute, which houses the multi-institutional Mass Open Cloud Alliance (MOC Alliance), is excited to have the MOC Alliance join the AI Alliance as a founding member. The MOC Alliance was formed to provide an affordable open cloud platform to the research community, thus furthering the goals of the Massachusetts Green High Performance Computing Center (MGHPCC) -- a joint venture of Boston University, Harvard, MIT, Northeastern, and UMass. The Hariri Institute has a history of supporting open-source efforts and is at the forefront of AI innovations with applications in science, biomedicine, and health care, where open, trusted, unbiased, and ethical AI models are particularly important.”

Andrew Feldman, co-founder and CEO, Cerebras Systems: “Cerebras’ mission is to transform AI compute and democratize access to AI for customers around the world. In alliance with this community of global technology leaders, we look forward to accelerating the progress and contributions of the open source AI community in the most efficient, collaborative, and responsible way possible.”

Tom Mihaljevic, M.D., Cleveland Clinic CEO and President, and holder of the Morton L. Mandel CEO Chair: “We are pleased to become a founding member of the AI Alliance. As a leader in healthcare AI, we recognize that it has the potential to quickly accelerate the pace of medical research and enhance patient care. AI capabilities are now constantly growing and improving, and it is critical that organizations from diverse fields come together to help advance AI discoveries and technologies while also addressing concerns around security and safety. We are looking forward to working on these important issues collaboratively with the Alliance members.”

Krystyn J Van Vliet, Vice President for Research and Innovation at Cornell University: “Cornell looks forward to participating in this AI Alliance, and to the range of participants and perspectives around this table. Open innovation has spurred incredible advances in many fields including AI and thrives on experimentation and dialogue. We look forward to contributing to the discussions, technologies, and advances that will help the world develop knowledge and tools using AI, as well as a shared sense of responsibility for positive impact on society.”

Dave Kotz, Provost and the Pat and John Rosenwald Professor in the Department of Computer Science, Dartmouth College: ""Dartmouth, where 'artificial intelligence' was first envisioned in 1956 , is excited to be part of this alliance and committed to ensuring AI is transparent, trustworthy, and serves all humanity going forward.”

Jeff Boudreau, chief AI officer, Dell Technologies: “AI progress that drives real value for humanity can only happen with open innovation and in open ecosystems. The AI Alliance is a positive step in making sure a diverse set of voices are collaborating transparently for the benefit of all.”

Marcel Salathé & Pascal Frossard, co-directors of the EPFL AI Center: “EPFL is committed to the open, transparent, and safe development of AI, recognizing its significance for everyone. Through global collaborations like the AI alliance, we aim to ensure that progress in AI technology is ethical, efficient, and universally beneficent.”

Jeremy Howard, founding researcher, Fast.ai: “Open source is the backbone of all leading artificial intelligence software. With open source, the entire community comes together to collaborate on solving the toughest problems, the most effective solutions rise to the top, and everyone benefits.”

Phong Nguyen, CAIO, FPT Software: “FPT Software is thrilled and honored to join the Alliance, where we believe that openness and transparency are key to ensuring the safe and responsible deployment of AI. Committed to this vision, we aim to work closely with various stakeholders to enhance open-science AI, ultimately bringing significant social benefits to the industry.”

President Asher Cohen, Hebrew University of Jerusalem: “Joining the AI Alliance marks a pivotal moment for Hebrew University, where collaboration meets innovation. We're thrilled to be part of this coalition driving the future of AI, fostering open technologies, nurturing talent, and championing ethical, trusted AI practices. Together, we'll forge pathways for global education, build robust frameworks, and advocate for policies that nurture a vibrant, open AI ecosystem. This alliance is a beacon illuminating our commitment to shaping a responsible, inclusive future powered by the boundless possibilities of artificial intelligence.”

Atish Dabholkar, ICTP Director: “ICTP is pleased to be part of the high-level, international research community underpinning the AI Alliance that includes major players in the field. We share a commitment to exploring new scientific horizons and to sharing new knowledge openly. AI has the potential to radically change the way science is done, and by joining this Alliance we will ensure that our large international network of scientists have open access to the latest in AI innovation, training and governance, regardless of geographical borders.”

Professor Varsha Apte, Head, Department of Computer Science and Engineering at IIT Bombay: “The Computer Science Department of IIT Bombay is excited to be part of the AI Alliance. We hope to contribute towards creating and deploying speech and language models over more diverse languages, accents and dialects, while also investigating foundation models for better representation of structurally rich information.”

Professor Bob Shorten, Head of Dyson School of Engineering, Imperial College London: “We are delighted to be a founding member of the AI Alliance. At Imperial College London, we believe that community engagement is essential for making AI trusted, responsible, transparent, as well as auditable. We look forward to engaging with the Alliance community to realise these objectives.”

Professor Martin Vechev, Scientific Director of INSAIT, Full Professor at ETH Zurich: “INSAIT is delighted to join the AI Alliance and help pave the way towards a more transparent, open, safe and trustworthy AI.""

Deepak Patil, CVP and GM Intel Datacenter AI Solutions: “Intel has long supported the open ecosystem to drive innovation and the future of technology. The AI Alliance provides an opportunity for a broad community to come together to advance the future of artificial intelligence technologies including data sets, models and tools rooted in responsibility, ethics, trustworthy and security practices. By collaborating across the industry, the advancement of this technology can be driven forward with speed and transparency. We look forward to working with this diverse group to help shape an open, responsible AI ecosystem.”

Prof. Kohei Itoh, President, Keio University: “Keio University is excited to be a part of this new alliance that will play a key role in our contribution to expanding the horizon of computing by combining supercomputing, quantum computing, semiconductor, and AI research.”

Jim Zemlin, Executive Director, Linux Foundation: “The AI Alliance is another milestone in the process of providing for openly shareable software, data, and other assets essential to the development of transparent, advanced and trustworthy AI. Open collaborative processes and open governance are essential to these efforts and working with our PyTorch Foundation, LF AI and Data Foundation, Cloud Native Computing Foundation, we look forward to participating in and assisting the Alliance by providing a neutral home for essential elements of the AI ecosystem.”

Jerry Liu, Co-founder and CEO, Llama Index: “Open-source software is one of the most important components in making sure that AI is understandable, explainable, and accessible to everyone. The mission of LlamaIndex is to empower all developers to connect LLMs with their private data. We support the development of open-source models and supporting tooling so that developers can take advantage of a strong ecosystem of different choices in achieving this mission.”

Orran Krieger, Professor ECE and CS and Director of the Mass Open Cloud Alliance (MOC Alliance): “The MOC Alliance is excited to have its open production cloud be a part of the Open AI Alliance. The AI Alliance's mission of enabling AI in an open and transparent matter is an exciting match for a cloud based on open source with all the operations state and telemetry available to enable innovation. We hope to both be a cost-effective platform for many of the research and non-profit users that are part of the AI Alliance, as well as a place where AI Alliance participants who are developing new tools and platforms can expose their innovation to our large community of existing AI researchers/users.""

Timothy Baldwin, Acting Provost, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI): “MBZUAI recognizes the transformative impact of collaborative efforts in fostering innovation. The open ecosystem cultivated by the alliance mirrors our core values, emphasizing principles such as scientific rigor, trust, ethics, transparency and responsibility. As one of the founding members, MBZUAI recognizes the transformative potential of foundation models and generative AI. Our recent launch of the Institute of Foundational Models solidifies our commitment to advancing academic research into foundational models with linguistic, visual, and biological capabilities and beyond. These models are driving transformation in industries as diverse as sustainability and healthcare. We are honored to be part of the AI Alliance, and we eagerly anticipate the collaborative strides we will make towards shaping a future where AI benefits people and society everywhere.”

David Kanter, MLCommons Executive Director: “MLCommons believes in collaborative engineering to build industry standard benchmarks and metrics to help drive AI forward in key areas such as performance and safety. We look forward to working with the AI Alliance in these areas to make all AI safer for everyone.”

Kevin Murphy, Chief Science Data Officer, NASA: “Open innovation is all but essential to ensuring equitable access and collaboration around AI and root this technology in principles that adhere to the strongest standards of diversity, trust and ingenuity. NASA is excited about efforts like the AI Alliance to continue enabling the global community of scientists, researchers, and practitioners committed to responsible, trustworthy AI technologies.”

Sethuraman Panchanathan, NSF Director: “The U.S. National Science Foundation is delighted to partner with the AI Alliance through the National AI Research Resource (NAIRR) pilot to meet our shared goals of supporting AI-driven discovery, innovation, and open science. The NAIRR pilot will connect America’s broad and diverse research community with the resources needed to pursue the frontiers of AI so that we can all be assured it is safe, secure, and trustworthy. The resources and tools developed by the AI Alliance will be made available to the research community through the NAIRR pilot, strengthening and extending the pilot’s ability to advance AI innovation in areas of societal and economic importance and ensuring the U.S. remains in the vanguard of competitiveness in this key technology.”

Leah Silen, Executive Director, NumFOCUS: “NumFOCUS supports many of the open-source scientific computing projects and communities foundational to AI research and innovation. We look forward to opportunities for collaboration among this community of support and advocacy for open-source AI.”

Stacie Bloom, Chief Research Officer and Vice Provost for Research, NYU: “NYU is excited about this opportunity to connect our world class AI research community with so many outstanding institutions including IBM and Meta. This Alliance aligns with our goals to drive science and technology solutions on a global scale, and in a way that is safe, responsible, accessible and equitable.”

Travis E. Oliphant, author of NumPy and SciPy, founder of Anaconda, CEO and founder of Quansight and OpenTeams: “We’re eager to contribute to an open, collaborative dialogue shaping the responsible development of AI. Embracing principles of transparency, ethics, and collaboration amplified by the open-source communities we serve, we envision a future where diverse voices continue to drive socially beneficial innovation in AI. Our commitment is to actively collaborate and share our expertise to ensure a responsible, open, and accessible AI ecosystem.”

Rebecca Finlay, CEO, Partnership on AI: “Partnership on AI is dedicated to creating a global community to catalyze AI developments that benefit people and society. It is important that open science and innovation contribute to safe and responsible AI. We look forward to continuing our work on this with PAI Partners, members of the Alliance, and the broader AI ecosystem.”

Matt Hicks, president and CEO, Red Hat: “Collaboration and open innovation have always been true catalysts for progress. The launch of the AI Alliance marks a visionary milestone, uniting industry giants, academia, and innovators with a shared responsibility to shape the future and advance open innovation, ensuring that the transformative power of AI is harnessed responsibly and ethically. As a founding member of the AI Alliance, Red Hat recognizes the collective potential of an open, healthy AI community — a force that will not only build technologies but also advocate for a future where innovation knows no bounds.""

Martin A. Schmidt, Ph.D., Rensselaer Polytechnic Institute President: “Rensselaer Polytechnic Institute is honored to become a founding member of the AI Alliance. We believe the responsible application of artificial intelligence enables infinite possibilities in discovery and innovation. We look forward to collaborating with the alliance as we accelerate our work in artificial intelligence and quantum computing.”

Rohan Malhotra, CEO and Founder, Roadzen: “The AI Alliance's focus on fostering an open and diverse ecosystem is a pivotal step in advancing AI research worldwide. It's a striking contrast to the idea of AI being tightly controlled by a few entities. As Roadzen joins this global initiative alongside the industry's leaders and researchers, our commitment is twofold: to support this vision and to leverage our expertise in building AI that is not just groundbreaking but also safe, accessible, and beneficial for all.”

CJ Desai, president & COO, ServiceNow: “AI innovation must remain open to drive positive and equitable societal impact, foster continued progress, and address potential risks collaboratively. There is no room for a winner-take-all approach; the development of responsible, secure LLMs comes in many forms. By joining this alliance, ServiceNow is doubling down on open innovation, harnessing the collective talent and expertise across the global open-source community. This move aims to ensure AI development is grounded in the needs of society, both today and in the future.”

David Spergel, President, Simons Foundation: “The Simons Foundation is strongly committed to open codes, open data and open scientific literature. We have been long supporters of the arXiv, a key site for open science. We see this support of open source code for AI as essential for both advancing the science and enabling ethical use of AI.”

Dr. Hiroaki Kitano, Senior Executive Vice President and CTO of Sony Group Corporation: “As a global technology and entertainment company, Sony believes in developing new technology for the benefit of the creative community and society at large in an ethical, legal, and responsible manner. We look forward to working together with representatives from the diverse international open-source community across industry, academic, research, and government organizations to develop AI models, data, guidelines and best practices that ensure safe and responsible usage of such technology.”

Emad Mostaque, CEO of Stability AI: “Stability AI, a leader in developing advanced open AI models, and IBM, a champion of open-source innovation, proudly announce our partnership with The AI Alliance. Together, we are poised to redefine the landscape of generative AI, focusing on breakthrough innovations with a steadfast commitment to trust and safety in this dynamic field.”

Vipul Ved Prakash, Co-founder and CEO of Together AI: “We are excited to join the alliance because we believe in the power of the community and openness. This is the path to safe and responsible AI, and we are interested in building the platform to enable the open models community to thrive.”

Thomas F. Hofmann, TUM president: “With this alliance, we want to accelerate the development of responsible, socially acceptable and scalable AI solutions in partnership with other leading universities and companies.”

Jennifer Chayes, Dean, UC Berkeley’s College of Computing, Data Science, and Society: “Pursuing open innovation levels the playing field, allowing everyone to share in the benefits of generative AI.”

Rashid Bashir, dean of The Grainger College of Engineering, University of Illinois Urbana-Champaign: “Grainger Engineers have been conducting AI research since the dawn of computing and UIUC is a hotbed of AI innovation today because of that rich history. Today, with three of the nation’s NSF AI Institutes, and numerous cross-sector partnerships, we are advancing research and workforce development to accelerate safe, responsible AI progress. UIUC is proud to join our esteemed peers and partners in this vital alliance.”

University President Rev. John I. Jenkins, C.S.C., University of Notre Dame: “Innovations in artificial intelligence offer, at the same time, the promise of serving the common good and the threat of undermining it. It is critical that we engage serious ethical questions about AI alongside the technological. Notre Dame has long been a place for researching and reflecting on the ethical implications of science and technology, and we are pleased that through the AI Alliance we will be able to lend a distinctive voice and perspective to the conversation about building technologies that are both innovative and ethical.”

Jeffrey F. Rhoads, vice president for research and professor in the Department of Aerospace and Mechanical Engineering at the University of Notre Dame: “The technologies that will truly move our world forward will be the ones that emerge from an inclusive, interdisciplinary innovation ecosystem. As a founding partner in the AI Alliance, Notre Dame’s engineers, data scientists, ethicists, and other researchers will be able to collaborate to build that ecosystem, joining both with AI labs around the world and with the industry partners who get new technologies into the hands of users. This will bring an unprecedented opportunity to our faculty and our students as they help steer innovations that bring benefits to society.”

Adam Klivans, Director, UT-Austin Machine Learning Lab: “The AI and ML researchers at The University of Texas at Austin, led by the Machine Learning Lab (MLL) and Good Systems, look forward to working closely with members of the AI Alliance to broaden access to generative AI for the purpose of accelerating research. We are excited about exploring ways to leverage its benefits, safeguard against AI dangers while simultaneously advancing the state of the art in AI capabilities.”

Prof. Hiroaki Aihara, Executive Vice President of the University of Tokyo: “The University Tokyo is thrilled to be part of the AI Alliance, together to support open innovation and open science in AI for the benefit of society at large.”

Jeffrey Brock, Dean of the Yale School of Engineering & Applied Science. “We are excited to join the AI Alliance as a founding partner. Its commitment to innovative and open AI development aligns with our vision at Yale Engineering. This partnership enables us to work with a broad range of university and industry leadership to pursue collaborative research, while formulating policy and standards for safe, explainable, and trustworthy AI."""
Meta_Blog,https://ai.meta.com/blog/ai-alliance/,,Introducing the AI Alliance,"To ensure open innovation in AI benefits everyone and that it is built responsibly, the AI Alliance consists of a broad range of organizations that are working across aspects of AI education, research, development and deployment, and governance.

How the AI Alliance Will Work

In addition to bringing together leading developers, scientists, academics, students, and business leaders in the field of artificial intelligence, the AI Alliance will plan to partner with important existing initiatives from governments, non-profit and civil society organizations who are doing valuable and aligned work in the AI space.

The AI Alliance will begin its work with the formation of member-driven working groups across all major topical areas listed above. The Alliance will also establish a governing board and technical oversight committee dedicated to advancing the above project areas, as well as establishing overall project standards and guidelines.

Member Quotes

Arvind Krishna, IBM Chairman and CEO: ""The progress we continue to witness in AI is a testament to open innovation and collaboration across communities of creators, scientists, academics and business leaders. This is a pivotal moment in defining the future of AI. IBM is proud to partner with like-minded organizations through the AI Alliance to ensure this open ecosystem drives an innovative AI agenda underpinned by safety, accountability and scientific rigor.""

Nick Clegg, President, Global Affairs of Meta: “We believe it’s better when AI is developed openly — more people can access the benefits, build innovative products and work on safety. The AI Alliance brings together researchers, developers and companies to share tools and knowledge that can help us all make progress whether models are shared openly or not. We’re looking forward to working with partners to advance the state-of-the-art in AI and help everyone build responsibly.”

Lisa Su, AMD CEO and Chair: “At AMD, we are committed to advancing technology through collaboration. The history of our industry highlights how open, standards-based development leveraging the capabilities of the entire industry both accelerate innovation and ensure technology advances have the largest positive impact. By embracing open standards and transparency across all aspects of the rapidly developing AI ecosystem, we can help ensure the transformational benefits of responsible AI are broadly available. We are proud to join with other industry leaders as a founding member of the AI Alliance and look forward to working together to ensure the rapid advances in AI are a force for positive change.”

Dr. Sun Sumei, Acting Executive Director, A*STAR’s Institute for Infocomm Research (I²R):“As a research institute under A*STAR, I²R spearheads research initiatives and actively strives to enhance the tangible impact of our innovations to address the national needs of Singapore and its people. We are proud to be a founding member of this AI Alliance, which will allow us to play a pivotal role in the ecosystem, contribute to the development of ethical and trustworthy AI innovations with well-regulated AI models for impactful real-world applications. Through the collective efforts with the rest of the members, we believe that the evolution, application and adoption of AI will reach new heights.”

Christopher Nguyen, CEO, Aitomatic & Industrial AI Leader: “AI independence is vital for industrial companies, focusing on specialized domain expertise as key to competitiveness. Open-source foundations, driven by the AI Alliance, are essential for seamless model deployment at the computing edge. Aitomatic and our partners are eager to both contribute to and benefit from the transformative impact of the global AI Alliance.”

Robert Nishihara, CEO of Anyscale: “AI will have a positive impact on our daily lives and address some of the world’s most pressing challenges, but like with any new technology or innovation, we need to consider the risks. To ensure that open source communities can continue to flourish, innovate, deliver rich technological progress and advance the broader AI ecosystem, it’s imperative that we advance AI ethics, governance and safety. The AI Alliance is an important step to ensuring that our society can benefit from AI responsibly and equitably.”

Yannis Paschalidis, Distinguished Professor of ECE and Director of the Hariri Institute for Computing and Computational Science & Engineering at Boston University: “The Hariri Institute, which houses the multi-institutional Mass Open Cloud Alliance (MOC Alliance), is excited to have the MOC Alliance join the AI Alliance as a founding member. The MOC Alliance was formed to provide an affordable open cloud platform to the research community, thus furthering the goals of the Massachusetts Green High Performance Computing Center (MGHPCC) -- a joint venture of Boston University, Harvard, MIT, Northeastern, and UMass. The Hariri Institute has a history of supporting open-source efforts and is at the forefront of AI innovations with applications in science, biomedicine, and health care, where open, trusted, unbiased, and ethical AI models are particularly important.”

Andrew Feldman, co-founder and CEO, Cerebras Systems: “Cerebras’ mission is to transform AI compute and democratize access to AI for customers around the world. In alliance with this community of global technology leaders, we look forward to accelerating the progress and contributions of the open source AI community in the most efficient, collaborative, and responsible way possible.”

Tom Mihaljevic, M.D., Cleveland Clinic CEO and President, and holder of the Morton L. Mandel CEO Chair: “We are pleased to become a founding member of the AI Alliance. As a leader in healthcare AI, we recognize that it has the potential to quickly accelerate the pace of medical research and enhance patient care. AI capabilities are now constantly growing and improving, and it is critical that organizations from diverse fields come together to help advance AI discoveries and technologies while also addressing concerns around security and safety. We are looking forward to working on these important issues collaboratively with the Alliance members.”

Krystyn J Van Vliet, Vice President for Research and Innovation at Cornell University: “Cornell looks forward to participating in this AI Alliance, and to the range of participants and perspectives around this table. Open innovation has spurred incredible advances in many fields including AI and thrives on experimentation and dialogue. We look forward to contributing to the discussions, technologies, and advances that will help the world develop knowledge and tools using AI, as well as a shared sense of responsibility for positive impact on society.”

Dave Kotz, Provost and the Pat and John Rosenwald Professor in the Department of Computer Science, Dartmouth College: ""Dartmouth, where 'artificial intelligence' was first envisioned in 1956 , is excited to be part of this alliance and committed to ensuring AI is transparent, trustworthy, and serves all humanity going forward.”

Jeff Boudreau, chief AI officer, Dell Technologies: “AI progress that drives real value for humanity can only happen with open innovation and in open ecosystems. The AI Alliance is a positive step in making sure a diverse set of voices are collaborating transparently for the benefit of all.”

Marcel Salathé & Pascal Frossard, co-directors of the EPFL AI Center: “EPFL is committed to the open, transparent, and safe development of AI, recognizing its significance for everyone. Through global collaborations like the AI alliance, we aim to ensure that progress in AI technology is ethical, efficient, and universally beneficent.”

Jeremy Howard, founding researcher, Fast.ai: “Open source is the backbone of all leading artificial intelligence software. With open source, the entire community comes together to collaborate on solving the toughest problems, the most effective solutions rise to the top, and everyone benefits.”

Phong Nguyen, CAIO, FPT Software: “FPT Software is thrilled and honored to join the Alliance, where we believe that openness and transparency are key to ensuring the safe and responsible deployment of AI. Committed to this vision, we aim to work closely with various stakeholders to enhance open-science AI, ultimately bringing significant social benefits to the industry.”

President Asher Cohen, Hebrew University of Jerusalem: “Joining the AI Alliance marks a pivotal moment for Hebrew University, where collaboration meets innovation. We're thrilled to be part of this coalition driving the future of AI, fostering open technologies, nurturing talent, and championing ethical, trusted AI practices. Together, we'll forge pathways for global education, build robust frameworks, and advocate for policies that nurture a vibrant, open AI ecosystem. This alliance is a beacon illuminating our commitment to shaping a responsible, inclusive future powered by the boundless possibilities of artificial intelligence.”

Atish Dabholkar, ICTP Director: “ICTP is pleased to be part of the high-level, international research community underpinning the AI Alliance that includes major players in the field. We share a commitment to exploring new scientific horizons and to sharing new knowledge openly. AI has the potential to radically change the way science is done, and by joining this Alliance we will ensure that our large international network of scientists have open access to the latest in AI innovation, training and governance, regardless of geographical borders.”

Professor Varsha Apte, Head, Department of Computer Science and Engineering at IIT Bombay: “The Computer Science Department of IIT Bombay is excited to be part of the AI Alliance. We hope to contribute towards creating and deploying speech and language models over more diverse languages, accents and dialects, while also investigating foundation models for better representation of structurally rich information.”

Professor Bob Shorten, Head of Dyson School of Engineering, Imperial College London: “We are delighted to be a founding member of the AI Alliance. At Imperial College London, we believe that community engagement is essential for making AI trusted, responsible, transparent, as well as auditable. We look forward to engaging with the Alliance community to realise these objectives.”

Professor Martin Vechev, Scientific Director of INSAIT, Full Professor at ETH Zurich: “INSAIT is delighted to join the AI Alliance and help pave the way towards a more transparent, open, safe and trustworthy AI.""

Deepak Patil, CVP and GM Intel Datacenter AI Solutions: “Intel has long supported the open ecosystem to drive innovation and the future of technology. The AI Alliance provides an opportunity for a broad community to come together to advance the future of artificial intelligence technologies including data sets, models and tools rooted in responsibility, ethics, trustworthy and security practices. By collaborating across the industry, the advancement of this technology can be driven forward with speed and transparency. We look forward to working with this diverse group to help shape an open, responsible AI ecosystem.”

Prof. Kohei Itoh, President, Keio University: “Keio University is excited to be a part of this new alliance that will play a key role in our contribution to expanding the horizon of computing by combining supercomputing, quantum computing, semiconductor, and AI research.”

Jim Zemlin, Executive Director, Linux Foundation: “The AI Alliance is another milestone in the process of providing for openly shareable software, data, and other assets essential to the development of transparent, advanced and trustworthy AI. Open collaborative processes and open governance are essential to these efforts and working with our PyTorch Foundation, LF AI and Data Foundation, Cloud Native Computing Foundation, we look forward to participating in and assisting the Alliance by providing a neutral home for essential elements of the AI ecosystem.”

Jerry Liu, Co-founder and CEO, Llama Index: “Open-source software is one of the most important components in making sure that AI is understandable, explainable, and accessible to everyone. The mission of LlamaIndex is to empower all developers to connect LLMs with their private data. We support the development of open-source models and supporting tooling so that developers can take advantage of a strong ecosystem of different choices in achieving this mission.”

Orran Krieger, Professor ECE and CS and Director of the Mass Open Cloud Alliance (MOC Alliance): “The MOC Alliance is excited to have its open production cloud be a part of the Open AI Alliance. The AI Alliance's mission of enabling AI in an open and transparent matter is an exciting match for a cloud based on open source with all the operations state and telemetry available to enable innovation. We hope to both be a cost-effective platform for many of the research and non-profit users that are part of the AI Alliance, as well as a place where AI Alliance participants who are developing new tools and platforms can expose their innovation to our large community of existing AI researchers/users.""

Timothy Baldwin, Acting Provost, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI): “MBZUAI recognizes the transformative impact of collaborative efforts in fostering innovation. The open ecosystem cultivated by the alliance mirrors our core values, emphasizing principles such as scientific rigor, trust, ethics, transparency and responsibility. As one of the founding members, MBZUAI recognizes the transformative potential of foundation models and generative AI. Our recent launch of the Institute of Foundational Models solidifies our commitment to advancing academic research into foundational models with linguistic, visual, and biological capabilities and beyond. These models are driving transformation in industries as diverse as sustainability and healthcare. We are honored to be part of the AI Alliance, and we eagerly anticipate the collaborative strides we will make towards shaping a future where AI benefits people and society everywhere.”

David Kanter, MLCommons Executive Director: “MLCommons believes in collaborative engineering to build industry standard benchmarks and metrics to help drive AI forward in key areas such as performance and safety. We look forward to working with the AI Alliance in these areas to make all AI safer for everyone.”

Kevin Murphy, Chief Science Data Officer, NASA: “Open innovation is all but essential to ensuring equitable access and collaboration around AI and root this technology in principles that adhere to the strongest standards of diversity, trust and ingenuity. NASA is excited about efforts like the AI Alliance to continue enabling the global community of scientists, researchers, and practitioners committed to responsible, trustworthy AI technologies.”

Sethuraman Panchanathan, NSF Director: “The U.S. National Science Foundation is delighted to partner with the AI Alliance through the National AI Research Resource (NAIRR) pilot to meet our shared goals of supporting AI-driven discovery, innovation, and open science. The NAIRR pilot will connect America’s broad and diverse research community with the resources needed to pursue the frontiers of AI so that we can all be assured it is safe, secure, and trustworthy. The resources and tools developed by the AI Alliance will be made available to the research community through the NAIRR pilot, strengthening and extending the pilot’s ability to advance AI innovation in areas of societal and economic importance and ensuring the U.S. remains in the vanguard of competitiveness in this key technology.”

Leah Silen, Executive Director, NumFOCUS: “NumFOCUS supports many of the open-source scientific computing projects and communities foundational to AI research and innovation. We look forward to opportunities for collaboration among this community of support and advocacy for open-source AI.”

Stacie Bloom, Chief Research Officer and Vice Provost for Research, NYU: “NYU is excited about this opportunity to connect our world class AI research community with so many outstanding institutions including IBM and Meta. This Alliance aligns with our goals to drive science and technology solutions on a global scale, and in a way that is safe, responsible, accessible and equitable.”

Travis E. Oliphant, author of NumPy and SciPy, founder of Anaconda, CEO and founder of Quansight and OpenTeams: “We’re eager to contribute to an open, collaborative dialogue shaping the responsible development of AI. Embracing principles of transparency, ethics, and collaboration amplified by the open-source communities we serve, we envision a future where diverse voices continue to drive socially beneficial innovation in AI. Our commitment is to actively collaborate and share our expertise to ensure a responsible, open, and accessible AI ecosystem.”

Rebecca Finlay, CEO, Partnership on AI: “Partnership on AI is dedicated to creating a global community to catalyze AI developments that benefit people and society. It is important that open science and innovation contribute to safe and responsible AI. We look forward to continuing our work on this with PAI Partners, members of the Alliance, and the broader AI ecosystem.”

Matt Hicks, president and CEO, Red Hat: “Collaboration and open innovation have always been true catalysts for progress. The launch of the AI Alliance marks a visionary milestone, uniting industry giants, academia, and innovators with a shared responsibility to shape the future and advance open innovation, ensuring that the transformative power of AI is harnessed responsibly and ethically. As a founding member of the AI Alliance, Red Hat recognizes the collective potential of an open, healthy AI community — a force that will not only build technologies but also advocate for a future where innovation knows no bounds.""

Martin A. Schmidt, Ph.D., Rensselaer Polytechnic Institute President: “Rensselaer Polytechnic Institute is honored to become a founding member of the AI Alliance. We believe the responsible application of artificial intelligence enables infinite possibilities in discovery and innovation. We look forward to collaborating with the alliance as we accelerate our work in artificial intelligence and quantum computing.”

Rohan Malhotra, CEO and Founder, Roadzen: “The AI Alliance's focus on fostering an open and diverse ecosystem is a pivotal step in advancing AI research worldwide. It's a striking contrast to the idea of AI being tightly controlled by a few entities. As Roadzen joins this global initiative alongside the industry's leaders and researchers, our commitment is twofold: to support this vision and to leverage our expertise in building AI that is not just groundbreaking but also safe, accessible, and beneficial for all.”

CJ Desai, president & COO, ServiceNow: “AI innovation must remain open to drive positive and equitable societal impact, foster continued progress, and address potential risks collaboratively. There is no room for a winner-take-all approach; the development of responsible, secure LLMs comes in many forms. By joining this alliance, ServiceNow is doubling down on open innovation, harnessing the collective talent and expertise across the global open-source community. This move aims to ensure AI development is grounded in the needs of society, both today and in the future.”

David Spergel, President, Simons Foundation: “The Simons Foundation is strongly committed to open codes, open data and open scientific literature. We have been long supporters of the arXiv, a key site for open science. We see this support of open source code for AI as essential for both advancing the science and enabling ethical use of AI.”

Dr. Hiroaki Kitano, Senior Executive Vice President and CTO of Sony Group Corporation: “As a global technology and entertainment company, Sony believes in developing new technology for the benefit of the creative community and society at large in an ethical, legal, and responsible manner. We look forward to working together with representatives from the diverse international open-source community across industry, academic, research, and government organizations to develop AI models, data, guidelines and best practices that ensure safe and responsible usage of such technology.”

Emad Mostaque, CEO of Stability AI: “Stability AI, a leader in developing advanced open AI models, and IBM, a champion of open-source innovation, proudly announce our partnership with The AI Alliance. Together, we are poised to redefine the landscape of generative AI, focusing on breakthrough innovations with a steadfast commitment to trust and safety in this dynamic field.”

Vipul Ved Prakash, Co-founder and CEO of Together AI: “We are excited to join the alliance because we believe in the power of the community and openness. This is the path to safe and responsible AI, and we are interested in building the platform to enable the open models community to thrive.”

Thomas F. Hofmann, TUM president: “With this alliance, we want to accelerate the development of responsible, socially acceptable and scalable AI solutions in partnership with other leading universities and companies.”

Jennifer Chayes, Dean, UC Berkeley’s College of Computing, Data Science, and Society: “Pursuing open innovation levels the playing field, allowing everyone to share in the benefits of generative AI.”

Rashid Bashir, dean of The Grainger College of Engineering, University of Illinois Urbana-Champaign: “Grainger Engineers have been conducting AI research since the dawn of computing and UIUC is a hotbed of AI innovation today because of that rich history. Today, with three of the nation’s NSF AI Institutes, and numerous cross-sector partnerships, we are advancing research and workforce development to accelerate safe, responsible AI progress. UIUC is proud to join our esteemed peers and partners in this vital alliance.”

University President Rev. John I. Jenkins, C.S.C., University of Notre Dame: “Innovations in artificial intelligence offer, at the same time, the promise of serving the common good and the threat of undermining it. It is critical that we engage serious ethical questions about AI alongside the technological. Notre Dame has long been a place for researching and reflecting on the ethical implications of science and technology, and we are pleased that through the AI Alliance we will be able to lend a distinctive voice and perspective to the conversation about building technologies that are both innovative and ethical.”

Jeffrey F. Rhoads, vice president for research and professor in the Department of Aerospace and Mechanical Engineering at the University of Notre Dame: “The technologies that will truly move our world forward will be the ones that emerge from an inclusive, interdisciplinary innovation ecosystem. As a founding partner in the AI Alliance, Notre Dame’s engineers, data scientists, ethicists, and other researchers will be able to collaborate to build that ecosystem, joining both with AI labs around the world and with the industry partners who get new technologies into the hands of users. This will bring an unprecedented opportunity to our faculty and our students as they help steer innovations that bring benefits to society.”

Adam Klivans, Director, UT-Austin Machine Learning Lab: “The AI and ML researchers at The University of Texas at Austin, led by the Machine Learning Lab (MLL) and Good Systems, look forward to working closely with members of the AI Alliance to broaden access to generative AI for the purpose of accelerating research. We are excited about exploring ways to leverage its benefits, safeguard against AI dangers while simultaneously advancing the state of the art in AI capabilities.”

Prof. Hiroaki Aihara, Executive Vice President of the University of Tokyo: “The University Tokyo is thrilled to be part of the AI Alliance, together to support open innovation and open science in AI for the benefit of society at large.”

Jeffrey Brock, Dean of the Yale School of Engineering & Applied Science. “We are excited to join the AI Alliance as a founding partner. Its commitment to innovative and open AI development aligns with our vision at Yale Engineering. This partnership enables us to work with a broad range of university and industry leadership to pursue collaborative research, while formulating policy and standards for safe, explainable, and trustworthy AI."""
Meta_Blog,https://ai.meta.com/blog/fair-10-year-anniversary-open-science-meta/,,Ten years of FAIR: Advancing the state-of-the-art through open research,"Meta is uniquely poised to solve AI’s biggest problems — not many companies have the resources or capacity to make the investments we have in software, hardware, and infrastructure to weave learnings from our research into products that billions of people can benefit from. FAIR is a critical piece to Meta’s success, and one of the only groups in the world with all the prerequisites for delivering true breakthroughs: some of the brightest minds in the industry, a culture of openness, and most importantly: the freedom to conduct exploratory research. This freedom has helped us stay agile and contribute to building the future of social connection.

The momentum shows no signs of slowing. Today, we announced new models, datasets, and updates spanning audio generation, translation, and multimodal perception. The successor to Voicebox , Audiobox ​​is advancing generative AI for audio by unifying generation and editing capabilities for speech, sound effects, and soundscapes with a variety of input mechanisms, including natural language prompts. Building on our work with SeamlessM4T , Seamless introduces a suite of AI language translation models that preserve expression and improve streamings. And Ego-Exo4D extends our work on egocentric perception with a foundational dataset and benchmark suite containing both ego- and exocentric views. While the egocentric perspective shows the participant’s point of view, the exocentric views reveal the surrounding scene and context. Together, these two perspectives give AI models a new window into complex human skill.

The future

While much of the progress in AI over the last decade was achieved by a divide and conquer approach, breaking up the problem into separate well-defined tasks, in the next decade, we are increasingly looking at ways of putting the puzzle pieces together to advance AI. The rise in Foundation models is just the beginning of this: large models with increasingly general abilities, which we can flexibly adapt to our specific needs and values. World models, which can be used to reason and plan, will be increasingly common, allowing us to overcome limitations of current AI models. Rather than a single AGI, we expect the future to feature numerous and diverse populations of AIs deployed across platforms, which will transform how we work, how we play, how we connect, how we create, how we live.

Pursuing this path requires that we also have a deep understanding of how to build AI models responsibly, from beginning to end. We remain committed to doing that work safely and responsibly. Our commitment to open science is a key part of this, and will continue to be part of FAIR’s DNA. When we aim to share our work openly, whether it be our papers, code, models, demos or responsible use guides, it helps us set the highest standards of quality and responsibility, which is the best way for us to help the community build better AI solutions. This also directly helps Meta build AI solutions that are safer, more robust, equitable and transparent, and can benefit the many different people that use our products around the world.

As I look forward to the next decade for FAIR, I am inspired by our vision and ambition to solve the hardest, most fundamental problems in AI. I am thankful for the many teams and people across Meta who are contributing to our success. And I look forward to seeing what the future holds if we continue to push towards solving AI, while staying true to our culture of responsibility, excellence, and openness!"
Meta_Blog,https://ai.meta.com/blog/fair-10-year-anniversary-open-science-meta/,,Ten years of FAIR: Advancing the state-of-the-art through open research,"Meta is uniquely poised to solve AI’s biggest problems — not many companies have the resources or capacity to make the investments we have in software, hardware, and infrastructure to weave learnings from our research into products that billions of people can benefit from. FAIR is a critical piece to Meta’s success, and one of the only groups in the world with all the prerequisites for delivering true breakthroughs: some of the brightest minds in the industry, a culture of openness, and most importantly: the freedom to conduct exploratory research. This freedom has helped us stay agile and contribute to building the future of social connection.

The momentum shows no signs of slowing. Today, we announced new models, datasets, and updates spanning audio generation, translation, and multimodal perception. The successor to Voicebox , Audiobox ​​is advancing generative AI for audio by unifying generation and editing capabilities for speech, sound effects, and soundscapes with a variety of input mechanisms, including natural language prompts. Building on our work with SeamlessM4T , Seamless introduces a suite of AI language translation models that preserve expression and improve streamings. And Ego-Exo4D extends our work on egocentric perception with a foundational dataset and benchmark suite containing both ego- and exocentric views. While the egocentric perspective shows the participant’s point of view, the exocentric views reveal the surrounding scene and context. Together, these two perspectives give AI models a new window into complex human skill.

The future

While much of the progress in AI over the last decade was achieved by a divide and conquer approach, breaking up the problem into separate well-defined tasks, in the next decade, we are increasingly looking at ways of putting the puzzle pieces together to advance AI. The rise in Foundation models is just the beginning of this: large models with increasingly general abilities, which we can flexibly adapt to our specific needs and values. World models, which can be used to reason and plan, will be increasingly common, allowing us to overcome limitations of current AI models. Rather than a single AGI, we expect the future to feature numerous and diverse populations of AIs deployed across platforms, which will transform how we work, how we play, how we connect, how we create, how we live.

Pursuing this path requires that we also have a deep understanding of how to build AI models responsibly, from beginning to end. We remain committed to doing that work safely and responsibly. Our commitment to open science is a key part of this, and will continue to be part of FAIR’s DNA. When we aim to share our work openly, whether it be our papers, code, models, demos or responsible use guides, it helps us set the highest standards of quality and responsibility, which is the best way for us to help the community build better AI solutions. This also directly helps Meta build AI solutions that are safer, more robust, equitable and transparent, and can benefit the many different people that use our products around the world.

As I look forward to the next decade for FAIR, I am inspired by our vision and ambition to solve the hardest, most fundamental problems in AI. I am thankful for the many teams and people across Meta who are contributing to our success. And I look forward to seeing what the future holds if we continue to push towards solving AI, while staying true to our culture of responsibility, excellence, and openness!"
Meta_Blog,https://ai.meta.com/blog/ego-exo4d-video-learning-perception/,,Introducing Ego-Exo4D: A foundational dataset for research on video learning and multimodal perception,"Update April 2, 2024:

Together with the Ego4D consortium, we just released Ego-Exo4D v2 , which includes nearly 1,300 hours of video capture (including 221 egocentric hours) across 5,035 videos. Ego-Exo4D v2 also includes significantly more annotations than our initial release.

With this update, Exo-Exo4D now contains the largest publicly available manually labeled egocentric body and hand pose dataset (and also ~25x/~60x auto-GT annotations for body/hands), as well as the largest collection of manually labeled video segmentation masks. And we’re excited to release expert commentary annotations, a first-of-its-kind video-language resource.

We’re also launching two Ego-Exo4D teaser challenges as part of the EgoVis Workshop at CVPR 2024. Using egocentric video from the Ego-Exo4D dataset, participants will either estimate the 3D body pose of the camera wearer or estimate the 3D locations of the defined hand joints for visible hand(s). We welcome researchers to take part in these challenges. Click here for details , including competition rules and prize information."
Meta_Blog,https://ai.meta.com/blog/ego-exo4d-video-learning-perception/,,Introducing Ego-Exo4D: A foundational dataset for research on video learning and multimodal perception,"Update April 2, 2024:

Together with the Ego4D consortium, we just released Ego-Exo4D v2 , which includes nearly 1,300 hours of video capture (including 221 egocentric hours) across 5,035 videos. Ego-Exo4D v2 also includes significantly more annotations than our initial release.

With this update, Exo-Exo4D now contains the largest publicly available manually labeled egocentric body and hand pose dataset (and also ~25x/~60x auto-GT annotations for body/hands), as well as the largest collection of manually labeled video segmentation masks. And we’re excited to release expert commentary annotations, a first-of-its-kind video-language resource.

We’re also launching two Ego-Exo4D teaser challenges as part of the EgoVis Workshop at CVPR 2024. Using egocentric video from the Ego-Exo4D dataset, participants will either estimate the 3D body pose of the camera wearer or estimate the 3D locations of the defined hand joints for visible hand(s). We welcome researchers to take part in these challenges. Click here for details , including competition rules and prize information."
Meta_Blog,https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts/,,Audiobox: Generating audio from voice and natural language prompts,"We’re releasing Audiobox to a hand-selected group of researchers and academic institutions with a track record in speech research to help further the state of the art in this research area and ensure we have a diverse set of partners to tackle the responsible AI aspects of this work. In the future, we believe research breakthroughs like Audiobox will lower the barrier of accessibility for audio creation and make it easy for anyone to become an audio content creator. Creators could use models like Audiobox to generate soundscapes for videos or podcasts, custom sound effects for games, or any of a number of other use cases.

Audio plays a fundamental role in many forms of media, from movies to podcasts, audiobooks, and video games. But producing quality audio can often be a challenging process that requires access to extensive sound libraries as well as deep domain expertise (sound engineering, foley, voice acting, etc.) to yield optimal results — expertise that the public, or even hobbyists, may not possess.

Audiobox demonstrates state-of-the-art controllability on speech and sound effects generation. Our own tests show it significantly surpasses prior best models (AudioLDM2, VoiceLDM, and TANGO) on quality and relevance (faithfulness to text description) in subjective evaluations. Audiobox outperforms Voicebox on style similarity by over 30 percent on a variety of speech styles.

Vocal restylization: Audiobox can restyle a voice to make it sound as though it’s in a different environment — in a large cathedral in this example.

The model also allows users to combine an audio voice input with a text style prompt to synthesize speech of that voice in any environment (e.g., “in a cathedral”) or any emotion (e.g., “speaks sadly and slowly”). To our knowledge, Audiobox is the first model to enable dual input (voice prompts and text description prompts) for freeform voice restyling.

Describe-and-generate speech: Users can provide a short description of the desired voice, along with the transcript to be narrated, and ask the model to generate speech.

Describe-and-generate sound: Users can provide a short description of the desired sound and ask the model to generate it.

Most notably, Audiobox lets people use natural language prompts to describe a sound or type of speech they want to generate. If someone wants to generate a soundscape, for example, they can give the model a text prompt like, “A running river and birds chirping.”

Now, Audiobox, the successor to Voicebox, is advancing generative AI for audio even further by unifying generation and editing capabilities for speech, sound effects (short, discrete sounds like a dog bark, car horn, a crack of thunder, etc.), and soundscapes, with a variety of input mechanisms to maximize controllability for each use case.

Earlier this year, Meta introduced Voicebox , a state-of-the-art AI model that can perform speech generation tasks like editing, sampling, and stylizing. It was a breakthrough in generative AI in that it could generalize to speech-generation tasks it wasn’t specifically trained to accomplish — and execute these tasks with state-of-the-art performance.

Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance

Audiobox’s capabilities

While Audiobox is built on top of the Voicebox framework, it can generate a larger variety of sounds, including speech in various environments and styles, non-speech sound effects, and soundscapes.

Describe-and-generate speech: A text prompt can be used to describe not only a voice, but also the acoustic environment, such as, “in a large cathedral” in this example.

Being able to use text and voice inputs also greatly enhances Audiobox’s controllability compared to Voicebox. Audiobox users can use text description prompts to specify the style of speech and sound effects, a feature that was not supported in Voicebox. When a voice input and text prompt are used together, the voice input anchors the timbre, and the text prompt can be used to change other aspects. Audiobox inherits Voicebox’s guided audio generation training objective and flow-matching modeling method to allow for audio infilling. With infilling, users can also use the model to polish sound effects (adding different thunder sounds into a raining soundscape, for example).

Sound editing with generative infilling: Users can crop an audio segment and regenerate it with Audiobox. By providing a text description, Audiobox can insert sound effects like “a dog barking” into an audio clip of the sound of rain.

Our invitation to collaborate on responsible research

AI for audio generation has made significant progress over the past year. But, as with all AI innovations, we must work to help ensure responsible use. The known issues with AI cannot be addressed by any individual or single organization alone. That’s why collaboration with the research community on state-of-the-art models is more important now than ever. For these tools to be better and safer for everyone, the AI community must be empowered to build on top of our work and continue to develop these innovations responsibly. But access must be shared in the right way. To honor this and our ongoing commitment to open science, we’re releasing Audiobox under a research-only license to a limited number of hand-selected researchers and institutions. We’re inviting researchers and institutions who have been previously involved in speech research, and who want to pursue responsibility and safety research on the latest Audiobox models, to apply. We've also released an interactive demo that showcases Audiobox’s capabilities.

Implementing Audiobox responsibly

Tools like Audiobox can raise concerns about voice impersonation or other abuses. As part of our commitment to building generative AI features responsibly, we’ve implemented new technologies to help address these issues. Both the Audiobox model and our interactive demo feature automatic audio watermarking so any audio created with Audiobox can be accurately traced to its origin. Our watermarking method embeds a signal into the audio that’s imperceptible to the human ear but can be detected all the way down to the frame level using a model capable of finding AI-generated segments in audio. We’ve tested this method against a broad range of attacks and found it to be more robust than even current state-of-the-art solutions, making it extremely difficult for bad actors to try and bypass detection by modifying the AI-generated audio. Additionally, similar to how websites use CAPTCHAs to deter bots and spam, our interactive demo includes a voice authentication feature to safeguard against impersonation. Anyone who wants to add a voice to the Audiobox demo will have to speak a voice prompt using their own voice. The prompt changes at regular, rapid intervals and makes it extremely difficult to add someone else’s voice with pre-recorded audio. To help ensure robustness across different groups of speakers, we tested the performance of Audiobox on speakers of different genders and with different native languages and verified that the performance is close across all groups of speakers.

Future use cases for Audiobox

In the long term, it will be crucial to move from building specialized audio generative models that can only generate one type of audio (such as speech or sound) to building generalized audio generative models that can generate any audio. With such models we can perform any generative audio task that requires understanding beyond a single modality. This will make it simpler for developers to build towards a more dynamic and wide range of use cases. Audiobox is an important step toward democratizing audio generation. We envision a future where everyone can more easily and efficiently create audio that is tailored to their use cases. Our hope is that we can see the same creativity sparked by advancements in text and image generation happen for audio as well, for both professionals and hobbyists. Content creation, narration, sound editing, game development, and even AI chatbots can all benefit from the capabilities of audio generation models.

Try the Audiobox demo

Read the paper

This blog post was made possible by the work of Akinniyi Akinyemi, Alice Rakotoarison, Andros Tjandra, Apoorv Vyas, Baishan Guo, Bapi Akula, Bowen Shi, Brian Ellis, Carleigh Wood, Chris Summers, Ivan Cruz, Joshua Lane, Jeff Wang, Jiemin Zhang, Liang Tan, Mary Williamson, Matt Le, Rashel Moritz, Robbie Adkins, Wei-Ning Hsu, William Ngan, Xinyue Zhang, Yael Yungster, and Yi-Chiao Wu."
Meta_Blog,https://ai.meta.com/blog/audiobox-generating-audio-voice-natural-language-prompts/,,Audiobox: Generating audio from voice and natural language prompts,"We’re releasing Audiobox to a hand-selected group of researchers and academic institutions with a track record in speech research to help further the state of the art in this research area and ensure we have a diverse set of partners to tackle the responsible AI aspects of this work. In the future, we believe research breakthroughs like Audiobox will lower the barrier of accessibility for audio creation and make it easy for anyone to become an audio content creator. Creators could use models like Audiobox to generate soundscapes for videos or podcasts, custom sound effects for games, or any of a number of other use cases.

Audio plays a fundamental role in many forms of media, from movies to podcasts, audiobooks, and video games. But producing quality audio can often be a challenging process that requires access to extensive sound libraries as well as deep domain expertise (sound engineering, foley, voice acting, etc.) to yield optimal results — expertise that the public, or even hobbyists, may not possess.

Audiobox demonstrates state-of-the-art controllability on speech and sound effects generation. Our own tests show it significantly surpasses prior best models (AudioLDM2, VoiceLDM, and TANGO) on quality and relevance (faithfulness to text description) in subjective evaluations. Audiobox outperforms Voicebox on style similarity by over 30 percent on a variety of speech styles.

Vocal restylization: Audiobox can restyle a voice to make it sound as though it’s in a different environment — in a large cathedral in this example.

The model also allows users to combine an audio voice input with a text style prompt to synthesize speech of that voice in any environment (e.g., “in a cathedral”) or any emotion (e.g., “speaks sadly and slowly”). To our knowledge, Audiobox is the first model to enable dual input (voice prompts and text description prompts) for freeform voice restyling.

Describe-and-generate speech: Users can provide a short description of the desired voice, along with the transcript to be narrated, and ask the model to generate speech.

Describe-and-generate sound: Users can provide a short description of the desired sound and ask the model to generate it.

Most notably, Audiobox lets people use natural language prompts to describe a sound or type of speech they want to generate. If someone wants to generate a soundscape, for example, they can give the model a text prompt like, “A running river and birds chirping.”

Now, Audiobox, the successor to Voicebox, is advancing generative AI for audio even further by unifying generation and editing capabilities for speech, sound effects (short, discrete sounds like a dog bark, car horn, a crack of thunder, etc.), and soundscapes, with a variety of input mechanisms to maximize controllability for each use case.

Earlier this year, Meta introduced Voicebox , a state-of-the-art AI model that can perform speech generation tasks like editing, sampling, and stylizing. It was a breakthrough in generative AI in that it could generalize to speech-generation tasks it wasn’t specifically trained to accomplish — and execute these tasks with state-of-the-art performance.

Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance

Audiobox’s capabilities

While Audiobox is built on top of the Voicebox framework, it can generate a larger variety of sounds, including speech in various environments and styles, non-speech sound effects, and soundscapes.

Describe-and-generate speech: A text prompt can be used to describe not only a voice, but also the acoustic environment, such as, “in a large cathedral” in this example.

Being able to use text and voice inputs also greatly enhances Audiobox’s controllability compared to Voicebox. Audiobox users can use text description prompts to specify the style of speech and sound effects, a feature that was not supported in Voicebox. When a voice input and text prompt are used together, the voice input anchors the timbre, and the text prompt can be used to change other aspects. Audiobox inherits Voicebox’s guided audio generation training objective and flow-matching modeling method to allow for audio infilling. With infilling, users can also use the model to polish sound effects (adding different thunder sounds into a raining soundscape, for example).

Sound editing with generative infilling: Users can crop an audio segment and regenerate it with Audiobox. By providing a text description, Audiobox can insert sound effects like “a dog barking” into an audio clip of the sound of rain.

Our invitation to collaborate on responsible research

AI for audio generation has made significant progress over the past year. But, as with all AI innovations, we must work to help ensure responsible use. The known issues with AI cannot be addressed by any individual or single organization alone. That’s why collaboration with the research community on state-of-the-art models is more important now than ever. For these tools to be better and safer for everyone, the AI community must be empowered to build on top of our work and continue to develop these innovations responsibly. But access must be shared in the right way. To honor this and our ongoing commitment to open science, we’re releasing Audiobox under a research-only license to a limited number of hand-selected researchers and institutions. We’re inviting researchers and institutions who have been previously involved in speech research, and who want to pursue responsibility and safety research on the latest Audiobox models, to apply. We've also released an interactive demo that showcases Audiobox’s capabilities.

Implementing Audiobox responsibly

Tools like Audiobox can raise concerns about voice impersonation or other abuses. As part of our commitment to building generative AI features responsibly, we’ve implemented new technologies to help address these issues. Both the Audiobox model and our interactive demo feature automatic audio watermarking so any audio created with Audiobox can be accurately traced to its origin. Our watermarking method embeds a signal into the audio that’s imperceptible to the human ear but can be detected all the way down to the frame level using a model capable of finding AI-generated segments in audio. We’ve tested this method against a broad range of attacks and found it to be more robust than even current state-of-the-art solutions, making it extremely difficult for bad actors to try and bypass detection by modifying the AI-generated audio. Additionally, similar to how websites use CAPTCHAs to deter bots and spam, our interactive demo includes a voice authentication feature to safeguard against impersonation. Anyone who wants to add a voice to the Audiobox demo will have to speak a voice prompt using their own voice. The prompt changes at regular, rapid intervals and makes it extremely difficult to add someone else’s voice with pre-recorded audio. To help ensure robustness across different groups of speakers, we tested the performance of Audiobox on speakers of different genders and with different native languages and verified that the performance is close across all groups of speakers.

Future use cases for Audiobox

In the long term, it will be crucial to move from building specialized audio generative models that can only generate one type of audio (such as speech or sound) to building generalized audio generative models that can generate any audio. With such models we can perform any generative audio task that requires understanding beyond a single modality. This will make it simpler for developers to build towards a more dynamic and wide range of use cases. Audiobox is an important step toward democratizing audio generation. We envision a future where everyone can more easily and efficiently create audio that is tailored to their use cases. Our hope is that we can see the same creativity sparked by advancements in text and image generation happen for audio as well, for both professionals and hobbyists. Content creation, narration, sound editing, game development, and even AI chatbots can all benefit from the capabilities of audio generation models.

Try the Audiobox demo

Read the paper

This blog post was made possible by the work of Akinniyi Akinyemi, Alice Rakotoarison, Andros Tjandra, Apoorv Vyas, Baishan Guo, Bapi Akula, Bowen Shi, Brian Ellis, Carleigh Wood, Chris Summers, Ivan Cruz, Joshua Lane, Jeff Wang, Jiemin Zhang, Liang Tan, Mary Williamson, Matt Le, Rashel Moritz, Robbie Adkins, Wei-Ning Hsu, William Ngan, Xinyue Zhang, Yael Yungster, and Yi-Chiao Wu."
Meta_Blog,https://ai.meta.com/blog/seamless-communication/,,Introducing a suite of AI language translation models that preserve expression and improve streaming,"In particular, we are updating our stopes library and SONAR encoders . With these tools, anyone can automatically create multimodal translation pairs from their own speech and/or text monolingual data through parallel data alignment methods.

SeamlessStreaming unlocks real-time conversations with someone who speaks a different language by generating the translation while the speaker is still talking. In contrast to conventional systems which translate when the speaker has finished their sentence, SeamlessStreaming translates while the speaker is still talking. This means that the person they're speaking to can hear a translation in closer to real-time - there is a delay of a few seconds - rather than waiting until the speaker has finished their sentence. SeamlessStreaming supports automatic speech recognition and speech-to-text translation for nearly 100 input and output languages, and speech-to-speech translation for nearly 100 input languages and 36 output languages. In keeping with our approach to open science, we’re publicly releasing all four models to allow researchers to build on this work.

Today, we are excited to share Seamless , the first publicly available system that unlocks expressive cross-lingual communication in real time. To build Seamless, we developed SeamlessExpressive, a model for preserving expression in speech-to-speech translation, and SeamlessStreaming, a streaming translation model that delivers state-of-the-art results with around two seconds of latency. All of the models are built on SeamlessM4T v2, the latest version of the foundational model we released in August. SeamlessM4T v2 demonstrates performance improvements for automatic speech recognition, speech-to-speech, speech-to text, and text-to-speech capabilities. Compared to previous efforts in expressive speech research, SeamlessExpressive addresses certain underexplored aspects of prosody, such as speech rate and pauses for rhythm, while also preserving emotion and style. The model currently preserves these elements in speech-to-speech translation between English, Spanish, German, French, Italian, and Chinese.

In our increasingly interconnected world, where language differences may present a barrier to communication, translation systems can enable people from different linguistic backgrounds to share knowledge and experiences more seamlessly. However, many of these systems today do not preserve key elements of speech that make human communication human. More specifically, it’s not just the words we choose that convey what we want to say—it’s also how we speak them. Tone of voice, pauses, and emphasis carry important signals that help us communicate emotions and intent. Moreover, human speech and translation are sensitive to nuances such as turn-taking and timing controls. Picture, for example, how human interpreters work: they find just the right balance between low-latency and accurate translations. Waiting too long stifles the flow of communication, while going too fast compromises the overall quality of a translation. Translation systems that enable authentic conversations should deliver across all of these elements of communication.

Instead of using an autoregressive text-to-unit model as in UnitY, we used a non-autoregressive model. Autoregressive models predict the next token based on the previously generated tokens. While autoregressive models model speech naturally, they scale poorly as sequence length increases. They are also more likely to exhibit repetitive degeneration. Non-autoregressive models predict the duration of each segment, which enables each segment to be decoded in parallel. This makes them robust to long sequences, and we see improvements over the initial iteration of UnitY. Since the model inherently predicts duration, it is much more easily adaptable to the streaming use case, because we know exactly how much speech is needed to be generated for each piece of text, which is not the case for autoregressive models.

UnitY2, a new architecture that has a non-autoregressive text-to-unit decoder, is also instrumental to our work. In SeamlessM4T v2, we used multitask-UnitY2 to enable text input (updated from v1's multitask-UnitY). We also used the architecture for SeamlessStreaming and SeamlessExpressive. As our next generation multitask model, UnitY2 has superior speech generation capabilities through its improved text-to-unit model. This implementation leads to improved consistency between text output and speech output, compared to the SeamlessM4T v1 model.

All our models run on fairseq2 , the latest update of our sequence modeling toolkit. Similar to our previous work on SeamlessM4T, fairseq2 offers an ideal framework for building our streaming and expressivity updates because it is lightweight, easily composable with other PyTorch ecosystem libraries, and has more efficient modeling and data loader APIs.

EMMA is our core streaming algorithm, which allows us to intelligently decide when we have enough information to generate the next speech segment or target text. It improves upon previous state-of-the-art algorithms especially for long input sequences, which is the case for speech-to-text or speech-to-speech translation. Further, this algorithm allows us to fine-tune from offline models, which allows us to reap the benefits of the Seamless M4T v2 foundation model. Finally, we show empirically that this algorithm generalizes well across many different language pairs, which is particularly challenging for streaming models because the language pairs may be structured differently.

Expressivity

Preserving expression also requires a new approach. We replaced the unit HiFi-GAN vocoder in SeamlessM4T v2 with PRETSSEL, an expressive unit-to-speech generator. PRETSSEL is conditioned on the source speech for waveform generation to transfer tones, emotional expression, and vocal style qualities. We initialize our model from SeamlessM4T v2 in order to achieve high translation quality, which is the most fundamental need for a speech-to-speech translation system. We also developed Prosody UnitY2, integrating an expressivity encoder in SeamlessM4T v2 to guide unit generation with proper rhythm, speaking rate, and pauses. In addition, we release a suite of evaluation tools to capture the preservation of these aspects of expressivity.

Results

The updates to UnitY2 have resulted in improved translation quality across a variety of tasks. SeamlessM4T v2 achieves sate of the art translation for speech-to-speech and speech-to-text results in 100 languages. In the same model, it also beats Whisper v3’s for automatic speech recognition on average and in particular for lower resource languages. For speech-to-text translation, SeamlessM4T v2 improves by 10% compared to the model we released in August and by more than 17% over the strongest cascaded models when translating into English. For speech-to-speech translation, SeamlessM4T v2, improves over SeamlessM4T (v1) by more than 15% when translating into English, and by 25% when translating from English. In other tasks, SeamlessM4T v2 is on par with No Language Left Behind (NLLB) in text-to-text translation. It is also on-par on average with MMS in automatic speech recognition (ASR) (with better performance on mid and high-resource languages while MMS has better performance on low resource languages), and improving over the recently released Whisper-Large-v3 by more than 25%. In the zero-shot task of text-to-speech translation, SeamlessM4T v2 is on-par with strong cascaded models into English, and improves over these baselines by 16 percent in English. We compared SeamlessExpressive against a cascaded speech-to-text and text-to-speech pipeline, where speech-to-text is from SeamlessM4T v2, and text-to-speech is from strong open-sourced cross-lingual text-to-speech system that supports vocal style and emotion transfer. Results show that SeamlessExpressive is more stable with respect to noise in the source speech such that the output speech maintains high content translation quality, and better preserves styles and speech rate. SeamlessStreaming achieves state of the art low latency quality with speech-to-speech translation.

How we built AI translation systems responsibly: Toxicity mitigation

Accuracy is paramount in translation systems. Translation errors or unintended toxicity can cause misunderstandings between two people who don’t speak the same language. Keeping with our commitment to building responsible AI, we explored the problem of hallucinated toxicity further. We focused our efforts on SeamlessM4T v2, which serves as the foundation for SeamlessStreaming, SeamlessExpressive, and our unified Seamless model. The primary root cause for hallucinated toxicity often lies in the training data. Training samples can be noisy and contain unbalanced toxicity. For example, the input language side and target language side can contain different amounts of toxic words by mistake. Prior to training, we discarded any sample that showed signs of this imbalance. However, filtering is only a passive technique and does not fully prevent hallucinated toxicity. We went one step further this time, and implemented a novel approach that actively mitigates this phenomenon. During the translation generation process, our model automatically detects generated toxic words. When there are misaligned levels of toxicity, we automatically re-adjust the generation process and use a different choice of words. This works at inference time and does not require any fine-tuning of the translation model. By doing so, we significantly reduce added toxicity while preserving translation quality. Finally, building upon our past work on toxicity and bias evaluation, we’ve extended our evaluation framework with a new hallucinated toxicity detection tool. While our previous approach relied on an intermediate transcription model (ASR), we are now capable of detecting toxicity directly in the speech signal. This is useful in cases where toxicity is not conveyed by individual words, but rather in tone or general style. This allows us to get a more precise picture of the potential toxicity profile of our model. Additional research needs to be done on responsible AI for machine translation; however, we believe these measures bring us closer to realizing safer and more human-centric translation systems.

Audio watermarking

While AI tools can help bring the world closer together, it’s just as important that we include measures to prevent the risk of imitation and other forms of misuse. Our watermarking method offers a better level of reliability compared to passive discriminators, which are becoming less effective at differentiating synthetic voices from human ones as voice preservation technology advances. Watermarking actively embeds a signal that is imperceptible to the human ear, but still detectable within the audio using a detector model. Through this watermark, the origin of the audio can be accurately traced. This helps promote the responsible use of voice preservation technology by establishing a verifiable audio provenance and helps prevent potential abuses. Beyond sheer detection accuracy, our watermarking solution needs to be robust to various attacks. For example, bad actors can try to modify the audio by adding noise, echo, or filtering some frequencies to dilute the watermark and bypass detection. We tested our watermarking method against a broad range of attack types and the results show that it is more robust than the current state-of-the-art. Our method can also pinpoint AI-generated segments in audio down to the frame level, surpassing the previous state-of-the-art (which only provides a one second resolution). As in any kind of neural-network based safety mechanism, the watermarking model can be fine-tuned in isolation to forget its core properties. However, fine-tuning SeamlessExpressive and Seamless for translation purposes would not involve any update to the watermarking model itself, which does not play any role on translation quality.

Providing access to our technology

The breakthroughs we’ve achieved with Seamless show that the dream of a universal, real-time translator isn’t science fiction—it’s becoming a technical reality. We invite everyone to try our expressive translation demo. We’re also making our code, model and data available to the research community.

Try the expressive translation demo

Try the Hugging Face demo

Download the code, model, and data"
Meta_Blog,https://ai.meta.com/blog/seamless-communication/,,Introducing a suite of AI language translation models that preserve expression and improve streaming,"In particular, we are updating our stopes library and SONAR encoders . With these tools, anyone can automatically create multimodal translation pairs from their own speech and/or text monolingual data through parallel data alignment methods.

SeamlessStreaming unlocks real-time conversations with someone who speaks a different language by generating the translation while the speaker is still talking. In contrast to conventional systems which translate when the speaker has finished their sentence, SeamlessStreaming translates while the speaker is still talking. This means that the person they're speaking to can hear a translation in closer to real-time - there is a delay of a few seconds - rather than waiting until the speaker has finished their sentence. SeamlessStreaming supports automatic speech recognition and speech-to-text translation for nearly 100 input and output languages, and speech-to-speech translation for nearly 100 input languages and 36 output languages. In keeping with our approach to open science, we’re publicly releasing all four models to allow researchers to build on this work.

Today, we are excited to share Seamless , the first publicly available system that unlocks expressive cross-lingual communication in real time. To build Seamless, we developed SeamlessExpressive, a model for preserving expression in speech-to-speech translation, and SeamlessStreaming, a streaming translation model that delivers state-of-the-art results with around two seconds of latency. All of the models are built on SeamlessM4T v2, the latest version of the foundational model we released in August. SeamlessM4T v2 demonstrates performance improvements for automatic speech recognition, speech-to-speech, speech-to text, and text-to-speech capabilities. Compared to previous efforts in expressive speech research, SeamlessExpressive addresses certain underexplored aspects of prosody, such as speech rate and pauses for rhythm, while also preserving emotion and style. The model currently preserves these elements in speech-to-speech translation between English, Spanish, German, French, Italian, and Chinese.

In our increasingly interconnected world, where language differences may present a barrier to communication, translation systems can enable people from different linguistic backgrounds to share knowledge and experiences more seamlessly. However, many of these systems today do not preserve key elements of speech that make human communication human. More specifically, it’s not just the words we choose that convey what we want to say—it’s also how we speak them. Tone of voice, pauses, and emphasis carry important signals that help us communicate emotions and intent. Moreover, human speech and translation are sensitive to nuances such as turn-taking and timing controls. Picture, for example, how human interpreters work: they find just the right balance between low-latency and accurate translations. Waiting too long stifles the flow of communication, while going too fast compromises the overall quality of a translation. Translation systems that enable authentic conversations should deliver across all of these elements of communication.

Instead of using an autoregressive text-to-unit model as in UnitY, we used a non-autoregressive model. Autoregressive models predict the next token based on the previously generated tokens. While autoregressive models model speech naturally, they scale poorly as sequence length increases. They are also more likely to exhibit repetitive degeneration. Non-autoregressive models predict the duration of each segment, which enables each segment to be decoded in parallel. This makes them robust to long sequences, and we see improvements over the initial iteration of UnitY. Since the model inherently predicts duration, it is much more easily adaptable to the streaming use case, because we know exactly how much speech is needed to be generated for each piece of text, which is not the case for autoregressive models.

UnitY2, a new architecture that has a non-autoregressive text-to-unit decoder, is also instrumental to our work. In SeamlessM4T v2, we used multitask-UnitY2 to enable text input (updated from v1's multitask-UnitY). We also used the architecture for SeamlessStreaming and SeamlessExpressive. As our next generation multitask model, UnitY2 has superior speech generation capabilities through its improved text-to-unit model. This implementation leads to improved consistency between text output and speech output, compared to the SeamlessM4T v1 model.

All our models run on fairseq2 , the latest update of our sequence modeling toolkit. Similar to our previous work on SeamlessM4T, fairseq2 offers an ideal framework for building our streaming and expressivity updates because it is lightweight, easily composable with other PyTorch ecosystem libraries, and has more efficient modeling and data loader APIs.

EMMA is our core streaming algorithm, which allows us to intelligently decide when we have enough information to generate the next speech segment or target text. It improves upon previous state-of-the-art algorithms especially for long input sequences, which is the case for speech-to-text or speech-to-speech translation. Further, this algorithm allows us to fine-tune from offline models, which allows us to reap the benefits of the Seamless M4T v2 foundation model. Finally, we show empirically that this algorithm generalizes well across many different language pairs, which is particularly challenging for streaming models because the language pairs may be structured differently.

Expressivity

Preserving expression also requires a new approach. We replaced the unit HiFi-GAN vocoder in SeamlessM4T v2 with PRETSSEL, an expressive unit-to-speech generator. PRETSSEL is conditioned on the source speech for waveform generation to transfer tones, emotional expression, and vocal style qualities. We initialize our model from SeamlessM4T v2 in order to achieve high translation quality, which is the most fundamental need for a speech-to-speech translation system. We also developed Prosody UnitY2, integrating an expressivity encoder in SeamlessM4T v2 to guide unit generation with proper rhythm, speaking rate, and pauses. In addition, we release a suite of evaluation tools to capture the preservation of these aspects of expressivity.

Results

The updates to UnitY2 have resulted in improved translation quality across a variety of tasks. SeamlessM4T v2 achieves sate of the art translation for speech-to-speech and speech-to-text results in 100 languages. In the same model, it also beats Whisper v3’s for automatic speech recognition on average and in particular for lower resource languages. For speech-to-text translation, SeamlessM4T v2 improves by 10% compared to the model we released in August and by more than 17% over the strongest cascaded models when translating into English. For speech-to-speech translation, SeamlessM4T v2, improves over SeamlessM4T (v1) by more than 15% when translating into English, and by 25% when translating from English. In other tasks, SeamlessM4T v2 is on par with No Language Left Behind (NLLB) in text-to-text translation. It is also on-par on average with MMS in automatic speech recognition (ASR) (with better performance on mid and high-resource languages while MMS has better performance on low resource languages), and improving over the recently released Whisper-Large-v3 by more than 25%. In the zero-shot task of text-to-speech translation, SeamlessM4T v2 is on-par with strong cascaded models into English, and improves over these baselines by 16 percent in English. We compared SeamlessExpressive against a cascaded speech-to-text and text-to-speech pipeline, where speech-to-text is from SeamlessM4T v2, and text-to-speech is from strong open-sourced cross-lingual text-to-speech system that supports vocal style and emotion transfer. Results show that SeamlessExpressive is more stable with respect to noise in the source speech such that the output speech maintains high content translation quality, and better preserves styles and speech rate. SeamlessStreaming achieves state of the art low latency quality with speech-to-speech translation.

How we built AI translation systems responsibly: Toxicity mitigation

Accuracy is paramount in translation systems. Translation errors or unintended toxicity can cause misunderstandings between two people who don’t speak the same language. Keeping with our commitment to building responsible AI, we explored the problem of hallucinated toxicity further. We focused our efforts on SeamlessM4T v2, which serves as the foundation for SeamlessStreaming, SeamlessExpressive, and our unified Seamless model. The primary root cause for hallucinated toxicity often lies in the training data. Training samples can be noisy and contain unbalanced toxicity. For example, the input language side and target language side can contain different amounts of toxic words by mistake. Prior to training, we discarded any sample that showed signs of this imbalance. However, filtering is only a passive technique and does not fully prevent hallucinated toxicity. We went one step further this time, and implemented a novel approach that actively mitigates this phenomenon. During the translation generation process, our model automatically detects generated toxic words. When there are misaligned levels of toxicity, we automatically re-adjust the generation process and use a different choice of words. This works at inference time and does not require any fine-tuning of the translation model. By doing so, we significantly reduce added toxicity while preserving translation quality. Finally, building upon our past work on toxicity and bias evaluation, we’ve extended our evaluation framework with a new hallucinated toxicity detection tool. While our previous approach relied on an intermediate transcription model (ASR), we are now capable of detecting toxicity directly in the speech signal. This is useful in cases where toxicity is not conveyed by individual words, but rather in tone or general style. This allows us to get a more precise picture of the potential toxicity profile of our model. Additional research needs to be done on responsible AI for machine translation; however, we believe these measures bring us closer to realizing safer and more human-centric translation systems.

Audio watermarking

While AI tools can help bring the world closer together, it’s just as important that we include measures to prevent the risk of imitation and other forms of misuse. Our watermarking method offers a better level of reliability compared to passive discriminators, which are becoming less effective at differentiating synthetic voices from human ones as voice preservation technology advances. Watermarking actively embeds a signal that is imperceptible to the human ear, but still detectable within the audio using a detector model. Through this watermark, the origin of the audio can be accurately traced. This helps promote the responsible use of voice preservation technology by establishing a verifiable audio provenance and helps prevent potential abuses. Beyond sheer detection accuracy, our watermarking solution needs to be robust to various attacks. For example, bad actors can try to modify the audio by adding noise, echo, or filtering some frequencies to dilute the watermark and bypass detection. We tested our watermarking method against a broad range of attack types and the results show that it is more robust than the current state-of-the-art. Our method can also pinpoint AI-generated segments in audio down to the frame level, surpassing the previous state-of-the-art (which only provides a one second resolution). As in any kind of neural-network based safety mechanism, the watermarking model can be fine-tuned in isolation to forget its core properties. However, fine-tuning SeamlessExpressive and Seamless for translation purposes would not involve any update to the watermarking model itself, which does not play any role on translation quality.

Providing access to our technology

The breakthroughs we’ve achieved with Seamless show that the dream of a universal, real-time translator isn’t science fiction—it’s becoming a technical reality. We invite everyone to try our expressive translation demo. We’re also making our code, model and data available to the research community.

Try the expressive translation demo

Try the Hugging Face demo

Download the code, model, and data"
Meta_Blog,https://ai.meta.com/blog/fair-progress-and-learnings-across-socially-responsible-ai-research/,,FAIR progress and learnings across socially responsible AI research,"Enhancements across fairness research can help produce AI innovation that works well for everyone, regardless of demographic characteristics. That’s why FAIR is building tests and tools that aim to minimize potential bias and help to enable AI inclusivity and accessibility.

We’re continuing our work to create and distribute more diverse datasets that represent a wide range of people and experiences. We released the Casual Conversations v2 dataset, a consent-driven, publicly available resource that enables researchers to better evaluate the fairness and robustness of certain types of AI models. We also introduced FACET (FAirness in Computer Vision EvaluaTion), a new comprehensive benchmark for evaluating the fairness of computer vision models across classification, detection, instance segmentation, and visual grounding tasks.

In the growing subfield of large language models (LLMs), bias and toxicity metrics cover different demographic axes and text domains. Using just one metric doesn’t provide a full picture so we developed Robust Bias Evaluation of Large Generative Language Models (ROBBIE), a tool that compares six different prompt-based bias and toxicity metrics across 12 demographic axes and five different LLMs. The combination of these metrics enables a better understanding of bias and toxicity in the models that are being compared. It also allows us to explore the frequency of demographic terms in the texts on which an LLM trains and provides insight into how this could affect potential model biases, as described in our paper.

Representative generative AI would help enable consistent and realistic generation of content that is diverse and inclusive. DIG In focuses on evaluating gaps in quality and diversity of content generated from text-to-image models between geographic regions. After auditing five state-of-the-art text-to-image models using DIG In, our results suggest that progress in image generation quality has come at the cost of real-world geographic representation. The insights we gathered helped identify important areas for improvement, such as reducing background stereotypes or ensuring diversity prompting doesn’t hurt image consistency.

Fairness and privacy often can be considered in conflict with each other since most fairness methods need access to sensitive information. We’ve developed a new paradigm for evaluating group fairness that uses social networks to alleviate this issue. The key observation of this work is that homophily in social networks lets us define group fairness without access to any group information. This allows us to mitigate unfair machine learning outcomes by adjusting outcomes according to the similarity of users that is induced by the network structure. Importantly, this approach works without access to group information and without ever inferring sensitive information. As such, social network information helps to respect the privacy of users while enabling fair machine learning."
Meta_Blog,https://ai.meta.com/blog/fair-progress-and-learnings-across-socially-responsible-ai-research/,,FAIR progress and learnings across socially responsible AI research,"Enhancements across fairness research can help produce AI innovation that works well for everyone, regardless of demographic characteristics. That’s why FAIR is building tests and tools that aim to minimize potential bias and help to enable AI inclusivity and accessibility.

We’re continuing our work to create and distribute more diverse datasets that represent a wide range of people and experiences. We released the Casual Conversations v2 dataset, a consent-driven, publicly available resource that enables researchers to better evaluate the fairness and robustness of certain types of AI models. We also introduced FACET (FAirness in Computer Vision EvaluaTion), a new comprehensive benchmark for evaluating the fairness of computer vision models across classification, detection, instance segmentation, and visual grounding tasks.

In the growing subfield of large language models (LLMs), bias and toxicity metrics cover different demographic axes and text domains. Using just one metric doesn’t provide a full picture so we developed Robust Bias Evaluation of Large Generative Language Models (ROBBIE), a tool that compares six different prompt-based bias and toxicity metrics across 12 demographic axes and five different LLMs. The combination of these metrics enables a better understanding of bias and toxicity in the models that are being compared. It also allows us to explore the frequency of demographic terms in the texts on which an LLM trains and provides insight into how this could affect potential model biases, as described in our paper.

Representative generative AI would help enable consistent and realistic generation of content that is diverse and inclusive. DIG In focuses on evaluating gaps in quality and diversity of content generated from text-to-image models between geographic regions. After auditing five state-of-the-art text-to-image models using DIG In, our results suggest that progress in image generation quality has come at the cost of real-world geographic representation. The insights we gathered helped identify important areas for improvement, such as reducing background stereotypes or ensuring diversity prompting doesn’t hurt image consistency.

Fairness and privacy often can be considered in conflict with each other since most fairness methods need access to sensitive information. We’ve developed a new paradigm for evaluating group fairness that uses social networks to alleviate this issue. The key observation of this work is that homophily in social networks lets us define group fairness without access to any group information. This allows us to mitigate unfair machine learning outcomes by adjusting outcomes according to the similarity of users that is induced by the network structure. Importantly, this approach works without access to group information and without ever inferring sensitive information. As such, social network information helps to respect the privacy of users while enabling fair machine learning."
Meta_Blog,https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/,,Emu Video and Emu Edit: Our latest generative AI research milestones,"Whether or not you’ve personally used an AI image generation tool, you’ve likely seen the results: Visually distinct, often highly stylized and detailed, these images on their own can be quite striking—and the impact increases when you bring them to life by adding movement.

The field of generative AI is rapidly evolving, showing remarkable potential to augment human creativity and self-expression. In 2022, we made the leap from image generation to video generation in the span of a few months. And at this year’s Meta Connect, we announced several new developments , including Emu , our first foundational model for image generation. Technology from Emu underpins many of our generative AI experiences, some AI image editing tools for Instagram that let you take a photo and change its visual style or background, and the Imagine feature within Meta AI that lets you generate photorealistic images directly in messages with that assistant or in group chats across our family of apps. Our work in this exciting field is ongoing, and today, we’re announcing new research into controlled image editing based solely on text instructions and a method for text-to-video generation based on diffusion models.

With Emu Video, which leverages our Emu model, we present a simple method for text-to-video generation based on diffusion models. This is a unified architecture for video generation tasks that can respond to a variety of inputs: text only, image only, and both text and image. We’ve split the process into two steps: first, generating images conditioned on a text prompt, and then generating video conditioned on both the text and the generated image. This “factorized” or split approach to video generation lets us train video generation models efficiently. We show that factorized video generation can be implemented via a single diffusion model. We present critical design decisions, like adjusting noise schedules for video diffusion, and multi-stage training that allows us to directly generate higher-resolution videos.



Unlike prior work that requires a deep cascade of models (e.g., five models for Make-A-Video ), our state-of-the-art approach is simple to implement and uses just two diffusion models to generate 512x512 four-second long videos at 16 frames per second. In human evaluations, our video generations are strongly preferred compared to prior work—in fact, this model was preferred over Make-A-Video by 96% of respondents based on quality and by 85% of respondents based on faithfulness to the text prompt. Finally, the same model can “animate” user-provided images based on a text prompt where it once again sets a new state-of-the-art outperforming prior work by a significant margin.





Emu Edit: Precise image editing via recognition and generation tasks

Of course, the use of generative AI is often a process. You try a prompt, the generated image isn’t quite what you had in mind, so you continue tweaking the prompt until you get to a more desired outcome. That’s why prompt engineering has become a thing. And while instructable image generative models have made significant strides in recent years, they still face limitations when it comes to offering precise control. That’s why we’re introducing Emu Edit, a novel approach that aims to streamline various image manipulation tasks and bring enhanced capabilities and precision to image editing.





Emu Edit is capable of free-form editing through instructions, encompassing tasks such as local and global editing, removing and adding a background, color and geometry transformations, detection and segmentation, and more. Current methods often lean towards either over-modifying or under-performing on various editing tasks. We argue that the primary objective shouldn’t just be about producing a “believable” image. Instead, the model should focus on precisely altering only the pixels relevant to the edit request. Unlike many generative AI models today, Emu Edit precisely follows instructions, ensuring that pixels in the input image unrelated to the instructions remain untouched. For instance, when adding the text “Aloha!” to a baseball cap, the cap itself should remain unchanged.



Our key insight is that incorporating computer vision tasks as instructions to image generation models offers unprecedented control in image generation and editing. Through a detailed examination of both local and global editing tasks, we highlight the vast potential of Emu Edit in executing detailed edit instructions. In order to train the model, we’ve developed a dataset that contains 10 million synthesized samples, each including an input image, a description of the task to be performed, and the targeted output image. We believe it’s the largest dataset of its kind to date. As a result, our model displays unprecedented edit results in terms of both instruction faithfulness and image quality. In our evaluations, Emu Edit demonstrates superior performance over current methods, producing new state-of-the-art results in both qualitative and quantitative evaluations for a range of image editing tasks.

The road ahead"
Meta_Blog,https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/,,Emu Video and Emu Edit: Our latest generative AI research milestones,"Whether or not you’ve personally used an AI image generation tool, you’ve likely seen the results: Visually distinct, often highly stylized and detailed, these images on their own can be quite striking—and the impact increases when you bring them to life by adding movement.

The field of generative AI is rapidly evolving, showing remarkable potential to augment human creativity and self-expression. In 2022, we made the leap from image generation to video generation in the span of a few months. And at this year’s Meta Connect, we announced several new developments , including Emu , our first foundational model for image generation. Technology from Emu underpins many of our generative AI experiences, some AI image editing tools for Instagram that let you take a photo and change its visual style or background, and the Imagine feature within Meta AI that lets you generate photorealistic images directly in messages with that assistant or in group chats across our family of apps. Our work in this exciting field is ongoing, and today, we’re announcing new research into controlled image editing based solely on text instructions and a method for text-to-video generation based on diffusion models.

With Emu Video, which leverages our Emu model, we present a simple method for text-to-video generation based on diffusion models. This is a unified architecture for video generation tasks that can respond to a variety of inputs: text only, image only, and both text and image. We’ve split the process into two steps: first, generating images conditioned on a text prompt, and then generating video conditioned on both the text and the generated image. This “factorized” or split approach to video generation lets us train video generation models efficiently. We show that factorized video generation can be implemented via a single diffusion model. We present critical design decisions, like adjusting noise schedules for video diffusion, and multi-stage training that allows us to directly generate higher-resolution videos.



Unlike prior work that requires a deep cascade of models (e.g., five models for Make-A-Video ), our state-of-the-art approach is simple to implement and uses just two diffusion models to generate 512x512 four-second long videos at 16 frames per second. In human evaluations, our video generations are strongly preferred compared to prior work—in fact, this model was preferred over Make-A-Video by 96% of respondents based on quality and by 85% of respondents based on faithfulness to the text prompt. Finally, the same model can “animate” user-provided images based on a text prompt where it once again sets a new state-of-the-art outperforming prior work by a significant margin.





Emu Edit: Precise image editing via recognition and generation tasks

Of course, the use of generative AI is often a process. You try a prompt, the generated image isn’t quite what you had in mind, so you continue tweaking the prompt until you get to a more desired outcome. That’s why prompt engineering has become a thing. And while instructable image generative models have made significant strides in recent years, they still face limitations when it comes to offering precise control. That’s why we’re introducing Emu Edit, a novel approach that aims to streamline various image manipulation tasks and bring enhanced capabilities and precision to image editing.





Emu Edit is capable of free-form editing through instructions, encompassing tasks such as local and global editing, removing and adding a background, color and geometry transformations, detection and segmentation, and more. Current methods often lean towards either over-modifying or under-performing on various editing tasks. We argue that the primary objective shouldn’t just be about producing a “believable” image. Instead, the model should focus on precisely altering only the pixels relevant to the edit request. Unlike many generative AI models today, Emu Edit precisely follows instructions, ensuring that pixels in the input image unrelated to the instructions remain untouched. For instance, when adding the text “Aloha!” to a baseball cap, the cap itself should remain unchanged.



Our key insight is that incorporating computer vision tasks as instructions to image generation models offers unprecedented control in image generation and editing. Through a detailed examination of both local and global editing tasks, we highlight the vast potential of Emu Edit in executing detailed edit instructions. In order to train the model, we’ve developed a dataset that contains 10 million synthesized samples, each including an input image, a description of the task to be performed, and the targeted output image. We believe it’s the largest dataset of its kind to date. As a result, our model displays unprecedented edit results in terms of both instruction faithfulness and image quality. In our evaluations, Emu Edit demonstrates superior performance over current methods, producing new state-of-the-art results in both qualitative and quantitative evaluations for a range of image editing tasks.

The road ahead"
Meta_Blog,https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/,,5 Steps to Getting Started with Llama 2,"Getting started with Llama 2

Step 1: Prerequisites and dependencies

For running this example, we will use the transformers and accelerate libraries from Hugging Face.

We will use Python to write our script to set up and run the pipeline. To install Python, visit the Python website , where you can choose your OS and download the version of Python you like.

Step 2: Download the model weights

Our models are available on our Llama 2 Github repo . To download the model through our Github repository: Visit the AI at Meta website , accept our License and submit the form. Once your request is approved, you will receive a pre-signed URL in your email.

, accept our License and submit the form. Once your request is approved, you will receive a pre-signed URL in your email. Clone the Llama 2 repository



git clone https://github.com/facebookresearch/llama

Launch the download.sh script (sh download.sh). When prompted, enter the presigned URL you receive in your email. Choose the model variant you want to download, for example: 7b-chat. This will download the tokenizer.model, and a directory llama-2-7b-chat with the weights in it. Run ln -h ./tokenizer.model ./llama-2-7b-chat/tokenizer.mode l to create a link to the tokenizer. This is needed for conversion (next step) Convert the model weights to run with Hugging Face:

TRANSFORM=`python -c ""import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')""` pip install protobuf && python $TRANSFORM --input_dir ./llama-2-7b-chat --model_size 7B --output_dir ./llama-2-7b-chat-hf



We also provide already converted Llama 2 weights on Hugging Face . To use the downloads on Hugging Face, you must first request a download as shown in the steps above making sure that you are using the same email address as your Hugging Face account.

Step 3: Write your Python script

Now, we will create a new Python script which we will use to run our example. This script will contain all the code necessary to load the model and to run inference with transformers.

Import necessary modules

First, we will need to import the following necessary modules in your script: LlamaForCausalLM is the Llama 2 model class, LlamaTokenizer prepares your prompt for the model to process, pipeline is an abstraction to generate model outputs, and torch allows us to use PyTorch and specify the datatype we’d like to use.

import torch import transformers from transformers import LlamaForCausalLM, LlamaTokenizer

Load your model

Next we load the Llama model with the weights we downloaded and converted (stored at ./llama-2-7b-chat-hf in this example).

model_dir = ""./llama-2-7b-chat-hf"" model = LlamaForCausalLM.from_pretrained(model_dir)

Define and instantiate the tokenizer and pipeline

We need to make sure that we have our inputs prepared for the model. This is done by loading the tokenizer associated with our model. In your script add the following that lets you initialize the tokenizer from the same model directory:

tokenizer = LlamaTokenizer.from_pretrained(model_dir)

Next we need a way to use our model for inference. Pipeline allows us to specify which type of task the pipeline needs to run ( “text-generation” ), specify the model that the pipeline should use to make predictions ( model ), define the precision to use this model ( torch.float16 ), device on which the pipeline should run ( device_map ) among various other options. In your script, add the following to instantiate the pipeline that we will use to run our example:

pipeline = transformers.pipeline( ""text-generation"", model=model, tokenizer=tokenizer, torch_dtype=torch.float16, device_map=""auto"", )

Run the pipeline

Now we have our pipeline defined, and we need to provide some text prompts as inputs to our pipeline to use when it runs to generate responses ( sequences ). The pipeline shown in the example below sets do_sample to True, which allows us to specify the decoding strategy we’d like to use to select the next token from the probability distribution over the entire vocabulary. In our example, we are using top_k sampling. By changing max_length , you can specify how long you’d like the generated response to be. Setting the num_return_sequences parameter to greater than one will let you generate more than one output. In your script, add the following to provide input, and information on how to run the pipeline:

sequences = pipeline( 'I have tomatoes, basil and cheese at home. What can I cook for dinner?

', do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=400, ) for seq in sequences: print(f""{seq['generated_text']}"")

Step 4: Run the model"
Meta_Blog,https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/,,5 Steps to Getting Started with Llama 2,"Getting started with Llama 2

Step 1: Prerequisites and dependencies

For running this example, we will use the transformers and accelerate libraries from Hugging Face.

We will use Python to write our script to set up and run the pipeline. To install Python, visit the Python website , where you can choose your OS and download the version of Python you like.

Step 2: Download the model weights

Our models are available on our Llama 2 Github repo . To download the model through our Github repository: Visit the AI at Meta website , accept our License and submit the form. Once your request is approved, you will receive a pre-signed URL in your email.

, accept our License and submit the form. Once your request is approved, you will receive a pre-signed URL in your email. Clone the Llama 2 repository



git clone https://github.com/facebookresearch/llama

Launch the download.sh script (sh download.sh). When prompted, enter the presigned URL you receive in your email. Choose the model variant you want to download, for example: 7b-chat. This will download the tokenizer.model, and a directory llama-2-7b-chat with the weights in it. Run ln -h ./tokenizer.model ./llama-2-7b-chat/tokenizer.mode l to create a link to the tokenizer. This is needed for conversion (next step) Convert the model weights to run with Hugging Face:

TRANSFORM=`python -c ""import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')""` pip install protobuf && python $TRANSFORM --input_dir ./llama-2-7b-chat --model_size 7B --output_dir ./llama-2-7b-chat-hf



We also provide already converted Llama 2 weights on Hugging Face . To use the downloads on Hugging Face, you must first request a download as shown in the steps above making sure that you are using the same email address as your Hugging Face account.

Step 3: Write your Python script

Now, we will create a new Python script which we will use to run our example. This script will contain all the code necessary to load the model and to run inference with transformers.

Import necessary modules

First, we will need to import the following necessary modules in your script: LlamaForCausalLM is the Llama 2 model class, LlamaTokenizer prepares your prompt for the model to process, pipeline is an abstraction to generate model outputs, and torch allows us to use PyTorch and specify the datatype we’d like to use.

import torch import transformers from transformers import LlamaForCausalLM, LlamaTokenizer

Load your model

Next we load the Llama model with the weights we downloaded and converted (stored at ./llama-2-7b-chat-hf in this example).

model_dir = ""./llama-2-7b-chat-hf"" model = LlamaForCausalLM.from_pretrained(model_dir)

Define and instantiate the tokenizer and pipeline

We need to make sure that we have our inputs prepared for the model. This is done by loading the tokenizer associated with our model. In your script add the following that lets you initialize the tokenizer from the same model directory:

tokenizer = LlamaTokenizer.from_pretrained(model_dir)

Next we need a way to use our model for inference. Pipeline allows us to specify which type of task the pipeline needs to run ( “text-generation” ), specify the model that the pipeline should use to make predictions ( model ), define the precision to use this model ( torch.float16 ), device on which the pipeline should run ( device_map ) among various other options. In your script, add the following to instantiate the pipeline that we will use to run our example:

pipeline = transformers.pipeline( ""text-generation"", model=model, tokenizer=tokenizer, torch_dtype=torch.float16, device_map=""auto"", )

Run the pipeline

Now we have our pipeline defined, and we need to provide some text prompts as inputs to our pipeline to use when it runs to generate responses ( sequences ). The pipeline shown in the example below sets do_sample to True, which allows us to specify the decoding strategy we’d like to use to select the next token from the probability distribution over the entire vocabulary. In our example, we are using top_k sampling. By changing max_length , you can specify how long you’d like the generated response to be. Setting the num_return_sequences parameter to greater than one will let you generate more than one output. In your script, add the following to provide input, and information on how to run the pipeline:

sequences = pipeline( 'I have tomatoes, basil and cheese at home. What can I cook for dinner?

', do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=400, ) for seq in sequences: print(f""{seq['generated_text']}"")

Step 4: Run the model"
Meta_Blog,https://ai.meta.com/blog/habitat-3-socially-intelligent-robots-siro/,,Embodied AI: Toward effective collaboration between humans and socially intelligent robots,"Habitat 3.0 builds on those advances and supports both robots and humanoid avatars to enable human-robot collaboration on everyday tasks (e.g., tidying up the living room, preparing a recipe in the kitchen). This opens up new avenues for research on human-robot collaboration in diverse, realistic, and visually and semantically rich tasks. Habitat 3.0 also supports human avatars with a realistic appearance, natural gait, and actions to model realistic low- and high-level interactions. These humanoid avatars are controllable both by learned policies as well as real humans using a human-in-the-loop interface. This interface can support different media, such as control via keyboard and mouse as well as through VR headsets. This cohabitation of humans and robots in the simulation environment allows us for the first time to learn robotics AI policies in the presence of humanoid avatars in home-like environments on everyday tasks and evaluate them with real humans-in-the-loop. This is significant on several fronts:

Reinforcement learning algorithms typically require millions of iterations to learn something meaningful, so it can take years to do these experiments in the physical world. In simulation, they can be completed in a few days.

It’s impractical to collect data in different houses in the physical world, as that requires moving the robots to different places, setting up the environment, etc. In simulation, we can change the environment in a fraction of a second and start experimenting in a new environment.

If the model isn’t trained well, there’s a risk that the robot could damage the environment or harm people in the physical world. Simulation allows us to test the methods in a safe environment before deploying them to the physical world to help mitigate those safety concerns.

Today’s state-of-the-art AI models require a lot of data for training purposes. Simulation enables us to easily scale up data collection, while in the physical world it can be quite costly and slow.

We also present two highly relevant tasks and a suite of baselines to establish benchmarks in the field of social embodied AI. The first task, Social Rearrangement, involves a robot and a humanoid avatar working collaboratively to perform a set of pick-and-place tasks, like cleaning up a house. In this task, the robot and the human must coordinate their actions to achieve a common goal. This intelligent behavior emerges from large-scale training in simulation. The second task, Social Navigation, involves the robot locating and following a person while maintaining a safe distance.

We believe Habitat 3.0 is the first simulator to support large-scale training on human-robot interaction tasks in diverse, realistic indoor environments. This training results in emergent collaborative behaviors in our learned policy, such as giving way to the human partner in narrow corridors and efficiently splitting up a task for faster completion than the human could accomplish alone."
Meta_Blog,https://ai.meta.com/blog/habitat-3-socially-intelligent-robots-siro/,,Embodied AI: Toward effective collaboration between humans and socially intelligent robots,"Habitat 3.0 builds on those advances and supports both robots and humanoid avatars to enable human-robot collaboration on everyday tasks (e.g., tidying up the living room, preparing a recipe in the kitchen). This opens up new avenues for research on human-robot collaboration in diverse, realistic, and visually and semantically rich tasks. Habitat 3.0 also supports human avatars with a realistic appearance, natural gait, and actions to model realistic low- and high-level interactions. These humanoid avatars are controllable both by learned policies as well as real humans using a human-in-the-loop interface. This interface can support different media, such as control via keyboard and mouse as well as through VR headsets. This cohabitation of humans and robots in the simulation environment allows us for the first time to learn robotics AI policies in the presence of humanoid avatars in home-like environments on everyday tasks and evaluate them with real humans-in-the-loop. This is significant on several fronts:

Reinforcement learning algorithms typically require millions of iterations to learn something meaningful, so it can take years to do these experiments in the physical world. In simulation, they can be completed in a few days.

It’s impractical to collect data in different houses in the physical world, as that requires moving the robots to different places, setting up the environment, etc. In simulation, we can change the environment in a fraction of a second and start experimenting in a new environment.

If the model isn’t trained well, there’s a risk that the robot could damage the environment or harm people in the physical world. Simulation allows us to test the methods in a safe environment before deploying them to the physical world to help mitigate those safety concerns.

Today’s state-of-the-art AI models require a lot of data for training purposes. Simulation enables us to easily scale up data collection, while in the physical world it can be quite costly and slow.

We also present two highly relevant tasks and a suite of baselines to establish benchmarks in the field of social embodied AI. The first task, Social Rearrangement, involves a robot and a humanoid avatar working collaboratively to perform a set of pick-and-place tasks, like cleaning up a house. In this task, the robot and the human must coordinate their actions to achieve a common goal. This intelligent behavior emerges from large-scale training in simulation. The second task, Social Navigation, involves the robot locating and following a person while maintaining a safe distance.

We believe Habitat 3.0 is the first simulator to support large-scale training on human-robot interaction tasks in diverse, realistic indoor environments. This training results in emergent collaborative behaviors in our learned policy, such as giving way to the human partner in narrow corridors and efficiently splitting up a task for faster completion than the human could accomplish alone."
Meta_Blog,https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/,,Toward a real-time decoding of images from brain activity,"At every moment of every day, our brains meticulously sculpt a wealth of sensory signals into meaningful representations of the world around us. Yet how this continuous process actually works remains poorly understood.

Today, Meta is announcing an important milestone in the pursuit of that fundamental question. Using magnetoencephalography (MEG), a non-invasive neuroimaging technique in which thousands of brain activity measurements are taken per second, we showcase an AI system capable of decoding the unfolding of visual representations in the brain with an unprecedented temporal resolution."
Meta_Blog,https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/,,Toward a real-time decoding of images from brain activity,"At every moment of every day, our brains meticulously sculpt a wealth of sensory signals into meaningful representations of the world around us. Yet how this continuous process actually works remains poorly understood.

Today, Meta is announcing an important milestone in the pursuit of that fundamental question. Using magnetoencephalography (MEG), a non-invasive neuroimaging technique in which thousands of brain activity measurements are taken per second, we showcase an AI system capable of decoding the unfolding of visual representations in the brain with an unprecedented temporal resolution."
Meta_Blog,https://ai.meta.com/blog/stable-signature-watermarking-generative-ai/,,Stable Signature: A new method for watermarking images created by open source generative AI,"Stable Signature closes the potential for removing the watermark by rooting it in the model with a watermark that can trace back to where the image was created.

While the fact that these safeguards exist is a start, this simple tactic shows there’s plenty of potential for this feature to be exploited. The work we’re sharing today is a solution for adding watermarks to images that come from open source generative AI models. We’re exploring how this research could potentially be used in our models. In keeping with our approach to open science, we want to share this research with the AI community in the hope of advancing the work being done in this space.

More than 11 billion images have been created using models from three open source repositories, according to Everypixel Journal . In this case, invisible watermarks can be removed simply by deleting the line that generates the watermark.

At FAIR, we’re excited about driving continued exploratory research in generative AI, but we also want to make sure we do so in a manner that prioritizes safety and responsibility. Today, together with Inria , we are excited to share a research paper and code that details Stable Signature, an invisible watermarking technique we created to distinguish when an image is created by an open source generative AI model. Invisible watermarking incorporates information into digital content. The watermark is invisible to the naked eye but can be detected by algorithms—even if people edit the images. While there have been other lines of research around watermarking, many existing methods create the watermark after an image is generated.

AI-powered image generation is booming and for good reason: It’s fun, entertaining, and easy to use. While these models enable new creative possibilities, they may raise concerns about potential misuse from bad actors who may intentionally generate images to deceive people. Even images created in good fun could still go viral and potentially mislead people. For example, earlier this year, images appearing to show Pope Francis wearing a flashy white puffy jacket went viral. The images weren’t actual photographs , but plenty of people were fooled, since there weren’t any clear indicators to distinguish that the content was created by generative AI.

Bob receives his version of the model and generates images. The generated images will carry the watermark of Bob. They can be analyzed by Alice or third parties to see if the image was generated by Bob, who used the generative AI model.

Alice trains a master generative model. Before distributing it, she fine-tunes a small part of the model (called the decoder) to root a given watermark for Bob. This watermark may identify the model version, a company, a user, etc.



We know that people enjoy sharing and reposting images. What if Bob shared the image he created with 10 friends, who each then shared it with 10 more friends? During this time, it’s possible that someone could have altered the image, such as by cropping it, compressing it, or changing the colors. We built Stable Signature to be robust to these changes. No matter how a person transforms an image, the original watermark will likely remain in the digital data and can be traced back to the generative model where it was created.





During our research, we discovered two major advantages of Stable Signature over passive detection methods. First, we were able to control and reduce the generation of false positives, which occur when we mistake an image produced by humans for one generated by AI. This is crucial given the prevalence of non-AI-generated images shared online. For example, the most effective existing detection method can spot approximately 50% of edited generated images but still generates a false positive rate of approximately 1/100. Put differently, on a user-generated content platform receiving 1 billion images daily, around 10 million images would be incorrectly flagged to detect just half of the generated ones. On the other hand, Stable Signature detects images with the same accuracy at a false positive rate of 1e-10 (which can be set to a specific desired value). Moreover, our watermarking method allows us to trace images from various versions of the same model—a capability not possible with passive techniques.



How Stable Signature works with fine-tuning



A common practice in AI is to take foundational models and fine-tune them to handle specific use cases that are sometimes even tailored to one person. For example, a model could be shown images of Alice’s dog, and then Alice could ask for the model to generate images of her dog at the beach. This is done through methods like DreamBooth , Textual Inversion , and ControlNet . These methods act at the latent model level, and they do not change the decoder. This means that our watermarking method is not affected by these fine-tunings. Overall, Stable Signature works well with vector-quantized image modeling (like VQGANs) and latent diffusion models (like Stable Diffusion). Since our method doesn’t modify the diffusion generation process, it’s compatible with the popular models mentioned above. We believe that, with some adaptation, Stable Signature could also be applied to other modeling methods.

Providing access to our technology



The use of generative AI is advancing at a rapid pace. Currently, there aren’t any common standards for identifying and labeling AI-generated content across the industry. In order to build better products, we believe advancements in responsibility research, like the work we’re sharing today, must exist in parallel. We’re excited to share our work and give the AI research community access to these tools in the hope of driving continued collaboration and iteration. While it’s still early days for generative AI, we believe that by sharing our research, engaging with the community, and listening to feedback, we can all work together to ensure this impressive new technology is built, operated, and used in a responsible way. The research we’re sharing today focuses on images, but in the future we hope to explore the potential of integrating our Stable Signature method across more generative AI modalities. Our model works with many popular open source models, however there are still limitations. It does not scale to non-latent generative models, so it may not be future proof to new generation technologies. By continuing to invest in this research, we believe we can chart a future where generative AI is used responsibly for exciting new creative endeavors.

This blog post reflects the work of Matthijs Douze and Pierre Fernandez. We'd like to acknowledge the contributions of Guillaume Couairon, Teddy Furon, and Hervé Jégou to this research.

Get the code"
Meta_Blog,https://ai.meta.com/blog/stable-signature-watermarking-generative-ai/,,Stable Signature: A new method for watermarking images created by open source generative AI,"Stable Signature closes the potential for removing the watermark by rooting it in the model with a watermark that can trace back to where the image was created.

While the fact that these safeguards exist is a start, this simple tactic shows there’s plenty of potential for this feature to be exploited. The work we’re sharing today is a solution for adding watermarks to images that come from open source generative AI models. We’re exploring how this research could potentially be used in our models. In keeping with our approach to open science, we want to share this research with the AI community in the hope of advancing the work being done in this space.

More than 11 billion images have been created using models from three open source repositories, according to Everypixel Journal . In this case, invisible watermarks can be removed simply by deleting the line that generates the watermark.

At FAIR, we’re excited about driving continued exploratory research in generative AI, but we also want to make sure we do so in a manner that prioritizes safety and responsibility. Today, together with Inria , we are excited to share a research paper and code that details Stable Signature, an invisible watermarking technique we created to distinguish when an image is created by an open source generative AI model. Invisible watermarking incorporates information into digital content. The watermark is invisible to the naked eye but can be detected by algorithms—even if people edit the images. While there have been other lines of research around watermarking, many existing methods create the watermark after an image is generated.

AI-powered image generation is booming and for good reason: It’s fun, entertaining, and easy to use. While these models enable new creative possibilities, they may raise concerns about potential misuse from bad actors who may intentionally generate images to deceive people. Even images created in good fun could still go viral and potentially mislead people. For example, earlier this year, images appearing to show Pope Francis wearing a flashy white puffy jacket went viral. The images weren’t actual photographs , but plenty of people were fooled, since there weren’t any clear indicators to distinguish that the content was created by generative AI.

Bob receives his version of the model and generates images. The generated images will carry the watermark of Bob. They can be analyzed by Alice or third parties to see if the image was generated by Bob, who used the generative AI model.

Alice trains a master generative model. Before distributing it, she fine-tunes a small part of the model (called the decoder) to root a given watermark for Bob. This watermark may identify the model version, a company, a user, etc.



We know that people enjoy sharing and reposting images. What if Bob shared the image he created with 10 friends, who each then shared it with 10 more friends? During this time, it’s possible that someone could have altered the image, such as by cropping it, compressing it, or changing the colors. We built Stable Signature to be robust to these changes. No matter how a person transforms an image, the original watermark will likely remain in the digital data and can be traced back to the generative model where it was created.





During our research, we discovered two major advantages of Stable Signature over passive detection methods. First, we were able to control and reduce the generation of false positives, which occur when we mistake an image produced by humans for one generated by AI. This is crucial given the prevalence of non-AI-generated images shared online. For example, the most effective existing detection method can spot approximately 50% of edited generated images but still generates a false positive rate of approximately 1/100. Put differently, on a user-generated content platform receiving 1 billion images daily, around 10 million images would be incorrectly flagged to detect just half of the generated ones. On the other hand, Stable Signature detects images with the same accuracy at a false positive rate of 1e-10 (which can be set to a specific desired value). Moreover, our watermarking method allows us to trace images from various versions of the same model—a capability not possible with passive techniques.



How Stable Signature works with fine-tuning



A common practice in AI is to take foundational models and fine-tune them to handle specific use cases that are sometimes even tailored to one person. For example, a model could be shown images of Alice’s dog, and then Alice could ask for the model to generate images of her dog at the beach. This is done through methods like DreamBooth , Textual Inversion , and ControlNet . These methods act at the latent model level, and they do not change the decoder. This means that our watermarking method is not affected by these fine-tunings. Overall, Stable Signature works well with vector-quantized image modeling (like VQGANs) and latent diffusion models (like Stable Diffusion). Since our method doesn’t modify the diffusion generation process, it’s compatible with the popular models mentioned above. We believe that, with some adaptation, Stable Signature could also be applied to other modeling methods.

Providing access to our technology



The use of generative AI is advancing at a rapid pace. Currently, there aren’t any common standards for identifying and labeling AI-generated content across the industry. In order to build better products, we believe advancements in responsibility research, like the work we’re sharing today, must exist in parallel. We’re excited to share our work and give the AI research community access to these tools in the hope of driving continued collaboration and iteration. While it’s still early days for generative AI, we believe that by sharing our research, engaging with the community, and listening to feedback, we can all work together to ensure this impressive new technology is built, operated, and used in a responsible way. The research we’re sharing today focuses on images, but in the future we hope to explore the potential of integrating our Stable Signature method across more generative AI modalities. Our model works with many popular open source models, however there are still limitations. It does not scale to non-latent generative models, so it may not be future proof to new generation technologies. By continuing to invest in this research, we believe we can chart a future where generative AI is used responsibly for exciting new creative endeavors.

This blog post reflects the work of Matthijs Douze and Pierre Fernandez. We'd like to acknowledge the contributions of Guillaume Couairon, Teddy Furon, and Hervé Jégou to this research.

Get the code"
Meta_Blog,https://ai.meta.com/blog/llama-2-updates-connect-2023/,,"The Llama Ecosystem: Past, Present, and Future","But we wanted to make the technology available more broadly. This is where Llama 2 came in.

Why Did We Release Our Models?

As our history shows, we believe deeply in the power of the open source community. We believe that state-of-the-art AI technology is safer and better aligned when it’s open and accessible to everyone.

Additionally, where there are areas of high entropy, it’s advantageous to build bridges and leverage the innovation that inevitably arises. This was true for PyTorch, where breakthroughs like Stable Diffusion, GPT 3, and GPT 4 continually disrupted the world of AI, and it’s true for Llama as well. For us at Meta, we can summarize the value back along three axes:

Research: New techniques, performance optimizations, tools, and evaluation methods, including work on safety, provide Meta leverage from the research community to more quickly incorporate learnings. Many of these communities are also nascent, and collaborating in the open makes it much easier to make progress;

Enterprise and commercialization: The more enterprises and startups build on our technology, the more we can learn about use cases, safe model deployment, and potential opportunities; and

Developer ecosystem: LLMs have fundamentally changed AI development, and new tools and approaches are emerging daily for manipulating, managing, and evaluating models. Having a lingua franca to the community enables us to quickly leverage these technologies, accelerating our internal stack."
Meta_Blog,https://ai.meta.com/blog/llama-2-updates-connect-2023/,,"The Llama Ecosystem: Past, Present, and Future","But we wanted to make the technology available more broadly. This is where Llama 2 came in.

Why Did We Release Our Models?

As our history shows, we believe deeply in the power of the open source community. We believe that state-of-the-art AI technology is safer and better aligned when it’s open and accessible to everyone.

Additionally, where there are areas of high entropy, it’s advantageous to build bridges and leverage the innovation that inevitably arises. This was true for PyTorch, where breakthroughs like Stable Diffusion, GPT 3, and GPT 4 continually disrupted the world of AI, and it’s true for Llama as well. For us at Meta, we can summarize the value back along three axes:

Research: New techniques, performance optimizations, tools, and evaluation methods, including work on safety, provide Meta leverage from the research community to more quickly incorporate learnings. Many of these communities are also nascent, and collaborating in the open makes it much easier to make progress;

Enterprise and commercialization: The more enterprises and startups build on our technology, the more we can learn about use cases, safe model deployment, and potential opportunities; and

Developer ecosystem: LLMs have fundamentally changed AI development, and new tools and approaches are emerging daily for manipulating, managing, and evaluating models. Having a lingua franca to the community enables us to quickly leverage these technologies, accelerating our internal stack."
Meta_Blog,https://ai.meta.com/blog/dinov2-facet-computer-vision-fairness-evaluation/,,"Announcing the commercial relicensing and expansion of DINOv2, plus the introduction of FACET","This high-level summary aggregates the absolute performance disparities between extreme categories across all classes in FACET. It shows that DINOv2 performs similarly to these other models: While it fares a bit worse than OpenCLIP with respect to disparity across perceived gender presentation, it performs better than other models with respect to perceived age group and skin tone.

The FACET evaluation lets us dive deeper into the potential biases of the model at the level of classes, which is where FACET adds great value compared to earlier fairness evaluations. For example, for one of its most gender-biased classes, the “nurse” class, DINOv2 model displays a disparity of +16.9 points between predictions on images with people who are perceived as having more stereotypically female attributes compared to those with stereotypically male attributes. SEERv2 and OpenCLIP exhibit a more pronounced bias for the “nurse” class, with disparities of +21.4 and +23.9 points respectively. As SEERv2 was pretrained on uncurated social media content, this might reflect a lack of diversity in the data source. And OpenCLIP, using web crawled data filtered via the CLIP vision-language model, could amplify occupational gender associations already present both in the image and text training data and this filtering model."
Meta_Blog,https://ai.meta.com/blog/dinov2-facet-computer-vision-fairness-evaluation/,,"Announcing the commercial relicensing and expansion of DINOv2, plus the introduction of FACET","This high-level summary aggregates the absolute performance disparities between extreme categories across all classes in FACET. It shows that DINOv2 performs similarly to these other models: While it fares a bit worse than OpenCLIP with respect to disparity across perceived gender presentation, it performs better than other models with respect to perceived age group and skin tone.

The FACET evaluation lets us dive deeper into the potential biases of the model at the level of classes, which is where FACET adds great value compared to earlier fairness evaluations. For example, for one of its most gender-biased classes, the “nurse” class, DINOv2 model displays a disparity of +16.9 points between predictions on images with people who are perceived as having more stereotypically female attributes compared to those with stereotypically male attributes. SEERv2 and OpenCLIP exhibit a more pronounced bias for the “nurse” class, with disparities of +21.4 and +23.9 points respectively. As SEERv2 was pretrained on uncurated social media content, this might reflect a lack of diversity in the data source. And OpenCLIP, using web crawled data filtered via the CLIP vision-language model, could amplify occupational gender associations already present both in the image and text training data and this filtering model."
Meta_Blog,https://ai.meta.com/blog/code-llama-large-language-model-coding/,,"Introducing Code Llama, a state-of-the-art large language model for coding","The generative AI space is evolving rapidly, and we believe an open approach to today’s AI is the best one for developing new AI tools that are innovative, safe, and responsible. We are releasing Code Llama under the same community license as Llama 2 .

Today, we are releasing Code Llama, a large language model (LLM) that can use text prompts to generate code. Code Llama is state-of-the-art for publicly available LLMs on code tasks, and has the potential to make workflows faster and more efficient for current developers and lower the barrier to entry for people who are learning to code. Code Llama has the potential to be used as a productivity and educational tool to help programmers write more robust, well-documented software.

How Code Llama works

Code Llama is a code-specialized version of Llama 2 that was created by further training Llama 2 on its code-specific datasets, sampling more data from that same dataset for longer. Essentially, Code Llama features enhanced coding capabilities, built on top of Llama 2. It can generate code, and natural language about code, from both code and natural language prompts (e.g., “Write me a function that outputs the fibonacci sequence.”) It can also be used for code completion and debugging. It supports many of the most popular languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.





We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens. The 7B and 13B base and instruct models have also been trained with fill-in-the-middle (FIM) capability, allowing them to insert code into existing code, meaning they can support tasks like code completion right out of the box. The three models address different serving and latency requirements. The 7B model, for example, can be served on a single GPU. The 34B and 70B models return the best results and allow for better coding assistance, but the smaller 7B and 13B models are faster and more suitable for tasks that require low latency, like real-time code completion.





The Code Llama models provide stable generations with up to 100,000 tokens of context. All models are trained on sequences of 16,000 tokens and show improvements on inputs with up to 100,000 tokens. Aside from being a prerequisite for generating longer programs, having longer input sequences unlocks exciting new use cases for a code LLM. For example, users can provide the model with more context from their codebase to make the generations more relevant. It also helps in debugging scenarios in larger codebases, where staying on top of all code related to a concrete issue can be challenging for developers. When developers are faced with debugging a large chunk of code they can pass the entire length of the code into the model.



Additionally, we have further fine-tuned two additional variations of Code Llama: Code Llama - Python and Code Llama - Instruct. Code Llama - Python is a language-specialized variation of Code Llama, further fine-tuned on 100B tokens of Python code. Because Python is the most benchmarked language for code generation – and because Python and PyTorch play an important role in the AI community – we believe a specialized model provides additional utility. Code Llama - Instruct is an instruction fine-tuned and aligned variation of Code Llama. Instruction tuning continues the training process, but with a different objective. The model is fed a “natural language instruction” input and the expected output. This makes it better at understanding what humans expect out of their prompts. We recommend using Code Llama - Instruct variants whenever using Code Llama for code generation since Code Llama - Instruct has been fine-tuned to generate helpful and safe answers in natural language. We do not recommend using Code Llama or Code Llama - Python to perform general natural language tasks since neither of these models are designed to follow natural language instructions. Code Llama is specialized for code-specific tasks and isn’t appropriate as a foundation model for other tasks. When using the Code Llama models, users must abide by our license and acceptable use policy.



Evaluating Code Llama’s performance

To test Code Llama’s performance against existing solutions, we used two popular coding benchmarks: HumanEval and Mostly Basic Python Programming (MBPP). HumanEval tests the model’s ability to complete code based on docstrings and MBPP tests the model’s ability to write code based on a description. Our benchmark testing showed that Code Llama performed better than open-source, code-specific LLMs and outperformed Llama 2. Code Llama 34B, for example, scored 53.7% on HumanEval and 56.2% on MBPP, the highest compared with other state-of-the-art open solutions, and on par with ChatGPT.





As with all cutting edge technology, Code Llama comes with risks. Building AI models responsibly is crucial, and we undertook numerous safety measures before releasing Code Llama. As part of our red teaming efforts, we ran a quantitative evaluation of Code Llama’s risk of generating malicious code. We created prompts that attempted to solicit malicious code with clear intent and scored Code Llama’s responses to those prompts against ChatGPT’s (GPT3.5 Turbo). Our results found that Code Llama answered with safer responses. Details about our red teaming efforts from domain experts in responsible AI, offensive security engineering, malware development, and software engineering are available in our research paper.

Releasing Code Llama

Programmers are already using LLMs to assist in a variety of tasks, ranging from writing new software to debugging existing code. The goal is to make developer workflows more efficient, so they can focus on the most human centric aspects of their job, rather than repetitive tasks. At Meta, we believe that AI models, but LLMs for coding in particular, benefit most from an open approach, both in terms of innovation and safety. Publicly available, code-specific models can facilitate the development of new technologies that improve peoples' lives. By releasing code models like Code Llama, the entire community can evaluate their capabilities, identify issues, and fix vulnerabilities. Code Llama’s training recipes are available on our Github repository. Model weights are also available.

Responsible use

Our research paper discloses details of Code Llama’s development as well as how we conducted our benchmarking tests. It also provides more information into the model’s limitations, known challenges we encountered, mitigations we’ve taken, and future challenges we intend to investigate. We’ve also updated our Responsible Use Guide and it includes guidance on developing downstream models responsibly, including: Defining content policies and mitigations.

Preparing data.

Fine-tuning the model.

Evaluating and improving performance.

Addressing input- and output-level risks.

Building transparency and reporting mechanisms in user interactions. Developers should evaluate their models using code-specific evaluation benchmarks and perform safety studies on code-specific use cases such as generating malware, computer viruses, or malicious code. We also recommend leveraging safety datasets for automatic and human evaluations, and red teaming on adversarial prompts.

The future of generative AI for coding Code Llama is designed to support software engineers in all sectors – including research, industry, open source projects, NGOs, and businesses. But there are still many more use cases to support than what our base and instruct models can serve. We hope that Code Llama will inspire others to leverage Llama 2 to create new innovative tools for research and commercial products.

Try Code Llama today

Read the research paper"
Meta_Blog,https://ai.meta.com/blog/code-llama-large-language-model-coding/,,"Introducing Code Llama, a state-of-the-art large language model for coding","The generative AI space is evolving rapidly, and we believe an open approach to today’s AI is the best one for developing new AI tools that are innovative, safe, and responsible. We are releasing Code Llama under the same community license as Llama 2 .

Today, we are releasing Code Llama, a large language model (LLM) that can use text prompts to generate code. Code Llama is state-of-the-art for publicly available LLMs on code tasks, and has the potential to make workflows faster and more efficient for current developers and lower the barrier to entry for people who are learning to code. Code Llama has the potential to be used as a productivity and educational tool to help programmers write more robust, well-documented software.

How Code Llama works

Code Llama is a code-specialized version of Llama 2 that was created by further training Llama 2 on its code-specific datasets, sampling more data from that same dataset for longer. Essentially, Code Llama features enhanced coding capabilities, built on top of Llama 2. It can generate code, and natural language about code, from both code and natural language prompts (e.g., “Write me a function that outputs the fibonacci sequence.”) It can also be used for code completion and debugging. It supports many of the most popular languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.





We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens. The 7B and 13B base and instruct models have also been trained with fill-in-the-middle (FIM) capability, allowing them to insert code into existing code, meaning they can support tasks like code completion right out of the box. The three models address different serving and latency requirements. The 7B model, for example, can be served on a single GPU. The 34B and 70B models return the best results and allow for better coding assistance, but the smaller 7B and 13B models are faster and more suitable for tasks that require low latency, like real-time code completion.





The Code Llama models provide stable generations with up to 100,000 tokens of context. All models are trained on sequences of 16,000 tokens and show improvements on inputs with up to 100,000 tokens. Aside from being a prerequisite for generating longer programs, having longer input sequences unlocks exciting new use cases for a code LLM. For example, users can provide the model with more context from their codebase to make the generations more relevant. It also helps in debugging scenarios in larger codebases, where staying on top of all code related to a concrete issue can be challenging for developers. When developers are faced with debugging a large chunk of code they can pass the entire length of the code into the model.



Additionally, we have further fine-tuned two additional variations of Code Llama: Code Llama - Python and Code Llama - Instruct. Code Llama - Python is a language-specialized variation of Code Llama, further fine-tuned on 100B tokens of Python code. Because Python is the most benchmarked language for code generation – and because Python and PyTorch play an important role in the AI community – we believe a specialized model provides additional utility. Code Llama - Instruct is an instruction fine-tuned and aligned variation of Code Llama. Instruction tuning continues the training process, but with a different objective. The model is fed a “natural language instruction” input and the expected output. This makes it better at understanding what humans expect out of their prompts. We recommend using Code Llama - Instruct variants whenever using Code Llama for code generation since Code Llama - Instruct has been fine-tuned to generate helpful and safe answers in natural language. We do not recommend using Code Llama or Code Llama - Python to perform general natural language tasks since neither of these models are designed to follow natural language instructions. Code Llama is specialized for code-specific tasks and isn’t appropriate as a foundation model for other tasks. When using the Code Llama models, users must abide by our license and acceptable use policy.



Evaluating Code Llama’s performance

To test Code Llama’s performance against existing solutions, we used two popular coding benchmarks: HumanEval and Mostly Basic Python Programming (MBPP). HumanEval tests the model’s ability to complete code based on docstrings and MBPP tests the model’s ability to write code based on a description. Our benchmark testing showed that Code Llama performed better than open-source, code-specific LLMs and outperformed Llama 2. Code Llama 34B, for example, scored 53.7% on HumanEval and 56.2% on MBPP, the highest compared with other state-of-the-art open solutions, and on par with ChatGPT.





As with all cutting edge technology, Code Llama comes with risks. Building AI models responsibly is crucial, and we undertook numerous safety measures before releasing Code Llama. As part of our red teaming efforts, we ran a quantitative evaluation of Code Llama’s risk of generating malicious code. We created prompts that attempted to solicit malicious code with clear intent and scored Code Llama’s responses to those prompts against ChatGPT’s (GPT3.5 Turbo). Our results found that Code Llama answered with safer responses. Details about our red teaming efforts from domain experts in responsible AI, offensive security engineering, malware development, and software engineering are available in our research paper.

Releasing Code Llama

Programmers are already using LLMs to assist in a variety of tasks, ranging from writing new software to debugging existing code. The goal is to make developer workflows more efficient, so they can focus on the most human centric aspects of their job, rather than repetitive tasks. At Meta, we believe that AI models, but LLMs for coding in particular, benefit most from an open approach, both in terms of innovation and safety. Publicly available, code-specific models can facilitate the development of new technologies that improve peoples' lives. By releasing code models like Code Llama, the entire community can evaluate their capabilities, identify issues, and fix vulnerabilities. Code Llama’s training recipes are available on our Github repository. Model weights are also available.

Responsible use

Our research paper discloses details of Code Llama’s development as well as how we conducted our benchmarking tests. It also provides more information into the model’s limitations, known challenges we encountered, mitigations we’ve taken, and future challenges we intend to investigate. We’ve also updated our Responsible Use Guide and it includes guidance on developing downstream models responsibly, including: Defining content policies and mitigations.

Preparing data.

Fine-tuning the model.

Evaluating and improving performance.

Addressing input- and output-level risks.

Building transparency and reporting mechanisms in user interactions. Developers should evaluate their models using code-specific evaluation benchmarks and perform safety studies on code-specific use cases such as generating malware, computer viruses, or malicious code. We also recommend leveraging safety datasets for automatic and human evaluations, and red teaming on adversarial prompts.

The future of generative AI for coding Code Llama is designed to support software engineers in all sectors – including research, industry, open source projects, NGOs, and businesses. But there are still many more use cases to support than what our base and instruct models can serve. We hope that Code Llama will inspire others to leverage Llama 2 to create new innovative tools for research and commercial products.

Try Code Llama today

Read the research paper"
Meta_Blog,https://ai.meta.com/blog/seamless-m4t/,,Introducing a foundational multimodal model for speech translation,"The world we live in has never been more interconnected—the global proliferation of the internet, mobile devices, social media, and communication platforms gives people access to more multilingual content than ever before. In such a context, having an on-demand ability to communicate and understand information in any language becomes increasingly important. While such a capability has long been dreamed of in science fiction, AI is on the verge of bringing this vision into technical reality.



In keeping with our approach to open science, we’re publicly releasing SeamlessM4T under CC BY-NC 4.0 to allow researchers and developers to build on this work. We’re also releasing the metadata of SeamlessAlign, the biggest open multimodal translation dataset to date, totaling 470,000 hours of mined speech and text alignments. We make it easy for the community to perform mining on their own monolingual datasets with SONAR, a complete suite of speech and text sentence encoders, and stopes, our library for multimodal data processing and parallel data mining. All research advancements are supported by fairseq2, our next-generation sequence modeling library. Building a universal language translator, like the fictional Babel Fish in The Hitchhiker’s Guide to the Galaxy, is challenging because existing speech-to-speech and speech-to-text systems only cover a small fraction of the world’s languages. SeamlessM4T represents a significant breakthrough in the field of speech-to-speech and speech-to-text by addressing the challenges of limited language coverage and a reliance on separate systems, which divide the task of speech-to-speech translation into multiple stages across subsystems. These systems can leverage large amounts of data and generally perform well for only one modality. Our challenge was to create a unified multilingual model that could do it all. We believe the work we’re announcing today is a significant step forward in this journey. Our single model provides on-demand translations that enable people who speak different languages to communicate more effectively. We significantly improve performance for the low and mid-resource languages we support. These are languages that have smaller digital linguistic footprints. We also maintain strong performance on high-resource languages, such as English, Spanish, and German. SeamlessM4T implicitly recognizes the source languages, without the need for a separate language identification model.



This work builds on advancements Meta and others have made over the years in the quest to create a universal translator. Last year, we released No Language Left Behind (NLLB), a text-to-text machine translation model that supports 200 languages and has since been integrated into Wikipedia as one of its translation providers. A few months later, we shared a demo of our Universal Speech Translator, which was the first direct speech-to-speech translation system for Hokkien, a language without a widely used writing system. Through this, we developed SpeechMatrix, the first large-scale multilingual speech-to-speech translation dataset, derived from SpeechLASER, a breakthrough in supervised representation learning. Earlier this year, we also shared Massively Multilingual Speech, which provides automatic speech recognition, language identification, and speech synthesis technology across more than 1,100 languages. SeamlessM4T draws on findings from all of these projects to enable a multilingual and multimodal translation experience stemming from a single model, built across a wide range of spoken data sources and with state-of-the-art results.

Our approach

Building a unified model requires a sequence modeling toolkit that is lightweight and easily composable with other modern PyTorch ecosystem libraries. We redesigned fairseq, our original sequence modeling toolkit. With more efficient modeling and data loader APIs, fairseq2 helps power the modeling behind SeamlessM4T. For the model, we use the multitask UnitY model architecture, which is capable of directly generating translated text and speech. This new architecture also supports automatic speech recognition, text-to-text, text-to-speech, speech-to text, and speech-to-speech translations that are already a part of the vanilla UnitY model. The multitask UnitY model consists of three main sequential components. Text and speech encoders have the task of recognizing speech input in nearly 100 languages. The text decoder then transfers that meaning into nearly 100 languages for text followed by a text-to-unit model to decode into discrete acoustic units for 36 speech languages. The self-supervised encoder, speech-to-text, text-to-text translation components, and text-to-unit model are pre-trained to improve the quality of the model and for training stability The decoded discrete units are then converted into speech using a multilingual HiFi-GAN unit vocoder.

How the encoder processes speech

Our self-supervised speech encoder, w2v-BERT 2.0 which is an improved version of w2v-BERT that improves its training stability and representation quality, learns to find structure and meaning in speech by analyzing millions of hours of multilingual speech. The encoder takes the audio signal, breaks it down into smaller parts, and builds an internal representation of what is being said. Because spoken words are made up of many of those sounds and characters, we use a length adaptor to roughly map them to actual words.

How the encoder processes text

Similarly, we have a text encoder that is based on the NLLB model. It has been trained to understand text in nearly 100 languages and produce representations that are useful for translation.

Producing text

Our text decoder is trained to take encoded speech representations or text representations. This can be applied to tasks in the same language, such as automatic speech recognition, and multilingual translation tasks. For example, someone can say the word “bonjour” in French, and expect the translated text in Swahili to be “habari.” With multitask training, we leverage the strengths of a strong text-to-text translation model (NLLB) to guide our speech-to-text translation model via token-level knowledge distillation.

Producing speech

We use acoustic units to represent speech on the target side. The text-to-unit (T2U) component in the UnitY model generates these discrete speech units based on the text output and is pre-trained on ASR data prior to UnitY fine-tuning. A multilingual HiFi-GAN unit vocoder is then used to convert these discrete units into audio waveforms.

Data scaling

Data-driven models like SeamlessM4T usually benefit from large amounts of high-quality end-to-end data, namely speech-to-text and speech-to-speech data. Relying only on human transcribed and translated speech does not scale to tackle the challenging task of speech translation for 100 languages. We build upon our pioneering work on text-to-text mining using a similarity measure in a joint embedding space, and initial work in speech mining to create additional resources to train the SeamlessM4T model. First, we build a new massively multilingual and -modal text embedding space for 200 languages, named SONAR (Sentence-level mOdality- and laNguage-Agnostic Representations), which substantially outperforms existing approaches like LASER3 or LaBSE in multilingual similarity search. We then apply a teacher-student approach to extend this embedding space to the speech modality and currently cover 36 languages. Mining is performed in data from publicly available repositories of web data (tens of billions of sentences) and speech (4 million hours). In total, we were able to automatically align more than 443,000 hours of speech with texts and create about 29,000 hours of speech-to-speech alignments. This corpus, dubbed SeamlessAlign, is the largest open speech/speech and speech/text parallel corpus in terms of total volume and language coverage to date.

Results

For these tasks and languages, SeamlessM4T achieves state-of-the-art results for nearly 100 languages and multitask support across automatic speech recognition, speech-to-text, speech-to-speech, text-to-speech, and text-to-text translation—all in a single model. We also significantly improve performance for low and mid-resource languages supported and maintain strong performance on high-resource languages.

To more accurately evaluate the system without depending on text-based metrics, we extended our text-less metric into BLASER 2.0, which now enables evaluation across speech and text units with similar accuracy compared to its predecessor. When tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks (average improvements of 37% and 48%, respectively) compared to the current state-of-the-art model. SeamlessM4T also outperforms previous state-of-the-art competitors.

How we built SeamlessM4T responsibly

It is important that translation systems are accurate. As with all AI systems, there are inherent risks that the model could mistranscribe what a person wants to say or generate outputs that are toxic or inaccurate. At Meta, our AI research and development follows a responsible framework that is guided by our five pillars of Responsible AI . In line with our commitment to responsible AI, we conducted research on toxicity and bias to help us understand which areas of the model might be sensitive. For toxicity, we extended our highly multilingual toxicity classifier to speech to help identify toxic words from speech inputs and outputs. We filtered unbalanced toxicity in training data. If input or output contained different amounts of toxicity, we removed that training pair. The demo we’re releasing today showcases the capabilities of SeamlessM4T and is an important part of the research. We detect toxicity in both the input and the output for the demo. If toxicity is only detected in the output, it means that toxicity is added. In this case, we include a warning and do not show the output. When comparing our models to the state of the art, we significantly reduce added toxicity on both speech-to-speech and speech-to-text translation. Gender bias, where the results unfairly favor a gender and sometimes default to gender stereotypes, is another area we are beginning to evaluate in languages at scale. We are now able to quantify gender bias in dozens of speech translation directions by extending our previously designed Multilingual HolisticBias dataset to speech. Our work around safety and security is an ongoing effort. We’ll continue to research and take action in this area to continuously improve SeamlessM4T and reduce any instances of toxicity we see in the model.

Providing access to our technology

With state-of-the-art results, we believe SeamlessM4T is an important breakthrough in the AI community’s quest to create universal multitask systems. In keeping with our approach to open science, we’re excited to share our model publicly to allow researchers and developers to build on this technology. This is only the latest step in our ongoing effort to build AI-powered technology that helps connect people across languages. In the future, we want to explore how this foundational model can enable new communication capabilities—ultimately bringing us closer to a world where everyone can be understood.



Read the paper"
Meta_Blog,https://ai.meta.com/blog/seamless-m4t/,,Introducing a foundational multimodal model for speech translation,"The world we live in has never been more interconnected—the global proliferation of the internet, mobile devices, social media, and communication platforms gives people access to more multilingual content than ever before. In such a context, having an on-demand ability to communicate and understand information in any language becomes increasingly important. While such a capability has long been dreamed of in science fiction, AI is on the verge of bringing this vision into technical reality.



In keeping with our approach to open science, we’re publicly releasing SeamlessM4T under CC BY-NC 4.0 to allow researchers and developers to build on this work. We’re also releasing the metadata of SeamlessAlign, the biggest open multimodal translation dataset to date, totaling 470,000 hours of mined speech and text alignments. We make it easy for the community to perform mining on their own monolingual datasets with SONAR, a complete suite of speech and text sentence encoders, and stopes, our library for multimodal data processing and parallel data mining. All research advancements are supported by fairseq2, our next-generation sequence modeling library. Building a universal language translator, like the fictional Babel Fish in The Hitchhiker’s Guide to the Galaxy, is challenging because existing speech-to-speech and speech-to-text systems only cover a small fraction of the world’s languages. SeamlessM4T represents a significant breakthrough in the field of speech-to-speech and speech-to-text by addressing the challenges of limited language coverage and a reliance on separate systems, which divide the task of speech-to-speech translation into multiple stages across subsystems. These systems can leverage large amounts of data and generally perform well for only one modality. Our challenge was to create a unified multilingual model that could do it all. We believe the work we’re announcing today is a significant step forward in this journey. Our single model provides on-demand translations that enable people who speak different languages to communicate more effectively. We significantly improve performance for the low and mid-resource languages we support. These are languages that have smaller digital linguistic footprints. We also maintain strong performance on high-resource languages, such as English, Spanish, and German. SeamlessM4T implicitly recognizes the source languages, without the need for a separate language identification model.



This work builds on advancements Meta and others have made over the years in the quest to create a universal translator. Last year, we released No Language Left Behind (NLLB), a text-to-text machine translation model that supports 200 languages and has since been integrated into Wikipedia as one of its translation providers. A few months later, we shared a demo of our Universal Speech Translator, which was the first direct speech-to-speech translation system for Hokkien, a language without a widely used writing system. Through this, we developed SpeechMatrix, the first large-scale multilingual speech-to-speech translation dataset, derived from SpeechLASER, a breakthrough in supervised representation learning. Earlier this year, we also shared Massively Multilingual Speech, which provides automatic speech recognition, language identification, and speech synthesis technology across more than 1,100 languages. SeamlessM4T draws on findings from all of these projects to enable a multilingual and multimodal translation experience stemming from a single model, built across a wide range of spoken data sources and with state-of-the-art results.

Our approach

Building a unified model requires a sequence modeling toolkit that is lightweight and easily composable with other modern PyTorch ecosystem libraries. We redesigned fairseq, our original sequence modeling toolkit. With more efficient modeling and data loader APIs, fairseq2 helps power the modeling behind SeamlessM4T. For the model, we use the multitask UnitY model architecture, which is capable of directly generating translated text and speech. This new architecture also supports automatic speech recognition, text-to-text, text-to-speech, speech-to text, and speech-to-speech translations that are already a part of the vanilla UnitY model. The multitask UnitY model consists of three main sequential components. Text and speech encoders have the task of recognizing speech input in nearly 100 languages. The text decoder then transfers that meaning into nearly 100 languages for text followed by a text-to-unit model to decode into discrete acoustic units for 36 speech languages. The self-supervised encoder, speech-to-text, text-to-text translation components, and text-to-unit model are pre-trained to improve the quality of the model and for training stability The decoded discrete units are then converted into speech using a multilingual HiFi-GAN unit vocoder.

How the encoder processes speech

Our self-supervised speech encoder, w2v-BERT 2.0 which is an improved version of w2v-BERT that improves its training stability and representation quality, learns to find structure and meaning in speech by analyzing millions of hours of multilingual speech. The encoder takes the audio signal, breaks it down into smaller parts, and builds an internal representation of what is being said. Because spoken words are made up of many of those sounds and characters, we use a length adaptor to roughly map them to actual words.

How the encoder processes text

Similarly, we have a text encoder that is based on the NLLB model. It has been trained to understand text in nearly 100 languages and produce representations that are useful for translation.

Producing text

Our text decoder is trained to take encoded speech representations or text representations. This can be applied to tasks in the same language, such as automatic speech recognition, and multilingual translation tasks. For example, someone can say the word “bonjour” in French, and expect the translated text in Swahili to be “habari.” With multitask training, we leverage the strengths of a strong text-to-text translation model (NLLB) to guide our speech-to-text translation model via token-level knowledge distillation.

Producing speech

We use acoustic units to represent speech on the target side. The text-to-unit (T2U) component in the UnitY model generates these discrete speech units based on the text output and is pre-trained on ASR data prior to UnitY fine-tuning. A multilingual HiFi-GAN unit vocoder is then used to convert these discrete units into audio waveforms.

Data scaling

Data-driven models like SeamlessM4T usually benefit from large amounts of high-quality end-to-end data, namely speech-to-text and speech-to-speech data. Relying only on human transcribed and translated speech does not scale to tackle the challenging task of speech translation for 100 languages. We build upon our pioneering work on text-to-text mining using a similarity measure in a joint embedding space, and initial work in speech mining to create additional resources to train the SeamlessM4T model. First, we build a new massively multilingual and -modal text embedding space for 200 languages, named SONAR (Sentence-level mOdality- and laNguage-Agnostic Representations), which substantially outperforms existing approaches like LASER3 or LaBSE in multilingual similarity search. We then apply a teacher-student approach to extend this embedding space to the speech modality and currently cover 36 languages. Mining is performed in data from publicly available repositories of web data (tens of billions of sentences) and speech (4 million hours). In total, we were able to automatically align more than 443,000 hours of speech with texts and create about 29,000 hours of speech-to-speech alignments. This corpus, dubbed SeamlessAlign, is the largest open speech/speech and speech/text parallel corpus in terms of total volume and language coverage to date.

Results

For these tasks and languages, SeamlessM4T achieves state-of-the-art results for nearly 100 languages and multitask support across automatic speech recognition, speech-to-text, speech-to-speech, text-to-speech, and text-to-text translation—all in a single model. We also significantly improve performance for low and mid-resource languages supported and maintain strong performance on high-resource languages.

To more accurately evaluate the system without depending on text-based metrics, we extended our text-less metric into BLASER 2.0, which now enables evaluation across speech and text units with similar accuracy compared to its predecessor. When tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks (average improvements of 37% and 48%, respectively) compared to the current state-of-the-art model. SeamlessM4T also outperforms previous state-of-the-art competitors.

How we built SeamlessM4T responsibly

It is important that translation systems are accurate. As with all AI systems, there are inherent risks that the model could mistranscribe what a person wants to say or generate outputs that are toxic or inaccurate. At Meta, our AI research and development follows a responsible framework that is guided by our five pillars of Responsible AI . In line with our commitment to responsible AI, we conducted research on toxicity and bias to help us understand which areas of the model might be sensitive. For toxicity, we extended our highly multilingual toxicity classifier to speech to help identify toxic words from speech inputs and outputs. We filtered unbalanced toxicity in training data. If input or output contained different amounts of toxicity, we removed that training pair. The demo we’re releasing today showcases the capabilities of SeamlessM4T and is an important part of the research. We detect toxicity in both the input and the output for the demo. If toxicity is only detected in the output, it means that toxicity is added. In this case, we include a warning and do not show the output. When comparing our models to the state of the art, we significantly reduce added toxicity on both speech-to-speech and speech-to-text translation. Gender bias, where the results unfairly favor a gender and sometimes default to gender stereotypes, is another area we are beginning to evaluate in languages at scale. We are now able to quantify gender bias in dozens of speech translation directions by extending our previously designed Multilingual HolisticBias dataset to speech. Our work around safety and security is an ongoing effort. We’ll continue to research and take action in this area to continuously improve SeamlessM4T and reduce any instances of toxicity we see in the model.

Providing access to our technology

With state-of-the-art results, we believe SeamlessM4T is an important breakthrough in the AI community’s quest to create universal multitask systems. In keeping with our approach to open science, we’re excited to share our model publicly to allow researchers and developers to build on this technology. This is only the latest step in our ongoing effort to build AI-powered technology that helps connect people across languages. In the future, we want to explore how this foundational model can enable new communication capabilities—ultimately bringing us closer to a world where everyone can be understood.



Read the paper"
Meta_Blog,https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/,,AudioCraft: A simple one-stop shop for audio modeling,"In recent years, generative AI models including language models have made huge strides and shown exceptional abilities: from the generation of a wide-variety of images and video from text descriptions exhibiting spatial understanding to text and speech models that perform machine translation or even text or speech dialogue agents . Yet while we’ve seen a lot of excitement around generative AI for images, video, and text, audio has always seemed to lag a bit behind. There’s some work out there, but it’s highly complicated and not very open, so people aren’t able to readily play with it.

Generating high-fidelity audio of any kind requires modeling complex signals and patterns at varying scales. Music is arguably the most challenging type of audio to generate because it’s composed of local and long-range patterns, from a suite of notes to a global musical structure with multiple instruments. Generating coherent music with AI has often been addressed through the use of symbolic representations like MIDI or piano rolls. However, these approaches are unable to fully grasp the expressive nuances and stylistic elements found in music. More recent advances leverage self-supervised audio representation learning and a number of hierarchical or cascaded models to generate music, feeding the raw audio into a complex system in order to capture long-range structures in the signal while generating quality audio. But we knew that more could be done in this field.

The AudioCraft family of models is capable of producing high-quality audio with long-term consistency, and it can be easily interacted with through a natural interface. With AudioCraft, we simplify the overall design of generative models for audio compared to prior work in the field — giving people the full recipe to play with the existing models that Meta has been developing over the past several years while also empowering them to push the limits and develop their own models.

AudioCraft works for music and sound generation and compression — all in the same place. Because it’s easy to build on and reuse, people who want to build better sound generators, compression algorithms, or music generators can do it all in the same code base and build on top of what others have done.

And while a lot of work went into making the models simple, the team was equally committed to ensuring that AudioCraft could support the state of the art. People can easily extend our models and adapt them to their use cases for research. There are nearly limitless possibilities once you give people access to the models to tune them to their needs. And that’s what we want to do with this family of models: give people the power to extend their work."
Meta_Blog,https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/,,AudioCraft: A simple one-stop shop for audio modeling,"In recent years, generative AI models including language models have made huge strides and shown exceptional abilities: from the generation of a wide-variety of images and video from text descriptions exhibiting spatial understanding to text and speech models that perform machine translation or even text or speech dialogue agents . Yet while we’ve seen a lot of excitement around generative AI for images, video, and text, audio has always seemed to lag a bit behind. There’s some work out there, but it’s highly complicated and not very open, so people aren’t able to readily play with it.

Generating high-fidelity audio of any kind requires modeling complex signals and patterns at varying scales. Music is arguably the most challenging type of audio to generate because it’s composed of local and long-range patterns, from a suite of notes to a global musical structure with multiple instruments. Generating coherent music with AI has often been addressed through the use of symbolic representations like MIDI or piano rolls. However, these approaches are unable to fully grasp the expressive nuances and stylistic elements found in music. More recent advances leverage self-supervised audio representation learning and a number of hierarchical or cascaded models to generate music, feeding the raw audio into a complex system in order to capture long-range structures in the signal while generating quality audio. But we knew that more could be done in this field.

The AudioCraft family of models is capable of producing high-quality audio with long-term consistency, and it can be easily interacted with through a natural interface. With AudioCraft, we simplify the overall design of generative models for audio compared to prior work in the field — giving people the full recipe to play with the existing models that Meta has been developing over the past several years while also empowering them to push the limits and develop their own models.

AudioCraft works for music and sound generation and compression — all in the same place. Because it’s easy to build on and reuse, people who want to build better sound generators, compression algorithms, or music generators can do it all in the same code base and build on top of what others have done.

And while a lot of work went into making the models simple, the team was equally committed to ensuring that AudioCraft could support the state of the art. People can easily extend our models and adapt them to their use cases for research. There are nearly limitless possibilities once you give people access to the models to tune them to their needs. And that’s what we want to do with this family of models: give people the power to extend their work."
Meta_Blog,https://ai.meta.com/blog/llama-2-update/,,Community-driven AI innovation comes alive with Llama 2,"Last week, we took an important step toward advancing access and opportunity in the creation of AI-powered products and experiences with the launch of Llama 2 . The open release of these new models to the research and business community is laying the foundation for the next wave of community-driven innovation in generative AI. We’ve seen an incredible response thus far with more than 150,000 download requests in the week since its release, and I’m excited to see what the future holds.

RECOMMENDED READS Meta and Microsoft Introduce the Next Generation of Llama At Meta, we believe in open sourcing technology. It’s part of our DNA and who we are. Our experiences with PyTorch and OCP have shown us the incredible advancements that can be achieved when we innovate together. We’ve taken these philosophies and embraced them with Llama 2, with an unbelievable group of partners and supporters to take these first steps together."
Meta_Blog,https://ai.meta.com/blog/llama-2-update/,,Community-driven AI innovation comes alive with Llama 2,"Last week, we took an important step toward advancing access and opportunity in the creation of AI-powered products and experiences with the launch of Llama 2 . The open release of these new models to the research and business community is laying the foundation for the next wave of community-driven innovation in generative AI. We’ve seen an incredible response thus far with more than 150,000 download requests in the week since its release, and I’m excited to see what the future holds.

RECOMMENDED READS Meta and Microsoft Introduce the Next Generation of Llama At Meta, we believe in open sourcing technology. It’s part of our DNA and who we are. Our experiences with PyTorch and OCP have shown us the incredible advancements that can be achieved when we innovate together. We’ve taken these philosophies and embraced them with Llama 2, with an unbelievable group of partners and supporters to take these first steps together."
Meta_Blog,https://ai.meta.com/blog/llama-2/,,Meta and Microsoft Introduce the Next Generation of Llama,"And we believe it’s safer. Opening access to today’s AI models means a generation of developers and researchers can stress test them, identifying and solving problems fast, as a community. By seeing how these tools are used by others, our own teams can learn from them, improve those tools, and fix vulnerabilities.

We believe an open approach is the right one for the development of today’s AI models, especially those in the generative space where the technology is rapidly advancing. By making AI models available openly, they can benefit everyone. Giving businesses, startups, entrepreneurs, and researchers access to tools developed at a scale that would be challenging to build themselves, backed by computing power they might not otherwise access, will open up a world of opportunities for them to experiment, innovate in exciting ways, and ultimately benefit from economically and socially.

Recent breakthroughs in AI, and generative AI in particular, have captured the public’s imagination and demonstrated what those developing these technologies have long known — they have the potential to help people do incredible things, create a new era of economic and social opportunities, and give individuals, creators, and businesses new ways to express themselves and connect with people.

Meta has put exploratory research, open source, and collaboration with academic and industry partners at the heart of our AI efforts for over a decade. We’ve seen first-hand how innovation in the open can lead to technologies that benefit more people. Dozens of large language models have already been released and are driving progress by developers and researchers. They’re being used by businesses as core ingredients for new generative AI-powered experiences. We’ve been blown away by the huge demand for Llama 1 from researchers — with more than 100,000 requests for access to the large language model — and the amazing things they’ve achieved by building on top of it .

We’re now ready to open source the next version of Llama 2 and are making it available free of charge for research and commercial use. We’re including model weights and starting code for the pretrained model and conversational fine-tuned versions too. As Satya Nadella announced on stage at Microsoft Inspire, we’re taking our partnership to the next level with Microsoft as our preferred partner for Llama 2 and expanding our efforts in generative AI. Starting today, Llama 2 is available in the Azure AI model catalog, enabling developers using Microsoft Azure to build with it and leverage their cloud-native tools for content filtering and safety features. It is also optimized to run locally on Windows, giving developers a seamless workflow as they bring generative AI experiences to customers across different platforms. Llama 2 is available through Amazon Web Services (AWS), Hugging Face, and other providers too.

People and businesses have benefited from the longstanding partnership between Microsoft and Meta. Together we’ve introduced an open ecosystem for interchangeable AI frameworks, and we’ve co-authored research papers to advance the state of the art in AI. We’ve collaborated to scale the adoption of PyTorch — today’s leading AI framework created by Meta and the AI community — on Azure, and we’re among the founding members of the PyTorch Foundation. Microsoft and Meta recently joined a cohort of supporters that endorse the Partnership on AI’s framework for collective action in the creation and sharing of synthetic media. Our partnership extends outside of AI and into the metaverse too to deliver immersive experiences for the future of work and play.



Now, with this expanded partnership, Microsoft and Meta are supporting an open approach to provide increased access to foundational AI technologies to the benefits of businesses globally. It’s not just Meta and Microsoft that believe in democratizing access to today’s AI models. We have a broad range of diverse supporters around the world who believe in this approach too — including companies that have given us early feedback and are excited to build new products with Llama 2, cloud providers that will include Llama 2 in their offerings for customers, research institutions who are collaborating with us on the safe and responsible deployment of large generative models, and people across tech, academia, and policy who see the benefits as we do.

A Focus on Responsibility

Our open source approach promotes transparency and access. We know that while AI has brought huge advances to society, it also comes with risk. We are committed to building responsibly and are providing a number of resources to help those who use Llama 2 do so too.



Red-Teaming Exercises: Our fine-tuned models have been red-teamed — tested for safety — through internal and external efforts. The team worked to generate adversarial prompts to facilitate model fine-tuning. In addition, we commissioned third parties to conduct external adversarial testing across our fine-tuned models to similarly identify gaps in performance. These safety fine-tuning processes are iterative; we will continue to invest in safety through fine-tuning and benchmarking and plan to release updated fine-tuned models based on these efforts. Transparency Schematic: We explain our fine-tuning and evaluation methods for the model and identify its shortcomings. Our transparency schematic, which is located within the research paper, discloses known challenges and issues we’ve experienced and provides insight into mitigations taken and future ones we intend to explore.

We explain our fine-tuning and evaluation methods for the model and identify its shortcomings. Our transparency schematic, which is located within the research paper, discloses known challenges and issues we’ve experienced and provides insight into mitigations taken and future ones we intend to explore. Responsible Use Guide: We created this guide as a resource to support developers with best practices for responsible development and safety evaluations. It outlines best practices reflective of current, state-of-the-art research on responsible generative AI discussed across the industry and the AI research community.

We created this guide as a resource to support developers with best practices for responsible development and safety evaluations. It outlines best practices reflective of current, state-of-the-art research on responsible generative AI discussed across the industry and the AI research community. Acceptable Use Policy: We put a policy in place that prohibits certain use cases to help ensure that these models are being used fairly and responsibly.

Meta has also created new initiatives to harness the insight and creativity of individuals, researchers, and developers around the world to get feedback on how the models are performing and how they might be improved.



Open Innovation AI Research Community: Today, we also launched a new partnership program for academic researchers that aims to deepen our understanding of the responsible development and sharing of large language models. Researchers may apply to join a community of practitioners to share learnings on this important topic, and the community will form a research agenda to pursue going forward. Llama Impact Challenge: We want to activate the community of innovators who aspire to use Llama to solve hard problems. We are launching a challenge to encourage a diverse set of public, non-profit, and for-profit entities to use Llama 2 to address environmental, education and other important challenges. The challenge rules will be available prior to the start of it.

Conclusion

Throughout our company’s history, we’ve experienced the benefits of an open source approach when innovating in other areas of the business. Our engineers developed and shared frameworks that are now industry standards — like React, a leading framework for making web and mobile applications, and PyTorch, which is now the leading framework for AI. These became commonly used infrastructure for the entire technology industry. We believe that openly sharing today’s large language models will support the development of helpful and safer generative AI too."
Meta_Blog,https://ai.meta.com/blog/llama-2/,,Meta and Microsoft Introduce the Next Generation of Llama,"And we believe it’s safer. Opening access to today’s AI models means a generation of developers and researchers can stress test them, identifying and solving problems fast, as a community. By seeing how these tools are used by others, our own teams can learn from them, improve those tools, and fix vulnerabilities.

We believe an open approach is the right one for the development of today’s AI models, especially those in the generative space where the technology is rapidly advancing. By making AI models available openly, they can benefit everyone. Giving businesses, startups, entrepreneurs, and researchers access to tools developed at a scale that would be challenging to build themselves, backed by computing power they might not otherwise access, will open up a world of opportunities for them to experiment, innovate in exciting ways, and ultimately benefit from economically and socially.

Recent breakthroughs in AI, and generative AI in particular, have captured the public’s imagination and demonstrated what those developing these technologies have long known — they have the potential to help people do incredible things, create a new era of economic and social opportunities, and give individuals, creators, and businesses new ways to express themselves and connect with people.

Meta has put exploratory research, open source, and collaboration with academic and industry partners at the heart of our AI efforts for over a decade. We’ve seen first-hand how innovation in the open can lead to technologies that benefit more people. Dozens of large language models have already been released and are driving progress by developers and researchers. They’re being used by businesses as core ingredients for new generative AI-powered experiences. We’ve been blown away by the huge demand for Llama 1 from researchers — with more than 100,000 requests for access to the large language model — and the amazing things they’ve achieved by building on top of it .

We’re now ready to open source the next version of Llama 2 and are making it available free of charge for research and commercial use. We’re including model weights and starting code for the pretrained model and conversational fine-tuned versions too. As Satya Nadella announced on stage at Microsoft Inspire, we’re taking our partnership to the next level with Microsoft as our preferred partner for Llama 2 and expanding our efforts in generative AI. Starting today, Llama 2 is available in the Azure AI model catalog, enabling developers using Microsoft Azure to build with it and leverage their cloud-native tools for content filtering and safety features. It is also optimized to run locally on Windows, giving developers a seamless workflow as they bring generative AI experiences to customers across different platforms. Llama 2 is available through Amazon Web Services (AWS), Hugging Face, and other providers too.

People and businesses have benefited from the longstanding partnership between Microsoft and Meta. Together we’ve introduced an open ecosystem for interchangeable AI frameworks, and we’ve co-authored research papers to advance the state of the art in AI. We’ve collaborated to scale the adoption of PyTorch — today’s leading AI framework created by Meta and the AI community — on Azure, and we’re among the founding members of the PyTorch Foundation. Microsoft and Meta recently joined a cohort of supporters that endorse the Partnership on AI’s framework for collective action in the creation and sharing of synthetic media. Our partnership extends outside of AI and into the metaverse too to deliver immersive experiences for the future of work and play.



Now, with this expanded partnership, Microsoft and Meta are supporting an open approach to provide increased access to foundational AI technologies to the benefits of businesses globally. It’s not just Meta and Microsoft that believe in democratizing access to today’s AI models. We have a broad range of diverse supporters around the world who believe in this approach too — including companies that have given us early feedback and are excited to build new products with Llama 2, cloud providers that will include Llama 2 in their offerings for customers, research institutions who are collaborating with us on the safe and responsible deployment of large generative models, and people across tech, academia, and policy who see the benefits as we do.

A Focus on Responsibility

Our open source approach promotes transparency and access. We know that while AI has brought huge advances to society, it also comes with risk. We are committed to building responsibly and are providing a number of resources to help those who use Llama 2 do so too.



Red-Teaming Exercises: Our fine-tuned models have been red-teamed — tested for safety — through internal and external efforts. The team worked to generate adversarial prompts to facilitate model fine-tuning. In addition, we commissioned third parties to conduct external adversarial testing across our fine-tuned models to similarly identify gaps in performance. These safety fine-tuning processes are iterative; we will continue to invest in safety through fine-tuning and benchmarking and plan to release updated fine-tuned models based on these efforts. Transparency Schematic: We explain our fine-tuning and evaluation methods for the model and identify its shortcomings. Our transparency schematic, which is located within the research paper, discloses known challenges and issues we’ve experienced and provides insight into mitigations taken and future ones we intend to explore.

We explain our fine-tuning and evaluation methods for the model and identify its shortcomings. Our transparency schematic, which is located within the research paper, discloses known challenges and issues we’ve experienced and provides insight into mitigations taken and future ones we intend to explore. Responsible Use Guide: We created this guide as a resource to support developers with best practices for responsible development and safety evaluations. It outlines best practices reflective of current, state-of-the-art research on responsible generative AI discussed across the industry and the AI research community.

We created this guide as a resource to support developers with best practices for responsible development and safety evaluations. It outlines best practices reflective of current, state-of-the-art research on responsible generative AI discussed across the industry and the AI research community. Acceptable Use Policy: We put a policy in place that prohibits certain use cases to help ensure that these models are being used fairly and responsibly.

Meta has also created new initiatives to harness the insight and creativity of individuals, researchers, and developers around the world to get feedback on how the models are performing and how they might be improved.



Open Innovation AI Research Community: Today, we also launched a new partnership program for academic researchers that aims to deepen our understanding of the responsible development and sharing of large language models. Researchers may apply to join a community of practitioners to share learnings on this important topic, and the community will form a research agenda to pursue going forward. Llama Impact Challenge: We want to activate the community of innovators who aspire to use Llama to solve hard problems. We are launching a challenge to encourage a diverse set of public, non-profit, and for-profit entities to use Llama 2 to address environmental, education and other important challenges. The challenge rules will be available prior to the start of it.

Conclusion

Throughout our company’s history, we’ve experienced the benefits of an open source approach when innovating in other areas of the business. Our engineers developed and shared frameworks that are now industry standards — like React, a leading framework for making web and mobile applications, and PyTorch, which is now the leading framework for AI. These became commonly used infrastructure for the entire technology industry. We believe that openly sharing today’s large language models will support the development of helpful and safer generative AI too."
Meta_Blog,https://ai.meta.com/blog/generative-ai-text-images-cm3leon/,,"Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images","CM3leon is the first multimodal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multitask supervised fine-tuning (SFT) stage. This recipe is simple, produces a strong model, and also shows that tokenizer-based transformers can be trained as efficiently as existing generative diffusion-based models. CM3leon achieves state-of-the-art performance for text-to-image generation, despite being trained with five times less compute than previous transformer-based methods. CM3leon has the versatility and effectiveness of autoregressive models, while maintaining low training costs and inference efficiency. It is a causal masked mixed-modal (CM3) model because it can generate sequences of text and images conditioned on arbitrary sequences of other image and text content. This greatly expands the functionality of previous models that were either only text-to-image or only image-to-text.

Although text-only generative models are commonly multitask instruction-tuned on a wide range of different tasks to improve their ability to follow instruction prompts, image generation models are instead typically specialized for particular tasks. We apply large-scale multitask instruction tuning to CM3leon for both image and text generation, and show that it significantly improves performance on tasks such as image caption generation, visual question answering, text-based editing, and conditional image generation. This provides another strong example of how the scaling recipes developed for text-only models generalize directly to our tokenization-based image generation models."
Meta_Blog,https://ai.meta.com/blog/generative-ai-text-images-cm3leon/,,"Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images","CM3leon is the first multimodal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multitask supervised fine-tuning (SFT) stage. This recipe is simple, produces a strong model, and also shows that tokenizer-based transformers can be trained as efficiently as existing generative diffusion-based models. CM3leon achieves state-of-the-art performance for text-to-image generation, despite being trained with five times less compute than previous transformer-based methods. CM3leon has the versatility and effectiveness of autoregressive models, while maintaining low training costs and inference efficiency. It is a causal masked mixed-modal (CM3) model because it can generate sequences of text and images conditioned on arbitrary sequences of other image and text content. This greatly expands the functionality of previous models that were either only text-to-image or only image-to-text.

Although text-only generative models are commonly multitask instruction-tuned on a wide range of different tasks to improve their ability to follow instruction prompts, image generation models are instead typically specialized for particular tasks. We apply large-scale multitask instruction tuning to CM3leon for both image and text generation, and show that it significantly improves performance on tasks such as image caption generation, visual question answering, text-based editing, and conditional image generation. This provides another strong example of how the scaling recipes developed for text-only models generalize directly to our tokenization-based image generation models."
Meta_Blog,https://ai.meta.com/blog/improving-fairness-and-robustness-in-speech-recognition/,,Improving fairness and robustness in speech recognition,"We believe fairness is a process. Meta continues to introduce research like this and produce new innovations to support AI systems that are robust, fair, and inclusive. In the future, both the modeling technique and the dataset could help improve automatic speech recognition systems for use cases including AI assistants, translation tools, and much more, supporting billions of people around the globe across a variety of languages.

During testing, we observed that a model trained in this manner improved speech recognition accuracy for all measured demographic groups, and in particular for different accents, which are identified in sociolinguistics as a way of pronouncing a language that is distinctive to a country, area, social class, or individual. While our proposed algorithm was built using English-language data, we hope these approaches can be extended to work for other languages as well.

This technique enables researchers to improve ASR performance without relying on data related to demographic characteristics, or data that represents someone’s speech, known as speaker embeddings. Instead of dividing a dataset based on speakers’ demographic information — such as their age group or gender — our proposed algorithm clusters speech at the utterance level. A single cluster will contain similar utterances from a diverse group of speakers. We can then train our model using the various clusters and use fairness datasets to measure how the model impacts outcomes across different demographic groups. The clustering is performed using unsupervised learning, leveraging algorithms to analyze and group unlabeled data sets without human intervention.

Speech recognition systems should be robust enough to work well for all groups of people — including those with different speaking styles, accents, and other characteristics. In order to measure how automatic speech recognition (ASR) tools perform for different demographic groups, AI practitioners need diverse speech data. In keeping with our approach to open science, we’re releasing a fairness-oriented evaluation dataset that consists of audio commands taken from a diverse group of consenting paid participants. Along with the dataset, we also developed a privacy-preserving approach that improves the robustness and fairness of an automatic speech recognition system by using an unsupervised clustering method.

First, we segment the training data into 10-second chunks. We then extract utterance level embeddings for each of the segments. Using these embeddings, we then train a principal component analysis model for dimensionality reduction and use that to cluster the data using the well-known K-means algorithm.

We then trained our model on de-identified, publicly available Facebook videos in English. It was evaluated on two datasets. The first was a de-identified dataset collected from a data supplier for ASR that includes 48,000 utterances from 867 speakers. Speakers had the option to self-identify across demographic categories such as age, gender, ethnicity, English accent, and first or home language. The second dataset is Casual Conversations v1 , a dataset of transcribed speech that Meta built and made publicly available in 2021. That data includes self-provided age and gender by participants in the U.S, as well as apparent skin tone categories that were annotated. (After we concluded our research, Meta released Casual Conversations v2 , an expanded dataset that includes additional self-identified and annotated categories of video recordings of people in seven countries.)

We run unsupervised clustering on the utterances. Since there isn’t a need for human annotators to label the clusters, this process means we have no insight into the content of each cluster and why some utterances are grouped together.

Because most parameters are implicitly shared by all the clusters, the speech recognition model learns to generalize across different clusters. We use a “masking” strategy by assigning a probability of sampling the current data into an “unknown” cluster. This ensures that, during training, the model will try to infer the right cluster for the unknown domain, making it more robust — even when the correct cluster id is missing. At inference time, in order to avoid running a speaker identification model on the incoming data, which would be problematic from both a privacy and latency perspective, we give an “unknown” cluster id to the test data. Because our proposed algorithm doesn’t rely on demographic information, it’s much easier to use it in production applications.



How the results of the proposed algorithm stack-up

Experimental results show that our approach improves model performance on all demographic groups in our evaluation datasets, though by far the largest gains are with respect to more inclusivity of accents. The results in our research paper show that we don’t have to sacrifice overall performance to improve the fairness of the model. The cluster-based model improves performance on all age groups, with the largest gain for people ages 66 – 85, which is typically an underrepresented group in ASR training data. Since this group is underrepresented in training data, there hasn’t been a clear way to improve the model for this group via normal data collection and training. With our method, we show improvements for this group without the need for actual training data to be collected. Overall, based on word error rates across different groups, such as age, gender, ethnicity, and native vs. non-native language, our results show a 10% relative improvement on an ASR model trained for dictation, messaging, and voice commands after training using our proposed method. We also found that geographic and self-identified ethnicity labels are poor indicators of accent or speaker variability. Therefore, we suggest that unsupervised clustering is better than using metadata to train for fairness and robustness. We’re exploring how to incorporate this fairness approach into our products.

Our open-sourced dataset

The current public datasets for speech recognition tend not to focus specifically on improving fairness. Our dataset includes approximately 27,055 utterances in recorded speech by 595 people in the U.S. who were paid to record and submit audio of themselves saying commands. They self-identified their demographic information, such as age, gender, ethnicity, geographic location and whether they consider themselves native English speakers. The verbal commands included in this dataset are categorized into seven domains, primarily serving voice assistant use cases — music, capture, utilities, notification control, messaging, calling, and dictation — that can support researchers who are building or have models in those areas. In response to prompts that relate to each of these domains, dataset participants provided their own audio commands. Some examples of prompts were asking how they would search for a song or make plans with friends, including deciding where to meet. Our dataset includes the audio and transcription of participants’ utterances. By releasing this dataset, we hope to further motivate the AI community to continue improving the fairness of speech recognition models, which will help everyone have a better experience using applications with ASR.

Building more inclusive models

We believe that introducing our privacy-preserving approach to improve fairness and robustness of ASR models will inspire AI systems that work well for different speakers under different speaking conditions. That can include voice commands for AI assistants or medical transcription for better health care. Measuring fairness often calls for the analysis of demographic data to see if an AI model is fair across all groups. Using collected data — in which participants self-identify across different dimensions — is one way in which we navigate the tension between measuring fairness and preserving privacy. Our proposed algorithm is part of Meta’s long-term focus on responsible AI and just one part of our holistic approach to address fairness issues . This new fairness dataset includes explicit permission from paid participants, while our new approach is one of our many efforts to build AI systems that work to preserve privacy . In the future, we want to explore the adaptation of this approach for other languages. Meanwhile, our team continues to invest in building more fair and inclusive models, while maintaining high standards of overall accuracy. The hope is that fairness will become an integral part of how speech models are trained and evaluated moving forward. This blog post was made possible by the work of Irina-Elena Veliche, Vineeth Ayyat Kochaniyan, Mike Seltzer, Fuchun Peng, and Pascale Fung.

Read the paper"
Meta_Blog,https://ai.meta.com/blog/improving-fairness-and-robustness-in-speech-recognition/,,Improving fairness and robustness in speech recognition,"We believe fairness is a process. Meta continues to introduce research like this and produce new innovations to support AI systems that are robust, fair, and inclusive. In the future, both the modeling technique and the dataset could help improve automatic speech recognition systems for use cases including AI assistants, translation tools, and much more, supporting billions of people around the globe across a variety of languages.

During testing, we observed that a model trained in this manner improved speech recognition accuracy for all measured demographic groups, and in particular for different accents, which are identified in sociolinguistics as a way of pronouncing a language that is distinctive to a country, area, social class, or individual. While our proposed algorithm was built using English-language data, we hope these approaches can be extended to work for other languages as well.

This technique enables researchers to improve ASR performance without relying on data related to demographic characteristics, or data that represents someone’s speech, known as speaker embeddings. Instead of dividing a dataset based on speakers’ demographic information — such as their age group or gender — our proposed algorithm clusters speech at the utterance level. A single cluster will contain similar utterances from a diverse group of speakers. We can then train our model using the various clusters and use fairness datasets to measure how the model impacts outcomes across different demographic groups. The clustering is performed using unsupervised learning, leveraging algorithms to analyze and group unlabeled data sets without human intervention.

Speech recognition systems should be robust enough to work well for all groups of people — including those with different speaking styles, accents, and other characteristics. In order to measure how automatic speech recognition (ASR) tools perform for different demographic groups, AI practitioners need diverse speech data. In keeping with our approach to open science, we’re releasing a fairness-oriented evaluation dataset that consists of audio commands taken from a diverse group of consenting paid participants. Along with the dataset, we also developed a privacy-preserving approach that improves the robustness and fairness of an automatic speech recognition system by using an unsupervised clustering method.

First, we segment the training data into 10-second chunks. We then extract utterance level embeddings for each of the segments. Using these embeddings, we then train a principal component analysis model for dimensionality reduction and use that to cluster the data using the well-known K-means algorithm.

We then trained our model on de-identified, publicly available Facebook videos in English. It was evaluated on two datasets. The first was a de-identified dataset collected from a data supplier for ASR that includes 48,000 utterances from 867 speakers. Speakers had the option to self-identify across demographic categories such as age, gender, ethnicity, English accent, and first or home language. The second dataset is Casual Conversations v1 , a dataset of transcribed speech that Meta built and made publicly available in 2021. That data includes self-provided age and gender by participants in the U.S, as well as apparent skin tone categories that were annotated. (After we concluded our research, Meta released Casual Conversations v2 , an expanded dataset that includes additional self-identified and annotated categories of video recordings of people in seven countries.)

We run unsupervised clustering on the utterances. Since there isn’t a need for human annotators to label the clusters, this process means we have no insight into the content of each cluster and why some utterances are grouped together.

Because most parameters are implicitly shared by all the clusters, the speech recognition model learns to generalize across different clusters. We use a “masking” strategy by assigning a probability of sampling the current data into an “unknown” cluster. This ensures that, during training, the model will try to infer the right cluster for the unknown domain, making it more robust — even when the correct cluster id is missing. At inference time, in order to avoid running a speaker identification model on the incoming data, which would be problematic from both a privacy and latency perspective, we give an “unknown” cluster id to the test data. Because our proposed algorithm doesn’t rely on demographic information, it’s much easier to use it in production applications.



How the results of the proposed algorithm stack-up

Experimental results show that our approach improves model performance on all demographic groups in our evaluation datasets, though by far the largest gains are with respect to more inclusivity of accents. The results in our research paper show that we don’t have to sacrifice overall performance to improve the fairness of the model. The cluster-based model improves performance on all age groups, with the largest gain for people ages 66 – 85, which is typically an underrepresented group in ASR training data. Since this group is underrepresented in training data, there hasn’t been a clear way to improve the model for this group via normal data collection and training. With our method, we show improvements for this group without the need for actual training data to be collected. Overall, based on word error rates across different groups, such as age, gender, ethnicity, and native vs. non-native language, our results show a 10% relative improvement on an ASR model trained for dictation, messaging, and voice commands after training using our proposed method. We also found that geographic and self-identified ethnicity labels are poor indicators of accent or speaker variability. Therefore, we suggest that unsupervised clustering is better than using metadata to train for fairness and robustness. We’re exploring how to incorporate this fairness approach into our products.

Our open-sourced dataset

The current public datasets for speech recognition tend not to focus specifically on improving fairness. Our dataset includes approximately 27,055 utterances in recorded speech by 595 people in the U.S. who were paid to record and submit audio of themselves saying commands. They self-identified their demographic information, such as age, gender, ethnicity, geographic location and whether they consider themselves native English speakers. The verbal commands included in this dataset are categorized into seven domains, primarily serving voice assistant use cases — music, capture, utilities, notification control, messaging, calling, and dictation — that can support researchers who are building or have models in those areas. In response to prompts that relate to each of these domains, dataset participants provided their own audio commands. Some examples of prompts were asking how they would search for a song or make plans with friends, including deciding where to meet. Our dataset includes the audio and transcription of participants’ utterances. By releasing this dataset, we hope to further motivate the AI community to continue improving the fairness of speech recognition models, which will help everyone have a better experience using applications with ASR.

Building more inclusive models

We believe that introducing our privacy-preserving approach to improve fairness and robustness of ASR models will inspire AI systems that work well for different speakers under different speaking conditions. That can include voice commands for AI assistants or medical transcription for better health care. Measuring fairness often calls for the analysis of demographic data to see if an AI model is fair across all groups. Using collected data — in which participants self-identify across different dimensions — is one way in which we navigate the tension between measuring fairness and preserving privacy. Our proposed algorithm is part of Meta’s long-term focus on responsible AI and just one part of our holistic approach to address fairness issues . This new fairness dataset includes explicit permission from paid participants, while our new approach is one of our many efforts to build AI systems that work to preserve privacy . In the future, we want to explore the adaptation of this approach for other languages. Meanwhile, our team continues to invest in building more fair and inclusive models, while maintaining high standards of overall accuracy. The hope is that fairness will become an integral part of how speech models are trained and evaluated moving forward. This blog post was made possible by the work of Irina-Elena Veliche, Vineeth Ayyat Kochaniyan, Mike Seltzer, Fuchun Peng, and Pascale Fung.

Read the paper"
Meta_Blog,https://ai.meta.com/blog/how-ai-powers-experiences-facebook-instagram-system-cards/,,Introducing 22 system cards that explain how AI powers experiences on Facebook and Instagram,"Our approach to creating system cards

One of our biggest challenges in creating these system cards was figuring out the best way to explain highly technical information in a way that everyone can understand. There isn’t an industry standard way to do this, so we created a uniform approach at Meta to explain these systems. By listening to people who use our services and speaking with a diverse group of experts throughout the design and development process, we gained insights that helped us determine how to present this information in a meaningful way that will reach our intended audiences. We heard that people want more transparency and control over what they see, so we added a customization section to each system card. We also learned that giving people too much technical detail can sometimes obfuscate transparency, which is why we present the top ten most important prediction models, rather than everything in the system.

In order to keep our approach consistent, we chose a glossary of terms to use when we talk about AI. When explaining terms that might be unfamiliar, we include a tooltips prompt to remind people what we mean when we say things like “connected content” and “AI system.” By taking a consistent approach to language, we enable people to compare and contrast multiple system cards.

To maintain consistency across sections in the system cards that explain how our AI systems work and deliver content, we developed internal tools to analyze the impact of the models that make up an AI system. With the help of our engineers, we translated this information–from signals to words–to help explain how each AI system makes predictions. Notably, these models and signals are dynamic as the system learns, and they change frequently over time."
Meta_Blog,https://ai.meta.com/blog/how-ai-powers-experiences-facebook-instagram-system-cards/,,Introducing 22 system cards that explain how AI powers experiences on Facebook and Instagram,"Our approach to creating system cards

One of our biggest challenges in creating these system cards was figuring out the best way to explain highly technical information in a way that everyone can understand. There isn’t an industry standard way to do this, so we created a uniform approach at Meta to explain these systems. By listening to people who use our services and speaking with a diverse group of experts throughout the design and development process, we gained insights that helped us determine how to present this information in a meaningful way that will reach our intended audiences. We heard that people want more transparency and control over what they see, so we added a customization section to each system card. We also learned that giving people too much technical detail can sometimes obfuscate transparency, which is why we present the top ten most important prediction models, rather than everything in the system.

In order to keep our approach consistent, we chose a glossary of terms to use when we talk about AI. When explaining terms that might be unfamiliar, we include a tooltips prompt to remind people what we mean when we say things like “connected content” and “AI system.” By taking a consistent approach to language, we enable people to compare and contrast multiple system cards.

To maintain consistency across sections in the system cards that explain how our AI systems work and deliver content, we developed internal tools to analyze the impact of the models that make up an AI system. With the help of our engineers, we translated this information–from signals to words–to help explain how each AI system makes predictions. Notably, these models and signals are dynamic as the system learns, and they change frequently over time."
Meta_Blog,https://ai.meta.com/blog/ai-unconnected-content-recommendations-facebook-instagram/,,The AI behind unconnected content recommendations on Facebook and Instagram,"We aim to show people the most relevant posts out of tens of billions of possibilities. That is much more challenging than merely surfacing the right recommendations from a person’s network of Facebook friends or from the accounts they follow on Instagram. This requires a deep analysis of each piece of content and each person’s interests. Is someone interested in mountain biking or road biking? Do they care about local races or professional cyclists, or do they just ride for fun with friends? We need to learn nuances like this to find the best content for the people on our platforms.

At a high level, our system takes the following steps to achieve this at scale:

Content understanding: Meta AI has focused on cutting-edge research work, including MViT , XLM-R / XLM-V , and FLAVA / Omnivore , to understand semantic meanings of content holistically across different modalities (such as image, text, audio, or videos). We have applied some of what we learned in our research efforts to develop better production models. These production models provide capabilities such as visual recognition, object detection, text extraction, and audio recognition. They also enable us to do more application-specific tasks, such as topic/genre classification, hashtag prediction, similarity matching, and clustering. These systems are also important to our efforts to remove content that violates our Community Standards and to reduce the spread of content that is problematic and that does not align with our Recommendations Guidelines or Content Distribution Guidelines .

Preference understanding, retrieval, and ranking: We built retrieval systems that take just hundredths of a second to narrow billions of pieces of content down to thousands and then to a few hundred that are relevant to a particular person’s interests. Our ranking systems then select the final items based on pointwise and listwise predictions. They also adjust recommendations to deliver a balanced, engaging mix, so that people can enjoy content on a variety of interests, and a mix of popular and niche posts can appear in people’s feeds.

These systems understand people’s behavior preferences utilizing very large-scale attention models, graph neural networks, few-shot learning, and other techniques. Recent key innovations include a novel hierarchical deep neural retrieval architecture, which allowed us to significantly outperform various state-of-the-art baselines without regressing inference latency; and a new ensemble architecture that leverages heterogeneous interaction modules to better model factors relevant to people’s interests. We believe our various AI algorithm advancements contributed to the 15 percent increase in watch time in the Reels video player on Facebook last fall, while at the same time enabling more people to connect with creators they love and reducing dissatisfaction, as measured by reduced skip rates and hide rates.

Incorporating people’s feedback: We believe it is important that people have the ability to guide the types of content they see. After a recommendation is delivered, our AI systems respond to feedback and refine how they model each person’s preferences — if a person watched an entire video or liked a post, for example. They also weigh additional signals that show someone is not interested, such as watching a recommended video for just a few seconds before stopping it or clicking away. We further incorporate explicit feedback through our Show More / Show Less feature , which lets people indicate whether they want to see more content like what they just saw. Our systems take these signals into account and tailor future recommendations closer to a person’s preferences. They can also use other options, such as explicitly hiding or snoozing posts, if they choose.

It is a complex engineering challenge to intelligently match tens of billions of pieces of content with the interests of nearly 3 billion people. To do it effectively, we pushed the envelope and built a system that delivered:

Scalability: In order to deeply understand and model people’s preferences, our recommendation models can have tens of trillions of parameters — orders of magnitude larger than even the biggest language models used today. We have built training superclusters , advanced model parallelism libraries in PyTorch , a new 4D model parallelism approach for efficient training of models with a massive amount of parameters, and low-precision/quantization techniques to significantly reduce model size and computational needs. These and other algorithmic and system optimizations ensure that these very large models can be trained and deployed efficiently at scale.

Real-time responsiveness: People expect our platforms to adapt to their changing preferences in real time, which is challenging given the massive size of our systems and models. We’ve built asynchronous update protocols into our training and serving stacks on top of PyTorch so these terabyte-scale models can be incrementally updated with minute-level freshness. These changes have helped us significantly increase meaningful engagements on our platforms.

Cold start: New people and new content enter our platforms every single day. This poses what are known as cold start problems, where there isn’t much data to learn from yet. To address this challenge, we developed a few-shot learning system called Meta Interest Learner to accurately match new content to prospective audiences based on their interests, even when there are very few engagements. We also leverage various online learning algorithms to help better distribute new content so that every new piece of high-quality content has a chance to be exposed to a large, relevant audience. This incentivizes creators to share great engaging and useful posts so they can find their audience and potentially get their big break.

Discovery: While we want Facebook and Instagram recommendations to enable people to deepen their existing interests, we also want to help them discover new things to enjoy. To learn how different interests are related, we use cutting-edge embedding learning and graph learning methods, and we leverage uncertainty modeling combined with reinforcement learning. Our recent work includes the development of a personalized methodology for delivery frequency control to optimize for long-term user value, and policy learning using large sequential models to help content discovery while reducing the prevalence of clickbait content on our platforms.

AI systems like these offer experiences tailored to each person at a massive scale. Significant research and engineering efforts went into developing them, yet such systems are ultimately empowered by and dedicated to those who use them. We're also sharing additional details about how our AI systems work with the release of 22 system cards that explain how people can customize and control the experiences they have on Facebook and Instagram. People teach our algorithms — through interactions and feedback — what they want to see. We hold this close to heart as we design our systems, so that they truly serve people’s needs and help further our mission to bring the world closer together."
Meta_Blog,https://ai.meta.com/blog/ai-unconnected-content-recommendations-facebook-instagram/,,The AI behind unconnected content recommendations on Facebook and Instagram,"We aim to show people the most relevant posts out of tens of billions of possibilities. That is much more challenging than merely surfacing the right recommendations from a person’s network of Facebook friends or from the accounts they follow on Instagram. This requires a deep analysis of each piece of content and each person’s interests. Is someone interested in mountain biking or road biking? Do they care about local races or professional cyclists, or do they just ride for fun with friends? We need to learn nuances like this to find the best content for the people on our platforms.

At a high level, our system takes the following steps to achieve this at scale:

Content understanding: Meta AI has focused on cutting-edge research work, including MViT , XLM-R / XLM-V , and FLAVA / Omnivore , to understand semantic meanings of content holistically across different modalities (such as image, text, audio, or videos). We have applied some of what we learned in our research efforts to develop better production models. These production models provide capabilities such as visual recognition, object detection, text extraction, and audio recognition. They also enable us to do more application-specific tasks, such as topic/genre classification, hashtag prediction, similarity matching, and clustering. These systems are also important to our efforts to remove content that violates our Community Standards and to reduce the spread of content that is problematic and that does not align with our Recommendations Guidelines or Content Distribution Guidelines .

Preference understanding, retrieval, and ranking: We built retrieval systems that take just hundredths of a second to narrow billions of pieces of content down to thousands and then to a few hundred that are relevant to a particular person’s interests. Our ranking systems then select the final items based on pointwise and listwise predictions. They also adjust recommendations to deliver a balanced, engaging mix, so that people can enjoy content on a variety of interests, and a mix of popular and niche posts can appear in people’s feeds.

These systems understand people’s behavior preferences utilizing very large-scale attention models, graph neural networks, few-shot learning, and other techniques. Recent key innovations include a novel hierarchical deep neural retrieval architecture, which allowed us to significantly outperform various state-of-the-art baselines without regressing inference latency; and a new ensemble architecture that leverages heterogeneous interaction modules to better model factors relevant to people’s interests. We believe our various AI algorithm advancements contributed to the 15 percent increase in watch time in the Reels video player on Facebook last fall, while at the same time enabling more people to connect with creators they love and reducing dissatisfaction, as measured by reduced skip rates and hide rates.

Incorporating people’s feedback: We believe it is important that people have the ability to guide the types of content they see. After a recommendation is delivered, our AI systems respond to feedback and refine how they model each person’s preferences — if a person watched an entire video or liked a post, for example. They also weigh additional signals that show someone is not interested, such as watching a recommended video for just a few seconds before stopping it or clicking away. We further incorporate explicit feedback through our Show More / Show Less feature , which lets people indicate whether they want to see more content like what they just saw. Our systems take these signals into account and tailor future recommendations closer to a person’s preferences. They can also use other options, such as explicitly hiding or snoozing posts, if they choose.

It is a complex engineering challenge to intelligently match tens of billions of pieces of content with the interests of nearly 3 billion people. To do it effectively, we pushed the envelope and built a system that delivered:

Scalability: In order to deeply understand and model people’s preferences, our recommendation models can have tens of trillions of parameters — orders of magnitude larger than even the biggest language models used today. We have built training superclusters , advanced model parallelism libraries in PyTorch , a new 4D model parallelism approach for efficient training of models with a massive amount of parameters, and low-precision/quantization techniques to significantly reduce model size and computational needs. These and other algorithmic and system optimizations ensure that these very large models can be trained and deployed efficiently at scale.

Real-time responsiveness: People expect our platforms to adapt to their changing preferences in real time, which is challenging given the massive size of our systems and models. We’ve built asynchronous update protocols into our training and serving stacks on top of PyTorch so these terabyte-scale models can be incrementally updated with minute-level freshness. These changes have helped us significantly increase meaningful engagements on our platforms.

Cold start: New people and new content enter our platforms every single day. This poses what are known as cold start problems, where there isn’t much data to learn from yet. To address this challenge, we developed a few-shot learning system called Meta Interest Learner to accurately match new content to prospective audiences based on their interests, even when there are very few engagements. We also leverage various online learning algorithms to help better distribute new content so that every new piece of high-quality content has a chance to be exposed to a large, relevant audience. This incentivizes creators to share great engaging and useful posts so they can find their audience and potentially get their big break.

Discovery: While we want Facebook and Instagram recommendations to enable people to deepen their existing interests, we also want to help them discover new things to enjoy. To learn how different interests are related, we use cutting-edge embedding learning and graph learning methods, and we leverage uncertainty modeling combined with reinforcement learning. Our recent work includes the development of a personalized methodology for delivery frequency control to optimize for long-term user value, and policy learning using large sequential models to help content discovery while reducing the prevalence of clickbait content on our platforms.

AI systems like these offer experiences tailored to each person at a massive scale. Significant research and engineering efforts went into developing them, yet such systems are ultimately empowered by and dedicated to those who use them. We're also sharing additional details about how our AI systems work with the release of 22 system cards that explain how people can customize and control the experiences they have on Facebook and Instagram. People teach our algorithms — through interactions and feedback — what they want to see. We hold this close to heart as we design our systems, so that they truly serve people’s needs and help further our mission to bring the world closer together."
Meta_Blog,https://ai.meta.com/blog/voicebox-generative-ai-model-speech/,,Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance,"Meta AI researchers have achieved a breakthrough in generative AI for speech. We’ve developed Voicebox, the first model that can generalize to speech-generation tasks it was not specifically trained to accomplish with state-of-the-art performance.

RECOMMENDED READS Introducing Make-A-Video: An AI system that generates videos from text

Greater creative control for AI image generation

Introducing LLaMA: A foundational, 65-billion-parameter large language model Like generative systems for images and text, Voicebox creates outputs in a vast variety of styles, and it can create outputs from scratch as well as modify a sample it’s given. But instead of creating a picture or a passage of text, Voicebox produces high-quality audio clips. The model can synthesize speech across six languages, as well as perform noise removal, content editing, style conversion, and diverse sample generation. Prior to Voicebox, generative AI for speech required specific training for each task using carefully prepared training data. Voicebox uses a new approach to learn just from raw audio and an accompanying transcription. Unlike autoregressive models for audio generation, Voicebox can modify any part of a given sample, not just the end of an audio clip it is given. Voicebox is based on a method called Flow Matching, which has been shown to improve upon diffusion models. Voicebox outperforms the current state of the art English model VALL-E on zero-shot text-to-speech in terms of both intelligibility (5.9 percent vs. 1.9 percent word error rates) and audio similarity (0.580 vs. 0.681), while being as much as 20 times faster. For cross-lingual style transfer, Voicebox outperforms YourTTS to reduce average word error rate from 10.9 percent to 5.2 percent, and improves audio similarity from 0.335 to 0.481.

Voicebox achieves new state-of-the-art results, outperforming Vall-E and YourTTS on word error rate.

Voicebox also achieves new state-of-the-art results on audio style similarity metrics on English and multilingual benchmarks, respectively.

There are many exciting use cases for generative speech models, but because of the potential risks of misuse, we are not making the Voicebox model or code publicly available at this time. While we believe it is important to be open with the AI community and to share our research to advance the state of the art in AI, it’s also necessary to strike the right balance between openness with responsibility. With these considerations, today we are sharing audio samples and a research paper detailing the approach and results we have achieved. In the paper , we also detail how we built a highly effective classifier that can distinguish between authentic speech and audio generated with Voicebox.

A new approach to speech generation

One of the main limitations of existing speech synthesizers is that they can only be trained on data that has been prepared expressly for that task. These inputs – known as monotonic, clean data – are difficult to produce, so they exist only in limited quantities, and they result in outputs that sound monotone. We built Voicebox upon the Flow Matching model , which is Meta’s latest advancement on non-autoregressive generative models that can learn highly non-deterministic mapping between text and speech. Non-deterministic mapping is useful because it enables Voicebox to learn from varied speech data without those variations having to be carefully labeled. This means Voicebox can train on more diverse data and a much larger scale of data.

We trained Voicebox with more than 50,000 hours of recorded speech and transcripts from public domain audiobooks in English, French, Spanish, German, Polish, and Portuguese. Voicebox is trained to predict a speech segment when given the surrounding speech and the transcript of the segment. Having learned to infill speech from context, the model can then apply this across speech generation tasks, including generating portions in the middle of an audio recording without having to re-create the entire input. This versatility enables Voicebox to perform well across a variety of tasks, including:

In-context text-to-speech synthesis: Using an input audio sample just two seconds in length, Voicebox can match the sample’s audio style and use it for text-to-speech generation. Future projects could build on this capability by bringing speech to people who are unable to speak, or by allowing people to customize the voices used by nonplayer characters and virtual assistants.

Cross-lingual style transfer: Given a sample of speech and a passage of text in English, French, German, Spanish, Polish, or Portuguese, Voicebox can produce a reading of the text in that language. This capability is exciting because in the future it could be used to help people communicate in a natural, authentic way — even if they don’t speak the same languages.

Speech denoising and editing: Voicebox’s in-context learning makes it good at generating speech to seamlessly edit segments within audio recordings. It can resynthesize the portion of speech corrupted by short-duration noise, or replace misspoken words without having to rerecord the entire speech. A person could identify which raw segment of the speech is corrupted by noise (like a dog barking), crop it, and instruct the model to regenerate that segment. This capability could one day be used to make cleaning up and editing audio as easy as popular image-editing tools have made adjusting photos.

Diverse speech sampling: Having learned from diverse in-the-wild data, Voicebox can generate speech that is more representative of how people talk in the real world and across the six languages listed above. In the future, this capability could be used to generate synthetic data to help better train a speech assistant model. Our results show that speech recognition models trained on Voicebox-generated synthetic speech perform almost as well as models trained on real speech, with 1 percent error rate degradation as opposed to 45 to 70 percent degradation with synthetic speech from previous text-to-speech models.

Sharing generative AI research responsibly"
Meta_Blog,https://ai.meta.com/blog/voicebox-generative-ai-model-speech/,,Introducing Voicebox: The first generative AI model for speech to generalize across tasks with state-of-the-art performance,"Meta AI researchers have achieved a breakthrough in generative AI for speech. We’ve developed Voicebox, the first model that can generalize to speech-generation tasks it was not specifically trained to accomplish with state-of-the-art performance.

RECOMMENDED READS Introducing Make-A-Video: An AI system that generates videos from text

Greater creative control for AI image generation

Introducing LLaMA: A foundational, 65-billion-parameter large language model Like generative systems for images and text, Voicebox creates outputs in a vast variety of styles, and it can create outputs from scratch as well as modify a sample it’s given. But instead of creating a picture or a passage of text, Voicebox produces high-quality audio clips. The model can synthesize speech across six languages, as well as perform noise removal, content editing, style conversion, and diverse sample generation. Prior to Voicebox, generative AI for speech required specific training for each task using carefully prepared training data. Voicebox uses a new approach to learn just from raw audio and an accompanying transcription. Unlike autoregressive models for audio generation, Voicebox can modify any part of a given sample, not just the end of an audio clip it is given. Voicebox is based on a method called Flow Matching, which has been shown to improve upon diffusion models. Voicebox outperforms the current state of the art English model VALL-E on zero-shot text-to-speech in terms of both intelligibility (5.9 percent vs. 1.9 percent word error rates) and audio similarity (0.580 vs. 0.681), while being as much as 20 times faster. For cross-lingual style transfer, Voicebox outperforms YourTTS to reduce average word error rate from 10.9 percent to 5.2 percent, and improves audio similarity from 0.335 to 0.481.

Voicebox achieves new state-of-the-art results, outperforming Vall-E and YourTTS on word error rate.

Voicebox also achieves new state-of-the-art results on audio style similarity metrics on English and multilingual benchmarks, respectively.

There are many exciting use cases for generative speech models, but because of the potential risks of misuse, we are not making the Voicebox model or code publicly available at this time. While we believe it is important to be open with the AI community and to share our research to advance the state of the art in AI, it’s also necessary to strike the right balance between openness with responsibility. With these considerations, today we are sharing audio samples and a research paper detailing the approach and results we have achieved. In the paper , we also detail how we built a highly effective classifier that can distinguish between authentic speech and audio generated with Voicebox.

A new approach to speech generation

One of the main limitations of existing speech synthesizers is that they can only be trained on data that has been prepared expressly for that task. These inputs – known as monotonic, clean data – are difficult to produce, so they exist only in limited quantities, and they result in outputs that sound monotone. We built Voicebox upon the Flow Matching model , which is Meta’s latest advancement on non-autoregressive generative models that can learn highly non-deterministic mapping between text and speech. Non-deterministic mapping is useful because it enables Voicebox to learn from varied speech data without those variations having to be carefully labeled. This means Voicebox can train on more diverse data and a much larger scale of data.

We trained Voicebox with more than 50,000 hours of recorded speech and transcripts from public domain audiobooks in English, French, Spanish, German, Polish, and Portuguese. Voicebox is trained to predict a speech segment when given the surrounding speech and the transcript of the segment. Having learned to infill speech from context, the model can then apply this across speech generation tasks, including generating portions in the middle of an audio recording without having to re-create the entire input. This versatility enables Voicebox to perform well across a variety of tasks, including:

In-context text-to-speech synthesis: Using an input audio sample just two seconds in length, Voicebox can match the sample’s audio style and use it for text-to-speech generation. Future projects could build on this capability by bringing speech to people who are unable to speak, or by allowing people to customize the voices used by nonplayer characters and virtual assistants.

Cross-lingual style transfer: Given a sample of speech and a passage of text in English, French, German, Spanish, Polish, or Portuguese, Voicebox can produce a reading of the text in that language. This capability is exciting because in the future it could be used to help people communicate in a natural, authentic way — even if they don’t speak the same languages.

Speech denoising and editing: Voicebox’s in-context learning makes it good at generating speech to seamlessly edit segments within audio recordings. It can resynthesize the portion of speech corrupted by short-duration noise, or replace misspoken words without having to rerecord the entire speech. A person could identify which raw segment of the speech is corrupted by noise (like a dog barking), crop it, and instruct the model to regenerate that segment. This capability could one day be used to make cleaning up and editing audio as easy as popular image-editing tools have made adjusting photos.

Diverse speech sampling: Having learned from diverse in-the-wild data, Voicebox can generate speech that is more representative of how people talk in the real world and across the six languages listed above. In the future, this capability could be used to generate synthetic data to help better train a speech assistant model. Our results show that speech recognition models trained on Voicebox-generated synthetic speech perform almost as well as models trained on real speech, with 1 percent error rate degradation as opposed to 45 to 70 percent degradation with synthetic speech from previous text-to-speech models.

Sharing generative AI research responsibly"
Meta_Blog,https://ai.meta.com/blog/multilingual-model-speech-recognition/,,"Introducing speech-to-text, text-to-speech, and more for 1,100+ languages","Our approach

Collecting audio data for thousands of languages was our first challenge because the largest existing speech datasets cover at most 100 languages. To overcome it, we turned to religious texts, such as the Bible, that have been translated in many different languages and whose translations have been widely studied for text-based language translation research. These translations have publicly available audio recordings of people reading these texts in different languages. As part of this project, we created a dataset of readings of the New Testament in over 1,100 languages, which provided on average 32 hours of data per language.

By considering unlabeled recordings of various other Christian religious readings, we increased the number of languages available to over 4,000. While this data is from a specific domain and is often read by male speakers, our analysis shows that our models perform equally well for male and female voices. And while the content of the audio recordings is religious, our analysis shows that this does not overly bias the model to produce more religious language. We believe this is because we use a Connectionist Temporal Classification approach, which is far more constrained compared with large language models (LLMs) or sequence to-sequence models for speech recognition."
Meta_Blog,https://ai.meta.com/blog/multilingual-model-speech-recognition/,,"Introducing speech-to-text, text-to-speech, and more for 1,100+ languages","Our approach

Collecting audio data for thousands of languages was our first challenge because the largest existing speech datasets cover at most 100 languages. To overcome it, we turned to religious texts, such as the Bible, that have been translated in many different languages and whose translations have been widely studied for text-based language translation research. These translations have publicly available audio recordings of people reading these texts in different languages. As part of this project, we created a dataset of readings of the New Testament in over 1,100 languages, which provided on average 32 hours of data per language.

By considering unlabeled recordings of various other Christian religious readings, we increased the number of languages available to over 4,000. While this data is from a specific domain and is often read by male speakers, our analysis shows that our models perform equally well for male and female voices. And while the content of the audio recordings is religious, our analysis shows that this does not overly bias the model to produce more religious language. We believe this is because we use a Connectionist Temporal Classification approach, which is far more constrained compared with large language models (LLMs) or sequence to-sequence models for speech recognition."
Meta_Blog,https://ai.meta.com/blog/supercomputer-meta-research-supercluster-2023/,,Pursuing groundbreaking scale and accelerating research using Meta’s Research SuperCluster,"Training large AI models is traditionally a resource- and time-intensive task. By adding an unprecedented number of GPUs to the mix, we can significantly reduce the time it takes to train and fine-tune a model.

There are three elements that influence the rate at which a model trains: tokens, which are the number of words, numbers, and phrases in the training dataset, the number of parameters, and the GPUs used for training. In the animation below, observe how a higher number of parameters and tokens increases the time needed from hours to months. As the number of GPUs allocated increases, that training time can be drastically reduced. We’ve used this massive scale to our advantage over the past year to train a number of projects that are already making an impact."
Meta_Blog,https://ai.meta.com/blog/supercomputer-meta-research-supercluster-2023/,,Pursuing groundbreaking scale and accelerating research using Meta’s Research SuperCluster,"Training large AI models is traditionally a resource- and time-intensive task. By adding an unprecedented number of GPUs to the mix, we can significantly reduce the time it takes to train and fine-tune a model.

There are three elements that influence the rate at which a model trains: tokens, which are the number of words, numbers, and phrases in the training dataset, the number of parameters, and the GPUs used for training. In the animation below, observe how a higher number of parameters and tokens increases the time needed from hours to months. As the number of GPUs allocated increases, that training time can be drastically reduced. We’ve used this massive scale to our advantage over the past year to train a number of projects that are already making an impact."
Meta_Blog,https://ai.meta.com/blog/meta-training-inference-accelerator-AI-MTIA/,,MTIA v1: Meta’s first-generation AI inference accelerator,"AI workloads are ubiquitous at Meta — forming the basis for a wide range of use cases, including content understanding, Feeds, generative AI, and ads ranking. These workloads run on PyTorch with first-class Python integration, eager-mode development, and the simplicity of APIs. Deep learning recommendation models (DLRMs), in particular, are important for improving experiences across Meta’s services and applications. But as these models increase in size and complexity, the underlying hardware systems need to provide exponentially more memory and compute while remaining efficient.

We found that GPUs were not always optimal for running Meta’s specific recommendation workloads at the levels of efficiency required at our scale. Our solution to this challenge was to design a family of recommendation-specific Meta Training and Inference Accelerator (MTIA) ASICs. We co-designed the first-generation ASIC with next-generation recommendation model requirements in mind and integrated it into PyTorch to create a wholly optimized ranking system. In addition, we maintained the user experience and developer efficiency offered by PyTorch eager-mode development. Developer efficiency is a journey as we continue to support PyTorch 2.0, which supercharges how PyTorch operates at the compiler level — under the hood."
Meta_Blog,https://ai.meta.com/blog/meta-training-inference-accelerator-AI-MTIA/,,MTIA v1: Meta’s first-generation AI inference accelerator,"AI workloads are ubiquitous at Meta — forming the basis for a wide range of use cases, including content understanding, Feeds, generative AI, and ads ranking. These workloads run on PyTorch with first-class Python integration, eager-mode development, and the simplicity of APIs. Deep learning recommendation models (DLRMs), in particular, are important for improving experiences across Meta’s services and applications. But as these models increase in size and complexity, the underlying hardware systems need to provide exponentially more memory and compute while remaining efficient.

We found that GPUs were not always optimal for running Meta’s specific recommendation workloads at the levels of efficiency required at our scale. Our solution to this challenge was to design a family of recommendation-specific Meta Training and Inference Accelerator (MTIA) ASICs. We co-designed the first-generation ASIC with next-generation recommendation model requirements in mind and integrated it into PyTorch to create a wholly optimized ranking system. In addition, we maintained the user experience and developer efficiency offered by PyTorch eager-mode development. Developer efficiency is a journey as we continue to support PyTorch 2.0, which supercharges how PyTorch operates at the compiler level — under the hood."
Meta_Blog,https://ai.meta.com/blog/meta-scalable-video-processor-MSVP/,,MSVP: Meta’s first ASIC for video transcoding,"Here are the primary stages of the transcoding process:

Decode Support for a variety of elementary input video stream formats, including H.264, HEVC, VP9, and AV1

All profiles, 8/10-bit pixel sample depths, and YUV420 color format Preprocessing Format conversion, including video overlays

Frame resampling operation from arbitrary input resolutions to multiple resolutions (up to 4x) with high precision and wide filters

Shot detection Encode Support for H.264 (AVC) and VP9 coding standards

8-bit pixel sample depth, YUV420 color format Quality metrics (QM) Full reference: SSIM, MS-SSIM, VIF, PSNR (Luma and Chroma)

No-reference blurriness

QM at multiple viewport resolutions

These stages are implemented as memory-to-memory operations, meaning intermediate buffers are stored back to DRAM and refetched as needed by the downstream operation.

Power and performance Each MSVP ASIC can offer a peak transcoding performance of 4K at 15fps at the highest quality configuration with 1-in, 5-out streams and can scale up to 4K at 60fps at the standard quality configuration. Performance scales linearly with resolution. This performance is achieved at ~10W of PCIe module power. We achieved a throughput gain of ~9x for H.264 when compared against libx264 SW encoding. For VP9, we achieved a throughput gain of ~50x when compared with libVPX speed 2 preset. ~9x faster throughput for H.264 compared to libx264 SW ~50x faster throughput for VP9 compared to libVPX speed 2

In video coding, we deploy a method to assess and compare compression efficiency, called the Bjontegaard delta rate (BD-Rate), which estimates the number of bits saved (if the BD-Rate is a negative figure) in order to deliver the same objective quality for a given video over a baseline configuration.

MSVP’s video encoding algorithms

The MSVP encoder has two main goals: to be highly power efficient and to deliver the same or better video quality as software encoders. There are existing video encoder IPs, but most of them are targeted at mobile devices with tight area/power constraints and cannot meet the quality bar set by current software encoders. Because software encoders offer very flexible control and fast evolution over time, it is quite challenging for ASIC video encoders to meet the same performance bar as software encoders. Here’s a simplified version of the data flow of modern hybrid (hardware and software) video encoders:

Simplified video encoder modules.

These encoders use intra-coding to reduce spatial redundancy and inter-coding to remove temporal redundancy. Different stages of motion estimation are applied in inter-coding to find out the best prediction among all possible block positions in available reference frames. Entropy coding is the lossless compression part that squeezes the statistical redundancy of all syntax elements, including encoding modes, motion vectors and quantized residual coefficients. For MSVP’s algorithms to perform the way we wanted, we had to find hardware-friendly alternatives for each of the above key modules. We mainly focused on three levels: block level, frame level, and group of picture (GOP) level. At the block level, we looked for coding tools with the highest return on investment, that were easy/economical (in terms of silicon area and power requirements) to implement in hardware, and that met our performance targets while maximizing compression efficiency. At frame level, we studied the best algorithms to make intelligent frame type decisions among I/P/B frames, and the best rate-control algorithms based on statistics collected from hardware. And at the GOP level, we had to figure out whether to use multiple-pass encoding with look-ahead, or to insert intra (key) frames at a given shot boundary.

Motion estimation

Motion estimation is one of the most computationally intensive algorithms in video encoding. To find accurate motion vectors that closely match the block currently being encoded, a full motion estimation pipeline often includes a multistage search to balance among large search range, computing complexity, and accuracy. MSVP’s motion search algorithm needs to be one that identifies which potential neighboring blocks can contribute more to quality and only searches around highly correlated neighbors with a limited cycle budget. Although we lack the flexibility of iterative software motion search algorithms, such as diamond or hexagon shapes, hardware motion estimation can search multiple blocks in parallel. Thus, it allows us to search more candidates, cover a larger search range and more reference frames in both single direction and bidirectional mode, and search all supported block partition shapes in parallel.

Rate distortion optimization (RDO)

Achieving high video encoding quality also requires RDO support. Since there are so many decisions to make in video encoding (intra/inter modes, partition block size, transform block types/sizes, etc.), RDO is one of the best practices in video compression to determine which mode is optimal given the current rate or quality target. MSVP supports exhaustive RDO at almost all mode decision stages. Distortion calculation is intensive but both straightforward and easily parallelizable. But the unique challenge is the bit rate estimation. Entropy coding for the final bitstream is sequential in nature, and each context model is dependent on the previously encoded ones. In a hardware encoder implementation, rate distortion (RD) cost for different blocks/partitions might be evaluated in parallel; thus, it is impossible to have very accurate bit rate estimation. We implemented a pretty accurate bit rate estimation model in MSVP. The model is hardware friendly, in that it is easy to evaluate multiple coding modes in parallel.

Smart quantization

Quantization is the only lossy part of video compression, and it is also the dominant bit rate control knob in any video coding standard. The corresponding parameter is called the quantization parameter (QP), and it is inversely related to quality: Low QP values result in small quantization errors, creating low distortion levels and, subsequently, high quality at the expense of higher bit rates. By making smart quantization choices, encoding bits can be allocated to areas that impact visual quality the most. We perform smart quantization using optimal QP selection and rounding decisions. Modern video coding standards allow different QP values to be applied to different coding units. In MSVP’s hardware encoder, block-level QP values are determined adaptively based on both spatial and temporal characteristics. In spatial adaptive QP (AQP) selection, since the human visual system is less sensitive to quality loss at high texture or high motion areas, a larger QP value can be applied to these coding blocks. In temporal AQP, coding blocks that are referenced more in the future can be quantized with a lower QP to get higher quality, such that future coding blocks that reference these blocks will benefit from it. Smart rounding tries to make a joint optimization on rounding decisions for all coefficients in each coding block. Since the choices of rounding at different coefficient positions are dependent on one another, we need better algorithms that remove the dependency while maintaining the rounding decision accuracy. To reduce compute cost, we’ve applied smart rounding to the final stage after the coding mode for each block is determined. This feature alone can achieve a ~1 percent to 2 percent BD-Rate improvement.

H.264

The frame-level algorithm for the MSVP H.264 encoder can be configured to be either two-pass or one-pass, depending on whether it is a VOD or live streaming use case. In the high quality (longer latency) VOD two-pass mode, MSVP looks ahead N frames and collects statistics, such as intra/inter cost and motion vectors, from these frames. Then, based on the statistics collected in the look-ahead, frame level control applies back-propagation on the reference tree in the look-ahead buffers for each reference frame to assign an importance to frames. Then, finally, the accumulated reference importance of the frame to be coded is modulated using temporal AQP of each block. Finally, the delta QP map is passed to the final encoding pass to be used as the encoding QP, also captured in the output bitstream.

MSVP H.264 encoder frame level control flow.

VP9

In MSVP’s VP9 encoder, multiple-pass encoding is also enabled for high-quality VOD use cases. An analysis pass (the first pass) is performed up front to capture the video characteristics into a set of statistics, and the statistics are used to determine the frame level parameters for filtering and encoding. Since VP9’s frame type is different from H.264’s, the strategy for making frame level decisions is also different, as shown in the following figure:

VP9 encoder frame level algorithm flow."
Meta_Blog,https://ai.meta.com/blog/meta-scalable-video-processor-MSVP/,,MSVP: Meta’s first ASIC for video transcoding,"Here are the primary stages of the transcoding process:

Decode Support for a variety of elementary input video stream formats, including H.264, HEVC, VP9, and AV1

All profiles, 8/10-bit pixel sample depths, and YUV420 color format Preprocessing Format conversion, including video overlays

Frame resampling operation from arbitrary input resolutions to multiple resolutions (up to 4x) with high precision and wide filters

Shot detection Encode Support for H.264 (AVC) and VP9 coding standards

8-bit pixel sample depth, YUV420 color format Quality metrics (QM) Full reference: SSIM, MS-SSIM, VIF, PSNR (Luma and Chroma)

No-reference blurriness

QM at multiple viewport resolutions

These stages are implemented as memory-to-memory operations, meaning intermediate buffers are stored back to DRAM and refetched as needed by the downstream operation.

Power and performance Each MSVP ASIC can offer a peak transcoding performance of 4K at 15fps at the highest quality configuration with 1-in, 5-out streams and can scale up to 4K at 60fps at the standard quality configuration. Performance scales linearly with resolution. This performance is achieved at ~10W of PCIe module power. We achieved a throughput gain of ~9x for H.264 when compared against libx264 SW encoding. For VP9, we achieved a throughput gain of ~50x when compared with libVPX speed 2 preset. ~9x faster throughput for H.264 compared to libx264 SW ~50x faster throughput for VP9 compared to libVPX speed 2

In video coding, we deploy a method to assess and compare compression efficiency, called the Bjontegaard delta rate (BD-Rate), which estimates the number of bits saved (if the BD-Rate is a negative figure) in order to deliver the same objective quality for a given video over a baseline configuration.

MSVP’s video encoding algorithms

The MSVP encoder has two main goals: to be highly power efficient and to deliver the same or better video quality as software encoders. There are existing video encoder IPs, but most of them are targeted at mobile devices with tight area/power constraints and cannot meet the quality bar set by current software encoders. Because software encoders offer very flexible control and fast evolution over time, it is quite challenging for ASIC video encoders to meet the same performance bar as software encoders. Here’s a simplified version of the data flow of modern hybrid (hardware and software) video encoders:

Simplified video encoder modules.

These encoders use intra-coding to reduce spatial redundancy and inter-coding to remove temporal redundancy. Different stages of motion estimation are applied in inter-coding to find out the best prediction among all possible block positions in available reference frames. Entropy coding is the lossless compression part that squeezes the statistical redundancy of all syntax elements, including encoding modes, motion vectors and quantized residual coefficients. For MSVP’s algorithms to perform the way we wanted, we had to find hardware-friendly alternatives for each of the above key modules. We mainly focused on three levels: block level, frame level, and group of picture (GOP) level. At the block level, we looked for coding tools with the highest return on investment, that were easy/economical (in terms of silicon area and power requirements) to implement in hardware, and that met our performance targets while maximizing compression efficiency. At frame level, we studied the best algorithms to make intelligent frame type decisions among I/P/B frames, and the best rate-control algorithms based on statistics collected from hardware. And at the GOP level, we had to figure out whether to use multiple-pass encoding with look-ahead, or to insert intra (key) frames at a given shot boundary.

Motion estimation

Motion estimation is one of the most computationally intensive algorithms in video encoding. To find accurate motion vectors that closely match the block currently being encoded, a full motion estimation pipeline often includes a multistage search to balance among large search range, computing complexity, and accuracy. MSVP’s motion search algorithm needs to be one that identifies which potential neighboring blocks can contribute more to quality and only searches around highly correlated neighbors with a limited cycle budget. Although we lack the flexibility of iterative software motion search algorithms, such as diamond or hexagon shapes, hardware motion estimation can search multiple blocks in parallel. Thus, it allows us to search more candidates, cover a larger search range and more reference frames in both single direction and bidirectional mode, and search all supported block partition shapes in parallel.

Rate distortion optimization (RDO)

Achieving high video encoding quality also requires RDO support. Since there are so many decisions to make in video encoding (intra/inter modes, partition block size, transform block types/sizes, etc.), RDO is one of the best practices in video compression to determine which mode is optimal given the current rate or quality target. MSVP supports exhaustive RDO at almost all mode decision stages. Distortion calculation is intensive but both straightforward and easily parallelizable. But the unique challenge is the bit rate estimation. Entropy coding for the final bitstream is sequential in nature, and each context model is dependent on the previously encoded ones. In a hardware encoder implementation, rate distortion (RD) cost for different blocks/partitions might be evaluated in parallel; thus, it is impossible to have very accurate bit rate estimation. We implemented a pretty accurate bit rate estimation model in MSVP. The model is hardware friendly, in that it is easy to evaluate multiple coding modes in parallel.

Smart quantization

Quantization is the only lossy part of video compression, and it is also the dominant bit rate control knob in any video coding standard. The corresponding parameter is called the quantization parameter (QP), and it is inversely related to quality: Low QP values result in small quantization errors, creating low distortion levels and, subsequently, high quality at the expense of higher bit rates. By making smart quantization choices, encoding bits can be allocated to areas that impact visual quality the most. We perform smart quantization using optimal QP selection and rounding decisions. Modern video coding standards allow different QP values to be applied to different coding units. In MSVP’s hardware encoder, block-level QP values are determined adaptively based on both spatial and temporal characteristics. In spatial adaptive QP (AQP) selection, since the human visual system is less sensitive to quality loss at high texture or high motion areas, a larger QP value can be applied to these coding blocks. In temporal AQP, coding blocks that are referenced more in the future can be quantized with a lower QP to get higher quality, such that future coding blocks that reference these blocks will benefit from it. Smart rounding tries to make a joint optimization on rounding decisions for all coefficients in each coding block. Since the choices of rounding at different coefficient positions are dependent on one another, we need better algorithms that remove the dependency while maintaining the rounding decision accuracy. To reduce compute cost, we’ve applied smart rounding to the final stage after the coding mode for each block is determined. This feature alone can achieve a ~1 percent to 2 percent BD-Rate improvement.

H.264

The frame-level algorithm for the MSVP H.264 encoder can be configured to be either two-pass or one-pass, depending on whether it is a VOD or live streaming use case. In the high quality (longer latency) VOD two-pass mode, MSVP looks ahead N frames and collects statistics, such as intra/inter cost and motion vectors, from these frames. Then, based on the statistics collected in the look-ahead, frame level control applies back-propagation on the reference tree in the look-ahead buffers for each reference frame to assign an importance to frames. Then, finally, the accumulated reference importance of the frame to be coded is modulated using temporal AQP of each block. Finally, the delta QP map is passed to the final encoding pass to be used as the encoding QP, also captured in the output bitstream.

MSVP H.264 encoder frame level control flow.

VP9

In MSVP’s VP9 encoder, multiple-pass encoding is also enabled for high-quality VOD use cases. An analysis pass (the first pass) is performed up front to capture the video characteristics into a set of statistics, and the statistics are used to determine the frame level parameters for filtering and encoding. Since VP9’s frame type is different from H.264’s, the strategy for making frame level decisions is also different, as shown in the following figure:

VP9 encoder frame level algorithm flow."
Meta_Blog,https://ai.meta.com/blog/meta-ai-infrastructure-overview/,,Reimagining Meta’s infrastructure for the AI age,"Meta’s AI compute needs will grow dramatically over the next decade as we break new ground in AI research, ship more cutting-edge AI applications and experiences for our family of apps, and build our long-term vision of the metaverse.

We are now executing on an ambitious plan to build the next generation of Meta’s infrastructure backbone – specifically built for AI – and in this blog post we’re sharing some details on our recent progress. The projects we’re announcing here touch many of the layers of our hardware and software stack as well as the customized network that connects these technologies from top to bottom. They include our first custom chip for running AI models, a new AI-optimized data center design, and phase 2 of our 16,000 GPU supercomputer for AI research.

These transformational efforts — and additional projects still underway — will enable us to develop much larger, more sophisticated AI models and then deploy them efficiently at scale. AI is already at the core of Meta’s products, enabling better personalization; safer, fairer products; and richer experiences, while also helping businesses reach the audiences they care about most. We are even reimagining how we code — deploying Code Compose, a generative AI–based coding assistant developed in-house at Meta as a key tool to make our developers more productive throughout the software development life cycle. By rethinking how we innovate across our infrastructure, we’re creating a scalable foundation to power emerging opportunities in the near term in areas like generative AI, and in the longer term as we bring new AI-powered experiences to the metaverse.

For more on the AI investments shared in this post, check out the Meta AI Infra @Scale page."
Meta_Blog,https://ai.meta.com/blog/meta-ai-infrastructure-overview/,,Reimagining Meta’s infrastructure for the AI age,"Meta’s AI compute needs will grow dramatically over the next decade as we break new ground in AI research, ship more cutting-edge AI applications and experiences for our family of apps, and build our long-term vision of the metaverse.

We are now executing on an ambitious plan to build the next generation of Meta’s infrastructure backbone – specifically built for AI – and in this blog post we’re sharing some details on our recent progress. The projects we’re announcing here touch many of the layers of our hardware and software stack as well as the customized network that connects these technologies from top to bottom. They include our first custom chip for running AI models, a new AI-optimized data center design, and phase 2 of our 16,000 GPU supercomputer for AI research.

These transformational efforts — and additional projects still underway — will enable us to develop much larger, more sophisticated AI models and then deploy them efficiently at scale. AI is already at the core of Meta’s products, enabling better personalization; safer, fairer products; and richer experiences, while also helping businesses reach the audiences they care about most. We are even reimagining how we code — deploying Code Compose, a generative AI–based coding assistant developed in-house at Meta as a key tool to make our developers more productive throughout the software development life cycle. By rethinking how we innovate across our infrastructure, we’re creating a scalable foundation to power emerging opportunities in the near term in areas like generative AI, and in the longer term as we bring new AI-powered experiences to the metaverse.

For more on the AI investments shared in this post, check out the Meta AI Infra @Scale page."
Meta_Blog,https://ai.meta.com/blog/ai-ads-performance-efficiency-meta-lattice/,,New AI advancements drive Meta’s ads system performance and efficiency,"AI has long been a crucial component of Meta’s ads system. We began with manual feature engineering for small models and progressed to building hundreds of deep neural network models with trillions of parameters. Each model is independently optimized for different goals — such as improving ad quality to provide better experiences for people or increasing conversion rates for a higher return on ad spend for our advertisers.

Meta continues to take bold steps to advance and deploy state-of-the-art AI and deliver a step change in our ads system performance. We’re rolling out more powerful AI models to improve performance across all ad types and ad surfaces. We aim to achieve this through deeper alignment with advertiser objectives, and utilizing the rapid expansion of high-growth areas, like short-form video, to provide enjoyable experiences to people — all while working to safeguard privacy.

Recently, we built and deployed Meta Lattice, a new model architecture that learns to predict an ad’s performance across a variety of datasets and optimization goals that were previously supported by numerous smaller, siloed models. It enhances Meta’s ads system in the following ways:

Better performance. Meta Lattice is capable of improving the performance of our ads system holistically. We’ve supercharged its performance with a high-capacity architecture that allows our ads system to more broadly and deeply understand new concepts and relationships in data and benefits advertisers through joint optimization of a large number of goals. Early results from our deployment on Instagram show that knowledge-sharing across its different surfaces (e.g., Feed, Story, and Reels) and across various advertiser objectives (e.g., clicks, video views, and conversions) increased performance for advertisers. Joint optimization of value for people and advertisers resulted in better ad experiences for people, showing an ~8 percent improvement in ads quality.

Improved AI efficiency. Maintaining and advancing fewer, more powerful models makes our overall ads system more nimble in adopting future AI innovations, driving greater value for our advertisers. We also expect that transitioning to Meta Lattice will allow our fleet to improve compute efficiency, freeing up resources to explore new frontiers in AI.

Faster adaptability to the shifting market landscape. As people’s expectations of how their data is used continue to evolve, regulations and policies from governments and industry players are also changing. Evolving data use regulations and platform practices change the type and amount of data available to machine learning models. We've designed Meta Lattice to drive advertiser performance in the new digital advertising environment where we have access to less granular data. Additionally, Lattice is capable of generalizing learnings across domains and objectives, which is especially crucial when the model has limited data to train on. Fewer models also means we can proactively and efficiently update our models and adapt to the fast-evolving market landscape.

Meta Lattice: A model architecture that’s greater than the sum of its parts

Something Went Wrong We're having trouble playing this video. Learn more

Before the use of this model architecture, Meta Ads model space was projected to grow substantially in the coming years as new surfaces, advertiser products, and privacy practices emerge. Maintaining a large model space often leads to slower proliferation of AI innovations and compute inefficiency.

We have built a holistic model architecture that incorporates disparate signals and balances performance across domains and objectives. Furthermore, balancing model performance with compute efficiency is a complex and compelling technical challenge.

To overcome these challenges, we built the following key components:

Holistic understanding of both advertisers’ and people's objectives. Meta Lattice can understand both the common usage patterns and the unique and latent people-advertiser engagement patterns from heterogeneous data sources, through multi-domain, multi-task learning and armed by sparse activation techniques. This mechanism is particularly useful for the “cold start” problem — people can receive more relevant ad recommendations on emerging products and surfaces, even though there is little data to learn from, through better generalization.

Handling delayed feedback. The engagement between an ad and a person viewing the ad can span from seconds (e.g., click, like) to days (e.g., considering a purchase, adding to a cart, and later making the purchase from a website or an app). Through multi-distribution modeling with temporal awareness, Meta Lattice can capture not only a person’s real-time intent from fresh signals but also long-term interest from slow, sparse, and delayed signals.

Balancing multiple domains and objectives. Meta Lattice is able to balance performance across multiple domains and objectives and reach a status where no objective can be further improved without hurting the rest (aka Pareto optimality). Techniques such as Pareto-front feature selection, MetaBalance , help avoid manually tuning the performance of thousands of different domains and tens of different objectives.

Advanced model scaling. Meta Lattice has trillions of parameters, is trained on hundreds of billions of examples from thousands of data domains, including Meta’s platform surfaces, and our advertiser-facing products. Our customized Deep and Hierarchical Ensemble Network model, built on top of a Transformers backbone, is highly scalable on GPUs.

Maximizing AI Capex efficiency. Previously, hundreds of models were separately trained, served, and optimized. Now we’ve introduced two levels of resource-sharing: (1) horizontal sharing across domains, objectives, and ranking stages through joint optimization; and (2) hierarchical sharing from large, high-capacity upstream models to lightweight downstream vertical models. Through resource-sharing enhancement, we can significantly reduce the amount of computational needs.

Ushering in a new era of AI-powered ads

As businesses face continuous shifts in consumer behavior, economic slowdowns, and ongoing changes to industry data use practices and restrictions, we need smarter, more flexible AI systems that can address these challenges quickly and efficiently.

Meta Lattice is one way we’re using AI more broadly and deeply to enhance Meta’s ads system. The system will now continuously learn the essential characteristics that improve ad performance across various surfaces, objectives, and ad types simultaneously. Going forward, we’ll continue to further iterate on Meta Lattice. This new model architecture creates a more nimble system — one that is more adaptable to broader market changes, can quickly utilize new AI innovations, and operates more efficiently to deliver the results that help businesses grow."
Meta_Blog,https://ai.meta.com/blog/ai-ads-performance-efficiency-meta-lattice/,,New AI advancements drive Meta’s ads system performance and efficiency,"AI has long been a crucial component of Meta’s ads system. We began with manual feature engineering for small models and progressed to building hundreds of deep neural network models with trillions of parameters. Each model is independently optimized for different goals — such as improving ad quality to provide better experiences for people or increasing conversion rates for a higher return on ad spend for our advertisers.

Meta continues to take bold steps to advance and deploy state-of-the-art AI and deliver a step change in our ads system performance. We’re rolling out more powerful AI models to improve performance across all ad types and ad surfaces. We aim to achieve this through deeper alignment with advertiser objectives, and utilizing the rapid expansion of high-growth areas, like short-form video, to provide enjoyable experiences to people — all while working to safeguard privacy.

Recently, we built and deployed Meta Lattice, a new model architecture that learns to predict an ad’s performance across a variety of datasets and optimization goals that were previously supported by numerous smaller, siloed models. It enhances Meta’s ads system in the following ways:

Better performance. Meta Lattice is capable of improving the performance of our ads system holistically. We’ve supercharged its performance with a high-capacity architecture that allows our ads system to more broadly and deeply understand new concepts and relationships in data and benefits advertisers through joint optimization of a large number of goals. Early results from our deployment on Instagram show that knowledge-sharing across its different surfaces (e.g., Feed, Story, and Reels) and across various advertiser objectives (e.g., clicks, video views, and conversions) increased performance for advertisers. Joint optimization of value for people and advertisers resulted in better ad experiences for people, showing an ~8 percent improvement in ads quality.

Improved AI efficiency. Maintaining and advancing fewer, more powerful models makes our overall ads system more nimble in adopting future AI innovations, driving greater value for our advertisers. We also expect that transitioning to Meta Lattice will allow our fleet to improve compute efficiency, freeing up resources to explore new frontiers in AI.

Faster adaptability to the shifting market landscape. As people’s expectations of how their data is used continue to evolve, regulations and policies from governments and industry players are also changing. Evolving data use regulations and platform practices change the type and amount of data available to machine learning models. We've designed Meta Lattice to drive advertiser performance in the new digital advertising environment where we have access to less granular data. Additionally, Lattice is capable of generalizing learnings across domains and objectives, which is especially crucial when the model has limited data to train on. Fewer models also means we can proactively and efficiently update our models and adapt to the fast-evolving market landscape.

Meta Lattice: A model architecture that’s greater than the sum of its parts

Something Went Wrong We're having trouble playing this video. Learn more

Before the use of this model architecture, Meta Ads model space was projected to grow substantially in the coming years as new surfaces, advertiser products, and privacy practices emerge. Maintaining a large model space often leads to slower proliferation of AI innovations and compute inefficiency.

We have built a holistic model architecture that incorporates disparate signals and balances performance across domains and objectives. Furthermore, balancing model performance with compute efficiency is a complex and compelling technical challenge.

To overcome these challenges, we built the following key components:

Holistic understanding of both advertisers’ and people's objectives. Meta Lattice can understand both the common usage patterns and the unique and latent people-advertiser engagement patterns from heterogeneous data sources, through multi-domain, multi-task learning and armed by sparse activation techniques. This mechanism is particularly useful for the “cold start” problem — people can receive more relevant ad recommendations on emerging products and surfaces, even though there is little data to learn from, through better generalization.

Handling delayed feedback. The engagement between an ad and a person viewing the ad can span from seconds (e.g., click, like) to days (e.g., considering a purchase, adding to a cart, and later making the purchase from a website or an app). Through multi-distribution modeling with temporal awareness, Meta Lattice can capture not only a person’s real-time intent from fresh signals but also long-term interest from slow, sparse, and delayed signals.

Balancing multiple domains and objectives. Meta Lattice is able to balance performance across multiple domains and objectives and reach a status where no objective can be further improved without hurting the rest (aka Pareto optimality). Techniques such as Pareto-front feature selection, MetaBalance , help avoid manually tuning the performance of thousands of different domains and tens of different objectives.

Advanced model scaling. Meta Lattice has trillions of parameters, is trained on hundreds of billions of examples from thousands of data domains, including Meta’s platform surfaces, and our advertiser-facing products. Our customized Deep and Hierarchical Ensemble Network model, built on top of a Transformers backbone, is highly scalable on GPUs.

Maximizing AI Capex efficiency. Previously, hundreds of models were separately trained, served, and optimized. Now we’ve introduced two levels of resource-sharing: (1) horizontal sharing across domains, objectives, and ranking stages through joint optimization; and (2) hierarchical sharing from large, high-capacity upstream models to lightweight downstream vertical models. Through resource-sharing enhancement, we can significantly reduce the amount of computational needs.

Ushering in a new era of AI-powered ads

As businesses face continuous shifts in consumer behavior, economic slowdowns, and ongoing changes to industry data use practices and restrictions, we need smarter, more flexible AI systems that can address these challenges quickly and efficiently.

Meta Lattice is one way we’re using AI more broadly and deeply to enhance Meta’s ads system. The system will now continuously learn the essential characteristics that improve ad performance across various surfaces, objectives, and ad types simultaneously. Going forward, we’ll continue to further iterate on Meta Lattice. This new model architecture creates a more nimble system — one that is more adaptable to broader market changes, can quickly utilize new AI innovations, and operates more efficiently to deliver the results that help businesses grow."
Meta_Blog,https://ai.meta.com/blog/imagebind-six-modalities-binding-ai/,,ImageBind: Holistic AI learning across six modalities,"When humans absorb information from the world, we innately use multiple senses, such as seeing a busy street and hearing the sounds of car engines. Today, we’re introducing an approach that brings machines one step closer to humans’ ability to learn simultaneously, holistically, and directly from many different forms of information — without the need for explicit supervision (the process of organizing and labeling raw data). We have built and are open-sourcing ImageBind, the first AI model capable of binding information from six modalities. The model learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position. ImageBind equips machines with a holistic understanding that connects objects in a photo with how they will sound, their 3D shape, how warm or cold they are, and how they move.

ImageBind can outperform prior specialist models trained individually for one particular modality, as described in our paper. But most important, it helps advance AI by enabling machines to better analyze many different forms of information together. For example, using ImageBind, Meta’s Make-A-Scene could create images from audio, such as creating an image based on the sounds of a rain forest or a bustling market. Other future possibilities include more accurate ways to recognize, connect, and moderate content, and to boost creative design, such as generating richer media more seamlessly and creating wider multimodal search functions.

ImageBind is part of Meta’s efforts to create multimodal AI systems that learn from all possible types of data around them. As the number of modalities increases, ImageBind opens the floodgates for researchers to try to develop new, holistic systems, such as combining 3D and IMU sensors to design or experience immersive, virtual worlds. ImageBind could also provide a rich way to explore memories — searching for pictures, videos, audio files or text messages using a combination of text, audio, and image.

In typical AI systems, there is a specific embedding (that is, vectors of numbers that can represent data and their relationships in machine learning) for each respective modality. ImageBind shows that it’s possible to create a joint embedding space across multiple modalities without needing to train on data with every different combination of modalities. This is important because it’s not feasible for researchers to create datasets with samples that contain, for example, audio data and thermal data from a busy city street, or depth data and a text description of a seaside cliff.

Just as there have been exciting recent advances in generating images, videos, and audio from text (such as Make-A-Scene and Meta’s Make-A-Video), ImageBind’s multimodal capabilities could allow researchers to use other modalities as input queries and retrieve outputs in other formats. ImageBind is also an important step toward building machines that can analyze different kinds of data holistically, as humans do.

By aligning six modalities’ embedding into a common space, ImageBind enables cross-modal retrieval of different types of content that aren’t observed together, the addition of embeddings from different modalities to naturally compose their semantics, and audio-to-image generation by using our audio embeddings with a pretrained DALLE-2 decoder to work with CLIP text embeddings.

ImageBind is a multimodal model that joins a recent series of Meta's open source AI tools. This includes computer vision models like DINOv2, a new method that doesn’t require fine tuning training high-performance computer vision models, and Segment Anything (SAM) a universal segmentation model that can segment any object in any image, based on any user prompt. ImageBind complements these models as it focuses on multimodal representation learning. It tries to learn a single aligned feature space for multiple modalities, including, but not limited to, images and videos. In the future, ImageBind can leverage the powerful visual features from DINOv2 to further improve its capabilities.

Learning a single embedding space by binding content with images

Humans have the ability to learn new concepts from only a few examples. We can typically read a description of an animal and then recognize it in real life. We can also look at a photo of an unfamiliar model of a car and anticipate how its engine might sound. This is partly because a single image, in fact, can “bind” together an entire sensory experience. In the field of AI, however, as the number of modalities increases, the lack of multiple sensory data can limit standard multimodal learning, which relies on paired data. Ideally, a single joint embedding space — where many different kinds of data are distributed — could allow a model to learn visual features along with other modalities.

Previously, learning such a joint embedding space for all modalities would require collecting all possible combinations of paired data — an infeasible feat.

ImageBind circumvented this challenge by leveraging recent large-scale vision-language models and extending their zero-shot capabilities to new modalities just by using their natural pairing with images, such as video-audio and image-depth data, to learn a single joint embedding space. For the four additional modalities (audio, depth, thermal, and IMU readings), we use naturally paired self-supervised data.

Training image-text models has been extensively studied because of the abundance of images and co-occurring text on the internet. ImageBind uses the binding property of images, meaning they co-occur with a variety of modalities and can serve as a bridge to connect them, such as linking text to image using web data or linking motion to video using video data captured from wearable cameras with IMU sensors.

The visual representations learned from large-scale web data can be used as targets to learn features for different modalities. This allows ImageBind to align any modality that co-occurs with images, naturally aligning those modalities among themselves. Modalities with a strong correlation to images, such as thermal and depth, are easier to align. Modalities that are not visual, such as audio and IMU, have a weaker correlation. Consider that there are particular sounds, like a baby’s cries, that could accompany any number of visual contexts.

ImageBind shows that image-paired data is sufficient to bind together these six modalities. The model can interpret content more holistically, allowing the different modalities to “talk” to each other and find links without observing them together. For example, ImageBind can associate audio and text without seeing them together. This enables other models to “understand” new modalities without any resource-intensive training. ImageBind’s strong scaling behavior allows the model to substitute or enhance many AI models by enabling them to use other modalities. For instance, while Make-A-Scene can generate images by using text prompts, ImageBind could upgrade it to generate images using audio sounds, such as laughter or rain.

ImageBind’s capabilities that outperform

Image-aligned, self-supervised learning shows that the performance of our model can actually improve by using very few training examples. Our model has new emergent capabilities, or scaling behavior — that is, abilities that didn’t exist in smaller models but appear in larger versions. This might include recognizing which audio fits with a certain image or predicting the depth of a scene from a photo.

Our analysis shows that ImageBind’s scaling behavior improves with the strength of the image encoder. In other words, ImageBind’s ability to align modalities increases with the strength and size of the vision model. This suggests that larger vision models benefit nonvision tasks, such as audio classification, and the benefits of training such models go beyond computer vision tasks.

Among our experiments, we used the audio and depth encoders from ImageBind and compared them with prior work in zero-shot retrieval as well as audio and depth classification tasks.

ImageBind outperformed specialist models in audio and depth, based on benchmarks.

We discovered that ImageBind features can be used for few-shot audio and depth classification tasks and can outperform prior methods tailored for those modalities. For example, ImageBind significantly outperforms Meta’s self-supervised AudioMAE model trained on Audioset and a supervised AudioMAE model fine-tuned on audio classification, with gains of approximately 40 percent accuracy in top-1 accuracy on ≤four-shot classification.

ImageBind also achieved new state-of-the-art performance on emergent zero-shot recognition tasks across modalities, even outperforming recent models that were trained to recognize concepts for that modality.

The future of multimodal learning

With the capability to use several modalities for input queries and retrieve outputs across other modalities, ImageBind shows new possibilities for creators. Imagine that someone could take a video recording of an ocean sunset and instantly add the perfect audio clip to enhance it, while an image of a brindle Shih Tzu could yield essays or depth models of similar dogs. Or when a model like Make-A-Video produces a video of a carnival, ImageBind can suggest background noise to accompany it, creating an immersive experience.

People could even segment and identify the objects in an image based on audio. This creates distinctive opportunities to create animations out of static images by combining them with audio prompts. For example, a creator could couple an image with an alarm clock and a rooster crowing, and use a crowing audio prompt to segment the rooster or the sound of an alarm to segment the clock and animate both into a video sequence.

While we explored six modalities in our current research, we believe that introducing new modalities that link as many senses as possible — like touch, speech, smell, and brain fMRI signals — will enable richer human-centric AI models.

There’s still a lot to uncover about multimodal learning. The AI research community has yet to effectively quantify scaling behaviors that appear only in larger models and understand their applications. ImageBind is a step toward evaluating them in a rigorous way and showing novel applications in image generation and retrieval.

We hope the research community will explore ImageBind and our accompanying published paper to find new ways to evaluate vision models and lead to novel applications.

Read the paper

See how ImageBind works

Access the ImageBind code"
Meta_Blog,https://ai.meta.com/blog/imagebind-six-modalities-binding-ai/,,ImageBind: Holistic AI learning across six modalities,"When humans absorb information from the world, we innately use multiple senses, such as seeing a busy street and hearing the sounds of car engines. Today, we’re introducing an approach that brings machines one step closer to humans’ ability to learn simultaneously, holistically, and directly from many different forms of information — without the need for explicit supervision (the process of organizing and labeling raw data). We have built and are open-sourcing ImageBind, the first AI model capable of binding information from six modalities. The model learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position. ImageBind equips machines with a holistic understanding that connects objects in a photo with how they will sound, their 3D shape, how warm or cold they are, and how they move.

ImageBind can outperform prior specialist models trained individually for one particular modality, as described in our paper. But most important, it helps advance AI by enabling machines to better analyze many different forms of information together. For example, using ImageBind, Meta’s Make-A-Scene could create images from audio, such as creating an image based on the sounds of a rain forest or a bustling market. Other future possibilities include more accurate ways to recognize, connect, and moderate content, and to boost creative design, such as generating richer media more seamlessly and creating wider multimodal search functions.

ImageBind is part of Meta’s efforts to create multimodal AI systems that learn from all possible types of data around them. As the number of modalities increases, ImageBind opens the floodgates for researchers to try to develop new, holistic systems, such as combining 3D and IMU sensors to design or experience immersive, virtual worlds. ImageBind could also provide a rich way to explore memories — searching for pictures, videos, audio files or text messages using a combination of text, audio, and image.

In typical AI systems, there is a specific embedding (that is, vectors of numbers that can represent data and their relationships in machine learning) for each respective modality. ImageBind shows that it’s possible to create a joint embedding space across multiple modalities without needing to train on data with every different combination of modalities. This is important because it’s not feasible for researchers to create datasets with samples that contain, for example, audio data and thermal data from a busy city street, or depth data and a text description of a seaside cliff.

Just as there have been exciting recent advances in generating images, videos, and audio from text (such as Make-A-Scene and Meta’s Make-A-Video), ImageBind’s multimodal capabilities could allow researchers to use other modalities as input queries and retrieve outputs in other formats. ImageBind is also an important step toward building machines that can analyze different kinds of data holistically, as humans do.

By aligning six modalities’ embedding into a common space, ImageBind enables cross-modal retrieval of different types of content that aren’t observed together, the addition of embeddings from different modalities to naturally compose their semantics, and audio-to-image generation by using our audio embeddings with a pretrained DALLE-2 decoder to work with CLIP text embeddings.

ImageBind is a multimodal model that joins a recent series of Meta's open source AI tools. This includes computer vision models like DINOv2, a new method that doesn’t require fine tuning training high-performance computer vision models, and Segment Anything (SAM) a universal segmentation model that can segment any object in any image, based on any user prompt. ImageBind complements these models as it focuses on multimodal representation learning. It tries to learn a single aligned feature space for multiple modalities, including, but not limited to, images and videos. In the future, ImageBind can leverage the powerful visual features from DINOv2 to further improve its capabilities.

Learning a single embedding space by binding content with images

Humans have the ability to learn new concepts from only a few examples. We can typically read a description of an animal and then recognize it in real life. We can also look at a photo of an unfamiliar model of a car and anticipate how its engine might sound. This is partly because a single image, in fact, can “bind” together an entire sensory experience. In the field of AI, however, as the number of modalities increases, the lack of multiple sensory data can limit standard multimodal learning, which relies on paired data. Ideally, a single joint embedding space — where many different kinds of data are distributed — could allow a model to learn visual features along with other modalities.

Previously, learning such a joint embedding space for all modalities would require collecting all possible combinations of paired data — an infeasible feat.

ImageBind circumvented this challenge by leveraging recent large-scale vision-language models and extending their zero-shot capabilities to new modalities just by using their natural pairing with images, such as video-audio and image-depth data, to learn a single joint embedding space. For the four additional modalities (audio, depth, thermal, and IMU readings), we use naturally paired self-supervised data.

Training image-text models has been extensively studied because of the abundance of images and co-occurring text on the internet. ImageBind uses the binding property of images, meaning they co-occur with a variety of modalities and can serve as a bridge to connect them, such as linking text to image using web data or linking motion to video using video data captured from wearable cameras with IMU sensors.

The visual representations learned from large-scale web data can be used as targets to learn features for different modalities. This allows ImageBind to align any modality that co-occurs with images, naturally aligning those modalities among themselves. Modalities with a strong correlation to images, such as thermal and depth, are easier to align. Modalities that are not visual, such as audio and IMU, have a weaker correlation. Consider that there are particular sounds, like a baby’s cries, that could accompany any number of visual contexts.

ImageBind shows that image-paired data is sufficient to bind together these six modalities. The model can interpret content more holistically, allowing the different modalities to “talk” to each other and find links without observing them together. For example, ImageBind can associate audio and text without seeing them together. This enables other models to “understand” new modalities without any resource-intensive training. ImageBind’s strong scaling behavior allows the model to substitute or enhance many AI models by enabling them to use other modalities. For instance, while Make-A-Scene can generate images by using text prompts, ImageBind could upgrade it to generate images using audio sounds, such as laughter or rain.

ImageBind’s capabilities that outperform

Image-aligned, self-supervised learning shows that the performance of our model can actually improve by using very few training examples. Our model has new emergent capabilities, or scaling behavior — that is, abilities that didn’t exist in smaller models but appear in larger versions. This might include recognizing which audio fits with a certain image or predicting the depth of a scene from a photo.

Our analysis shows that ImageBind’s scaling behavior improves with the strength of the image encoder. In other words, ImageBind’s ability to align modalities increases with the strength and size of the vision model. This suggests that larger vision models benefit nonvision tasks, such as audio classification, and the benefits of training such models go beyond computer vision tasks.

Among our experiments, we used the audio and depth encoders from ImageBind and compared them with prior work in zero-shot retrieval as well as audio and depth classification tasks.

ImageBind outperformed specialist models in audio and depth, based on benchmarks.

We discovered that ImageBind features can be used for few-shot audio and depth classification tasks and can outperform prior methods tailored for those modalities. For example, ImageBind significantly outperforms Meta’s self-supervised AudioMAE model trained on Audioset and a supervised AudioMAE model fine-tuned on audio classification, with gains of approximately 40 percent accuracy in top-1 accuracy on ≤four-shot classification.

ImageBind also achieved new state-of-the-art performance on emergent zero-shot recognition tasks across modalities, even outperforming recent models that were trained to recognize concepts for that modality.

The future of multimodal learning

With the capability to use several modalities for input queries and retrieve outputs across other modalities, ImageBind shows new possibilities for creators. Imagine that someone could take a video recording of an ocean sunset and instantly add the perfect audio clip to enhance it, while an image of a brindle Shih Tzu could yield essays or depth models of similar dogs. Or when a model like Make-A-Video produces a video of a carnival, ImageBind can suggest background noise to accompany it, creating an immersive experience.

People could even segment and identify the objects in an image based on audio. This creates distinctive opportunities to create animations out of static images by combining them with audio prompts. For example, a creator could couple an image with an alarm clock and a rooster crowing, and use a crowing audio prompt to segment the rooster or the sound of an alarm to segment the clock and animate both into a video sequence.

While we explored six modalities in our current research, we believe that introducing new modalities that link as many senses as possible — like touch, speech, smell, and brain fMRI signals — will enable richer human-centric AI models.

There’s still a lot to uncover about multimodal learning. The AI research community has yet to effectively quantify scaling behaviors that appear only in larger models and understand their applications. ImageBind is a step toward evaluating them in a rigorous way and showing novel applications in image generation and retrieval.

We hope the research community will explore ImageBind and our accompanying published paper to find new ways to evaluate vision models and lead to novel applications.

Read the paper

See how ImageBind works

Access the ImageBind code"
Meta_Blog,https://ai.meta.com/blog/self-supervised-learning-practical-guide/,,The self-supervised learning cookbook,"Self-supervised learning (SSL), dubbed ""the dark matter of intelligence,” is a key ingredient in recent AI breakthroughs.

It has pushed the bounds of deep learning in multiple domains by enabling learning from vast amounts of unlabeled data, rather than relying on carefully annotated datasets. Today it underpins cutting-edge models across modalities in natural language (e.g. translation and large language models), audio (e.g., data2vec), and unlocks flexible new computer vision models (e.g. SEER model trained on one billion images and DINOv2).

But training SSL is like cooking a gourmet meal — it’s an intricate art with a high barrier to entry. While many ingredients may be familiar, a successful SSL recipe involves a dizzying set of choices, from selecting the right pretext tasks to training with carefully curated and seasoned hyper-parameters.

We have released a new ""Cookbook of Self-Supervised Learning,” a practical guide for AI researchers and practitioners on how to navigate SSL recipes, understand its various knobs and levers, and gain the know-how needed to experiment with SSL's untapped flavors. This is part of our efforts to lower the barrier and help democratize access to SSL research. You’ll also find tips and tricks from more than a dozen authors across multiple universities, including New York University, University of Maryland, UC Davis, University of Montreal; as well as leading Meta AI researchers, such as Yann LeCun.

The SSL specialty

Unlike supervised learning, in which the objective is to match inputs to labels, SSL can learn without labels by defining a learning objective based on the underlying structure of data, also known as a pretext task. In natural language, for example, a common SSL objective is to mask a word in the text and predict the surrounding words. This objective encourages the model to capture relationships among words in the text without the need for labels. The same SSL model representations can then be used across a range of downstream tasks, such as translating text across languages, summarizing, or even generating text, among many others. In computer vision, analogous objectives to predict masked patches of an image (MAE: masked autoencoders) or representation (BYOL: bootstrap your own latent). Other SSL objectives encourage two views of the same image, formed by say adding color or cropping, to be mapped to similar representations.

The elusivity of SSL recipes

While straightforward in principle, there’s a confluence of factors that lead to SSL’s difficult barrier to entry. First, the computational cost of processing vast volumes of unlabeled data is very high for both training and evaluation. Second, there aren’t many detailed papers showcasing the intricate implementation choices needed to realize SSL’s potential. Third, because SSL establishes a notably distinct paradigm, there’s an absence of a unified vocabulary and theoretical view of SSL. Without a common ground to characterize the different components, it’s challenging for researchers to understand, compare, and develop SSL methods.

Plus, from an implementation perspective, SSL is a fast-paced emerging field with each method taking on its own precisely tuned training recipe. Standard codebases are hard to find and they often use cutting edge, difficult-to-understand optimizations.

A cook’s guide to SSL

Our new paper lays the foundation of SSL and its recipes in a style that’s easy for any researcher to use.

Just as a cook first learns the basic techniques, like chopping and sautéing, researchers can use this cookbook to learn the fundamental techniques and vocabulary of SSL. Specifically, we describe the families of methods along with theoretical threads to connect their objectives in a unified perspective. You’ll find key concepts, such as loss terms or training objects, in easy-to-follow concept boxes.

Researchers can look at common training recipes, including hyper parameter choices, how to assemble components like architectures and optimizers, and how to evaluate SSL methods. You’ll find in one place the key practical considerations to implement SSL methods successfully.

Untapped potential

There are still numerous open research questions in SSL, including generalization guarantees, fairness properties, and robustness to adversarial attacks or even naturally occurring variations. The research community needs to better understand how seemingly different yet overlapping methods can produce state-of-the-art results, and more generally advance theoretical understanding of SSL and best practices for real-world deployment.

New researchers are needed to help tackle these open questions and continue to push the field forward. We hope that our SSL cookbook will help make this possible.

Get the SSL Cookbook

We’d like to acknowledge the contributions of: Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun and Micah Goldblum"
Meta_Blog,https://ai.meta.com/blog/self-supervised-learning-practical-guide/,,The self-supervised learning cookbook,"Self-supervised learning (SSL), dubbed ""the dark matter of intelligence,” is a key ingredient in recent AI breakthroughs.

It has pushed the bounds of deep learning in multiple domains by enabling learning from vast amounts of unlabeled data, rather than relying on carefully annotated datasets. Today it underpins cutting-edge models across modalities in natural language (e.g. translation and large language models), audio (e.g., data2vec), and unlocks flexible new computer vision models (e.g. SEER model trained on one billion images and DINOv2).

But training SSL is like cooking a gourmet meal — it’s an intricate art with a high barrier to entry. While many ingredients may be familiar, a successful SSL recipe involves a dizzying set of choices, from selecting the right pretext tasks to training with carefully curated and seasoned hyper-parameters.

We have released a new ""Cookbook of Self-Supervised Learning,” a practical guide for AI researchers and practitioners on how to navigate SSL recipes, understand its various knobs and levers, and gain the know-how needed to experiment with SSL's untapped flavors. This is part of our efforts to lower the barrier and help democratize access to SSL research. You’ll also find tips and tricks from more than a dozen authors across multiple universities, including New York University, University of Maryland, UC Davis, University of Montreal; as well as leading Meta AI researchers, such as Yann LeCun.

The SSL specialty

Unlike supervised learning, in which the objective is to match inputs to labels, SSL can learn without labels by defining a learning objective based on the underlying structure of data, also known as a pretext task. In natural language, for example, a common SSL objective is to mask a word in the text and predict the surrounding words. This objective encourages the model to capture relationships among words in the text without the need for labels. The same SSL model representations can then be used across a range of downstream tasks, such as translating text across languages, summarizing, or even generating text, among many others. In computer vision, analogous objectives to predict masked patches of an image (MAE: masked autoencoders) or representation (BYOL: bootstrap your own latent). Other SSL objectives encourage two views of the same image, formed by say adding color or cropping, to be mapped to similar representations.

The elusivity of SSL recipes

While straightforward in principle, there’s a confluence of factors that lead to SSL’s difficult barrier to entry. First, the computational cost of processing vast volumes of unlabeled data is very high for both training and evaluation. Second, there aren’t many detailed papers showcasing the intricate implementation choices needed to realize SSL’s potential. Third, because SSL establishes a notably distinct paradigm, there’s an absence of a unified vocabulary and theoretical view of SSL. Without a common ground to characterize the different components, it’s challenging for researchers to understand, compare, and develop SSL methods.

Plus, from an implementation perspective, SSL is a fast-paced emerging field with each method taking on its own precisely tuned training recipe. Standard codebases are hard to find and they often use cutting edge, difficult-to-understand optimizations.

A cook’s guide to SSL

Our new paper lays the foundation of SSL and its recipes in a style that’s easy for any researcher to use.

Just as a cook first learns the basic techniques, like chopping and sautéing, researchers can use this cookbook to learn the fundamental techniques and vocabulary of SSL. Specifically, we describe the families of methods along with theoretical threads to connect their objectives in a unified perspective. You’ll find key concepts, such as loss terms or training objects, in easy-to-follow concept boxes.

Researchers can look at common training recipes, including hyper parameter choices, how to assemble components like architectures and optimizers, and how to evaluate SSL methods. You’ll find in one place the key practical considerations to implement SSL methods successfully.

Untapped potential

There are still numerous open research questions in SSL, including generalization guarantees, fairness properties, and robustness to adversarial attacks or even naturally occurring variations. The research community needs to better understand how seemingly different yet overlapping methods can produce state-of-the-art results, and more generally advance theoretical understanding of SSL and best practices for real-world deployment.

New researchers are needed to help tackle these open questions and continue to push the field forward. We hope that our SSL cookbook will help make this possible.

Get the SSL Cookbook

We’d like to acknowledge the contributions of: Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun and Micah Goldblum"
Meta_Blog,https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/,,DINOv2: State-of-the-art computer vision models with self-supervised learning,"Something Went Wrong We're having trouble playing this video. Learn more

DINOv2 is able to take a video and generate a higher-quality segmentation than the original DINO method. DINOv2 allows remarkable properties to emerge, such as a robust understanding of object parts, and robust semantic and low-level understanding of images.

Meta AI has built DINOv2, a new method for training high-performance computer vision models.

DINOv2 delivers strong performance and does not require fine-tuning. This makes it suitable for use as a backbone for many different computer vision tasks.

Because it uses self-supervision, DINOv2 can learn from any collection of images. It can also learn features, such as depth estimation, that the current standard approach cannot.

We are open-sourcing our model and sharing an interactive demo.

Today, we are open-sourcing DINOv2, the first method for training computer vision models that uses self-supervised learning to achieve results that match or surpass the standard approach used in the field.

Self-supervised learning — the same method that’s used to create cutting-edge large language models for text applications — is a powerful, flexible way to train AI models because it does not require large amounts of labeled data. Like with other self-supervised systems, models using the DINOv2 method can be trained on any collection of images, without needing any associated metadata. Think of it as being able to learn from all the images it’s given, rather than only those that contain a specific set of hashtags or alt text or caption.

Unlike many recent reconstruction-based self-supervised learning methods, our model requires no fine-tuning. DINOv2 provides high-performance features that can be directly used as inputs for simple linear classifiers. This flexibility means DINOv2 can be used to create multipurpose backbones for many different computer vision tasks. Our measurements show very strong prediction capabilities on tasks such as classification, segmentation, and image retrieval. Surprisingly, on depth estimation, our features significantly outperform specialized state-of-the-art pipelines evaluated both in-domain and out-of-domain. We believe that this strong out-of-domain performance is due to the combination of self-supervised feature learning and the use of lightweight task-specific modules, such as linear classifiers. Finally, because we don’t resort to fine-tuning, the backbone remains general and the same features can be used simultaneously on many different tasks.

Self-supervised computer vision models like DINOv2 will be useful in a wide variety of applications. Meta collaborated with the World Resources Institute to use AI to map forests, tree by tree, across areas the size of continents. Our self-supervised model was trained on data from forests in North America, but evaluations confirm that it generalizes well and delivers accurate maps in other locations around the world.

DINOv2 complements our other recent computer vision research, including Segment Anything. Segment Anything is a promptable segmentation system focused on zero-shot generalization to diverse set of segmentation tasks. DINOv2 combines with simple linear classifiers to achieve strong results across multiple tasks beyond the segmentation sub-field, creating horizontal impact.

Overcoming the limitations of image-text pretraining

In recent years, a different technique, known as image-text pretraining, has been the standard approach for many computer vision tasks. But because the method relies on handwritten captions to learn the semantic content of an image, it ignores important information that typically isn’t explicitly mentioned in those text descriptions. For instance, a caption of a picture of a chair in a vast purple room might read “single oak chair.” Yet, the caption misses important information about the background, such as where the chair is spatially located in the purple room. Because of that, we believe caption-based features lack a proper understanding of local information and can lead to poor performance on downstream tasks requiring detailed localized information. Because DINOv2 is based on self-supervised learning, we avoid this problem by not relying on text descriptions. This, in turn, coupled with strong execution, allows DINOv2 to provide state-of-the-art results for monocular depth estimation. For context, monocular depth estimation is a task where the goal is to predict which objects are in the foreground and which are in the background.

In general, the need for human annotations of images is a bottleneck because it limits how much data you can use to train a model. In specialized application domains, images are hard or even impossible to label. Training machine learning models on labeled cellular imaging, for instance, is challenging, as there are a limited number of experts who can annotate the cells, and certainly not at the scale required. Self-supervised training on microscopic cellular imagery, however, opens up the way for foundational cell imagery models and, consequently, biological discovery, as it becomes possible to compare known treatments with new ones, for example. The same story holds for the estimation of animal density and abundance, allowing the identification of sources of biodiversity decline and the effectiveness of conservation efforts. Both examples are based on the original open source DINO algorithm, and we hope DINOv2 can improve such lines of work. DINOv2’s training stability and scalability will fuel further advances in applicative domains. One application already underway is our forest-mapping collaboration with the World Resources Institute noted above.

Our release comes at a time when the performance of joint embedding models that train features by matching data augmentations is plateauing. Specifically, the evaluation performance on ImageNet had moved by 10 percent between 2019 and 2021, and not much since then (+1 percent since 2021). The community focused more on developing alternatives, such as masked-image modeling, limiting progress in that field. In addition, the DINO class of models, among other SSL methods, was difficult to train outside of the classical scope of ImageNet, limiting their adoption for research.

Making progress from DINO to DINOv2 required overcoming several challenges: creating a large and curated training dataset, improving the training algorithm and implementation, and designing a functional distillation pipeline.

Building a large, curated, and diverse dataset to train the models

One of the key components of our work is training larger architectures, and to increase the performance, larger models require more data for training. But accessing more data is not always possible. With no sufficiently large curated dataset available to suit our needs, we looked into leveraging a publicly available repository of crawled web data and built a pipeline to select useful data inspired by LASER. Two key ingredients are required for building a large-scale pretraining dataset from such a source: discarding irrelevant images and balancing the dataset across concepts. Such delicate curation can’t realistically be done manually, and we wanted a method that allowed capturing distributions not easily associated with metadata. This was achieved by curating a set of seed images from a collection of about 25 third-party datasets and extending it by retrieving images sufficiently close to those seed images. This approach enabled us to produce a pretraining dataset totaling 142 million images out of the 1.2 billion source images.

Algorithmic and technical improvements

With more training data, larger models perform better than smaller ones, but their training poses two major challenges. First, increasing the model size makes the training more challenging because of potential instability. In DINOv2, we included additional regularization methods inspired by the similarity search and classification literature, making the training algorithm much more stable. Second, in order to remain tractable, larger models require more efficient implementations. The DINOv2 training code integrates the latest mixed-precision and distributed training implementations proposed in the cutting-edge PyTorch 2 (fully sharded data parallel), an efficient implementation of the stochastic depth technique, as well as the latest compute algorithm implementations of xFormers (in particular, variable-length memory-efficient attention). This allows faster and more efficient iteration cycles. Overall, with equivalent hardware, our code runs around twice as fast with only a third of the memory usage, allowing scaling in data, model size, and hardware.

Strong, lightweight models with distillation

Running inference for larger models requires more powerful hardware, potentially limiting many practical use cases. To circumvent this problem, researchers typically resort to model distillation, to compress the knowledge of a large model into a smaller one. Our training algorithm is based on self-distillation, making it straightforward to compress our large models into smaller ones. This procedure allows us to compress our highest-performance architecture into significantly smaller ones at only a minimal cost in accuracy, for a dramatically decreased inference cost, leading to remarkably strong ViT-Small, ViT-Base, and ViT-Large models.

The DINOv2 family of models drastically improves over the previous state of the art in self-supervised learning (SSL), and reaches performance comparable with weakly-supervised features (WSL).

Releasing a family of high-performance pretrained models

We release DINOv2 pretrained models to the community with a matching stable, accurate, and scaled implementation: We share pretraining code and recipe for ViT-L/16 (300 M params) and ViT-g/14 (1.1 B params) architectures, as well as checkpoints for a range of pretrained models from the larger ViT-g/14 down to smaller distilled models (ViT-S/14, ViT-B/14 and ViT-L/14). The performance of our approach is competitive or better than the performance of text-image models such as CLIP and OpenCLIP on a wide array of tasks, some of which are illustrated in our demo. Don’t hesitate to play with it! Our features can be used out of the box for nearest neighbor classification or paired with linear classification, yielding strong performance. DINOv2 allows skipping the model adaptation phase (fine-tuning) — our linear evaluation performance is close to their fine-tuned counterpart (within 2 percent on ImageNet-1k) .

Our features can be used out of the box for nearest neighbor classification or paired with linear classification, yielding strong performance. DINOv2 allows skipping the model adaptation phase (fine-tuning) — our linear evaluation performance is close to their fine-tuned counterpart (within 2 percent on ImageNet-1k) .

Going forward, the team plans to integrate this model, which can function as a building block, in a larger, more complex AI system that could interact with large language models. A visual backbone providing rich information on images will allow complex AI systems to reason on images in a deeper way than describing them with a single text sentence. Models trained with text supervision are ultimately limited by the image captions. With DINOv2, there is no such built-in limitation.

Read the paper

Get the code

Explore the demo"
Meta_Blog,https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/,,DINOv2: State-of-the-art computer vision models with self-supervised learning,"Something Went Wrong We're having trouble playing this video. Learn more

DINOv2 is able to take a video and generate a higher-quality segmentation than the original DINO method. DINOv2 allows remarkable properties to emerge, such as a robust understanding of object parts, and robust semantic and low-level understanding of images.

Meta AI has built DINOv2, a new method for training high-performance computer vision models.

DINOv2 delivers strong performance and does not require fine-tuning. This makes it suitable for use as a backbone for many different computer vision tasks.

Because it uses self-supervision, DINOv2 can learn from any collection of images. It can also learn features, such as depth estimation, that the current standard approach cannot.

We are open-sourcing our model and sharing an interactive demo.

Today, we are open-sourcing DINOv2, the first method for training computer vision models that uses self-supervised learning to achieve results that match or surpass the standard approach used in the field.

Self-supervised learning — the same method that’s used to create cutting-edge large language models for text applications — is a powerful, flexible way to train AI models because it does not require large amounts of labeled data. Like with other self-supervised systems, models using the DINOv2 method can be trained on any collection of images, without needing any associated metadata. Think of it as being able to learn from all the images it’s given, rather than only those that contain a specific set of hashtags or alt text or caption.

Unlike many recent reconstruction-based self-supervised learning methods, our model requires no fine-tuning. DINOv2 provides high-performance features that can be directly used as inputs for simple linear classifiers. This flexibility means DINOv2 can be used to create multipurpose backbones for many different computer vision tasks. Our measurements show very strong prediction capabilities on tasks such as classification, segmentation, and image retrieval. Surprisingly, on depth estimation, our features significantly outperform specialized state-of-the-art pipelines evaluated both in-domain and out-of-domain. We believe that this strong out-of-domain performance is due to the combination of self-supervised feature learning and the use of lightweight task-specific modules, such as linear classifiers. Finally, because we don’t resort to fine-tuning, the backbone remains general and the same features can be used simultaneously on many different tasks.

Self-supervised computer vision models like DINOv2 will be useful in a wide variety of applications. Meta collaborated with the World Resources Institute to use AI to map forests, tree by tree, across areas the size of continents. Our self-supervised model was trained on data from forests in North America, but evaluations confirm that it generalizes well and delivers accurate maps in other locations around the world.

DINOv2 complements our other recent computer vision research, including Segment Anything. Segment Anything is a promptable segmentation system focused on zero-shot generalization to diverse set of segmentation tasks. DINOv2 combines with simple linear classifiers to achieve strong results across multiple tasks beyond the segmentation sub-field, creating horizontal impact.

Overcoming the limitations of image-text pretraining

In recent years, a different technique, known as image-text pretraining, has been the standard approach for many computer vision tasks. But because the method relies on handwritten captions to learn the semantic content of an image, it ignores important information that typically isn’t explicitly mentioned in those text descriptions. For instance, a caption of a picture of a chair in a vast purple room might read “single oak chair.” Yet, the caption misses important information about the background, such as where the chair is spatially located in the purple room. Because of that, we believe caption-based features lack a proper understanding of local information and can lead to poor performance on downstream tasks requiring detailed localized information. Because DINOv2 is based on self-supervised learning, we avoid this problem by not relying on text descriptions. This, in turn, coupled with strong execution, allows DINOv2 to provide state-of-the-art results for monocular depth estimation. For context, monocular depth estimation is a task where the goal is to predict which objects are in the foreground and which are in the background.

In general, the need for human annotations of images is a bottleneck because it limits how much data you can use to train a model. In specialized application domains, images are hard or even impossible to label. Training machine learning models on labeled cellular imaging, for instance, is challenging, as there are a limited number of experts who can annotate the cells, and certainly not at the scale required. Self-supervised training on microscopic cellular imagery, however, opens up the way for foundational cell imagery models and, consequently, biological discovery, as it becomes possible to compare known treatments with new ones, for example. The same story holds for the estimation of animal density and abundance, allowing the identification of sources of biodiversity decline and the effectiveness of conservation efforts. Both examples are based on the original open source DINO algorithm, and we hope DINOv2 can improve such lines of work. DINOv2’s training stability and scalability will fuel further advances in applicative domains. One application already underway is our forest-mapping collaboration with the World Resources Institute noted above.

Our release comes at a time when the performance of joint embedding models that train features by matching data augmentations is plateauing. Specifically, the evaluation performance on ImageNet had moved by 10 percent between 2019 and 2021, and not much since then (+1 percent since 2021). The community focused more on developing alternatives, such as masked-image modeling, limiting progress in that field. In addition, the DINO class of models, among other SSL methods, was difficult to train outside of the classical scope of ImageNet, limiting their adoption for research.

Making progress from DINO to DINOv2 required overcoming several challenges: creating a large and curated training dataset, improving the training algorithm and implementation, and designing a functional distillation pipeline.

Building a large, curated, and diverse dataset to train the models

One of the key components of our work is training larger architectures, and to increase the performance, larger models require more data for training. But accessing more data is not always possible. With no sufficiently large curated dataset available to suit our needs, we looked into leveraging a publicly available repository of crawled web data and built a pipeline to select useful data inspired by LASER. Two key ingredients are required for building a large-scale pretraining dataset from such a source: discarding irrelevant images and balancing the dataset across concepts. Such delicate curation can’t realistically be done manually, and we wanted a method that allowed capturing distributions not easily associated with metadata. This was achieved by curating a set of seed images from a collection of about 25 third-party datasets and extending it by retrieving images sufficiently close to those seed images. This approach enabled us to produce a pretraining dataset totaling 142 million images out of the 1.2 billion source images.

Algorithmic and technical improvements

With more training data, larger models perform better than smaller ones, but their training poses two major challenges. First, increasing the model size makes the training more challenging because of potential instability. In DINOv2, we included additional regularization methods inspired by the similarity search and classification literature, making the training algorithm much more stable. Second, in order to remain tractable, larger models require more efficient implementations. The DINOv2 training code integrates the latest mixed-precision and distributed training implementations proposed in the cutting-edge PyTorch 2 (fully sharded data parallel), an efficient implementation of the stochastic depth technique, as well as the latest compute algorithm implementations of xFormers (in particular, variable-length memory-efficient attention). This allows faster and more efficient iteration cycles. Overall, with equivalent hardware, our code runs around twice as fast with only a third of the memory usage, allowing scaling in data, model size, and hardware.

Strong, lightweight models with distillation

Running inference for larger models requires more powerful hardware, potentially limiting many practical use cases. To circumvent this problem, researchers typically resort to model distillation, to compress the knowledge of a large model into a smaller one. Our training algorithm is based on self-distillation, making it straightforward to compress our large models into smaller ones. This procedure allows us to compress our highest-performance architecture into significantly smaller ones at only a minimal cost in accuracy, for a dramatically decreased inference cost, leading to remarkably strong ViT-Small, ViT-Base, and ViT-Large models.

The DINOv2 family of models drastically improves over the previous state of the art in self-supervised learning (SSL), and reaches performance comparable with weakly-supervised features (WSL).

Releasing a family of high-performance pretrained models

We release DINOv2 pretrained models to the community with a matching stable, accurate, and scaled implementation: We share pretraining code and recipe for ViT-L/16 (300 M params) and ViT-g/14 (1.1 B params) architectures, as well as checkpoints for a range of pretrained models from the larger ViT-g/14 down to smaller distilled models (ViT-S/14, ViT-B/14 and ViT-L/14). The performance of our approach is competitive or better than the performance of text-image models such as CLIP and OpenCLIP on a wide array of tasks, some of which are illustrated in our demo. Don’t hesitate to play with it! Our features can be used out of the box for nearest neighbor classification or paired with linear classification, yielding strong performance. DINOv2 allows skipping the model adaptation phase (fine-tuning) — our linear evaluation performance is close to their fine-tuned counterpart (within 2 percent on ImageNet-1k) .

Our features can be used out of the box for nearest neighbor classification or paired with linear classification, yielding strong performance. DINOv2 allows skipping the model adaptation phase (fine-tuning) — our linear evaluation performance is close to their fine-tuned counterpart (within 2 percent on ImageNet-1k) .

Going forward, the team plans to integrate this model, which can function as a building block, in a larger, more complex AI system that could interact with large language models. A visual backbone providing rich information on images will allow complex AI systems to reason on images in a deeper way than describing them with a single text sentence. Models trained with text supervision are ultimately limited by the image captions. With DINOv2, there is no such built-in limitation.

Read the paper

Get the code

Explore the demo"
Meta_Blog,https://ai.meta.com/blog/ai-dataset-animation-drawings/,,"A new, unique AI dataset for animating amateur drawings","From a young age, people express themselves and their creativity through drawing. We created an AI system research demo to easily bring artwork to life through animation, and we are now releasing the animation code along with a novel dataset of nearly 180,000 annotated amateur drawings to help other AI researchers and creators to innovate further. To our knowledge, this is the first annotated dataset to feature this kind of artwork.

Drawing is a near-universal way for people to capture a character, scene, or idea quickly. But while the content or meaning of a drawing is often clear to other human observers, an abstract or non-realistic appearance can make a drawing incomprehensible to AI models trained on images of real-life objects. To teach AI to recognize all the different ways someone might draw a humanlike figure would require a large dataset of sketches from budding artists. With the new dataset we are sharing today (described in detail in this research paper), researchers and practitioners can build tools to more easily and accurately analyze the contents of amateur drawings. And this can unlock new digital-physical hybrid experiences, such as new forms of storytelling and greater accessibility in art.

When we released our Animated Drawings Demo in late 2021, we invited people to opt in to contribute to a dataset of in-the-wild amateur drawings. The browser-based demo allowed people to upload images, verify or fix a few annotation predictions, and receive a short animation of their humanlike character within their drawing. More than 3.2 million people from around the world visited the site, including those who posted on social media about their creations. In total, 6.7 million images were uploaded to the demo. The drawings were created, photographed, and shared with Meta by participants in a de-identified manner. Human reviewers then filtered a subset of images that people had chosen to share with our research team.

Prior to releasing the Amateur Drawings Dataset, we performed several levels of filtration to ensure a high level of quality and implemented privacy safeguards, which are described in detail in our research paper.

While our demo allows for only a limited set of movements, many users of the Animated Drawings Demo provided feedback requesting more features, such as multiple characters, additional actions, smiling, blinking, and gazing cues. The GIF with dancing figures (see above) is an example of expanding upon the open source code and dataset for other creative and educational purposes. With these resources, other researchers can add to our methods of analyzing and augmenting amateur drawings to expand upon the original demo features.

Analyzing and understanding human imagination through drawings

The range of figure drawings is as wide as any person’s imagination. How do you train a model to perform well in the presence of such variation? One way would be to train new models using annotated drawings. However, such drawings are difficult to find in the numbers needed to train a neural network. Another approach would be to create the drawings synthetically. This is problematic as well. Generative methods require a large set of sample data to learn from, and style transfer methods (e.g., creating a “colored pencil” rendering of a photograph) may not capture all the nuanced ways in which a drawing differs from a photo. In addition, creating data synthetically may not capture all the relevant sources of nuisance variation actually seen in in-the-wild photographs of amateur drawings, such paper creases, erased lines, light glare, and shadows.

We structured the task of generating an animation from a single drawing of a figure as a series of subtasks: human figure detection, segmentation, pose estimation, and animation.

After someone uses our demo to upload a drawing, they have the option to adjust the detected bounding box, segmentation mask, and joint locations, and choose an action to animate.

Our system incorporates repurposed computer vision models trained on photographs of real-world objects. Because the domain of drawings, including that of children, is significantly different in appearance, we fine-tune the models using the Amateur Drawings Dataset.

With this dataset and animation code, we believe that the domain of amateur drawings can inspire a new generation of creators with its expressive and accessible possibilities. We hope they will be an asset to other researchers interested in exploring potential applications for their work.

How we collected the Amateur Drawings Dataset

For those in the AI community targeting any tool or algorithm that uses pen-and-paper drawings, this dataset is distinctive for its size and in-the-wild nature: it reflects real-world conditions (e.g., blurriness, hard shadows, crinkled surfaces, and background elements) that aren’t present in digital drawings and high-resolution scans. In addition to the images, the dataset includes annotations of bounding boxes, segmentation masks, and joint locations — features that could provide more ways for models to identify or animate drawn figures.

Here’s how we built the dataset. As part of the demo, people had the option to let us retain their uploaded image and annotations to be included in our ongoing research. As researchers, we respect the right of individuals to be cautious about sharing their data, and we wanted people to be able to animate their drawings either way. The data collection process was also designed with safety in mind. In doing so, we aimed to reduce the potential for misuse of the data as much as possible.

We also filtered the submitted images to ensure that they showed amateur drawings and met our standards for collecting research data responsibly. We performed this refinement in two steps. First, we used a self-supervised clustering approach to identify and filter out-of-domain images, such as photographs of real people. Second, a contracted agency manually reviewed the remaining images to ensure that they met our standards. Reviewers were instructed to check that images were freehand drawings on paper, with at least one full-body humanlike figure. They also checked to make sure images did not contain characters that were protected intellectual property or any private or vulgar content. Because the reviewers were primarily English speakers, images that contained non-English words were excluded on the basis that they might contain inappropriate content.

Inspiring creativity, and more ways to analyze and animate drawings

In keeping with our approach to open science, we are sharing the animation pipeline code and this dataset in hope that it will be of interest to other practitioners – both AI researchers and members of the broader research community.

Drawing is a natural and expressive modality that is accessible to most of the world’s population. We hope our work will make it easier for other researchers to explore tools and techniques specifically tailored to using AI to complement human creativity.

Read the technical paper

Access the animation code

Access the Amateur Drawings Dataset

Acknowledgements

We’d like to thank the FAIR Interfaces for their assistance in creating the original demo."
Meta_Blog,https://ai.meta.com/blog/ai-dataset-animation-drawings/,,"A new, unique AI dataset for animating amateur drawings","From a young age, people express themselves and their creativity through drawing. We created an AI system research demo to easily bring artwork to life through animation, and we are now releasing the animation code along with a novel dataset of nearly 180,000 annotated amateur drawings to help other AI researchers and creators to innovate further. To our knowledge, this is the first annotated dataset to feature this kind of artwork.

Drawing is a near-universal way for people to capture a character, scene, or idea quickly. But while the content or meaning of a drawing is often clear to other human observers, an abstract or non-realistic appearance can make a drawing incomprehensible to AI models trained on images of real-life objects. To teach AI to recognize all the different ways someone might draw a humanlike figure would require a large dataset of sketches from budding artists. With the new dataset we are sharing today (described in detail in this research paper), researchers and practitioners can build tools to more easily and accurately analyze the contents of amateur drawings. And this can unlock new digital-physical hybrid experiences, such as new forms of storytelling and greater accessibility in art.

When we released our Animated Drawings Demo in late 2021, we invited people to opt in to contribute to a dataset of in-the-wild amateur drawings. The browser-based demo allowed people to upload images, verify or fix a few annotation predictions, and receive a short animation of their humanlike character within their drawing. More than 3.2 million people from around the world visited the site, including those who posted on social media about their creations. In total, 6.7 million images were uploaded to the demo. The drawings were created, photographed, and shared with Meta by participants in a de-identified manner. Human reviewers then filtered a subset of images that people had chosen to share with our research team.

Prior to releasing the Amateur Drawings Dataset, we performed several levels of filtration to ensure a high level of quality and implemented privacy safeguards, which are described in detail in our research paper.

While our demo allows for only a limited set of movements, many users of the Animated Drawings Demo provided feedback requesting more features, such as multiple characters, additional actions, smiling, blinking, and gazing cues. The GIF with dancing figures (see above) is an example of expanding upon the open source code and dataset for other creative and educational purposes. With these resources, other researchers can add to our methods of analyzing and augmenting amateur drawings to expand upon the original demo features.

Analyzing and understanding human imagination through drawings

The range of figure drawings is as wide as any person’s imagination. How do you train a model to perform well in the presence of such variation? One way would be to train new models using annotated drawings. However, such drawings are difficult to find in the numbers needed to train a neural network. Another approach would be to create the drawings synthetically. This is problematic as well. Generative methods require a large set of sample data to learn from, and style transfer methods (e.g., creating a “colored pencil” rendering of a photograph) may not capture all the nuanced ways in which a drawing differs from a photo. In addition, creating data synthetically may not capture all the relevant sources of nuisance variation actually seen in in-the-wild photographs of amateur drawings, such paper creases, erased lines, light glare, and shadows.

We structured the task of generating an animation from a single drawing of a figure as a series of subtasks: human figure detection, segmentation, pose estimation, and animation.

After someone uses our demo to upload a drawing, they have the option to adjust the detected bounding box, segmentation mask, and joint locations, and choose an action to animate.

Our system incorporates repurposed computer vision models trained on photographs of real-world objects. Because the domain of drawings, including that of children, is significantly different in appearance, we fine-tune the models using the Amateur Drawings Dataset.

With this dataset and animation code, we believe that the domain of amateur drawings can inspire a new generation of creators with its expressive and accessible possibilities. We hope they will be an asset to other researchers interested in exploring potential applications for their work.

How we collected the Amateur Drawings Dataset

For those in the AI community targeting any tool or algorithm that uses pen-and-paper drawings, this dataset is distinctive for its size and in-the-wild nature: it reflects real-world conditions (e.g., blurriness, hard shadows, crinkled surfaces, and background elements) that aren’t present in digital drawings and high-resolution scans. In addition to the images, the dataset includes annotations of bounding boxes, segmentation masks, and joint locations — features that could provide more ways for models to identify or animate drawn figures.

Here’s how we built the dataset. As part of the demo, people had the option to let us retain their uploaded image and annotations to be included in our ongoing research. As researchers, we respect the right of individuals to be cautious about sharing their data, and we wanted people to be able to animate their drawings either way. The data collection process was also designed with safety in mind. In doing so, we aimed to reduce the potential for misuse of the data as much as possible.

We also filtered the submitted images to ensure that they showed amateur drawings and met our standards for collecting research data responsibly. We performed this refinement in two steps. First, we used a self-supervised clustering approach to identify and filter out-of-domain images, such as photographs of real people. Second, a contracted agency manually reviewed the remaining images to ensure that they met our standards. Reviewers were instructed to check that images were freehand drawings on paper, with at least one full-body humanlike figure. They also checked to make sure images did not contain characters that were protected intellectual property or any private or vulgar content. Because the reviewers were primarily English speakers, images that contained non-English words were excluded on the basis that they might contain inappropriate content.

Inspiring creativity, and more ways to analyze and animate drawings

In keeping with our approach to open science, we are sharing the animation pipeline code and this dataset in hope that it will be of interest to other practitioners – both AI researchers and members of the broader research community.

Drawing is a natural and expressive modality that is accessible to most of the world’s population. We hope our work will make it easier for other researchers to explore tools and techniques specifically tailored to using AI to complement human creativity.

Read the technical paper

Access the animation code

Access the Amateur Drawings Dataset

Acknowledgements

We’d like to thank the FAIR Interfaces for their assistance in creating the original demo."
Meta_Blog,https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/,,Introducing Segment Anything: Working toward the first foundation model for image segmentation,"Something Went Wrong We're having trouble playing this video. Learn more

Segmentation — identifying which image pixels belong to an object — is a core task in computer vision and is used in a broad array of applications, from analyzing scientific imagery to editing photos. But creating an accurate segmentation model for specific tasks typically requires highly specialized work by technical experts with access to AI training infrastructure and large volumes of carefully annotated in-domain data.

Today, we aim to democratize segmentation by introducing the Segment Anything project: a new task, dataset, and model for image segmentation, as we explain in our research paper. We are releasing both our general Segment Anything Model (SAM) and our Segment Anything 1-Billion mask dataset (SA-1B), the largest ever segmentation dataset, to enable a broad set of applications and foster further research into foundation models for computer vision. We are making the SA-1B dataset available for research purposes and the Segment Anything Model is available under a permissive open license (Apache 2.0). Check out the demo to try SAM with your own images.

Reducing the need for task-specific modeling expertise, training compute, and custom data annotation for image segmentation is at the core of the Segment Anything project. To realize this vision, our goal was to build a foundation model for image segmentation: a promptable model that is trained on diverse data and that can adapt to specific tasks, analogous to how prompting is used in natural language processing models. However, the segmentation data needed to train such a model is not readily available online or elsewhere, unlike images, videos, and text, which are abundant on the internet. Thus, with Segment Anything, we set out to simultaneously develop a general, promptable segmentation model and use it to create a segmentation dataset of unprecedented scale.

SAM has learned a general notion of what objects are, and it can generate masks for any object in any image or any video, even including objects and image types that it had not encountered during training. SAM is general enough to cover a broad set of use cases and can be used out of the box on new image “domains” — whether underwater photos or cell microscopy — without requiring additional training (a capability often referred to as zero-shot transfer).

In the future, SAM could be used to help power applications in numerous domains that require finding and segmenting any object in any image. For the AI research community and others, SAM could become a component in larger AI systems for more general multimodal understanding of the world, for example, understanding both the visual and text content of a webpage. In the AR/VR domain, SAM could enable selecting an object based on a user’s gaze and then “lifting” it into 3D. For content creators, SAM can improve creative applications such as extracting image regions for collages or video editing. SAM could also be used to aid scientific study of natural occurrences on Earth or even in space, for example, by localizing animals or objects to study and track in video. We believe the possibilities are broad, and we are excited by the many potential use cases we haven’t even imagined yet.

Something Went Wrong We're having trouble playing this video. Learn more

Segment Anything’s promptable design enables flexible integration with other systems. SAM could receive input prompts, such as a user’s gaze from an AR/VR headset, like Project Aria

SAM: A generalized approach to segmentation

Previously, to solve any kind of segmentation problem, there were two classes of approaches. The first, interactive segmentation, allowed for segmenting any class of object but required a person to guide the method by iteratively refining a mask. The second, automatic segmentation, allowed for segmentation of specific object categories defined ahead of time (e.g., cats or chairs) but required substantial amounts of manually annotated objects to train (e.g., thousands or even tens of thousands of examples of segmented cats), along with the compute resources and technical expertise to train the segmentation model. Neither approach provided a general, fully automatic approach to segmentation.

SAM is a generalization of these two classes of approaches. It is a single model that can easily perform both interactive segmentation and automatic segmentation. The model’s promptable interface (described shortly) allows it to be used in flexible ways that make a wide range of segmentation tasks possible simply by engineering the right prompt for the model (clicks, boxes, text, and so on). Moreover, SAM is trained on a diverse, high-quality dataset of over 1 billion masks (collected as part of this project), which enables it to generalize to new types of objects and images beyond what it observed during training. This ability to generalize means that, by and large, practitioners will no longer need to collect their own segmentation data and fine-tune a model for their use case.

Taken together, these capabilities enable SAM to generalize both to new tasks and to new domains. This flexibility is the first of its kind for image segmentation.

Here is a short video showcasing some of SAM’s capabilities:

Something Went Wrong We're having trouble playing this video. Learn more

(1) SAM allows users to segment objects with just a click or by interactively clicking points to include and exclude from the object. The model can also be prompted with a bounding box.

(2) SAM can output multiple valid masks when faced with ambiguity about the object being segmented, an important and necessary capability for solving segmentation in the real world.

(3) SAM can automatically find and mask all objects in an image.

(4) SAM can generate a segmentation mask for any prompt in real time after precomputing the image embedding, allowing for real-time interaction with the model.

How SAM works: Promptable segmentation

In natural language processing and, more recently, computer vision, one of the most exciting developments is that of foundation models that can perform zero-shot and few-shot learning for new datasets and tasks using “prompting” techniques. We took inspiration from this line of work.

We trained SAM to return a valid segmentation mask for any prompt, where a prompt can be foreground/background points, a rough box or mask, freeform text, or, in general, any information indicating what to segment in an image. The requirement of a valid mask simply means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for one of those objects. This task is used to pretrain the model and to solve general downstream segmentation tasks via prompting.

We observed that the pretraining task and interactive data collection imposed specific constraints on the model design. In particular, the model needs to run in real time on a CPU in a web browser to allow our annotators to use SAM interactively in real time to annotate efficiently. While the runtime constraint implies a trade-off between quality and runtime, we find that a simple design yields good results in practice.

Under the hood, an image encoder produces a one-time embedding for the image, while a lightweight encoder converts any prompt into an embedding vector in real time. These two information sources are then combined in a lightweight decoder that predicts segmentation masks. After the image embedding is computed, SAM can produce a segment in just 50 milliseconds given any prompt in a web browser.

In a web browser, SAM efficiently maps the image features and a set of prompt embeddings to produce a segmentation mask.

Segmenting 1 billion masks: How we built SA-1B

To train our model, we needed a massive and diverse source of data, which did not exist at the start of our work. The segmentation dataset we are releasing today is the largest to date (by far). The data was collected using SAM. In particular, annotators used SAM to interactively annotate images, and then the newly annotated data was used to update SAM in turn. We repeated this cycle many times to iteratively improve both the model and dataset.

With SAM, collecting new segmentation masks is faster than ever before. With our tool, it only takes about 14 seconds to interactively annotate a mask. Our per-mask annotation process is only 2x slower than annotating bounding boxes, which takes about 7 seconds using the fastest annotation interfaces. In comparison with previous large-scale segmentation data collection efforts, our model is 6.5x faster than COCO fully manual polygon-based mask annotation and 2x faster than the previous largest data annotation effort, which was also model-assisted.

However, relying on interactively annotating masks does not scale sufficiently to create our 1 billion mask dataset. Therefore, we built a data engine for creating our SA-1B dataset. This data engine has three “gears.” In the first gear, the model assists annotators, as described above. The second gear is a mix of fully automatic annotation combined with assisted annotation, helping increase the diversity of collected masks. The last gear of the data engine is fully automatic mask creation, allowing our dataset to scale.

Our final dataset includes more than 1.1 billion segmentation masks collected on about 11 million licensed and privacy-preserving images. SA-1B has 400x more masks than any existing segmentation dataset, and as verified by human evaluation studies, the masks are of high quality and diversity, and in some cases even comparable in quality to masks from the previous much smaller, fully manually annotated datasets.

Segment Anything’s capabilities are the result of training on millions of images and masks collected using a data engine. The result is a dataset of more than 1 billion segmentation masks – 400x larger than any prior segmentation dataset.

Images for SA-1B were sourced via a photo provider from multiple countries that span a diverse set of geographic regions and income levels. While we recognize that certain geographic regions are still underrepresented, SA-1B has a larger number of images and overall better representation across all regions than previous segmentation datasets. Moreover, we analyzed potential biases of our model across the perceived gender presentation, perceived skin tone and perceived age range of people, and we found that SAM performs similarly across different groups. Together, we hope this will make our work more equitable for use in real-world use cases.

While SA-1B made our research possible, it can also enable other researchers to train foundation models for image segmentation. We further hope that this data can become a basis for new datasets with additional annotations, such as a text description associated with each mask.

What lies ahead

In the future, SAM could be used to identify everyday items via AR glasses that could prompt users with reminders and instructions.

SAM has the potential to impact a wide range of domains — perhaps one day helping farmers in the agricultural sector or assisting biologists in their research.

By sharing our research and dataset, we hope to further accelerate research into segmentation and more general image and video understanding. Our promptable segmentation model can perform a segmentation task by acting as a component in a larger system. Composition is a powerful tool that allows a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. We anticipate that composable system design, enabled by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks, and that SAM can become a powerful component in domains such as AR/VR, content creation, scientific domains, and more general AI systems. And as we look ahead, we see tighter coupling between understanding images at the pixel level and higher-level semantic understanding of visual content, unlocking even more powerful AI systems.

Try the Segment Anything demo

Learn more about SA-1B

Download SAM

Read the paper"
Meta_Blog,https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/,,Introducing Segment Anything: Working toward the first foundation model for image segmentation,"Something Went Wrong We're having trouble playing this video. Learn more

Segmentation — identifying which image pixels belong to an object — is a core task in computer vision and is used in a broad array of applications, from analyzing scientific imagery to editing photos. But creating an accurate segmentation model for specific tasks typically requires highly specialized work by technical experts with access to AI training infrastructure and large volumes of carefully annotated in-domain data.

Today, we aim to democratize segmentation by introducing the Segment Anything project: a new task, dataset, and model for image segmentation, as we explain in our research paper. We are releasing both our general Segment Anything Model (SAM) and our Segment Anything 1-Billion mask dataset (SA-1B), the largest ever segmentation dataset, to enable a broad set of applications and foster further research into foundation models for computer vision. We are making the SA-1B dataset available for research purposes and the Segment Anything Model is available under a permissive open license (Apache 2.0). Check out the demo to try SAM with your own images.

Reducing the need for task-specific modeling expertise, training compute, and custom data annotation for image segmentation is at the core of the Segment Anything project. To realize this vision, our goal was to build a foundation model for image segmentation: a promptable model that is trained on diverse data and that can adapt to specific tasks, analogous to how prompting is used in natural language processing models. However, the segmentation data needed to train such a model is not readily available online or elsewhere, unlike images, videos, and text, which are abundant on the internet. Thus, with Segment Anything, we set out to simultaneously develop a general, promptable segmentation model and use it to create a segmentation dataset of unprecedented scale.

SAM has learned a general notion of what objects are, and it can generate masks for any object in any image or any video, even including objects and image types that it had not encountered during training. SAM is general enough to cover a broad set of use cases and can be used out of the box on new image “domains” — whether underwater photos or cell microscopy — without requiring additional training (a capability often referred to as zero-shot transfer).

In the future, SAM could be used to help power applications in numerous domains that require finding and segmenting any object in any image. For the AI research community and others, SAM could become a component in larger AI systems for more general multimodal understanding of the world, for example, understanding both the visual and text content of a webpage. In the AR/VR domain, SAM could enable selecting an object based on a user’s gaze and then “lifting” it into 3D. For content creators, SAM can improve creative applications such as extracting image regions for collages or video editing. SAM could also be used to aid scientific study of natural occurrences on Earth or even in space, for example, by localizing animals or objects to study and track in video. We believe the possibilities are broad, and we are excited by the many potential use cases we haven’t even imagined yet.

Something Went Wrong We're having trouble playing this video. Learn more

Segment Anything’s promptable design enables flexible integration with other systems. SAM could receive input prompts, such as a user’s gaze from an AR/VR headset, like Project Aria

SAM: A generalized approach to segmentation

Previously, to solve any kind of segmentation problem, there were two classes of approaches. The first, interactive segmentation, allowed for segmenting any class of object but required a person to guide the method by iteratively refining a mask. The second, automatic segmentation, allowed for segmentation of specific object categories defined ahead of time (e.g., cats or chairs) but required substantial amounts of manually annotated objects to train (e.g., thousands or even tens of thousands of examples of segmented cats), along with the compute resources and technical expertise to train the segmentation model. Neither approach provided a general, fully automatic approach to segmentation.

SAM is a generalization of these two classes of approaches. It is a single model that can easily perform both interactive segmentation and automatic segmentation. The model’s promptable interface (described shortly) allows it to be used in flexible ways that make a wide range of segmentation tasks possible simply by engineering the right prompt for the model (clicks, boxes, text, and so on). Moreover, SAM is trained on a diverse, high-quality dataset of over 1 billion masks (collected as part of this project), which enables it to generalize to new types of objects and images beyond what it observed during training. This ability to generalize means that, by and large, practitioners will no longer need to collect their own segmentation data and fine-tune a model for their use case.

Taken together, these capabilities enable SAM to generalize both to new tasks and to new domains. This flexibility is the first of its kind for image segmentation.

Here is a short video showcasing some of SAM’s capabilities:

Something Went Wrong We're having trouble playing this video. Learn more

(1) SAM allows users to segment objects with just a click or by interactively clicking points to include and exclude from the object. The model can also be prompted with a bounding box.

(2) SAM can output multiple valid masks when faced with ambiguity about the object being segmented, an important and necessary capability for solving segmentation in the real world.

(3) SAM can automatically find and mask all objects in an image.

(4) SAM can generate a segmentation mask for any prompt in real time after precomputing the image embedding, allowing for real-time interaction with the model.

How SAM works: Promptable segmentation

In natural language processing and, more recently, computer vision, one of the most exciting developments is that of foundation models that can perform zero-shot and few-shot learning for new datasets and tasks using “prompting” techniques. We took inspiration from this line of work.

We trained SAM to return a valid segmentation mask for any prompt, where a prompt can be foreground/background points, a rough box or mask, freeform text, or, in general, any information indicating what to segment in an image. The requirement of a valid mask simply means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for one of those objects. This task is used to pretrain the model and to solve general downstream segmentation tasks via prompting.

We observed that the pretraining task and interactive data collection imposed specific constraints on the model design. In particular, the model needs to run in real time on a CPU in a web browser to allow our annotators to use SAM interactively in real time to annotate efficiently. While the runtime constraint implies a trade-off between quality and runtime, we find that a simple design yields good results in practice.

Under the hood, an image encoder produces a one-time embedding for the image, while a lightweight encoder converts any prompt into an embedding vector in real time. These two information sources are then combined in a lightweight decoder that predicts segmentation masks. After the image embedding is computed, SAM can produce a segment in just 50 milliseconds given any prompt in a web browser.

In a web browser, SAM efficiently maps the image features and a set of prompt embeddings to produce a segmentation mask.

Segmenting 1 billion masks: How we built SA-1B

To train our model, we needed a massive and diverse source of data, which did not exist at the start of our work. The segmentation dataset we are releasing today is the largest to date (by far). The data was collected using SAM. In particular, annotators used SAM to interactively annotate images, and then the newly annotated data was used to update SAM in turn. We repeated this cycle many times to iteratively improve both the model and dataset.

With SAM, collecting new segmentation masks is faster than ever before. With our tool, it only takes about 14 seconds to interactively annotate a mask. Our per-mask annotation process is only 2x slower than annotating bounding boxes, which takes about 7 seconds using the fastest annotation interfaces. In comparison with previous large-scale segmentation data collection efforts, our model is 6.5x faster than COCO fully manual polygon-based mask annotation and 2x faster than the previous largest data annotation effort, which was also model-assisted.

However, relying on interactively annotating masks does not scale sufficiently to create our 1 billion mask dataset. Therefore, we built a data engine for creating our SA-1B dataset. This data engine has three “gears.” In the first gear, the model assists annotators, as described above. The second gear is a mix of fully automatic annotation combined with assisted annotation, helping increase the diversity of collected masks. The last gear of the data engine is fully automatic mask creation, allowing our dataset to scale.

Our final dataset includes more than 1.1 billion segmentation masks collected on about 11 million licensed and privacy-preserving images. SA-1B has 400x more masks than any existing segmentation dataset, and as verified by human evaluation studies, the masks are of high quality and diversity, and in some cases even comparable in quality to masks from the previous much smaller, fully manually annotated datasets.

Segment Anything’s capabilities are the result of training on millions of images and masks collected using a data engine. The result is a dataset of more than 1 billion segmentation masks – 400x larger than any prior segmentation dataset.

Images for SA-1B were sourced via a photo provider from multiple countries that span a diverse set of geographic regions and income levels. While we recognize that certain geographic regions are still underrepresented, SA-1B has a larger number of images and overall better representation across all regions than previous segmentation datasets. Moreover, we analyzed potential biases of our model across the perceived gender presentation, perceived skin tone and perceived age range of people, and we found that SAM performs similarly across different groups. Together, we hope this will make our work more equitable for use in real-world use cases.

While SA-1B made our research possible, it can also enable other researchers to train foundation models for image segmentation. We further hope that this data can become a basis for new datasets with additional annotations, such as a text description associated with each mask.

What lies ahead

In the future, SAM could be used to identify everyday items via AR glasses that could prompt users with reminders and instructions.

SAM has the potential to impact a wide range of domains — perhaps one day helping farmers in the agricultural sector or assisting biologists in their research.

By sharing our research and dataset, we hope to further accelerate research into segmentation and more general image and video understanding. Our promptable segmentation model can perform a segmentation task by acting as a component in a larger system. Composition is a powerful tool that allows a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. We anticipate that composable system design, enabled by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks, and that SAM can become a powerful component in domains such as AR/VR, content creation, scientific domains, and more general AI systems. And as we look ahead, we see tighter coupling between understanding images at the pixel level and higher-level semantic understanding of visual content, unlocking even more powerful AI systems.

Try the Segment Anything demo

Learn more about SA-1B

Download SAM

Read the paper"
Meta_Blog,https://ai.meta.com/blog/robots-learning-video-simulation-artificial-visual-cortex-vc-1/,,Robots that learn from videos of human activities and simulated interactions,"Something Went Wrong We're having trouble playing this video. Learn more

Optimistic science fiction typically imagines a future where humans create art and pursue fulfilling pastimes while AI-enabled robots handle dull or dangerous tasks. In contrast, the AI systems of today display increasingly sophisticated generative abilities on ostensible creative tasks. But where are the robots? This gap is known as Moravec’s paradox, the thesis that the hardest problems in AI involve sensorimotor skills, not abstract thought or reasoning. To put it another way, “The hard problems are easy, and the easy problems are hard.”

Today, we are announcing two major advancements toward general-purpose embodied AI agents capable of performing challenging sensorimotor skills:

An artificial visual cortex (called VC-1): a single perception model that, for the first time, supports a diverse range of sensorimotor skills, environments, and embodiments. VC-1 is trained on videos of people performing everyday tasks from the groundbreaking Ego4D dataset created by Meta AI and academic partners. And VC-1 matches or outperforms best-known results on 17 different sensorimotor tasks in virtual environments.

A new approach called adaptive (sensorimotor) skill coordination (ASC), which achieves near-perfect performance (98 percent success) on the challenging task of robotic mobile manipulation (navigating to an object, picking it up, navigating to another location, placing the object, repeating) in physical environments.

Data powers both of these breakthroughs. AI needs data to learn from — and, specifically, embodied AI needs data that captures interactions with the environment. Traditionally, this interaction data is collected either by collecting large amounts of demonstrations or by allowing the robot to learn from interactions from scratch. Both approaches are too resource-intensive to scale toward the learning of a general embodied AI agent. In both of these works, we are developing new ways for robots to learn, using videos of human interactions with the real world and simulated interactions within photorealistic simulated worlds.

First, we’ve built a way for robots to learn from real-world human interactions, by training a general-purpose visual representation model (an artificial visual cortex) from a large number of egocentric videos. The videos include our open source Ego4D dataset, which shows first-person views of people doing everyday tasks, like going to the grocery store and cooking lunch. Second, we’ve built a way to pretrain our robot to perform long-horizon rearrangement tasks in simulation. Specifically, we train a policy in Habitat environments and transfer the policy zero-shot to a real Spot robot to perform such tasks in unfamiliar real-world spaces.

Toward an artificial visual cortex for embodied intelligence

A visual cortex is the region of the brain that (together with the motor cortex) enables an organism to convert vision into movement. We are interested in developing an artificial visual cortex — the module in an AI system that enables an artificial agent to convert camera input into actions.

Our FAIR team, together with academic collaborators, has been at the forefront of developing general-purpose visual representations for embodied AI trained from egocentric video datasets. The Ego4D dataset has been especially useful, since it contains thousands of hours of wearable camera video from research participants around the world performing daily life activities, including cooking, cleaning, sports, and crafts.

For instance, one prior work from our team (R3M) uses temporal and text-video alignment within Ego4D video frames to learn compact universal visual representations for robotic manipulation. Another work (VIP) uses Ego4D frames to learn an effective actionable visual representation that can also perform zero-shot visual reward-specification for training embodied agents. These are illustrative of the broader trend in the research community (e.g., PVR, OVRL, MVP) toward pretraining visual representations from web images and egocentric videos.

Although prior work has focused on a small set of robotic tasks, a visual cortex for embodied AI should work well for a diverse set of sensorimotor tasks in diverse environments across diverse embodiments. While prior works on pretraining visual representations give us a glimpse of what may be feasible, they are fundamentally incommensurable, with different ways of pretraining the visual representations on different datasets, evaluated on different embodied AI tasks. The lack of consistency meant there was no way of knowing which of the existing pretrained visual representations were best.

As a first step, we curated CortexBench, consisting of 17 different sensorimotor tasks in simulation, spanning locomotion, navigation, and dexterous and mobile manipulation, implementing the community standard for learning the policy for each task. The visual environments span from flat infinite planes to tabletop settings to photorealistic 3D scans of real-world indoor spaces. The agent embodiments vary from stationary arms to dexterous hands to idealized cylindrical navigation agents to articulated mobile manipulators. The learning conditions vary from few-shot imitation learning to large-scale reinforcement learning. This allowed us to perform a rigorous and consistent evaluation of existing and new pretrained models. Prior to our work, the best performance for each task in CortexBench was achieved by a model or algorithm specifically designed for that task. In contrast, what we want is one model and/or algorithm that achieves competitive performance on all tasks. Biological organisms have one general-purpose visual cortex, and that is what we seek for embodied agents.

We set out to pretrain a single general-purpose visual cortex that can perform well on all of these tasks. A critical choice for pretraining is the choice of dataset. It was entirely unclear what a good pretraining dataset for embodied AI would look like. There are massive amounts of video data available online, yet it isn’t practical to try out all combinations of those existing datasets.

We start with Ego4D as our core dataset and then explore whether adding additional datasets improves pretrained models. Having egocentric video is important because it enables robots to learn to see from a first-person perspective. Since Ego4D is heavily focused on everyday activities like cooking, gardening, and crafting, we also consider egocentric video datasets that explore houses and apartments. Finally, we also study whether static image datasets help improve our models.

Cumulatively, our work represents the largest and most comprehensive empirical study to date of visual representations for embodied AI, spanning 5+ pretrained visual representations from prior work, and multiple ablations of VC-1 trained on 4,000+ hours of egocentric human video from seven diverse datasets, which required over 10,000 GPU-hours of training and evaluation.

Today, we are open-sourcing VC-1, our best visual cortex model following FAIR’s values of open research for the benefit of all. Our results show VC-1 representations match or outperform learning from scratch on all 17 tasks. We also find that adapting VC-1 on task-relevant data results in it becoming competitive with or outperforming best-known results on all tasks in CortexBench. To the best of our knowledge, VC-1 is the first visual pretrained model that has shown to be competitive with state-of-the art results on such a diverse set of embodied AI tasks. We are sharing our detailed learnings, such as how scaling model size, dataset size, and diversity impact performance of pretrained models, in a related research paper.

Adaptive skill coordination for robotic mobile manipulation

While VC-1 demonstrates strong performance on sensorimotor skills in CortexBench, these are short-horizon tasks (navigating, picking up an object, in-hand manipulation of an object, etc.). The next generation of embodied AI agents (deployed on robots) will also need to accomplish long-horizon tasks and adapt to new and changing environments, including unexpected real-world disturbances.

Our second announcement focuses on mobile pick-and-place — a robot is initialized in a new environment and tasked with moving objects from initial to desired locations, emulating the task of tidying a house. The robot must navigate to a receptacle with objects, like the kitchen counter (the approximate location is provided to it), search for and pick an object, navigate to its desired place receptacle, place the object, and repeat.

Something Went Wrong We're having trouble playing this video. Learn more

To tackle such long-horizon tasks, we and our collaborators at Georgia Tech developed a new technique called Adaptive Skill Coordination (ASC), which consists of three components:

A library of basic sensorimotor skills (navigation, pick, place)

A skill coordination policy that chooses which skills are appropriate to use at which time

A corrective policy that adapts pretrained skills when out-of-distribution states are perceived

All sensorimotor policies are “model-free.” We use sensor-to-actions neural networks with no task-specific modules, like mapping or planning. The robot is trained entirely in simulation and transferred to the physical world without any real-world training data.

Something Went Wrong We're having trouble playing this video. Learn more

We demonstrate the effectiveness of ASC by deploying it on the Boston Dynamics' Spot robot in new/unknown real-world environments. We chose the Boston Dynamics Spot robot because of robust sensing, navigation, and manipulation capabilities. However, operating Spot today involves a large amount of human intervention. For example, picking an object requires a person to click on the object on the robot’s tablet. Our aim is to build AI models that can sense the world from onboard sensing and motor commands through Boston Dynamics APIs.

Using the Habitat simulator, and the HM3D and ReplicaCAD datasets, which include indoor 3D scans of 1,000 homes, we teach a simulated Spot robot to move around an unseen house, pick up out-of-place objects, and put them in the right location. Next, we deploy this policy zero-shot in the real-world (sim2real) without explicitly building a map in the real world, and instead rely on our robot to use its learned notion of what houses look like.

When we put our work to the test, we used two significantly different real-world environments where Spot was asked to rearrange a variety of objects — a fully furnished 185-square-meter apartment and a 65-square-meter university lab. Overall, ASC achieved near-perfect performance, succeeding on 59 of 60 (98 percent) episodes, overcoming hardware instabilities, picking failures, and adversarial disturbances like moving obstacles or blocked paths. In comparison, traditional baselines like task and motion planning succeed in only 73 percent of cases, because of an inability to recover from real-world disturbances. We also study robustness to adversarial perturbations, such as changing the layout of the environment, walking in front of the robot to repeatedly block its path, or moving target objects mid-episode. Despite being trained entirely in simulation, ASC is robust to such disturbances, making it well suited for many long-horizon problems in robotics and reinforcement learning.

This opens avenues for sim2real research to expand to even more challenging real-world tasks, such as assistance in everyday tasks like cooking and cleaning, and even human-robot collaboration. Our work is a step toward scalable, robust, and diverse robot assistants of the future that can operate in new environments out of the box and do not require expensive real-world data collection.

Rethinking sim2real transfer

One of the most important tasks in sim2real learning is to build simulation models that truthfully reflect the robot’s behavior in the real world. However, this is challenging, since the real world is vast and constantly changing, and the simulator needs to capture this diversity. No simulator is a perfect replica of reality, and the main challenge is overcoming the gap between the robot’s performance in simulation and in the real world. The default operating hypothesis of this field is that reducing the sim2real gap involves creating simulators of high physics fidelity and using them to learn robot policies.

Over the past year, we have taken a counterintuitive approach to sim2real. Instead of building high-fidelity simulations of the world, we built an abstracted simulator of Spot, which does not model low-level physics in simulation, and learn a policy that can reason on a higher level (like where to go rather than how to move the legs). We call this a kinematic simulation, where the robot is teleported to a location and the target object is attached to the robot arm, when it is close to the gripper and in view. In the real world, Boston Dynamics controllers are used for achieving the actions commanded by this high-level policy.

Something Went Wrong We're having trouble playing this video. Learn more

Robots pretrained in sim2real have mostly been limited to short-horizon tasks and visual navigation, without any interaction with the environment. Mobile pick-and-place is a long-horizon task, and it requires interacting with the environment and switching between different phases of navigation, picking, placing, etc. This is typically very challenging for reinforcement learning, and requires demonstrations, or sophisticated hand-designed rewards. Our high-level abstraction and kinematic simulation let us learn long-horizon tasks, with sparse rewards, without requiring to reason about low-level physics.

Future areas of exploration

While we haven’t yet applied visual cortex to our object rearrangement robot, we hope to integrate it into a single system. With so many unpredictable variables in the real world, having strong visual representations and pretraining on a diverse number of egocentric videos showing many different activities and environments will be an important step toward building even better robots.

Voice is one area we are particularly interested in exploring. For example, instead of providing a task definition, natural language processing could be integrated, so someone could use their voice to tell their assistant to take the dishes from the dining room and move them to the kitchen sink. We also want to explore how our robot can better perform around people, such as by anticipating their needs and helping them with a multistep task, like baking a cake.

These are just some of the many areas that call for more research and exploration. We believe that with a strong visual cortex pretrained on egocentric video and visuomotor skills pretrained in simulation, these advancements could one day serve as building blocks for AI-powered experiences where virtual assistants and physical robots can assist humans and interact seamlessly with the virtual and physical world.

Read the paper: Adaptive Skill Coordination (ASC)

Read the paper: Visual Cortex

Get the Visual Cortex code

We would like to acknowledge the contributions of the following people:

Visual Cortex: Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, Yixin Lin, Oleksandr Maksymets, and Aravind Rajeswaran

Adaptive Skill Coordination: Naoki Yokoyama, Alexander William Clegg, Eric Undersander, Sergio Arnaud, Jimmy Yang, and Sehoon Ha"
Meta_Blog,https://ai.meta.com/blog/robots-learning-video-simulation-artificial-visual-cortex-vc-1/,,Robots that learn from videos of human activities and simulated interactions,"Something Went Wrong We're having trouble playing this video. Learn more

Optimistic science fiction typically imagines a future where humans create art and pursue fulfilling pastimes while AI-enabled robots handle dull or dangerous tasks. In contrast, the AI systems of today display increasingly sophisticated generative abilities on ostensible creative tasks. But where are the robots? This gap is known as Moravec’s paradox, the thesis that the hardest problems in AI involve sensorimotor skills, not abstract thought or reasoning. To put it another way, “The hard problems are easy, and the easy problems are hard.”

Today, we are announcing two major advancements toward general-purpose embodied AI agents capable of performing challenging sensorimotor skills:

An artificial visual cortex (called VC-1): a single perception model that, for the first time, supports a diverse range of sensorimotor skills, environments, and embodiments. VC-1 is trained on videos of people performing everyday tasks from the groundbreaking Ego4D dataset created by Meta AI and academic partners. And VC-1 matches or outperforms best-known results on 17 different sensorimotor tasks in virtual environments.

A new approach called adaptive (sensorimotor) skill coordination (ASC), which achieves near-perfect performance (98 percent success) on the challenging task of robotic mobile manipulation (navigating to an object, picking it up, navigating to another location, placing the object, repeating) in physical environments.

Data powers both of these breakthroughs. AI needs data to learn from — and, specifically, embodied AI needs data that captures interactions with the environment. Traditionally, this interaction data is collected either by collecting large amounts of demonstrations or by allowing the robot to learn from interactions from scratch. Both approaches are too resource-intensive to scale toward the learning of a general embodied AI agent. In both of these works, we are developing new ways for robots to learn, using videos of human interactions with the real world and simulated interactions within photorealistic simulated worlds.

First, we’ve built a way for robots to learn from real-world human interactions, by training a general-purpose visual representation model (an artificial visual cortex) from a large number of egocentric videos. The videos include our open source Ego4D dataset, which shows first-person views of people doing everyday tasks, like going to the grocery store and cooking lunch. Second, we’ve built a way to pretrain our robot to perform long-horizon rearrangement tasks in simulation. Specifically, we train a policy in Habitat environments and transfer the policy zero-shot to a real Spot robot to perform such tasks in unfamiliar real-world spaces.

Toward an artificial visual cortex for embodied intelligence

A visual cortex is the region of the brain that (together with the motor cortex) enables an organism to convert vision into movement. We are interested in developing an artificial visual cortex — the module in an AI system that enables an artificial agent to convert camera input into actions.

Our FAIR team, together with academic collaborators, has been at the forefront of developing general-purpose visual representations for embodied AI trained from egocentric video datasets. The Ego4D dataset has been especially useful, since it contains thousands of hours of wearable camera video from research participants around the world performing daily life activities, including cooking, cleaning, sports, and crafts.

For instance, one prior work from our team (R3M) uses temporal and text-video alignment within Ego4D video frames to learn compact universal visual representations for robotic manipulation. Another work (VIP) uses Ego4D frames to learn an effective actionable visual representation that can also perform zero-shot visual reward-specification for training embodied agents. These are illustrative of the broader trend in the research community (e.g., PVR, OVRL, MVP) toward pretraining visual representations from web images and egocentric videos.

Although prior work has focused on a small set of robotic tasks, a visual cortex for embodied AI should work well for a diverse set of sensorimotor tasks in diverse environments across diverse embodiments. While prior works on pretraining visual representations give us a glimpse of what may be feasible, they are fundamentally incommensurable, with different ways of pretraining the visual representations on different datasets, evaluated on different embodied AI tasks. The lack of consistency meant there was no way of knowing which of the existing pretrained visual representations were best.

As a first step, we curated CortexBench, consisting of 17 different sensorimotor tasks in simulation, spanning locomotion, navigation, and dexterous and mobile manipulation, implementing the community standard for learning the policy for each task. The visual environments span from flat infinite planes to tabletop settings to photorealistic 3D scans of real-world indoor spaces. The agent embodiments vary from stationary arms to dexterous hands to idealized cylindrical navigation agents to articulated mobile manipulators. The learning conditions vary from few-shot imitation learning to large-scale reinforcement learning. This allowed us to perform a rigorous and consistent evaluation of existing and new pretrained models. Prior to our work, the best performance for each task in CortexBench was achieved by a model or algorithm specifically designed for that task. In contrast, what we want is one model and/or algorithm that achieves competitive performance on all tasks. Biological organisms have one general-purpose visual cortex, and that is what we seek for embodied agents.

We set out to pretrain a single general-purpose visual cortex that can perform well on all of these tasks. A critical choice for pretraining is the choice of dataset. It was entirely unclear what a good pretraining dataset for embodied AI would look like. There are massive amounts of video data available online, yet it isn’t practical to try out all combinations of those existing datasets.

We start with Ego4D as our core dataset and then explore whether adding additional datasets improves pretrained models. Having egocentric video is important because it enables robots to learn to see from a first-person perspective. Since Ego4D is heavily focused on everyday activities like cooking, gardening, and crafting, we also consider egocentric video datasets that explore houses and apartments. Finally, we also study whether static image datasets help improve our models.

Cumulatively, our work represents the largest and most comprehensive empirical study to date of visual representations for embodied AI, spanning 5+ pretrained visual representations from prior work, and multiple ablations of VC-1 trained on 4,000+ hours of egocentric human video from seven diverse datasets, which required over 10,000 GPU-hours of training and evaluation.

Today, we are open-sourcing VC-1, our best visual cortex model following FAIR’s values of open research for the benefit of all. Our results show VC-1 representations match or outperform learning from scratch on all 17 tasks. We also find that adapting VC-1 on task-relevant data results in it becoming competitive with or outperforming best-known results on all tasks in CortexBench. To the best of our knowledge, VC-1 is the first visual pretrained model that has shown to be competitive with state-of-the art results on such a diverse set of embodied AI tasks. We are sharing our detailed learnings, such as how scaling model size, dataset size, and diversity impact performance of pretrained models, in a related research paper.

Adaptive skill coordination for robotic mobile manipulation

While VC-1 demonstrates strong performance on sensorimotor skills in CortexBench, these are short-horizon tasks (navigating, picking up an object, in-hand manipulation of an object, etc.). The next generation of embodied AI agents (deployed on robots) will also need to accomplish long-horizon tasks and adapt to new and changing environments, including unexpected real-world disturbances.

Our second announcement focuses on mobile pick-and-place — a robot is initialized in a new environment and tasked with moving objects from initial to desired locations, emulating the task of tidying a house. The robot must navigate to a receptacle with objects, like the kitchen counter (the approximate location is provided to it), search for and pick an object, navigate to its desired place receptacle, place the object, and repeat.

Something Went Wrong We're having trouble playing this video. Learn more

To tackle such long-horizon tasks, we and our collaborators at Georgia Tech developed a new technique called Adaptive Skill Coordination (ASC), which consists of three components:

A library of basic sensorimotor skills (navigation, pick, place)

A skill coordination policy that chooses which skills are appropriate to use at which time

A corrective policy that adapts pretrained skills when out-of-distribution states are perceived

All sensorimotor policies are “model-free.” We use sensor-to-actions neural networks with no task-specific modules, like mapping or planning. The robot is trained entirely in simulation and transferred to the physical world without any real-world training data.

Something Went Wrong We're having trouble playing this video. Learn more

We demonstrate the effectiveness of ASC by deploying it on the Boston Dynamics' Spot robot in new/unknown real-world environments. We chose the Boston Dynamics Spot robot because of robust sensing, navigation, and manipulation capabilities. However, operating Spot today involves a large amount of human intervention. For example, picking an object requires a person to click on the object on the robot’s tablet. Our aim is to build AI models that can sense the world from onboard sensing and motor commands through Boston Dynamics APIs.

Using the Habitat simulator, and the HM3D and ReplicaCAD datasets, which include indoor 3D scans of 1,000 homes, we teach a simulated Spot robot to move around an unseen house, pick up out-of-place objects, and put them in the right location. Next, we deploy this policy zero-shot in the real-world (sim2real) without explicitly building a map in the real world, and instead rely on our robot to use its learned notion of what houses look like.

When we put our work to the test, we used two significantly different real-world environments where Spot was asked to rearrange a variety of objects — a fully furnished 185-square-meter apartment and a 65-square-meter university lab. Overall, ASC achieved near-perfect performance, succeeding on 59 of 60 (98 percent) episodes, overcoming hardware instabilities, picking failures, and adversarial disturbances like moving obstacles or blocked paths. In comparison, traditional baselines like task and motion planning succeed in only 73 percent of cases, because of an inability to recover from real-world disturbances. We also study robustness to adversarial perturbations, such as changing the layout of the environment, walking in front of the robot to repeatedly block its path, or moving target objects mid-episode. Despite being trained entirely in simulation, ASC is robust to such disturbances, making it well suited for many long-horizon problems in robotics and reinforcement learning.

This opens avenues for sim2real research to expand to even more challenging real-world tasks, such as assistance in everyday tasks like cooking and cleaning, and even human-robot collaboration. Our work is a step toward scalable, robust, and diverse robot assistants of the future that can operate in new environments out of the box and do not require expensive real-world data collection.

Rethinking sim2real transfer

One of the most important tasks in sim2real learning is to build simulation models that truthfully reflect the robot’s behavior in the real world. However, this is challenging, since the real world is vast and constantly changing, and the simulator needs to capture this diversity. No simulator is a perfect replica of reality, and the main challenge is overcoming the gap between the robot’s performance in simulation and in the real world. The default operating hypothesis of this field is that reducing the sim2real gap involves creating simulators of high physics fidelity and using them to learn robot policies.

Over the past year, we have taken a counterintuitive approach to sim2real. Instead of building high-fidelity simulations of the world, we built an abstracted simulator of Spot, which does not model low-level physics in simulation, and learn a policy that can reason on a higher level (like where to go rather than how to move the legs). We call this a kinematic simulation, where the robot is teleported to a location and the target object is attached to the robot arm, when it is close to the gripper and in view. In the real world, Boston Dynamics controllers are used for achieving the actions commanded by this high-level policy.

Something Went Wrong We're having trouble playing this video. Learn more

Robots pretrained in sim2real have mostly been limited to short-horizon tasks and visual navigation, without any interaction with the environment. Mobile pick-and-place is a long-horizon task, and it requires interacting with the environment and switching between different phases of navigation, picking, placing, etc. This is typically very challenging for reinforcement learning, and requires demonstrations, or sophisticated hand-designed rewards. Our high-level abstraction and kinematic simulation let us learn long-horizon tasks, with sparse rewards, without requiring to reason about low-level physics.

Future areas of exploration

While we haven’t yet applied visual cortex to our object rearrangement robot, we hope to integrate it into a single system. With so many unpredictable variables in the real world, having strong visual representations and pretraining on a diverse number of egocentric videos showing many different activities and environments will be an important step toward building even better robots.

Voice is one area we are particularly interested in exploring. For example, instead of providing a task definition, natural language processing could be integrated, so someone could use their voice to tell their assistant to take the dishes from the dining room and move them to the kitchen sink. We also want to explore how our robot can better perform around people, such as by anticipating their needs and helping them with a multistep task, like baking a cake.

These are just some of the many areas that call for more research and exploration. We believe that with a strong visual cortex pretrained on egocentric video and visuomotor skills pretrained in simulation, these advancements could one day serve as building blocks for AI-powered experiences where virtual assistants and physical robots can assist humans and interact seamlessly with the virtual and physical world.

Read the paper: Adaptive Skill Coordination (ASC)

Read the paper: Visual Cortex

Get the Visual Cortex code

We would like to acknowledge the contributions of the following people:

Visual Cortex: Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, Yixin Lin, Oleksandr Maksymets, and Aravind Rajeswaran

Adaptive Skill Coordination: Naoki Yokoyama, Alexander William Clegg, Eric Undersander, Sergio Arnaud, Jimmy Yang, and Sehoon Ha"
Meta_Blog,https://ai.meta.com/blog/meta-ai-ecosystem-management-metrics/,,How Meta measures the management of its AI ecosystem,"At Meta, we have developed measurement processes for specific metrics about AI systems that can be used to make managing models more effective and efficient, and we’ve tested these processes across a diverse ecosystem of tools and systems. We believe these techniques can be applied broadly in other organizations managing AI ecosystems, so we are sharing them here.

AI development ecosystems are increasingly complex and challenging to maintain, and technology companies need to develop highly efficient systems to build, serve, and improve their AI models for production applications.

AI model management is particularly important for large-scale technology companies like Meta, where billions of people trust us to respect their privacy, keep their data secure, and deliver consistent, high-quality product experiences. Meta has thousands of AI models as well as a diverse set of development tools and model serving platforms that need to comply with regulations in multiple areas of the world.

Meta uses extensive machine learning in our products, including in serving content to user feeds (center) and personalizing ads (upper right).

To manage these models, Meta has developed a machine learning operations (ML-Ops) ecosystem that consolidates key elements such as measurement while also offering flexible, decentralized tools for different product teams with different needs. At Meta, we have extensive experience with ML-Ops, and have pioneered new ways to navigate the process for a wide range of product applications. In this blog, we will walk through approaches we’ve developed for measuring the maturity of our AI models and tracking management outcomes. These approaches are foundationally important for any AI infrastructure, because a first step in managing an ecosystem of models is developing a unified way of understanding particular ML assets via metrics.

We’ll start by exploring the way we think about AI model management (one of the core components of Meta’s broader AI governance strategy), then discuss how to move toward consistently defining concepts across authoring, training, and deploying AI models. We’ll then walk through how to apply those concepts to platformize measurement and reporting, and describe how to build a cohesive metric framework on that platform. These measurement processes are being leveraged to more efficiently, reliably, and responsibly operate many AI platforms, products, and infrastructure components at Meta.

AI model management

What are the goals of managing an AI ecosystem?

The following items represent goals and principles that AI practitioners should consider as they aim to effectively manage models across their systems:

Data governance : Effective AI systems management factors in data governance considerations to help ensure models only consume data for a limited, clearly stated purpose, that the minimum necessary data is used, and that data is retained only as long as it is needed. At Meta, these efforts support certain regulatory requirements and internal policies related to data governance.

Security : Leverage automation to better prevent misuse or accidental loss of models and their data, and to provide access to the same.

Ownership accountability : Ensure that models are actively maintained and documented with a clear owner, taking into account other accountability goals for your organization.

Fairness : Minimize risk of amplifying societal biases in the model building process, and ensure consistent outcomes across different demographic groups, taking into account other fairness goals for your organization.

Robustness : Promote AI best practices to help models run optimally. At Meta, we work to drive best practices for things like frequent retraining to combat concept drift.

Efficiency: Maximize usage of machine resources and developer time by lessening friction in the process and reducing unnecessary compute and storage used for modeling.

Meta’s decentralized AI ecosystem

At Meta, we provide product teams with opportunities to match AI tooling to their needs. Some product teams use tightly coupled custom solutions to meet exceptional feature and scale requirements, while others benefit from the ease of use of simpler, more general tools. This variance allows for effective AI development and fast iterations, but it also adds complexity to model management. Various tools, linked by product area, mean more touchpoints to manage. In our measurement of AI models, features, and datasets, we look to enable this decentralization by efficiently bridging across these varied tools to create visibility to outcomes without requiring tooling consolidation.

Consistently defining AI concepts

What is a model?

Measurement begins from a clear definition of an AI model. This seemingly simple question can get quite ambiguous in the context of complex modeling ecosystems. For example, if you ask different practitioners to look at the same set of model artifacts and tell you how many models there are, you will often get several different answers. Some people may say there are five trained binaries, thus five models; others may say there are three deployments serving traffic, so three models; and still others may say these artifacts all came from one source code file, so there is one model.

All this becomes a challenge when you have to ask such questions as, “Are all my models being retrained regularly?” and need a uniform way to understand how many models to assess. In order to consistently manage models, we need consistent definitions for key concepts, so we’ll walk through how we established standardized definitions for ML model artifacts in technical systems.

What are model artifacts?

The first step in assessing the artifacts of AI modeling is selecting the right component for each use case. To do so, it can be helpful to picture a general structure of model development, pictured below as it looks at Meta. Significant complexity is added in considering nonmodel assets (such as features and data sources), so this section will focus on models themselves.

Model source code is written by engineers to solve a given purpose (such as ranking posts), and then one or more model binaries are trained from this source, often being created periodically for either iterative model improvements or retraining. Finally, some of these versions are published to inference systems as deployments to serve traffic. Separate deployments occur for various reasons, including for infrastructure considerations (such as one streaming deployment and one batch), for experiments, or for reusing generic or pretrained models in different areas.

A simplified view of the artifacts created for a single deployed machine learning model.

Artifact labeling

After selecting the model artifact to measure, additional complexity arises when artifacts with the same function have different labels in different tools. This causes confusion when elements with certain labels are selected and relevant components from some systems are missed, or when selecting the same label across systems unintentionally groups together non comparable concepts in AI.

Take the example below, where system 1 calls the model source code the “Model,” with another term denoting the trained binaries, and system 2 considers binaries the “Model,” calling the model source code something else. If we were to select all “Models” from these systems, we’d get a mix of source code and binaries, which are not comparable. This confusion can be minimized with a clear metadata architecture for the organization that bridges across specific system implementations via common labels.

When different machine learning tools take different approaches to iterative training and deployment, selecting artifacts with the same label can unintentionally group together non comparable AI concepts.

Consolidating measurement onto a single platform

Meta uses decentralized AI tooling to create customized product experiences in various domains, from video feed ranking in Reels to helping people discover new VR experiences in Meta Horizon Worlds. This wide array of use cases has shown us that measurements can sometimes be inconsistent because they cover different subsets of the model population.

To avoid inconsistency while maintaining decentralization of our AI systems, we’ve worked toward consolidating certain logging information into a single platform that can then serve various needs when queried. This platformizing of measurement involves creating modular data collection endpoints that can be reused in different areas and ingesting data from these endpoints into a nuanced metadata graph. Together, decentralized data ingestion and a central data store create a scalable structure that gives us the flexibility to work with a diverse range of AI systems.

Two key elements in this process are the metadata graph and data interoperability standards.

Because AI at Meta involves a combination of systems linked by product area, we’ve worked toward consolidating certain machine learning logging information to a central metadata graph, which is then interpreted through interoperability standards.

AI metadata graph

The central graph stitches together data between systems and helps ensure coverage in diverse contexts. This is done by bridging across systems and creating common definitions for various artifacts.

A feature source record stores the origin of data for a particular feature. These sources are linked to any number of logical feature records, which store the logic to transform the raw data into a training dataset. Model code and its training dataset are connected to a workflow run, which is a pipeline that executes to produce one or more model binaries. Finally, the model binaries are linked to one or more model deployments, which track each place where binaries are serving inference requests.

This graph is implemented via Ent framework and is referenced in various systems for model management where we need to call and edit information about specific models.

Meta’s approach to an AI metadata graph, a database that stores asset metadata and connections. Markers on the connections here (1, #..n, and #..m) denote them as single and one-to-many.

Data interoperability standards

The metadata graph has various flexible units and data flags to cover a vast number of infrequent edge cases, such as a single model deployment location running thousands of model binaries at the same time. Because of this, it can be parsed in many ways that produce different views from the same data. For company-wide data applications, where consistency is important, we establish standard ways to interpret the graph. To do this, practical trade-offs are necessary to determine what edge cases are appropriate to include for common uses and to set global definitions, such as what we mean by an “active model.” This allows us to transform the graph into flat tables for broad reporting and to interact with traditional (non-AI) programs that want to leverage AI data.

These tables are the first stop for most analytics or exploratory use cases. When more nuanced use cases with specific needs arise, these can return to the graph to look to meet their needs via a custom data pull. The process of transforming a large graph into tables involves significant ETL operations, so we use Presto to make sure pipelines are efficient and reliable, and supplement with Spark to handle areas that are particularly memory intensive.

Measuring model management outcomes with a top-line metric framework

After consistently defining artifacts and aggregating data from diverse sources, the final step in getting a broad picture of model management is to define metrics and interactions between them to describe model outcomes.

One way to do this is via a machine learning maturity framework.

Maturity metrics

The first problem we need to solve is how to compare metric outcomes from across areas of model management. The framework does this by setting out design principles for each component metric. These describe the outcomes expected in models with a standardized, bucketed format. Translation to consistent labels allows for apples-to-apples comparisons across diverse domains, and the bucketing allows for comparison of both continuous variables (e.g., time since a model has been retrained) and logical states (e.g., investigation in progress or investigation completed).

This design is flexible enough to work in various contexts, where expectations could be set out to mean anything from compliance with access control policy to proper annotation tags on models, depending on specific needs.

For instance, at Meta we use it to shape a company-wide view of exposure to concept drift among deployed models. Concept drift is when the statistical representation that a model learns from its data changes, making the model’s predictions less valid. By measuring the time since a model was last trained, we can proxy this risk of drift and understand the likelihood that the model learned a representation that no longer applies.

Framework structure

Even with consistently defined metrics, it can be challenging to make sense of outcomes across each metric individually. Implementing a hierarchical framework structure to group metrics can help alleviate this problem. These groupings constitute a graph of metrics and their aggregations for each model, and various nodes on that graph can be reported for different purposes. Aggregations from component metrics to groups and to overall maturity can use different methods based on the nature of the metrics they aggregate.

At Meta, we use a maturity framework to coordinate key top-line metrics for ML model management and give a holistic view of how they interact. Each metric has a config file that defines its place in the graph hierarchy (lines 5 and 6, below), and also defines how it is aggregated from finer model concepts, such as deployments, to coarser model concepts, such as source code (line 8, below).

Example config file fields for one maturity metric.

How to think about AI model management at scale

If you are looking to measure management of an AI development ecosystem, our experience shows some concrete ways to approach the situation:

Think about how different tools contribute to a generic AI development process, and map outputs from various tools to consistent artifacts in that process.

If your AI systems are complex and can’t be consolidated, think of measurement as a platform with a metadata graph and common interoperability standards.

For a broad view of outcomes, consider a metric framework that can compare and aggregate across model management domains.

Thanks to Michael Ashikhmin, Thomas Dudziak, Cindy Niu, and Reza Sharifi Sedeh for their significant contributions to this work, and Dany Daher, Prashant Dhamdhere, Maurizio Ferconi, Xin Fu, Giri Kumaran, Noah Lee, Joon Lim, and Jiang Wu for their guidance and support."
Meta_Blog,https://ai.meta.com/blog/meta-ai-ecosystem-management-metrics/,,How Meta measures the management of its AI ecosystem,"At Meta, we have developed measurement processes for specific metrics about AI systems that can be used to make managing models more effective and efficient, and we’ve tested these processes across a diverse ecosystem of tools and systems. We believe these techniques can be applied broadly in other organizations managing AI ecosystems, so we are sharing them here.

AI development ecosystems are increasingly complex and challenging to maintain, and technology companies need to develop highly efficient systems to build, serve, and improve their AI models for production applications.

AI model management is particularly important for large-scale technology companies like Meta, where billions of people trust us to respect their privacy, keep their data secure, and deliver consistent, high-quality product experiences. Meta has thousands of AI models as well as a diverse set of development tools and model serving platforms that need to comply with regulations in multiple areas of the world.

Meta uses extensive machine learning in our products, including in serving content to user feeds (center) and personalizing ads (upper right).

To manage these models, Meta has developed a machine learning operations (ML-Ops) ecosystem that consolidates key elements such as measurement while also offering flexible, decentralized tools for different product teams with different needs. At Meta, we have extensive experience with ML-Ops, and have pioneered new ways to navigate the process for a wide range of product applications. In this blog, we will walk through approaches we’ve developed for measuring the maturity of our AI models and tracking management outcomes. These approaches are foundationally important for any AI infrastructure, because a first step in managing an ecosystem of models is developing a unified way of understanding particular ML assets via metrics.

We’ll start by exploring the way we think about AI model management (one of the core components of Meta’s broader AI governance strategy), then discuss how to move toward consistently defining concepts across authoring, training, and deploying AI models. We’ll then walk through how to apply those concepts to platformize measurement and reporting, and describe how to build a cohesive metric framework on that platform. These measurement processes are being leveraged to more efficiently, reliably, and responsibly operate many AI platforms, products, and infrastructure components at Meta.

AI model management

What are the goals of managing an AI ecosystem?

The following items represent goals and principles that AI practitioners should consider as they aim to effectively manage models across their systems:

Data governance : Effective AI systems management factors in data governance considerations to help ensure models only consume data for a limited, clearly stated purpose, that the minimum necessary data is used, and that data is retained only as long as it is needed. At Meta, these efforts support certain regulatory requirements and internal policies related to data governance.

Security : Leverage automation to better prevent misuse or accidental loss of models and their data, and to provide access to the same.

Ownership accountability : Ensure that models are actively maintained and documented with a clear owner, taking into account other accountability goals for your organization.

Fairness : Minimize risk of amplifying societal biases in the model building process, and ensure consistent outcomes across different demographic groups, taking into account other fairness goals for your organization.

Robustness : Promote AI best practices to help models run optimally. At Meta, we work to drive best practices for things like frequent retraining to combat concept drift.

Efficiency: Maximize usage of machine resources and developer time by lessening friction in the process and reducing unnecessary compute and storage used for modeling.

Meta’s decentralized AI ecosystem

At Meta, we provide product teams with opportunities to match AI tooling to their needs. Some product teams use tightly coupled custom solutions to meet exceptional feature and scale requirements, while others benefit from the ease of use of simpler, more general tools. This variance allows for effective AI development and fast iterations, but it also adds complexity to model management. Various tools, linked by product area, mean more touchpoints to manage. In our measurement of AI models, features, and datasets, we look to enable this decentralization by efficiently bridging across these varied tools to create visibility to outcomes without requiring tooling consolidation.

Consistently defining AI concepts

What is a model?

Measurement begins from a clear definition of an AI model. This seemingly simple question can get quite ambiguous in the context of complex modeling ecosystems. For example, if you ask different practitioners to look at the same set of model artifacts and tell you how many models there are, you will often get several different answers. Some people may say there are five trained binaries, thus five models; others may say there are three deployments serving traffic, so three models; and still others may say these artifacts all came from one source code file, so there is one model.

All this becomes a challenge when you have to ask such questions as, “Are all my models being retrained regularly?” and need a uniform way to understand how many models to assess. In order to consistently manage models, we need consistent definitions for key concepts, so we’ll walk through how we established standardized definitions for ML model artifacts in technical systems.

What are model artifacts?

The first step in assessing the artifacts of AI modeling is selecting the right component for each use case. To do so, it can be helpful to picture a general structure of model development, pictured below as it looks at Meta. Significant complexity is added in considering nonmodel assets (such as features and data sources), so this section will focus on models themselves.

Model source code is written by engineers to solve a given purpose (such as ranking posts), and then one or more model binaries are trained from this source, often being created periodically for either iterative model improvements or retraining. Finally, some of these versions are published to inference systems as deployments to serve traffic. Separate deployments occur for various reasons, including for infrastructure considerations (such as one streaming deployment and one batch), for experiments, or for reusing generic or pretrained models in different areas.

A simplified view of the artifacts created for a single deployed machine learning model.

Artifact labeling

After selecting the model artifact to measure, additional complexity arises when artifacts with the same function have different labels in different tools. This causes confusion when elements with certain labels are selected and relevant components from some systems are missed, or when selecting the same label across systems unintentionally groups together non comparable concepts in AI.

Take the example below, where system 1 calls the model source code the “Model,” with another term denoting the trained binaries, and system 2 considers binaries the “Model,” calling the model source code something else. If we were to select all “Models” from these systems, we’d get a mix of source code and binaries, which are not comparable. This confusion can be minimized with a clear metadata architecture for the organization that bridges across specific system implementations via common labels.

When different machine learning tools take different approaches to iterative training and deployment, selecting artifacts with the same label can unintentionally group together non comparable AI concepts.

Consolidating measurement onto a single platform

Meta uses decentralized AI tooling to create customized product experiences in various domains, from video feed ranking in Reels to helping people discover new VR experiences in Meta Horizon Worlds. This wide array of use cases has shown us that measurements can sometimes be inconsistent because they cover different subsets of the model population.

To avoid inconsistency while maintaining decentralization of our AI systems, we’ve worked toward consolidating certain logging information into a single platform that can then serve various needs when queried. This platformizing of measurement involves creating modular data collection endpoints that can be reused in different areas and ingesting data from these endpoints into a nuanced metadata graph. Together, decentralized data ingestion and a central data store create a scalable structure that gives us the flexibility to work with a diverse range of AI systems.

Two key elements in this process are the metadata graph and data interoperability standards.

Because AI at Meta involves a combination of systems linked by product area, we’ve worked toward consolidating certain machine learning logging information to a central metadata graph, which is then interpreted through interoperability standards.

AI metadata graph

The central graph stitches together data between systems and helps ensure coverage in diverse contexts. This is done by bridging across systems and creating common definitions for various artifacts.

A feature source record stores the origin of data for a particular feature. These sources are linked to any number of logical feature records, which store the logic to transform the raw data into a training dataset. Model code and its training dataset are connected to a workflow run, which is a pipeline that executes to produce one or more model binaries. Finally, the model binaries are linked to one or more model deployments, which track each place where binaries are serving inference requests.

This graph is implemented via Ent framework and is referenced in various systems for model management where we need to call and edit information about specific models.

Meta’s approach to an AI metadata graph, a database that stores asset metadata and connections. Markers on the connections here (1, #..n, and #..m) denote them as single and one-to-many.

Data interoperability standards

The metadata graph has various flexible units and data flags to cover a vast number of infrequent edge cases, such as a single model deployment location running thousands of model binaries at the same time. Because of this, it can be parsed in many ways that produce different views from the same data. For company-wide data applications, where consistency is important, we establish standard ways to interpret the graph. To do this, practical trade-offs are necessary to determine what edge cases are appropriate to include for common uses and to set global definitions, such as what we mean by an “active model.” This allows us to transform the graph into flat tables for broad reporting and to interact with traditional (non-AI) programs that want to leverage AI data.

These tables are the first stop for most analytics or exploratory use cases. When more nuanced use cases with specific needs arise, these can return to the graph to look to meet their needs via a custom data pull. The process of transforming a large graph into tables involves significant ETL operations, so we use Presto to make sure pipelines are efficient and reliable, and supplement with Spark to handle areas that are particularly memory intensive.

Measuring model management outcomes with a top-line metric framework

After consistently defining artifacts and aggregating data from diverse sources, the final step in getting a broad picture of model management is to define metrics and interactions between them to describe model outcomes.

One way to do this is via a machine learning maturity framework.

Maturity metrics

The first problem we need to solve is how to compare metric outcomes from across areas of model management. The framework does this by setting out design principles for each component metric. These describe the outcomes expected in models with a standardized, bucketed format. Translation to consistent labels allows for apples-to-apples comparisons across diverse domains, and the bucketing allows for comparison of both continuous variables (e.g., time since a model has been retrained) and logical states (e.g., investigation in progress or investigation completed).

This design is flexible enough to work in various contexts, where expectations could be set out to mean anything from compliance with access control policy to proper annotation tags on models, depending on specific needs.

For instance, at Meta we use it to shape a company-wide view of exposure to concept drift among deployed models. Concept drift is when the statistical representation that a model learns from its data changes, making the model’s predictions less valid. By measuring the time since a model was last trained, we can proxy this risk of drift and understand the likelihood that the model learned a representation that no longer applies.

Framework structure

Even with consistently defined metrics, it can be challenging to make sense of outcomes across each metric individually. Implementing a hierarchical framework structure to group metrics can help alleviate this problem. These groupings constitute a graph of metrics and their aggregations for each model, and various nodes on that graph can be reported for different purposes. Aggregations from component metrics to groups and to overall maturity can use different methods based on the nature of the metrics they aggregate.

At Meta, we use a maturity framework to coordinate key top-line metrics for ML model management and give a holistic view of how they interact. Each metric has a config file that defines its place in the graph hierarchy (lines 5 and 6, below), and also defines how it is aggregated from finer model concepts, such as deployments, to coarser model concepts, such as source code (line 8, below).

Example config file fields for one maturity metric.

How to think about AI model management at scale

If you are looking to measure management of an AI development ecosystem, our experience shows some concrete ways to approach the situation:

Think about how different tools contribute to a generic AI development process, and map outputs from various tools to consistent artifacts in that process.

If your AI systems are complex and can’t be consolidated, think of measurement as a platform with a metadata graph and common interoperability standards.

For a broad view of outcomes, consider a metric framework that can compare and aggregate across model management domains.

Thanks to Michael Ashikhmin, Thomas Dudziak, Cindy Niu, and Reza Sharifi Sedeh for their significant contributions to this work, and Dany Daher, Prashant Dhamdhere, Maurizio Ferconi, Xin Fu, Giri Kumaran, Noah Lee, Joon Lim, and Jiang Wu for their guidance and support."
Meta_Blog,https://ai.meta.com/blog/casual-conversations-v2-dataset-measure-fairness/,,Introducing Casual Conversations v2: A more inclusive dataset to measure fairness,"Something Went Wrong We're having trouble playing this video. Learn more

For AI to serve communities fairly, researchers need diverse and inclusive datasets in order to rigorously and thoughtfully evaluate fairness in the models they build. In applications of computer vision and speech recognition in particular, AI researchers need data to assess how well a model works for different demographic groups. And this data can be difficult to gather due to complex geographic and cultural contexts, inconsistency between different sources, and challenges with accuracy in labeling.

Today, we are releasing Casual Conversations v2, a consent-driven, publicly available resource that enables researchers to better evaluate the fairness and robustness of certain types of AI models. The dataset was informed and shaped by a comprehensive literature review around relevant demographic categories, and was created in consultation with internal experts in fields such as civil rights. This dataset offers a granular list of 11 self-provided and annotated categories to further measure algorithmic fairness and robustness in these AI systems. The dataset features 26,467 video monologues recorded in seven countries featuring 5,567 paid participants who provided self-identified attributes such as age and gender, and is the next generation following the original Casual Conversations consent-driven dataset, which we released in 2021. To our knowledge, it’s the first open source dataset with videos collected from multiple countries using highly accurate and detailed demographic information to help test AI models for fairness and robustness.

The Casual Conversations v2 dataset permits multiple video submissions per participant, plus detailed labels, including age and recording setup.

Since launching the original dataset two years ago, we’ve continued to collaborate with experts and expand the dataset, including adding expert human speech transcriptions, to help the research community assess fairness concerns in additional domains. Like the original dataset, Casual Conversations v2 is made available to the public under a dataset license agreement to aid as many researchers as possible in their efforts to measure fairness and support robustness. By leveraging this dataset, researchers can investigate, for example, whether a speech recognition system is working consistently across a variety of demographic characteristics and environments.

Granular categories to help pinpoint fairness gaps

While the first version of Casual Conversations was a major step to help researchers establish fairness benchmarks, there were some limitations. The first dataset’s labels included only age, three subcategories of gender (female, male, and other), apparent skin tone, and ambient lighting. With the understanding that there are numerous underrepresented communities of people, languages, and attributes, we wanted to dig deeper into subcategories to identify potential model gaps in fairness and robustness.

Gender is one of the categories commonly used to assess fairness in computer vision tasks. The Casual Conversations v2 dataset invited participants to disclose their gender with the additional option of completing a freeform field.

Along with Prof. Pascale Fung, director of the Centre for AI Research, and other researchers from Hong Kong University of Science and Technology, we conducted a robust literature review of governmental and academic resources for potential categories and then published our findings for other researchers to build upon this work. We also consulted internal civil rights experts and domain experts at Meta.

To be more inclusive and to mitigate issues of subjective annotations for many of the dataset’s categories, we asked participants to provide some of their information in their preferred language. Even though participants agreed to share their information to be used in AI tasks via consent forms, providing this self-labeled information was optional. Therefore, we allowed participants to input certain information in their own words.

In the dataset, participants self-labeled their spoken and native languages and were able to speak in more than one language or dialect across their video submissions.

Of the 11 categories included in Casual Conversations v2, seven were provided by the participants, while the remaining were manually labeled by annotators. The self-provided categories are age, gender, language/dialect, geolocation, disability, physical adornments, and physical attributes. For the remaining categories (voice timbre, apparent skin tones, recording setup, and activity), we trained vendors with detailed guidelines to enhance consistency and reduce the likelihood of subjective annotations during the labeling process.

How we collected and categorized v2 data: skin tone and more

With the expansion of Casual Conversations, we wanted to support a multilingual dataset, particularly as language understanding can support the development of inclusive natural language processing models.

In addition to an expanded list of categories, Casual Conversations v2 differs from the first version with the inclusion of participant monologues recorded outside the United States. The seven countries included in v2 are Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the United States. In the future, we hope to further expand the dataset to additional geographies. Another difference in the latest dataset is that participants were given the chance to speak in both their primary and secondary languages. The types of monologues include both scripted and nonscripted speech.

While the introduction of recording participants from multiple geographies provided a new set of logistical challenges and opportunities, it also added further complexity in identifying categories relevant to even more diverse communities. With increasing concerns over the performance of AI systems across different skin tone scales, we decided to leverage two different scales for skin tone annotation. The first is the six-tone Fitzpatrick scale, the most commonly used numerical classification scheme for skin tone due to its simplicity and widespread use. The second is the 10-tone Skin Tone scale, which was introduced by Google and is used in its search and photo services. Including both scales in Casual Conversations v2 provides a clearer comparison with previous works that use the Fitzpatrick scale while also enabling measurement based on the more inclusive Monk scale.

The Casual Conversations v2 dataset includes apparent skin tone labels according to the six-tone Fitzpatrick scale and the 10-tone Monk scale.

“To increase nondiscrimination, fairness, and safety in AI, it’s important to have inclusive data and diversity within the data categories so researchers can better assess how well a specific model or AI-powered product is working for different demographic groups,” said Roy Austin, Vice President and Deputy General Counsel for Civil Rights at Meta. “This dataset has an important role in ensuring the technology we build has equity in mind for all from the outset.""

Previously, researchers measuring algorithmic fairness and robustness typically identified gaps in models only for categories available in public datasets. For example, the categories of age, gender, and apparent skin tone typically support computer vision tasks, while language/dialect and voice timbre are used in audio/speech research.

Because we have expanded data collection with multiple categories and countries, we hope researchers can leverage this dataset and the expanded categories, such as apparent skin tone, disability, accent, dialect, location, and recording setup.

Leveraging Casual Conversations v2 to support diversity and inclusion

Because researchers train and evaluate AI systems with data, identifying fairness concerns and increasing robustness remains an urgent priority as AI increasingly affects individuals and communities. The goal for Casual Conversations has been to support more inclusive AI and technology by providing a standardized resource to identify ways in which models can perform more robustly.

Researchers and practitioners in dozens of countries have utilized the first version of Casual Conversations. At Meta, we proactively leverage this dataset along with other available datasets for model assessment in computer vision, language, and speech models. Given the positive reception from the AI community for the first version, we believe v2 addresses the large gap in data for which we are still in the early stages of recognizing.

While there is no single solution to address AI fairness and robustness, nor are there universally accepted metrics to detect fairness concerns, we are committed to collaborating with experts in the field to tackle these issues and hope to spark further research in these areas.

Access the Casual Conversations v2 dataset

Read the paper

Acknowledgements

We would also like to thank Prof. Pascale Fung and Dr. Ellis Monk for their contributions."
Meta_Blog,https://ai.meta.com/blog/casual-conversations-v2-dataset-measure-fairness/,,Introducing Casual Conversations v2: A more inclusive dataset to measure fairness,"Something Went Wrong We're having trouble playing this video. Learn more

For AI to serve communities fairly, researchers need diverse and inclusive datasets in order to rigorously and thoughtfully evaluate fairness in the models they build. In applications of computer vision and speech recognition in particular, AI researchers need data to assess how well a model works for different demographic groups. And this data can be difficult to gather due to complex geographic and cultural contexts, inconsistency between different sources, and challenges with accuracy in labeling.

Today, we are releasing Casual Conversations v2, a consent-driven, publicly available resource that enables researchers to better evaluate the fairness and robustness of certain types of AI models. The dataset was informed and shaped by a comprehensive literature review around relevant demographic categories, and was created in consultation with internal experts in fields such as civil rights. This dataset offers a granular list of 11 self-provided and annotated categories to further measure algorithmic fairness and robustness in these AI systems. The dataset features 26,467 video monologues recorded in seven countries featuring 5,567 paid participants who provided self-identified attributes such as age and gender, and is the next generation following the original Casual Conversations consent-driven dataset, which we released in 2021. To our knowledge, it’s the first open source dataset with videos collected from multiple countries using highly accurate and detailed demographic information to help test AI models for fairness and robustness.

The Casual Conversations v2 dataset permits multiple video submissions per participant, plus detailed labels, including age and recording setup.

Since launching the original dataset two years ago, we’ve continued to collaborate with experts and expand the dataset, including adding expert human speech transcriptions, to help the research community assess fairness concerns in additional domains. Like the original dataset, Casual Conversations v2 is made available to the public under a dataset license agreement to aid as many researchers as possible in their efforts to measure fairness and support robustness. By leveraging this dataset, researchers can investigate, for example, whether a speech recognition system is working consistently across a variety of demographic characteristics and environments.

Granular categories to help pinpoint fairness gaps

While the first version of Casual Conversations was a major step to help researchers establish fairness benchmarks, there were some limitations. The first dataset’s labels included only age, three subcategories of gender (female, male, and other), apparent skin tone, and ambient lighting. With the understanding that there are numerous underrepresented communities of people, languages, and attributes, we wanted to dig deeper into subcategories to identify potential model gaps in fairness and robustness.

Gender is one of the categories commonly used to assess fairness in computer vision tasks. The Casual Conversations v2 dataset invited participants to disclose their gender with the additional option of completing a freeform field.

Along with Prof. Pascale Fung, director of the Centre for AI Research, and other researchers from Hong Kong University of Science and Technology, we conducted a robust literature review of governmental and academic resources for potential categories and then published our findings for other researchers to build upon this work. We also consulted internal civil rights experts and domain experts at Meta.

To be more inclusive and to mitigate issues of subjective annotations for many of the dataset’s categories, we asked participants to provide some of their information in their preferred language. Even though participants agreed to share their information to be used in AI tasks via consent forms, providing this self-labeled information was optional. Therefore, we allowed participants to input certain information in their own words.

In the dataset, participants self-labeled their spoken and native languages and were able to speak in more than one language or dialect across their video submissions.

Of the 11 categories included in Casual Conversations v2, seven were provided by the participants, while the remaining were manually labeled by annotators. The self-provided categories are age, gender, language/dialect, geolocation, disability, physical adornments, and physical attributes. For the remaining categories (voice timbre, apparent skin tones, recording setup, and activity), we trained vendors with detailed guidelines to enhance consistency and reduce the likelihood of subjective annotations during the labeling process.

How we collected and categorized v2 data: skin tone and more

With the expansion of Casual Conversations, we wanted to support a multilingual dataset, particularly as language understanding can support the development of inclusive natural language processing models.

In addition to an expanded list of categories, Casual Conversations v2 differs from the first version with the inclusion of participant monologues recorded outside the United States. The seven countries included in v2 are Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the United States. In the future, we hope to further expand the dataset to additional geographies. Another difference in the latest dataset is that participants were given the chance to speak in both their primary and secondary languages. The types of monologues include both scripted and nonscripted speech.

While the introduction of recording participants from multiple geographies provided a new set of logistical challenges and opportunities, it also added further complexity in identifying categories relevant to even more diverse communities. With increasing concerns over the performance of AI systems across different skin tone scales, we decided to leverage two different scales for skin tone annotation. The first is the six-tone Fitzpatrick scale, the most commonly used numerical classification scheme for skin tone due to its simplicity and widespread use. The second is the 10-tone Skin Tone scale, which was introduced by Google and is used in its search and photo services. Including both scales in Casual Conversations v2 provides a clearer comparison with previous works that use the Fitzpatrick scale while also enabling measurement based on the more inclusive Monk scale.

The Casual Conversations v2 dataset includes apparent skin tone labels according to the six-tone Fitzpatrick scale and the 10-tone Monk scale.

“To increase nondiscrimination, fairness, and safety in AI, it’s important to have inclusive data and diversity within the data categories so researchers can better assess how well a specific model or AI-powered product is working for different demographic groups,” said Roy Austin, Vice President and Deputy General Counsel for Civil Rights at Meta. “This dataset has an important role in ensuring the technology we build has equity in mind for all from the outset.""

Previously, researchers measuring algorithmic fairness and robustness typically identified gaps in models only for categories available in public datasets. For example, the categories of age, gender, and apparent skin tone typically support computer vision tasks, while language/dialect and voice timbre are used in audio/speech research.

Because we have expanded data collection with multiple categories and countries, we hope researchers can leverage this dataset and the expanded categories, such as apparent skin tone, disability, accent, dialect, location, and recording setup.

Leveraging Casual Conversations v2 to support diversity and inclusion

Because researchers train and evaluate AI systems with data, identifying fairness concerns and increasing robustness remains an urgent priority as AI increasingly affects individuals and communities. The goal for Casual Conversations has been to support more inclusive AI and technology by providing a standardized resource to identify ways in which models can perform more robustly.

Researchers and practitioners in dozens of countries have utilized the first version of Casual Conversations. At Meta, we proactively leverage this dataset along with other available datasets for model assessment in computer vision, language, and speech models. Given the positive reception from the AI community for the first version, we believe v2 addresses the large gap in data for which we are still in the early stages of recognizing.

While there is no single solution to address AI fairness and robustness, nor are there universally accepted metrics to detect fairness concerns, we are committed to collaborating with experts in the field to tackle these issues and hope to spark further research in these areas.

Access the Casual Conversations v2 dataset

Read the paper

Acknowledgements

We would also like to thank Prof. Pascale Fung and Dr. Ellis Monk for their contributions."
Meta_Blog,https://ai.meta.com/blog/muavic-audio-visual-speech-translation-benchmark/,,MuAViC: The first audio-video speech translation benchmark,"In countless everyday situations, background noise — the sound of traffic, music, other people speaking – makes it more difficult to understand what other people are saying. Humans often use information from our other senses, especially vision, to help us communicate (as pointed out by Harry McGurk and John MacDonald in their 1976 study “Hearing Lips and Seeing Voices”). For example, if you're speaking to a friend at a loud concert, you will likely focus on their face to supplement what you can hear.

AI researchers have recently built systems (such as Meta AI’s publicly available AV-HuBERT and RAVen models) that use visual information to improve performance for English speech recognition tasks. Now, Meta AI is releasing MuAViC (Multilingual Audio-Visual Corpus), the first benchmark that makes it possible to use audio-visual learning for highly accurate speech translation. We’ve used MuAViC to train our AV-HuBERT model to translate speech in noisy, challenging settings, where it outperforms other leading translation models.

Something Went Wrong We're having trouble playing this video. Learn more

In this example, the AV-HuBERT model's transcription contains one error (""Either"" instead of ""Hi there"") but still achieves much greater accuracy than the other model.

With No Language Left Behind and Universal Speech Translator, Meta has focused on speech translation research because it has incredible potential to break down communication barriers and bring people together. We’re excited to see how others in the research community use MuAViC to create translation systems that work well in real-world conditions.

Creating MuAViC

Because of the scarcity of suitable training data, extending audio-visual understanding to speech translation was previously unexplored. Collecting and processing audio-video data typically requires more resources than collecting audio data alone.

MuAViC is the first benchmark for audio-video speech translation and the largest multilingual benchmark for audio-video speech recognition. It contains roughly 1,200 hours of transcribed data spanning 9 languages.

For English talks, we reuse audio-visual data from the LRS3 dataset and align it with a machine translation corpus using a text-matching algorithm. Matched examples are then paired with the corresponding target sentences in the machine translation corpus for translation labels. We apply exact text matching for development set and test set examples to ensure the best accuracy. For training set examples without a match, we acquire pseudo-translation labels from a machine translation model.

For non-English talks, we reuse the audio-only data, transcriptions, and text translations collected in the speech translation dataset. To add the visual modality, we acquire video tracks of the original recordings and align processed video data with the audio data to create audio-visual data. Although all the audio data is transcribed, only a subset of it is translated. We acquire pseudo-translation labels using the same machine translation model as earlier.

Training end-to-end models

We used Meta’s AV-HuBERT architecture to create end-to-end audio-video speech recognition and audio-video speech translation models. Given an aligned pair of audio-video data, our model is able to process both modalities and fuse their representations into a unified space that can be used for either speech recognition or translation tasks. And if either modality is missing, AV-HuBERT can still process the available input modality (but with less efficiency).

Something Went Wrong We're having trouble playing this video. Learn more

In this video, the model must deal with background music (rather than background noise, as in the first video above).

Our model’s most noteworthy feature is its robustness to noise. If the audio modality is distorted because of noise or any other factor, the model will rely more on the visual modality to perform the task properly. We tested our models against a state-of-the-art model for speech recognition and X-En speech translation tasks in environments both with and without noise.

This chart compares model performance on speech recognition tasks spanning nine different languages. Meta's AV-HuBERT model doesn't degrade significantly in noisy environments, while the current state-of-the-art model does.

Similarly, the performance of Meta’s AV-HuBERT model does not significantly degrade compared with that of the state-of-the-art model, using the X-En speech translation task spanning six different languages.

Toward robust speech translation

MuAVic enables researchers to build robust speech recognition and translation systems for different languages. We’ve released the corpus as well as our audio-visual speech recognition and translation models covering nine different languages. We hope this will help the community build even better, more robust speech models. We are excited about the future of powerful robust models.

Read the full papers:

We’d like to acknowledge the contributions of Vedanju Goswami, Wei-Ning Hsu, Bowen Shi to the work discussed in this blog post."
Meta_Blog,https://ai.meta.com/blog/muavic-audio-visual-speech-translation-benchmark/,,MuAViC: The first audio-video speech translation benchmark,"In countless everyday situations, background noise — the sound of traffic, music, other people speaking – makes it more difficult to understand what other people are saying. Humans often use information from our other senses, especially vision, to help us communicate (as pointed out by Harry McGurk and John MacDonald in their 1976 study “Hearing Lips and Seeing Voices”). For example, if you're speaking to a friend at a loud concert, you will likely focus on their face to supplement what you can hear.

AI researchers have recently built systems (such as Meta AI’s publicly available AV-HuBERT and RAVen models) that use visual information to improve performance for English speech recognition tasks. Now, Meta AI is releasing MuAViC (Multilingual Audio-Visual Corpus), the first benchmark that makes it possible to use audio-visual learning for highly accurate speech translation. We’ve used MuAViC to train our AV-HuBERT model to translate speech in noisy, challenging settings, where it outperforms other leading translation models.

Something Went Wrong We're having trouble playing this video. Learn more

In this example, the AV-HuBERT model's transcription contains one error (""Either"" instead of ""Hi there"") but still achieves much greater accuracy than the other model.

With No Language Left Behind and Universal Speech Translator, Meta has focused on speech translation research because it has incredible potential to break down communication barriers and bring people together. We’re excited to see how others in the research community use MuAViC to create translation systems that work well in real-world conditions.

Creating MuAViC

Because of the scarcity of suitable training data, extending audio-visual understanding to speech translation was previously unexplored. Collecting and processing audio-video data typically requires more resources than collecting audio data alone.

MuAViC is the first benchmark for audio-video speech translation and the largest multilingual benchmark for audio-video speech recognition. It contains roughly 1,200 hours of transcribed data spanning 9 languages.

For English talks, we reuse audio-visual data from the LRS3 dataset and align it with a machine translation corpus using a text-matching algorithm. Matched examples are then paired with the corresponding target sentences in the machine translation corpus for translation labels. We apply exact text matching for development set and test set examples to ensure the best accuracy. For training set examples without a match, we acquire pseudo-translation labels from a machine translation model.

For non-English talks, we reuse the audio-only data, transcriptions, and text translations collected in the speech translation dataset. To add the visual modality, we acquire video tracks of the original recordings and align processed video data with the audio data to create audio-visual data. Although all the audio data is transcribed, only a subset of it is translated. We acquire pseudo-translation labels using the same machine translation model as earlier.

Training end-to-end models

We used Meta’s AV-HuBERT architecture to create end-to-end audio-video speech recognition and audio-video speech translation models. Given an aligned pair of audio-video data, our model is able to process both modalities and fuse their representations into a unified space that can be used for either speech recognition or translation tasks. And if either modality is missing, AV-HuBERT can still process the available input modality (but with less efficiency).

Something Went Wrong We're having trouble playing this video. Learn more

In this video, the model must deal with background music (rather than background noise, as in the first video above).

Our model’s most noteworthy feature is its robustness to noise. If the audio modality is distorted because of noise or any other factor, the model will rely more on the visual modality to perform the task properly. We tested our models against a state-of-the-art model for speech recognition and X-En speech translation tasks in environments both with and without noise.

This chart compares model performance on speech recognition tasks spanning nine different languages. Meta's AV-HuBERT model doesn't degrade significantly in noisy environments, while the current state-of-the-art model does.

Similarly, the performance of Meta’s AV-HuBERT model does not significantly degrade compared with that of the state-of-the-art model, using the X-En speech translation task spanning six different languages.

Toward robust speech translation

MuAVic enables researchers to build robust speech recognition and translation systems for different languages. We’ve released the corpus as well as our audio-visual speech recognition and translation models covering nine different languages. We hope this will help the community build even better, more robust speech models. We are excited about the future of powerful robust models.

Read the full papers:

We’d like to acknowledge the contributions of Vedanju Goswami, Wei-Ning Hsu, Bowen Shi to the work discussed in this blog post."
Meta_Blog,https://ai.meta.com/blog/large-language-model-llama-meta-ai/,,"Introducing LLaMA: A foundational, 65-billion-parameter language model","UPDATE: We just launched Llama 2 - for more information on the latest see our blog post on Llama 2.

As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI), a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models, further democratizing access in this important, fast-changing field.

Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches, validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks. We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.

Over the last year, large language models — natural language processing (NLP) systems with billions of parameters — have shown new capabilities to generate creative text, solve mathematical theorems, predict protein structures, answer reading comprehension questions, and more. They are one of the clearest cases of the substantial potential benefits AI can offer at scale to billions of people.

Even with all the recent advancements in large language models, full research access to them remains limited because of the resources that are required to train and run such large models. This restricted access has limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues, such as bias, toxicity, and the potential for generating misinformation.

Smaller models trained on more tokens — which are pieces of words — are easier to retrain and fine-tune for specific potential product use cases. We trained LLaMA 65B and LLaMA 33B on 1.4 trillion tokens. Our smallest model, LLaMA 7B, is trained on one trillion tokens.

Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text. To train our model, we chose text from the 20 languages with the most speakers, focusing on those with Latin and Cyrillic alphabets.

There is still more research that needs to be done to address the risks of bias, toxic comments, and hallucinations in large language models. Like other models, LLaMA shares these challenges. As a foundation model, LLaMA is designed to be versatile and can be applied to many different use cases, versus a fine-tuned model that is designed for a specific task. By sharing the code for LLaMA, other researchers can more easily test new approaches to limiting or eliminating these problems in large language models. We also provide in the paper a set of evaluations on benchmarks evaluating model biases and toxicity to show the model’s limitations and to support further research in this crucial area.

To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license focused on research use cases. Access to the model will be granted on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world. People interested in applying for access can find the link to the application in our research paper.

We believe that the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular. We look forward to seeing what the community can learn — and eventually build — using LLaMA.

Read the paper

Read the model card

Apply for access to LLaMA"
Meta_Blog,https://ai.meta.com/blog/large-language-model-llama-meta-ai/,,"Introducing LLaMA: A foundational, 65-billion-parameter language model","UPDATE: We just launched Llama 2 - for more information on the latest see our blog post on Llama 2.

As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI), a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models, further democratizing access in this important, fast-changing field.

Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches, validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks. We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.

Over the last year, large language models — natural language processing (NLP) systems with billions of parameters — have shown new capabilities to generate creative text, solve mathematical theorems, predict protein structures, answer reading comprehension questions, and more. They are one of the clearest cases of the substantial potential benefits AI can offer at scale to billions of people.

Even with all the recent advancements in large language models, full research access to them remains limited because of the resources that are required to train and run such large models. This restricted access has limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues, such as bias, toxicity, and the potential for generating misinformation.

Smaller models trained on more tokens — which are pieces of words — are easier to retrain and fine-tune for specific potential product use cases. We trained LLaMA 65B and LLaMA 33B on 1.4 trillion tokens. Our smallest model, LLaMA 7B, is trained on one trillion tokens.

Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text. To train our model, we chose text from the 20 languages with the most speakers, focusing on those with Latin and Cyrillic alphabets.

There is still more research that needs to be done to address the risks of bias, toxic comments, and hallucinations in large language models. Like other models, LLaMA shares these challenges. As a foundation model, LLaMA is designed to be versatile and can be applied to many different use cases, versus a fine-tuned model that is designed for a specific task. By sharing the code for LLaMA, other researchers can more easily test new approaches to limiting or eliminating these problems in large language models. We also provide in the paper a set of evaluations on benchmarks evaluating model biases and toxicity to show the model’s limitations and to support further research in this crucial area.

To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license focused on research use cases. Access to the model will be granted on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world. People interested in applying for access can find the link to the application in our research paper.

We believe that the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular. We look forward to seeing what the community can learn — and eventually build — using LLaMA.

Read the paper

Read the model card

Apply for access to LLaMA"
Meta_Blog,https://ai.meta.com/blog/responsible-ai-progress-meta-2022/,,Meta’s progress and learnings in AI fairness and transparency,"While AI has brought huge advancements to humanity and our planet, it also has the potential to cause unintended consequences, and technology companies must proactively work to mitigate these issues. So, as we did in 2021, we’re providing an update on our Responsible AI efforts over the past year.

The work described in this blog post includes datasets, balancing privacy and fairness, preventing bias in ad delivery systems, avoiding harmful or disrespectful associations, giving people more control over what they see, offering more transparency into AI models, and collaborating on standards and governance.

Our Responsible AI efforts are propelled by a cross-disciplinary team whose mission is to help ensure that AI at Meta benefits people and society. Our Civil Rights Team, for example, has been integral to our work, bringing subject-matter expertise with technical, policy, and legal assessments and collaboratively designing technical solutions.

Meta’s work on Responsible AI is driven by our belief that everyone should have equitable access to information, services, and opportunities. We believe that the responsible foundation we are building will ultimately shape future technologies, including the metaverse. As we reflect on the progress we made in 2022, we hope to foster more collaborative and transparent dialogue across disciplines and audiences about the path ahead for these critical issues.

Building diverse datasets and powerful tools for more inclusive AI products

One way we are addressing AI fairness through research is the creation and distribution of more diverse datasets. Datasets that are used to train AI models can reflect biases, which are then passed on to the system. But biases might also be due to what isn’t the training data. A lack of diverse data — or data that represents a wide range of people and experiences — can lead to AI-powered outcomes that reflect problematic stereotypes or fail to work equally well for everyone.

In 2022, we worked to prepare the Casual Conversations v2 (CCv2) dataset, which is unique in the field in terms of its proposed categories and the countries where the data collection will take place. This work, which we will release in 2023, is a continuation of the Casual Conversations dataset we released in 2021, which is composed of more than 45,000 videos designed to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of ages, genders, skin tones, and ambient lighting conditions.

In 2022, we also introduced and open-sourced two new datasets to help measure fairness and mitigate potential bias in natural language processing (NLP) models. In particular, these datasets include a more comprehensive representation of different demographic dimensions to measure fairness in these models, including terms for gender identity, age, race, and people with disabilities.

Developing reliable, large-scale ways of measuring fairness and mitigating bias gives AI researchers and practitioners helpful benchmarks that can be used to test NLP systems, driving progress toward the goal of ensuring that AI systems treat everyone fairly. We shared these datasets with the research community so that people can better assess the fairness of their text-based AI systems and expand their terminology.

In addition to our work on datasets, we’ve continued to explore new ways of understanding fairness in complex AI systems. For example, we proposed a method for considering preference-based fairness in personalized recommender systems that would allow practitioners to get a more holistic view of fairness across all groups. We detailed this work in a research paper that was named Outstanding Paper at the 2022 AAAI Conference.

By fostering the development of more inclusive datasets, we can create AI systems with the potential to bring the world closer together, helping people communicate across languages and cultures and creating experiences that reflect the diversity of the more than 3 billion people who use Meta’s platforms.

Protecting privacy while addressing fairness concerns

Improving fairness will often require measuring the impact of AI systems on different demographic populations and mitigating unfair differences. Yet the data necessary to do so is not always available — and even when it is, collecting it and storing it can raise privacy concerns. After engaging with civil rights advocates and human rights groups that further confirmed the fairness challenges, we identified new approaches to help us access data with the potential to meaningfully measure the fairness of the AI models on our platforms across races.

We made advancements in 2022 in our ability to measure whether people’s experiences with our technology differ across race. We launched a research project on Instagram to make progress in assessing and improving our technologies to advance fairness. We worked with Oasis Labs, a privacy-focused technology company that, while honoring their privacy, lets people in the United States who choose to self-identify their race for research purposes do so. We worked with research partners to safeguard survey responses by adapting a well-established privacy-enhancing method called secure multiparty computation (SMPC), in which data is securely distributed among multiple facilitators who together can perform computations over the combined encrypted information.

This encryption-based approach, in which data is split between multiple parties, has been used for years in other fields, such as to secure auctions, distributed voting, and statistical analysis. SMPC provides a strong guarantee that individuals’ responses to the survey cannot be accessed by any party, including Meta.

In the area of ad fairness , our privacy-enhanced version of the Bayesian Improved Surname Geocoding (BISG) method, which we announced in 2021, is being used to make progress by enabling iterative, aggregate measurement (discussed below).

Through approaches like these, we can better examine our systems to identify areas where we can make improvements that get us even closer to our goal of building and operating AI systems that treat everyone fairly.

Innovating to improve fairness in ad delivery

A critical aspect of fairness is ensuring that people of all backgrounds have equitable access to information about important life opportunities, like jobs, credit, and housing. Our policies already prohibit advertisers from using our ad products to discriminate against individuals or groups of people. Specifically, to better protect against discrimination, we give advertisers running housing ads on our platforms a limited number of targeting options while setting up their campaigns, including a restriction on using age, gender, or zip code. However, even with neutral targeting options and model features, factors such as people’s interests, their activity on the platform, or competition across all ad auctions for different audiences could affect how ads are distributed to different demographic groups. Therefore, as part of our settlement with the Department of Justice and our ongoing work with the Department of Housing and Urban Development, we designed and started to roll out a new technology called the Variance Reduction System (VRS), which aims to better ensure an equitable distribution of housing ads and eventually employment and credit ads.

The VRS uses reinforcement learning, a type of machine learning (ML) that learns from trial and error to optimize toward a predefined outcome, so that the audience that ends up seeing an ad for housing, employment, or credit more closely reflects the population of people who are eligible to see that ad.

We’ll do this by regularly measuring the actual audience for a particular ad to see how it compares with the demographic distribution (age, gender, and estimated race or ethnicity) of the audience the advertiser has selected. Importantly, the Variance Reduction System will not be provided with individual-level age, gender, or estimated race/ethnicity to make these determinations but instead will leverage aggregate demographic measurements. It will use the privacy-enhanced BISG method (described above) to measure estimated race. If our measurements show a wide variance in the demographics of the selected audience compared with who is actually seeing the ad, the VRS will automatically act to reduce that variance. In the process, we’ll help ensure that ads are delivered more equitably and can be seen by audiences that otherwise might not have.

Generating responsible associations

Fairness doesn’t just mean improving equitable access to positive opportunities or ensuring that our products work equally well regardless of someone’s demographic characteristics or what language they are using. It also means working to ensure that our AI systems don’t generate content that is harmful or disrespectful to historically marginalized communities.

Meta already has numerous systems and policies in place to detect and remove harmful content such as hate speech. But harmful content of a different type can be generated inadvertently when AI-driven recommendation systems produce a harmful association. This can arise when pieces of content that are otherwise harmless as standalone topics are paired in an offensive way. When associated with groups of people, even benign topics form associations that can become potentially problematic or degrading, exacerbating existing stereotypes.

The risk of problematic associations is a shared challenge across platforms that use AI to make recommendations or generate content, including social media platforms. The harmful conceptual association between groups of people and negative semantic terms can arise through a variety of routes, which can reflect and reinforce biases and bigotries embedded in social and semantic data.

In 2022, we assembled a cross-disciplinary team, including people from our Civil Rights, Engineering, AI Research, Policy, and Product teams, to better understand problematic content associations in several of our end-to-end systems and to implement technical mitigations to reduce the chance of them occurring on our platforms that use AI models.

As part of this collaborative effort, we carefully constructed and systematically reviewed the knowledge base of interest topics for usage in advanced mitigations that more precisely target the problematic associations. As more research is done in this area and shared with the greater community, we expect to build on this progress and to continue to improve our systems.

Giving more control over AI-driven feeds and recommendations

AI-driven feeds and recommendations are a powerful tool for helping people find the people and content they are most interested in, but we want to make sure that people can manage their experience in ways that don’t necessarily rely on AI-based ranking. While we already allow people to adjust their Feed preferences in a variety of ways, this year we also introduced an AI-based feature called Show More/Show Less that lets people directly influence their AI-driven personal recommendations.

When people click to show more or less of a type of content when the buttons are featured on select posts, our AI systems work to understand the intent behind the click. For example, clicking Show Less on a cousin’s post about their new convertible might mean “show me fewer posts by that person” or “show me fewer posts about cars.” An effective recommendation system must be able to differentiate the person’s intent in order to successfully deliver the type of content people want to see, and as more people use this feature, the better it will get at understanding what they do and don’t want to see.

This year on Facebook, we also introduced a Feeds tab, which helps people find the most recent posts from their friends, Favorites, Pages, and groups. People can curate a Favorites list of the friends and Pages they care about most and filter their content in this new tab. We rolled out a similar feature on Instagram, where people can choose to see posts from their favorite accounts in a chronological feed. We also rolled out Following, a feature that allows people to see posts only from people they’re following

Developing new methods for explaining our AI systems

Because AI systems are complex, it is important that we develop documentation that explains how systems work in a way that experts and nonexperts alike can understand. One way we’ve done this is by prototyping an AI System Card tool that provides insight into an AI system’s underlying architecture and helps better explain how the system operates.

System cards are one of the many ways to practice transparency about AI models. For a close look at individual AI models, we shared Model Cards for some of our most impactful research releases, including BlenderBot, an open source language generation model, and OPT-175B, the first 175-billion-parameter language model to be made available to the broader AI research community. We also shared the code and a detailed look into the development process. By sharing these sorts of details about OPT-175B, we aim to accelerate research around language generation systems so the broader field can work toward making these systems safer, more useful, and more robust.

We also introduced our proposal for Method Cards, intended to guide ML engineers on how to mitigate potential shortcomings in order to fix bugs or improve a system’s performance. Our research aims to increase the transparency and reproducibility of ML systems by allowing stakeholders to reproduce the models, understand the rationale behind their designs, and introduce adaptations in an informed way.

As the industry evolves and discussions about model and system documentation and transparency continue, we will further identify opportunities to undertake and iterate on our approach over time, so we can reflect product changes, evolving industry standards, and expectations around AI transparency.

Testing new policy approaches to AI transparency, explainability, and governance

The rapid advance of emerging technologies makes it difficult to fully understand and anticipate how they might eventually impact communities around the world. To help develop forward-looking policies around the development and use of new technology Meta supports Open Loop, a global experimental governance program. By involving governments, tech companies, academia, and civil society, Open Loop aims to connect tech and policy innovation for closer collaboration between those building emerging technologies and those regulating them.

AI transparency and explainability were the focus of two of our recent Open Loop programs. In 2022, we published the findings and recommendations of our policy prototyping program on AI transparency and explainability in Singapore, which was rolled out in the Asia-Pacific (APAC) region in partnership with Singapore’s Infocomm Media Development Authority and Personal Data Protection Commission with 12 APAC companies to test Singapore's Model AI Governance Framework. The report summarizes the feedback received from the participating companies in implementing Singapore’s Model AI Governance Framework, and makes recommendations to policymakers on how to further improve the frameworks based on real-world implementation experience and feedback from industry. A similar exercise was deployed in Mexico, with the support of Mexico’s National Institute for Transparency, Access to Information and Personal Data Protection, where we tested a framework on transparency and explainability for AI systems in the country (forthcoming).

Moreover, as part of our second policy prototyping program in Europe, we recently published our first report and recommendations on the European Artificial Intelligence Act in partnership with European governments and regulatory authorities. Among the provisions tested by 53 AI startups and companies operating in the European Union, we assessed transparency requirements of the draft AI Act, commenting on their clarity, feasibility, and cost-effectiveness. We asked companies what level of technical skill would be necessary to meet the requirement of enabling “human oversight,” as well as the required skill level to enable interpretability. One of the recommendations from the report was to consider distinguishing more clearly between different audiences for explanations and other transparency requirements. We also assessed transparency obligations for individuals interacting with AI systems via a survey with 469 participants from European Union countries who were shown different variations of AI notifications mockups (report forthcoming). The study provides preliminary insights on the effectiveness of disclosure notifications on users’ understanding, trust, and sense of agency.

These Open Loop programs are emblematic of Meta’s evidence-based, broadly collaborative approach to establishing standards around the future of AI governance, and we look forward to continuing these programs in partnership with a broad selection of participating regulators, companies, and expert stakeholders.

Working together to build AI responsibly

By listening to people with lived experiences, subject matter experts, and policymakers, we hope to proactively promote and advance the responsible design and operation of AI systems.

We look forward to sharing more updates in the future and will continue to engage with diverse stakeholders about how we can move forward together responsibly."
Meta_Blog,https://ai.meta.com/blog/responsible-ai-progress-meta-2022/,,Meta’s progress and learnings in AI fairness and transparency,"While AI has brought huge advancements to humanity and our planet, it also has the potential to cause unintended consequences, and technology companies must proactively work to mitigate these issues. So, as we did in 2021, we’re providing an update on our Responsible AI efforts over the past year.

The work described in this blog post includes datasets, balancing privacy and fairness, preventing bias in ad delivery systems, avoiding harmful or disrespectful associations, giving people more control over what they see, offering more transparency into AI models, and collaborating on standards and governance.

Our Responsible AI efforts are propelled by a cross-disciplinary team whose mission is to help ensure that AI at Meta benefits people and society. Our Civil Rights Team, for example, has been integral to our work, bringing subject-matter expertise with technical, policy, and legal assessments and collaboratively designing technical solutions.

Meta’s work on Responsible AI is driven by our belief that everyone should have equitable access to information, services, and opportunities. We believe that the responsible foundation we are building will ultimately shape future technologies, including the metaverse. As we reflect on the progress we made in 2022, we hope to foster more collaborative and transparent dialogue across disciplines and audiences about the path ahead for these critical issues.

Building diverse datasets and powerful tools for more inclusive AI products

One way we are addressing AI fairness through research is the creation and distribution of more diverse datasets. Datasets that are used to train AI models can reflect biases, which are then passed on to the system. But biases might also be due to what isn’t the training data. A lack of diverse data — or data that represents a wide range of people and experiences — can lead to AI-powered outcomes that reflect problematic stereotypes or fail to work equally well for everyone.

In 2022, we worked to prepare the Casual Conversations v2 (CCv2) dataset, which is unique in the field in terms of its proposed categories and the countries where the data collection will take place. This work, which we will release in 2023, is a continuation of the Casual Conversations dataset we released in 2021, which is composed of more than 45,000 videos designed to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of ages, genders, skin tones, and ambient lighting conditions.

In 2022, we also introduced and open-sourced two new datasets to help measure fairness and mitigate potential bias in natural language processing (NLP) models. In particular, these datasets include a more comprehensive representation of different demographic dimensions to measure fairness in these models, including terms for gender identity, age, race, and people with disabilities.

Developing reliable, large-scale ways of measuring fairness and mitigating bias gives AI researchers and practitioners helpful benchmarks that can be used to test NLP systems, driving progress toward the goal of ensuring that AI systems treat everyone fairly. We shared these datasets with the research community so that people can better assess the fairness of their text-based AI systems and expand their terminology.

In addition to our work on datasets, we’ve continued to explore new ways of understanding fairness in complex AI systems. For example, we proposed a method for considering preference-based fairness in personalized recommender systems that would allow practitioners to get a more holistic view of fairness across all groups. We detailed this work in a research paper that was named Outstanding Paper at the 2022 AAAI Conference.

By fostering the development of more inclusive datasets, we can create AI systems with the potential to bring the world closer together, helping people communicate across languages and cultures and creating experiences that reflect the diversity of the more than 3 billion people who use Meta’s platforms.

Protecting privacy while addressing fairness concerns

Improving fairness will often require measuring the impact of AI systems on different demographic populations and mitigating unfair differences. Yet the data necessary to do so is not always available — and even when it is, collecting it and storing it can raise privacy concerns. After engaging with civil rights advocates and human rights groups that further confirmed the fairness challenges, we identified new approaches to help us access data with the potential to meaningfully measure the fairness of the AI models on our platforms across races.

We made advancements in 2022 in our ability to measure whether people’s experiences with our technology differ across race. We launched a research project on Instagram to make progress in assessing and improving our technologies to advance fairness. We worked with Oasis Labs, a privacy-focused technology company that, while honoring their privacy, lets people in the United States who choose to self-identify their race for research purposes do so. We worked with research partners to safeguard survey responses by adapting a well-established privacy-enhancing method called secure multiparty computation (SMPC), in which data is securely distributed among multiple facilitators who together can perform computations over the combined encrypted information.

This encryption-based approach, in which data is split between multiple parties, has been used for years in other fields, such as to secure auctions, distributed voting, and statistical analysis. SMPC provides a strong guarantee that individuals’ responses to the survey cannot be accessed by any party, including Meta.

In the area of ad fairness , our privacy-enhanced version of the Bayesian Improved Surname Geocoding (BISG) method, which we announced in 2021, is being used to make progress by enabling iterative, aggregate measurement (discussed below).

Through approaches like these, we can better examine our systems to identify areas where we can make improvements that get us even closer to our goal of building and operating AI systems that treat everyone fairly.

Innovating to improve fairness in ad delivery

A critical aspect of fairness is ensuring that people of all backgrounds have equitable access to information about important life opportunities, like jobs, credit, and housing. Our policies already prohibit advertisers from using our ad products to discriminate against individuals or groups of people. Specifically, to better protect against discrimination, we give advertisers running housing ads on our platforms a limited number of targeting options while setting up their campaigns, including a restriction on using age, gender, or zip code. However, even with neutral targeting options and model features, factors such as people’s interests, their activity on the platform, or competition across all ad auctions for different audiences could affect how ads are distributed to different demographic groups. Therefore, as part of our settlement with the Department of Justice and our ongoing work with the Department of Housing and Urban Development, we designed and started to roll out a new technology called the Variance Reduction System (VRS), which aims to better ensure an equitable distribution of housing ads and eventually employment and credit ads.

The VRS uses reinforcement learning, a type of machine learning (ML) that learns from trial and error to optimize toward a predefined outcome, so that the audience that ends up seeing an ad for housing, employment, or credit more closely reflects the population of people who are eligible to see that ad.

We’ll do this by regularly measuring the actual audience for a particular ad to see how it compares with the demographic distribution (age, gender, and estimated race or ethnicity) of the audience the advertiser has selected. Importantly, the Variance Reduction System will not be provided with individual-level age, gender, or estimated race/ethnicity to make these determinations but instead will leverage aggregate demographic measurements. It will use the privacy-enhanced BISG method (described above) to measure estimated race. If our measurements show a wide variance in the demographics of the selected audience compared with who is actually seeing the ad, the VRS will automatically act to reduce that variance. In the process, we’ll help ensure that ads are delivered more equitably and can be seen by audiences that otherwise might not have.

Generating responsible associations

Fairness doesn’t just mean improving equitable access to positive opportunities or ensuring that our products work equally well regardless of someone’s demographic characteristics or what language they are using. It also means working to ensure that our AI systems don’t generate content that is harmful or disrespectful to historically marginalized communities.

Meta already has numerous systems and policies in place to detect and remove harmful content such as hate speech. But harmful content of a different type can be generated inadvertently when AI-driven recommendation systems produce a harmful association. This can arise when pieces of content that are otherwise harmless as standalone topics are paired in an offensive way. When associated with groups of people, even benign topics form associations that can become potentially problematic or degrading, exacerbating existing stereotypes.

The risk of problematic associations is a shared challenge across platforms that use AI to make recommendations or generate content, including social media platforms. The harmful conceptual association between groups of people and negative semantic terms can arise through a variety of routes, which can reflect and reinforce biases and bigotries embedded in social and semantic data.

In 2022, we assembled a cross-disciplinary team, including people from our Civil Rights, Engineering, AI Research, Policy, and Product teams, to better understand problematic content associations in several of our end-to-end systems and to implement technical mitigations to reduce the chance of them occurring on our platforms that use AI models.

As part of this collaborative effort, we carefully constructed and systematically reviewed the knowledge base of interest topics for usage in advanced mitigations that more precisely target the problematic associations. As more research is done in this area and shared with the greater community, we expect to build on this progress and to continue to improve our systems.

Giving more control over AI-driven feeds and recommendations

AI-driven feeds and recommendations are a powerful tool for helping people find the people and content they are most interested in, but we want to make sure that people can manage their experience in ways that don’t necessarily rely on AI-based ranking. While we already allow people to adjust their Feed preferences in a variety of ways, this year we also introduced an AI-based feature called Show More/Show Less that lets people directly influence their AI-driven personal recommendations.

When people click to show more or less of a type of content when the buttons are featured on select posts, our AI systems work to understand the intent behind the click. For example, clicking Show Less on a cousin’s post about their new convertible might mean “show me fewer posts by that person” or “show me fewer posts about cars.” An effective recommendation system must be able to differentiate the person’s intent in order to successfully deliver the type of content people want to see, and as more people use this feature, the better it will get at understanding what they do and don’t want to see.

This year on Facebook, we also introduced a Feeds tab, which helps people find the most recent posts from their friends, Favorites, Pages, and groups. People can curate a Favorites list of the friends and Pages they care about most and filter their content in this new tab. We rolled out a similar feature on Instagram, where people can choose to see posts from their favorite accounts in a chronological feed. We also rolled out Following, a feature that allows people to see posts only from people they’re following

Developing new methods for explaining our AI systems

Because AI systems are complex, it is important that we develop documentation that explains how systems work in a way that experts and nonexperts alike can understand. One way we’ve done this is by prototyping an AI System Card tool that provides insight into an AI system’s underlying architecture and helps better explain how the system operates.

System cards are one of the many ways to practice transparency about AI models. For a close look at individual AI models, we shared Model Cards for some of our most impactful research releases, including BlenderBot, an open source language generation model, and OPT-175B, the first 175-billion-parameter language model to be made available to the broader AI research community. We also shared the code and a detailed look into the development process. By sharing these sorts of details about OPT-175B, we aim to accelerate research around language generation systems so the broader field can work toward making these systems safer, more useful, and more robust.

We also introduced our proposal for Method Cards, intended to guide ML engineers on how to mitigate potential shortcomings in order to fix bugs or improve a system’s performance. Our research aims to increase the transparency and reproducibility of ML systems by allowing stakeholders to reproduce the models, understand the rationale behind their designs, and introduce adaptations in an informed way.

As the industry evolves and discussions about model and system documentation and transparency continue, we will further identify opportunities to undertake and iterate on our approach over time, so we can reflect product changes, evolving industry standards, and expectations around AI transparency.

Testing new policy approaches to AI transparency, explainability, and governance

The rapid advance of emerging technologies makes it difficult to fully understand and anticipate how they might eventually impact communities around the world. To help develop forward-looking policies around the development and use of new technology Meta supports Open Loop, a global experimental governance program. By involving governments, tech companies, academia, and civil society, Open Loop aims to connect tech and policy innovation for closer collaboration between those building emerging technologies and those regulating them.

AI transparency and explainability were the focus of two of our recent Open Loop programs. In 2022, we published the findings and recommendations of our policy prototyping program on AI transparency and explainability in Singapore, which was rolled out in the Asia-Pacific (APAC) region in partnership with Singapore’s Infocomm Media Development Authority and Personal Data Protection Commission with 12 APAC companies to test Singapore's Model AI Governance Framework. The report summarizes the feedback received from the participating companies in implementing Singapore’s Model AI Governance Framework, and makes recommendations to policymakers on how to further improve the frameworks based on real-world implementation experience and feedback from industry. A similar exercise was deployed in Mexico, with the support of Mexico’s National Institute for Transparency, Access to Information and Personal Data Protection, where we tested a framework on transparency and explainability for AI systems in the country (forthcoming).

Moreover, as part of our second policy prototyping program in Europe, we recently published our first report and recommendations on the European Artificial Intelligence Act in partnership with European governments and regulatory authorities. Among the provisions tested by 53 AI startups and companies operating in the European Union, we assessed transparency requirements of the draft AI Act, commenting on their clarity, feasibility, and cost-effectiveness. We asked companies what level of technical skill would be necessary to meet the requirement of enabling “human oversight,” as well as the required skill level to enable interpretability. One of the recommendations from the report was to consider distinguishing more clearly between different audiences for explanations and other transparency requirements. We also assessed transparency obligations for individuals interacting with AI systems via a survey with 469 participants from European Union countries who were shown different variations of AI notifications mockups (report forthcoming). The study provides preliminary insights on the effectiveness of disclosure notifications on users’ understanding, trust, and sense of agency.

These Open Loop programs are emblematic of Meta’s evidence-based, broadly collaborative approach to establishing standards around the future of AI governance, and we look forward to continuing these programs in partnership with a broad selection of participating regulators, companies, and expert stakeholders.

Working together to build AI responsibly

By listening to people with lived experiences, subject matter experts, and policymakers, we hope to proactively promote and advance the responsible design and operation of AI systems.

We look forward to sharing more updates in the future and will continue to engage with diverse stakeholders about how we can move forward together responsibly."
Meta_Blog,https://ai.meta.com/blog/advertising-fairness-variance-reduction-system-vrs/,,A new system to help ensure ads are delivered fairly to different demographic groups,"Advocates, researchers, and others have long been focused on the relationship between advertising and economic mobility, especially when it comes to ads that pertain to housing, employment, and credit opportunities. People can more easily pursue a new job or consider moving to a new home when they’re aware of the options out there, but the enduring effects of historically unequal treatment in these areas still shape the economic opportunities of too many. Across both the tech industry and the AI research community, approaches to fairness are still evolving, including in the realm of personalized, auction-based advertising systems. But we know we can’t wait for a consensus to make progress in addressing these important concerns, so today we’re sharing early but significant changes Meta has made to our advertising system that build on and reflect the evolution of our approach to ad fairness.

After more than a year of collaboration with the DOJ and HUD, Meta has developed a new method called the Variance Reduction System (VRS) to help ensure an equitable distribution of ads on our services. Our policies already prohibit advertisers from using our ad products to discriminate against individuals or groups of people, and we’ve implemented additional safeguards, such as disallowing the use of gender, age, or zip code targeting for certain ads. But even without these sorts of targeting options, factors such as people’s interests or activity on a service could affect how ads are distributed to different demographic groups. The goal of the VRS is to help ensure that the audience that ends up seeing a housing, employment, or credit ad more closely reflects the eligible targeted audience for that ad. We’ll do this by regularly measuring the actual audience for a particular ad to see how it compares with the demographic distribution — age, gender, and estimated race or ethnicity — of the audience the advertiser has selected. (Our initial launch will focus on gender and estimated race.) To implement this system in a way that respects people’s privacy, the system relies on aggregate measurements as well as privacy-enhancing approaches to measure race and ethnicity at the aggregate level. We will begin by applying the VRS to housing ads in the United States, and will expand it to employment and credit ads in the United States over the next year.

This system complements and extends our other longstanding efforts to help advance fairness in our ads system, such as limiting the targeting options available to advertisers running housing ads on our services, to make progress toward a more equitable distribution of ads through our ad delivery process.

In addition to sharing this blog post summarizing how the VRS works, we’re publishing technical details in a new white paper, available here:

Toward fairness in personalized ads

How the VRS works

The VRS is an offline reinforcement learning framework with the explicit goal of minimizing the variance in number of ad views between people who have seen the ad and the broader eligible audience of people who could have seen the ad. Reinforcement learning is a type of machine learning that learns from trial and error to optimize toward a predefined outcome — in this case, to minimize ad impression variance across demographic subgroups no matter the cause of that variance. Importantly, the VRS will not be provided with individual-level age, gender, or estimated race/ethnicity to make these determinations. The system will instead receive aggregate measurements of variance across these demographics.

The system starts by measuring the demographic distribution of the baseline or eligible ratio of the age, gender, and estimated racial/ethnic distribution of the population of users to whom an advertiser has indicated they would like their ad to be displayed. As the ad is being delivered, we periodically measure the delivery ratio, or the demographic distribution of the impressions of an ad.

The VRS relies on a controller that has the ability to change one of the values used to calculate an ad’s total value in our ad auction, which will influence the probability that a given ad will win an auction and be shown to a user.

When there is an opportunity to show an ad to someone, all the ads that are eligible to be shown to that person are narrowed down through the ad auction, where the total value of each ad is calculated and compared. There are three key components of total value: the advertiser bid (how much they are willing to pay), the estimated action rate, and ad quality. We use AI models to predict estimated action rate, or a person’s likelihood of taking the advertiser’s desired action (such as visiting a website, watching a video, or completing a purchase). This prediction is based on ad delivery inputs, such as clicking an ad, engaging with a Page, or installing an app.

Then a process in our system called pacing adjusts the ad’s total value by adjusting what’s called the pacing multiplier. Pacing is already used in ad auctions to help ensure that an advertiser’s entire campaign budget is not spent in just a few days, and now it will help the VRS accomplish its goal.

The VRS begins when an ad for housing, employment, or credit wins the auction and starts being shown to people. After the ad has been shown to a large enough group of people, the system measures the aggregate age, gender, and estimated race/ethnicity distribution of those who have seen the ad. (To measure the estimated race/ethnicity distribution of these groups, VRS relies on a widely used method of measurement called Bayesian Improved Surname Geocoding, which we built with added privacy enhancements, including differential privacy.) These measurements are compared with measurements of the population of people who are more broadly eligible to see the ad, and if there is a difference in distributions, the system is instructed to adjust pacing multipliers.

The VRS remeasures the audience’s demographic distribution and updates the pacing of ads throughout the campaign, working to reduce variance between the audiences. When there’s a new chance to show someone an ad, the system uses the latest demographic measurements, along with limited information about that person, to determine how to best adjust the pacing of the bid in order to encourage the ad to be distributed to an audience that more closely reflects the ad’s eligible targeted audience.

Reinforcement learning helps the VRS learn how to do this effectively before it is used in our ad auction. With reinforcement learning, the system has been trained offline to reduce the difference between the distribution of people who have seen the ad and the distribution of people who could have seen the ad, based on these demographic measurements.

Privacy safeguards

One of the key priorities of the VRS is to reduce variance for ads delivery in a privacy-preserving way. In particular, we aim to avoid demographic information leaking into the VRS or being discernable to human analysts reviewing the VRS or its outputs. To implement the VRS while also taking into account people’s privacy, we use the following privacy-preserving approaches:

The VRS will not have access to individuals’ age, gender, or estimated race/ethnicity.

Estimated race/ethnicity will be measured using Meta’s privacy-enhanced implementation of Bayesian Improved Surname Geocoding.

Aggregate demographic measurements that are generated and used by the VRS will include differential privacy noise to help prevent the system from learning and subsequently acting on individual-level demographic information with high fidelity.

We are also exploring ways to obfuscate the generation of user summary vectors by randomly rotating these summary vectors using a private rotation matrix, which could help prevent adversarial actors from reconstructing the user summarization process.

Challenges and limitations

In developing this new method, we encountered a number of challenges and questions. We hope that by highlighting some of the questions and tensions we have navigated or are still working through, we can help inform conversations across industry and with experts in civil society, academia, and policymaking who have similar goals of advancing the equitable distribution of ads. Ultimately, we believe our collective progress will be fastest if we work together to chart a responsible path forward. Some of the technical challenges include:

Availability of demographic data: Gender and age are data more readily available to tech services since they are commonly collected at account creation. But research has shown that for many companies, the absence of labeled demographic data, particularly race and ethnicity, has raised significant barriers to systematically investigating the potential for differences across those protected characteristics, or to monitoring the progress of equity efforts over time.

Multiobjective learning: The VRS simultaneously prioritizes multiple objectives: helping reduce variance for gender, for age, and for estimated race/ethnicity. To do so for all of these effectively, it must not consistently prioritize any one of those goals over the others. Like most societal challenges, navigating multiple objectives and potential trade-offs will likely mean that multiobjective systems are unlikely to be able to achieve every component goal perfectly.

Low-volume ads: The number of ad impressions in a given ad’s run relate directly to how many opportunities the system has to find successful strategies to reduce variance.

System latency: Because the VRS works based on iterative measurements that rely on several pieces of technical infrastructure, there will be latency between when measurements are taken and when those measurements are shared back to the VRS.

Important questions also arise in terms of how services should continue navigating trade-offs between privacy and fairness, including whether to apply similar tools to other demographic categories for which where privacy-preserving measurement methods have not yet been developed. Additionally, we expect important discussions around how baselines for such systems are selected and measured, and how to ensure that fairness goals are sustained beyond individual services, since people might use multiple online and offline sources to find information about housing, jobs, or financial tools.

Evolving the field of AI fairness

Fairness in AI is a dynamic and evolving field. The changes described in our paper were informed by substantial consultation with a broad array of stakeholders and represent a significant technological advancement in how AI can be responsibly used to deliver personalized ads. We are excited to pioneer this effort, and we hope that our sharing details on this work will help other AI and digital advertising practitioners make progress to advance fairness and equity and avoid amplifying societal biases, whose impact extends far beyond any one service. We know that our ongoing progress — in both ad fairness and broader civil rights initiatives — will be determined not just by our commitment to this work but also by the concrete changes we make in our products. With the deployment of the VRS, we’re pleased to be making a tangible impact on Meta’s ad-serving systems.

Download the paper"
Meta_Blog,https://ai.meta.com/blog/advertising-fairness-variance-reduction-system-vrs/,,A new system to help ensure ads are delivered fairly to different demographic groups,"Advocates, researchers, and others have long been focused on the relationship between advertising and economic mobility, especially when it comes to ads that pertain to housing, employment, and credit opportunities. People can more easily pursue a new job or consider moving to a new home when they’re aware of the options out there, but the enduring effects of historically unequal treatment in these areas still shape the economic opportunities of too many. Across both the tech industry and the AI research community, approaches to fairness are still evolving, including in the realm of personalized, auction-based advertising systems. But we know we can’t wait for a consensus to make progress in addressing these important concerns, so today we’re sharing early but significant changes Meta has made to our advertising system that build on and reflect the evolution of our approach to ad fairness.

After more than a year of collaboration with the DOJ and HUD, Meta has developed a new method called the Variance Reduction System (VRS) to help ensure an equitable distribution of ads on our services. Our policies already prohibit advertisers from using our ad products to discriminate against individuals or groups of people, and we’ve implemented additional safeguards, such as disallowing the use of gender, age, or zip code targeting for certain ads. But even without these sorts of targeting options, factors such as people’s interests or activity on a service could affect how ads are distributed to different demographic groups. The goal of the VRS is to help ensure that the audience that ends up seeing a housing, employment, or credit ad more closely reflects the eligible targeted audience for that ad. We’ll do this by regularly measuring the actual audience for a particular ad to see how it compares with the demographic distribution — age, gender, and estimated race or ethnicity — of the audience the advertiser has selected. (Our initial launch will focus on gender and estimated race.) To implement this system in a way that respects people’s privacy, the system relies on aggregate measurements as well as privacy-enhancing approaches to measure race and ethnicity at the aggregate level. We will begin by applying the VRS to housing ads in the United States, and will expand it to employment and credit ads in the United States over the next year.

This system complements and extends our other longstanding efforts to help advance fairness in our ads system, such as limiting the targeting options available to advertisers running housing ads on our services, to make progress toward a more equitable distribution of ads through our ad delivery process.

In addition to sharing this blog post summarizing how the VRS works, we’re publishing technical details in a new white paper, available here:

Toward fairness in personalized ads

How the VRS works

The VRS is an offline reinforcement learning framework with the explicit goal of minimizing the variance in number of ad views between people who have seen the ad and the broader eligible audience of people who could have seen the ad. Reinforcement learning is a type of machine learning that learns from trial and error to optimize toward a predefined outcome — in this case, to minimize ad impression variance across demographic subgroups no matter the cause of that variance. Importantly, the VRS will not be provided with individual-level age, gender, or estimated race/ethnicity to make these determinations. The system will instead receive aggregate measurements of variance across these demographics.

The system starts by measuring the demographic distribution of the baseline or eligible ratio of the age, gender, and estimated racial/ethnic distribution of the population of users to whom an advertiser has indicated they would like their ad to be displayed. As the ad is being delivered, we periodically measure the delivery ratio, or the demographic distribution of the impressions of an ad.

The VRS relies on a controller that has the ability to change one of the values used to calculate an ad’s total value in our ad auction, which will influence the probability that a given ad will win an auction and be shown to a user.

When there is an opportunity to show an ad to someone, all the ads that are eligible to be shown to that person are narrowed down through the ad auction, where the total value of each ad is calculated and compared. There are three key components of total value: the advertiser bid (how much they are willing to pay), the estimated action rate, and ad quality. We use AI models to predict estimated action rate, or a person’s likelihood of taking the advertiser’s desired action (such as visiting a website, watching a video, or completing a purchase). This prediction is based on ad delivery inputs, such as clicking an ad, engaging with a Page, or installing an app.

Then a process in our system called pacing adjusts the ad’s total value by adjusting what’s called the pacing multiplier. Pacing is already used in ad auctions to help ensure that an advertiser’s entire campaign budget is not spent in just a few days, and now it will help the VRS accomplish its goal.

The VRS begins when an ad for housing, employment, or credit wins the auction and starts being shown to people. After the ad has been shown to a large enough group of people, the system measures the aggregate age, gender, and estimated race/ethnicity distribution of those who have seen the ad. (To measure the estimated race/ethnicity distribution of these groups, VRS relies on a widely used method of measurement called Bayesian Improved Surname Geocoding, which we built with added privacy enhancements, including differential privacy.) These measurements are compared with measurements of the population of people who are more broadly eligible to see the ad, and if there is a difference in distributions, the system is instructed to adjust pacing multipliers.

The VRS remeasures the audience’s demographic distribution and updates the pacing of ads throughout the campaign, working to reduce variance between the audiences. When there’s a new chance to show someone an ad, the system uses the latest demographic measurements, along with limited information about that person, to determine how to best adjust the pacing of the bid in order to encourage the ad to be distributed to an audience that more closely reflects the ad’s eligible targeted audience.

Reinforcement learning helps the VRS learn how to do this effectively before it is used in our ad auction. With reinforcement learning, the system has been trained offline to reduce the difference between the distribution of people who have seen the ad and the distribution of people who could have seen the ad, based on these demographic measurements.

Privacy safeguards

One of the key priorities of the VRS is to reduce variance for ads delivery in a privacy-preserving way. In particular, we aim to avoid demographic information leaking into the VRS or being discernable to human analysts reviewing the VRS or its outputs. To implement the VRS while also taking into account people’s privacy, we use the following privacy-preserving approaches:

The VRS will not have access to individuals’ age, gender, or estimated race/ethnicity.

Estimated race/ethnicity will be measured using Meta’s privacy-enhanced implementation of Bayesian Improved Surname Geocoding.

Aggregate demographic measurements that are generated and used by the VRS will include differential privacy noise to help prevent the system from learning and subsequently acting on individual-level demographic information with high fidelity.

We are also exploring ways to obfuscate the generation of user summary vectors by randomly rotating these summary vectors using a private rotation matrix, which could help prevent adversarial actors from reconstructing the user summarization process.

Challenges and limitations

In developing this new method, we encountered a number of challenges and questions. We hope that by highlighting some of the questions and tensions we have navigated or are still working through, we can help inform conversations across industry and with experts in civil society, academia, and policymaking who have similar goals of advancing the equitable distribution of ads. Ultimately, we believe our collective progress will be fastest if we work together to chart a responsible path forward. Some of the technical challenges include:

Availability of demographic data: Gender and age are data more readily available to tech services since they are commonly collected at account creation. But research has shown that for many companies, the absence of labeled demographic data, particularly race and ethnicity, has raised significant barriers to systematically investigating the potential for differences across those protected characteristics, or to monitoring the progress of equity efforts over time.

Multiobjective learning: The VRS simultaneously prioritizes multiple objectives: helping reduce variance for gender, for age, and for estimated race/ethnicity. To do so for all of these effectively, it must not consistently prioritize any one of those goals over the others. Like most societal challenges, navigating multiple objectives and potential trade-offs will likely mean that multiobjective systems are unlikely to be able to achieve every component goal perfectly.

Low-volume ads: The number of ad impressions in a given ad’s run relate directly to how many opportunities the system has to find successful strategies to reduce variance.

System latency: Because the VRS works based on iterative measurements that rely on several pieces of technical infrastructure, there will be latency between when measurements are taken and when those measurements are shared back to the VRS.

Important questions also arise in terms of how services should continue navigating trade-offs between privacy and fairness, including whether to apply similar tools to other demographic categories for which where privacy-preserving measurement methods have not yet been developed. Additionally, we expect important discussions around how baselines for such systems are selected and measured, and how to ensure that fairness goals are sustained beyond individual services, since people might use multiple online and offline sources to find information about housing, jobs, or financial tools.

Evolving the field of AI fairness

Fairness in AI is a dynamic and evolving field. The changes described in our paper were informed by substantial consultation with a broad array of stakeholders and represent a significant technological advancement in how AI can be responsibly used to deliver personalized ads. We are excited to pioneer this effort, and we hope that our sharing details on this work will help other AI and digital advertising practitioners make progress to advance fairness and equity and avoid amplifying societal biases, whose impact extends far beyond any one service. We know that our ongoing progress — in both ad fairness and broader civil rights initiatives — will be determined not just by our commitment to this work but also by the concrete changes we make in our products. With the deployment of the VRS, we’re pleased to be making a tangible impact on Meta’s ad-serving systems.

Download the paper"
Meta_Blog,https://ai.meta.com/blog/ai-conversational-summarization-research/,,Advancing AI-driven conversational summarization,"Summarization using automated AI-based techniques can help counter information overload. But for this to be useful in everyday life, we need AI models that can create concise summaries of conversations from a variety of formats, including meeting notes, email threads, and discussions happening in forums and chats. These differ from well-structured sources like newspaper articles, which often contain large amounts of valuable but noisy, conversational, and verbose content from multiple participants. Meta AI has made a series of research advancements that enable machines to achieve close to human performance for automatic summaries of conversations across diverse domains. We’re sharing details in this blog post on our progress.

Our research covers key aspects of AI modeling to create a comprehensive approach to a solution: new dataset collection, new benchmark definition, a novel model training approach, and methods that cover both short- and long-form text content. Specifically, we used publicly available text in online community discussion sites to generate an additional dataset for multiperspective answer summarization and then defined a comprehensive benchmark for conversational summarization across diverse domains. We achieved state-of-the-art results on those benchmarks and significantly reduced occurrences of factual errors by using a novel linguistically informed contrastive fine-tuning approach. We also deliver state-of-the-art results when handling long document summarization. Finally, since labeling summarization data is a resource-heavy process, we proposed a general method for improving zero and few-shot abstractive summarization.

We are exploring how this work can be applied across a variety of use cases. Summarized information is particularly valuable for augmented and virtual reality devices, due to their limited screen space. We believe summarization can also be a useful capability for smart assistants, by creating intelligent, natural interactions between people and AI – which could have many potential future use cases as we help build the metaverse.

Enabling conversational summarization

While documents, articles, and scientific papers contain specific linguistic structures that make them easier to summarize, conversational text scatters the main points across multiple utterances and participants, covering a vast amount of information in many different formats.

We addressed this research gap by collecting the ConvoSumm benchmark for research purposes, which is the first comprehensive benchmark for conversational summarization across diverse domains. It includes newly collected summaries for news article comments, discussion forums and debate, community question answering, email threads, and existing data for dialog and meeting summarization.

We used the “issues-viewpoints-assertions” graph framework to unify modeling across these domains. We constructed the argument graph using entailment relations. We then linearized the graph and trained a graph-to-text model, and experimented with argument mining as a way to reduce noise in long-text input. Our results showed improved performance over the previous state-of-the-art model on the ConvoSumm benchmark in both automatic and human evaluations.

Another challenge we addressed was how to accurately summarize multiple perspectives in community question answering. Previously there was no way to ensure that different viewpoints were reflected in the summary. We addressed this by collecting the AnswerSumm dataset for research purposes. This follows the pipeline of relevant sentence selection, sentence grouping based on perspectives, summarizing each perspective, and producing an overall fused summary. We then introduced a novel unsupervised approach that automatically creates multi-perspective bullet-point answer summaries for data augmentation, further boosting overall summarization performance. Furthermore, we proposed to use reinforcement learning with two additional rewards based on textual entailment and semantic area to improve factual consistency and answer coverage.

Improving faithfulness of conversational summarization

Abstractive summarization models can hallucinate – generating information that might not be relevant or faithful to the input. For example, a common factual error for dialogue summaries is wrong reference error. To better understand the types of hallucinations generated by state-of-the-art, pre-trained models on dialogue summarization, we devised a new linguistically motivated taxonomy of factual errors and conducted human evaluation on popular dialogue summarization datasets.

We detailed a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual error types based on our annotation, we introduced additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. The results show that our model significantly reduces many types of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using automatic metrics, ROUGE and BARTScore, with 2.2 points increase in ROUGE-1 on SAMSum and 2.4 points increase on AMI. We also demonstrated the effectiveness of our approach through human evaluation with 30 percent improvement on SAMSum and 15 percent improvement on AMI on the human faithfulness score.

Scaling with zero or few examples

To cut laborious tasks for manually creating summaries for each new domain, we introduced a generalizable method called WikiTransfer, which fine-tunes pretrained models on pseudo-summaries that are produced from generic Wikipedia data. Each summary contains characteristics of the target dataset, such as the length and level of abstraction of the desired summaries.

Using WikiTransfer , we achieved a new state-of-the-art result for zero-shot abstractive summarization and demonstrated the effectiveness of our approach on four datasets from diverse domains. With this method, we also achieved better results in few-shot summarization compared to transfer from other summarization datasets. We also improved few-shot performance further with data augmentation techniques, and introduced a regularization term for few-shot transfer. Human assessments of the resulting summaries do not show significant differences between the WikiTransfer few-shot summaries and fully supervised summaries, demonstrating the efficiency of our approach.

Finding the key points to summarize in longer conversations

Conversations can be long and varied, adding to the challenge of using AI to accurately and concisely summarize what was discussed. Most state-of-the-art summarization models, such as BART or T5, rely on full-attention transformers to effectively capture global information in the input documents. However, applying these models to long inputs is prohibitive due to efficiency constraints – the self-attention mechanism has a quadratic complexity with respect to the input length.

Through a series of studies on efficient transformer variants (published in NAACL 2022), we identify a simpler yet still effective architecture that achieves a nice trade-off of performance and efficiency on long-text tasks. This architecture augments block-wise attention with pooling operations on the top layers of the transformer encoder. To further improve the performance, we pretrain this model on a large dataset of long text sequences constructed from the C4 corpus, using a masked span prediction objective that includes both long and short target spans. Our final model establishes state of the art on five summarization tasks. See our new preprint for more details.

Building the future of AI-driven summarization

Natural language processing is advancing at an exciting pace, with recent innovations such as Open AI’s ChatGPT. However, ChatGPT is not specifically designed for summarization. Our research helps address some of the areas in AI-generated text that warrant further exploration. For conversation summarization to be successful, responses will need to stick to the given source document as context and be able to cover multiple perspectives.

However, there are still additional challenges that will need to be addressed. We are actively working on zero- and few-shot conversation summarization generalization, using one model for conversation summarization across multiple domains. In addition, we are looking at on-device summarization to preserve user privacy for certain use cases on augmented and virtual reality devices.

We hope our research in this area can help the AI community advance conversation summarization and enable more use cases that bring concise information to people where and when they need it.

Read the papers:

We'd like to acknowledge contributions to this work from Anchit Gupta, Asish Ghosal, and our collaborators in FAIR. We are also grateful for the contributions of Dragomir Radev, director of LILY (Language, Information, and Learning) Lab at Yale, and his team."
Meta_Blog,https://ai.meta.com/blog/ai-conversational-summarization-research/,,Advancing AI-driven conversational summarization,"Summarization using automated AI-based techniques can help counter information overload. But for this to be useful in everyday life, we need AI models that can create concise summaries of conversations from a variety of formats, including meeting notes, email threads, and discussions happening in forums and chats. These differ from well-structured sources like newspaper articles, which often contain large amounts of valuable but noisy, conversational, and verbose content from multiple participants. Meta AI has made a series of research advancements that enable machines to achieve close to human performance for automatic summaries of conversations across diverse domains. We’re sharing details in this blog post on our progress.

Our research covers key aspects of AI modeling to create a comprehensive approach to a solution: new dataset collection, new benchmark definition, a novel model training approach, and methods that cover both short- and long-form text content. Specifically, we used publicly available text in online community discussion sites to generate an additional dataset for multiperspective answer summarization and then defined a comprehensive benchmark for conversational summarization across diverse domains. We achieved state-of-the-art results on those benchmarks and significantly reduced occurrences of factual errors by using a novel linguistically informed contrastive fine-tuning approach. We also deliver state-of-the-art results when handling long document summarization. Finally, since labeling summarization data is a resource-heavy process, we proposed a general method for improving zero and few-shot abstractive summarization.

We are exploring how this work can be applied across a variety of use cases. Summarized information is particularly valuable for augmented and virtual reality devices, due to their limited screen space. We believe summarization can also be a useful capability for smart assistants, by creating intelligent, natural interactions between people and AI – which could have many potential future use cases as we help build the metaverse.

Enabling conversational summarization

While documents, articles, and scientific papers contain specific linguistic structures that make them easier to summarize, conversational text scatters the main points across multiple utterances and participants, covering a vast amount of information in many different formats.

We addressed this research gap by collecting the ConvoSumm benchmark for research purposes, which is the first comprehensive benchmark for conversational summarization across diverse domains. It includes newly collected summaries for news article comments, discussion forums and debate, community question answering, email threads, and existing data for dialog and meeting summarization.

We used the “issues-viewpoints-assertions” graph framework to unify modeling across these domains. We constructed the argument graph using entailment relations. We then linearized the graph and trained a graph-to-text model, and experimented with argument mining as a way to reduce noise in long-text input. Our results showed improved performance over the previous state-of-the-art model on the ConvoSumm benchmark in both automatic and human evaluations.

Another challenge we addressed was how to accurately summarize multiple perspectives in community question answering. Previously there was no way to ensure that different viewpoints were reflected in the summary. We addressed this by collecting the AnswerSumm dataset for research purposes. This follows the pipeline of relevant sentence selection, sentence grouping based on perspectives, summarizing each perspective, and producing an overall fused summary. We then introduced a novel unsupervised approach that automatically creates multi-perspective bullet-point answer summaries for data augmentation, further boosting overall summarization performance. Furthermore, we proposed to use reinforcement learning with two additional rewards based on textual entailment and semantic area to improve factual consistency and answer coverage.

Improving faithfulness of conversational summarization

Abstractive summarization models can hallucinate – generating information that might not be relevant or faithful to the input. For example, a common factual error for dialogue summaries is wrong reference error. To better understand the types of hallucinations generated by state-of-the-art, pre-trained models on dialogue summarization, we devised a new linguistically motivated taxonomy of factual errors and conducted human evaluation on popular dialogue summarization datasets.

We detailed a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual error types based on our annotation, we introduced additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. The results show that our model significantly reduces many types of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using automatic metrics, ROUGE and BARTScore, with 2.2 points increase in ROUGE-1 on SAMSum and 2.4 points increase on AMI. We also demonstrated the effectiveness of our approach through human evaluation with 30 percent improvement on SAMSum and 15 percent improvement on AMI on the human faithfulness score.

Scaling with zero or few examples

To cut laborious tasks for manually creating summaries for each new domain, we introduced a generalizable method called WikiTransfer, which fine-tunes pretrained models on pseudo-summaries that are produced from generic Wikipedia data. Each summary contains characteristics of the target dataset, such as the length and level of abstraction of the desired summaries.

Using WikiTransfer , we achieved a new state-of-the-art result for zero-shot abstractive summarization and demonstrated the effectiveness of our approach on four datasets from diverse domains. With this method, we also achieved better results in few-shot summarization compared to transfer from other summarization datasets. We also improved few-shot performance further with data augmentation techniques, and introduced a regularization term for few-shot transfer. Human assessments of the resulting summaries do not show significant differences between the WikiTransfer few-shot summaries and fully supervised summaries, demonstrating the efficiency of our approach.

Finding the key points to summarize in longer conversations

Conversations can be long and varied, adding to the challenge of using AI to accurately and concisely summarize what was discussed. Most state-of-the-art summarization models, such as BART or T5, rely on full-attention transformers to effectively capture global information in the input documents. However, applying these models to long inputs is prohibitive due to efficiency constraints – the self-attention mechanism has a quadratic complexity with respect to the input length.

Through a series of studies on efficient transformer variants (published in NAACL 2022), we identify a simpler yet still effective architecture that achieves a nice trade-off of performance and efficiency on long-text tasks. This architecture augments block-wise attention with pooling operations on the top layers of the transformer encoder. To further improve the performance, we pretrain this model on a large dataset of long text sequences constructed from the C4 corpus, using a masked span prediction objective that includes both long and short target spans. Our final model establishes state of the art on five summarization tasks. See our new preprint for more details.

Building the future of AI-driven summarization

Natural language processing is advancing at an exciting pace, with recent innovations such as Open AI’s ChatGPT. However, ChatGPT is not specifically designed for summarization. Our research helps address some of the areas in AI-generated text that warrant further exploration. For conversation summarization to be successful, responses will need to stick to the given source document as context and be able to cover multiple perspectives.

However, there are still additional challenges that will need to be addressed. We are actively working on zero- and few-shot conversation summarization generalization, using one model for conversation summarization across multiple domains. In addition, we are looking at on-device summarization to preserve user privacy for certain use cases on augmented and virtual reality devices.

We hope our research in this area can help the AI community advance conversation summarization and enable more use cases that bring concise information to people where and when they need it.

Read the papers:

We'd like to acknowledge contributions to this work from Anchit Gupta, Asish Ghosal, and our collaborators in FAIR. We are also grateful for the contributions of Dragomir Radev, director of LILY (Language, Information, and Learning) Lab at Yale, and his team."
Meta_Blog,https://ai.meta.com/blog/ai-self-supervised-learning-data2vec/,,"Data2vec 2.0: Highly efficient self-supervised learning for vision, speech and text","Many recent breakthroughs in AI have been powered by self-supervised learning, which enables machines to learn without relying on labeled data. But current algorithms have several significant limitations, often including being specialized for a single modality (such as images or text) and requiring lots of computational power. This contrasts with human learning: People appear to learn much more efficiently than current AI, and also learn from different kinds of information in a similar way, rather than relying on separate learning mechanisms for text, speech, and other modalities.

Meta AI addressed one of these limitations earlier this year when we released data2vec, the first high-performance self-supervised algorithm to learn the same way for three different modalities: speech, vision, and text. Data2vec made it much easier to apply research advances in, say, text understanding to an image segmentation or speech translation task.

Today, we’re sharing data2vec 2.0, a new algorithm that is vastly more efficient and outperforms its predecessor’s strong performance. It achieves the same accuracy as the most popular existing self-supervised algorithm for computer vision but does so 16x faster.

To make our research accessible to other researchers, we are now sharing the code and pretrained models.

How data2vec 2.0 works

The general idea of self-supervised learning is for machines to learn the structure of images, speech, and text simply by observing the world. Advances in this area have led to many breakthroughs in speech (e.g., wav2vec 2.0), computer vision (e.g., masked autoencoders), and natural language processing (e.g., BERT). But modern systems can be computationally demanding, as training very large models requires many GPUs.

Illustration of how data2vec 2.0 training works. It can be trained separately on text, speech, or images.

Similar to the original data2vec algorithm, data2vec 2.0 predicts contextualized representations of the data — or the layers of a neural network — instead of the pixels of an image, the words of a text passage, or the sounds of speech. Unlike with most other algorithms, these so-called target representations are contextualized, meaning they take the entire training example into account. For instance, the representation of the word bank is based on the entire sentence that the word appears in, and it is therefore easier to represent the correct meaning of the word (“financial institution” or “ground next to river”). We believe that contextualized targets lead to a richer learning task and enable data2vec 2.0 to learn faster than other algorithms.

We improved the efficiency of the original data2vec algorithm in several ways: First, we take target representations built for a particular training example and reuse them for masked versions (where we hide different random parts of the training example). We take each version and feed it into the student model, which predicts the same contextualized target representation for the different masked versions. This effectively amortizes the computational effort required to create target representations. Second, and similar to masked autoencoders, we do not run the student encoder network for the parts of the training examples that are blanked out (which is about 80 percent of an image in our case), thereby saving significant compute cycles. Finally, we use a more efficient decoder model that relies not on Transformer networks but on a multilayer convolutional network.

Relative training time improvements when training data2vec 2.0 to the same accuracy as popular existing algorithms on the same hardware.

Efficiency gains with data2vec 2.0

To get a better understanding of how much more efficient data2vec 2.0 is than its predecessor and other algorithms, we tested it on computer vision, speech, and text tasks on widely used benchmarks. We were looking at the final accuracy and the time it took to pretrain the model. We measured the speed of algorithms on the same hardware (number of GPUs, etc.).

For computer vision, we evaluated data2vec 2.0 on the standard ImageNet-1K image classification benchmark, where it learned to represent images. Data2vec 2.0 can equal the accuracy of masked autoencoders (MAE) but is 16x faster (measured in wall clock time in a like-for-like setting). If we give the algorithm more time, it can achieve even higher accuracy while still being faster than MAE.

Data2vec 2.0 for computer vision: The graph shows speed vs. image classification accuracy for different algorithms on the popular ImageNet-1K benchmark.

For speech, we tested it on the LibriSpeech speech recognition benchmark, where it performed more than 11 times faster than wav2vec 2.0 with similar accuracy. For natural language processing (NLP), we evaluated data2vec 2.0 on the popular General Language Understanding Evaluation (GLUE) benchmark, where it achieved the same accuracy as RoBERTa, a reimplementation of BERT, in half the training time.

Data2vec 2.0 for speech and NLP: The top graph shows speed vs. speech recognition word error rate for models pretrained on LibriSpeech, fine-tuned on 10 hours of Libri-light data, and then evaluated on dev-other. The bottom graph shows natural language understanding accuracy on the GLUE benchmark when using the original BERT setup.

Toward machines that learn efficiently

We are on a journey to build more general and efficient self-supervised algorithms that can use a single learning objective to learn from different modalities. The ability to learn more efficiently is particularly important for modalities such as video, which require a lot of computational effort to process. We hope that more efficient self-supervised learning algorithms such as data2vec 2.0 will lead to machines that can deeply understand extremely complex data, such as the contents of an entire movie.

Access the open source code and pretrained models here, and read the paper hear.

Get it on GitHub:

https://github.com/facebookresearch/fairseq/tree/master/examples/data2vec

Read the paper:

Efficient self-supervised learning with contextualized target representations for speech, vision, and language

This blog post was made possible by the work of Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli.

The second graphic in this post has been updated to correct a typographical error."
Meta_Blog,https://ai.meta.com/blog/ai-self-supervised-learning-data2vec/,,"Data2vec 2.0: Highly efficient self-supervised learning for vision, speech and text","Many recent breakthroughs in AI have been powered by self-supervised learning, which enables machines to learn without relying on labeled data. But current algorithms have several significant limitations, often including being specialized for a single modality (such as images or text) and requiring lots of computational power. This contrasts with human learning: People appear to learn much more efficiently than current AI, and also learn from different kinds of information in a similar way, rather than relying on separate learning mechanisms for text, speech, and other modalities.

Meta AI addressed one of these limitations earlier this year when we released data2vec, the first high-performance self-supervised algorithm to learn the same way for three different modalities: speech, vision, and text. Data2vec made it much easier to apply research advances in, say, text understanding to an image segmentation or speech translation task.

Today, we’re sharing data2vec 2.0, a new algorithm that is vastly more efficient and outperforms its predecessor’s strong performance. It achieves the same accuracy as the most popular existing self-supervised algorithm for computer vision but does so 16x faster.

To make our research accessible to other researchers, we are now sharing the code and pretrained models.

How data2vec 2.0 works

The general idea of self-supervised learning is for machines to learn the structure of images, speech, and text simply by observing the world. Advances in this area have led to many breakthroughs in speech (e.g., wav2vec 2.0), computer vision (e.g., masked autoencoders), and natural language processing (e.g., BERT). But modern systems can be computationally demanding, as training very large models requires many GPUs.

Illustration of how data2vec 2.0 training works. It can be trained separately on text, speech, or images.

Similar to the original data2vec algorithm, data2vec 2.0 predicts contextualized representations of the data — or the layers of a neural network — instead of the pixels of an image, the words of a text passage, or the sounds of speech. Unlike with most other algorithms, these so-called target representations are contextualized, meaning they take the entire training example into account. For instance, the representation of the word bank is based on the entire sentence that the word appears in, and it is therefore easier to represent the correct meaning of the word (“financial institution” or “ground next to river”). We believe that contextualized targets lead to a richer learning task and enable data2vec 2.0 to learn faster than other algorithms.

We improved the efficiency of the original data2vec algorithm in several ways: First, we take target representations built for a particular training example and reuse them for masked versions (where we hide different random parts of the training example). We take each version and feed it into the student model, which predicts the same contextualized target representation for the different masked versions. This effectively amortizes the computational effort required to create target representations. Second, and similar to masked autoencoders, we do not run the student encoder network for the parts of the training examples that are blanked out (which is about 80 percent of an image in our case), thereby saving significant compute cycles. Finally, we use a more efficient decoder model that relies not on Transformer networks but on a multilayer convolutional network.

Relative training time improvements when training data2vec 2.0 to the same accuracy as popular existing algorithms on the same hardware.

Efficiency gains with data2vec 2.0

To get a better understanding of how much more efficient data2vec 2.0 is than its predecessor and other algorithms, we tested it on computer vision, speech, and text tasks on widely used benchmarks. We were looking at the final accuracy and the time it took to pretrain the model. We measured the speed of algorithms on the same hardware (number of GPUs, etc.).

For computer vision, we evaluated data2vec 2.0 on the standard ImageNet-1K image classification benchmark, where it learned to represent images. Data2vec 2.0 can equal the accuracy of masked autoencoders (MAE) but is 16x faster (measured in wall clock time in a like-for-like setting). If we give the algorithm more time, it can achieve even higher accuracy while still being faster than MAE.

Data2vec 2.0 for computer vision: The graph shows speed vs. image classification accuracy for different algorithms on the popular ImageNet-1K benchmark.

For speech, we tested it on the LibriSpeech speech recognition benchmark, where it performed more than 11 times faster than wav2vec 2.0 with similar accuracy. For natural language processing (NLP), we evaluated data2vec 2.0 on the popular General Language Understanding Evaluation (GLUE) benchmark, where it achieved the same accuracy as RoBERTa, a reimplementation of BERT, in half the training time.

Data2vec 2.0 for speech and NLP: The top graph shows speed vs. speech recognition word error rate for models pretrained on LibriSpeech, fine-tuned on 10 hours of Libri-light data, and then evaluated on dev-other. The bottom graph shows natural language understanding accuracy on the GLUE benchmark when using the original BERT setup.

Toward machines that learn efficiently

We are on a journey to build more general and efficient self-supervised algorithms that can use a single learning objective to learn from different modalities. The ability to learn more efficiently is particularly important for modalities such as video, which require a lot of computational effort to process. We hope that more efficient self-supervised learning algorithms such as data2vec 2.0 will lead to machines that can deeply understand extremely complex data, such as the contents of an entire movie.

Access the open source code and pretrained models here, and read the paper hear.

Get it on GitHub:

https://github.com/facebookresearch/fairseq/tree/master/examples/data2vec

Read the paper:

Efficient self-supervised learning with contextualized target representations for speech, vision, and language

This blog post was made possible by the work of Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli.

The second graphic in this post has been updated to correct a typographical error."
Meta_Blog,https://ai.meta.com/blog/pytorch-ai-smartphones-computers/,,How PyTorch is bringing the power of AI to computers and smartphones,"Many of the experiences people enjoy on Facebook and Instagram are powered by artificial intelligence (AI). A number of them, like Assistant, Avatars, and AR effects, cannot be powered by server-side AI due to latency, network bandwidth, and other constraints. Running AI on-device —that is, directly on a phone, tablet, or even a pair of smart glasses — offers huge advantages over constantly sending data back to a server. It’s faster, and it creates a privacy-enhancing experience for people who use our platforms. However, on-device AI presents new challenges, since it requires coping with devices that have a small battery, far less powerful processors, and less memory than a server in a data center.

Today, we are sharing more information about how Meta is using PyTorch, an open source machine learning framework we developed alongside the AI community that is now part of the Linux Foundation, to bring more AI-powered experiences to personal devices.

In order to provide the best AI-based product experiences, the AI models need to be optimized according to the different sets of constraints of the devices where they will be deployed. Teams iterate on the AI models to achieve state-of-the-art battery life, power, compute, size, and memory utilization.

This is where PyTorch can help. PyTorch has developed infrastructure that allows developers to execute AI models across a wide variety of devices with efficiency and performance. The PyTorch mobile runtime is small enough to fit across many mobile devices while supporting the variety of operations used to author those AI models with optimizations across different compute resources. In 2021, we announced the migration of all of our AI systems to PyTorch. PyTorch now powers the machine learning (ML) stack and workflow tools used across all Meta products to scale the adoption of on-device AI, using the same solution (PyTorch Mobile) available in the open source since release 1.9.

We have seen exponential growth in these on-device production use cases. On Meta’s family of mobile applications, PyTorch on-device powers:

70 billion daily inferences

50 on-device AI models across different mobile applications

All on-device ML in Instagram, Facebook, Messenger, and Meta Spark

Our on-device AI also powers several critical use cases across Meta’s mobile applications. Here are a few examples:

Real-time video calling: Our video background selection models let people select a blurred or unique AR background for their video calls, so they have a more private communication experience in their own space.

Privacy-preserving ML: Our models are used on-device to rank a user’s feed and friends list in a more private way on products like Messenger, keeping the data needed for these predictions on the person’s device.

Business integrity: Our text and image models detect and shut down cloaking feed ads, one way that malicious parties try to reach people on our platforms.

Smart Target Quick Promotion: Quick Promotion is a platform that enables Facebook to communicate with people in a timely, well-targeted way through products like Feed, notifications, and Messenger. This includes product information and public announcements. With PyTorch, we shipped a smarter targeting algorithm with on-device AI for quick promotions.

PyTorch also powers more engaging AI experiences on Instagram Reels, which people experience in the form of AR effects they can “try on” and use to create content that can do a variety of things, such as changing their background and adding fun AR effects and experiences to their selfies. On the recently launched Meta Quest Pro, Reality Labs leverages PyTorch to enable capabilities such as hand and eye tracking, Natural Facial Expressions, and tracked keyboard.

The future of PyTorch-powered on-device experiences

We believe AI is driving the creation of new products and experiences, and can further improve the capabilities of existing ones. With PyTorch bringing AI to run directly on the devices, this will only open the door to further innovation with increased reliability, interactivity, and privacy.

Based on our experience supporting applications and users across a wide variety of devices, we believe we can push the state of the art of on-device AI even further. Next year, we will be pushing the next-generation PyTorch framework for on-device AI, which will deliver a step function in efficiency, performance, and portability on more devices (including microcontrollers), all while maintaining consistency with the rest of PyTorch for ease of deployment."
Meta_Blog,https://ai.meta.com/blog/pytorch-ai-smartphones-computers/,,How PyTorch is bringing the power of AI to computers and smartphones,"Many of the experiences people enjoy on Facebook and Instagram are powered by artificial intelligence (AI). A number of them, like Assistant, Avatars, and AR effects, cannot be powered by server-side AI due to latency, network bandwidth, and other constraints. Running AI on-device —that is, directly on a phone, tablet, or even a pair of smart glasses — offers huge advantages over constantly sending data back to a server. It’s faster, and it creates a privacy-enhancing experience for people who use our platforms. However, on-device AI presents new challenges, since it requires coping with devices that have a small battery, far less powerful processors, and less memory than a server in a data center.

Today, we are sharing more information about how Meta is using PyTorch, an open source machine learning framework we developed alongside the AI community that is now part of the Linux Foundation, to bring more AI-powered experiences to personal devices.

In order to provide the best AI-based product experiences, the AI models need to be optimized according to the different sets of constraints of the devices where they will be deployed. Teams iterate on the AI models to achieve state-of-the-art battery life, power, compute, size, and memory utilization.

This is where PyTorch can help. PyTorch has developed infrastructure that allows developers to execute AI models across a wide variety of devices with efficiency and performance. The PyTorch mobile runtime is small enough to fit across many mobile devices while supporting the variety of operations used to author those AI models with optimizations across different compute resources. In 2021, we announced the migration of all of our AI systems to PyTorch. PyTorch now powers the machine learning (ML) stack and workflow tools used across all Meta products to scale the adoption of on-device AI, using the same solution (PyTorch Mobile) available in the open source since release 1.9.

We have seen exponential growth in these on-device production use cases. On Meta’s family of mobile applications, PyTorch on-device powers:

70 billion daily inferences

50 on-device AI models across different mobile applications

All on-device ML in Instagram, Facebook, Messenger, and Meta Spark

Our on-device AI also powers several critical use cases across Meta’s mobile applications. Here are a few examples:

Real-time video calling: Our video background selection models let people select a blurred or unique AR background for their video calls, so they have a more private communication experience in their own space.

Privacy-preserving ML: Our models are used on-device to rank a user’s feed and friends list in a more private way on products like Messenger, keeping the data needed for these predictions on the person’s device.

Business integrity: Our text and image models detect and shut down cloaking feed ads, one way that malicious parties try to reach people on our platforms.

Smart Target Quick Promotion: Quick Promotion is a platform that enables Facebook to communicate with people in a timely, well-targeted way through products like Feed, notifications, and Messenger. This includes product information and public announcements. With PyTorch, we shipped a smarter targeting algorithm with on-device AI for quick promotions.

PyTorch also powers more engaging AI experiences on Instagram Reels, which people experience in the form of AR effects they can “try on” and use to create content that can do a variety of things, such as changing their background and adding fun AR effects and experiences to their selfies. On the recently launched Meta Quest Pro, Reality Labs leverages PyTorch to enable capabilities such as hand and eye tracking, Natural Facial Expressions, and tracked keyboard.

The future of PyTorch-powered on-device experiences

We believe AI is driving the creation of new products and experiences, and can further improve the capabilities of existing ones. With PyTorch bringing AI to run directly on the devices, this will only open the door to further innovation with increased reliability, interactivity, and privacy.

Based on our experience supporting applications and users across a wide variety of devices, we believe we can push the state of the art of on-device AI even further. Next year, we will be pushing the next-generation PyTorch framework for on-device AI, which will deliver a step function in efficiency, performance, and portability on more devices (including microcontrollers), all while maintaining consistency with the rest of PyTorch for ease of deployment."
Meta_Blog,https://ai.meta.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/,,"CICERO: An AI agent that negotiates, persuades, and cooperates with people","Games have long been a proving ground for new AI advancements — from Deep Blue’s victory over chess grandmaster Garry Kasparov, to AlphaGo’s mastery of Go, to Pluribus out-bluffing the best humans in poker. But truly useful, versatile agents will need to go beyond just moving pieces on a board. Can we build more effective and flexible agents that can use language to negotiate, persuade, and work with people to achieve strategic goals similar to the way humans do?

Today, we’re announcing a breakthrough toward building AI that has mastered these skills. We’ve built an agent – CICERO – that is the first AI to achieve human-level performance in the popular strategy game Diplomacy*. CICERO demonstrated this by playing on webDiplomacy.net, an online version of the game, where CICERO achieved more than double the average score of the human players and ranked in the top 10 percent of participants who played more than one game.

Something Went Wrong We're having trouble playing this video. Learn more

Diplomacy has been viewed for decades as a near-impossible grand challenge in AI because it requires players to master the art of understanding other people’s motivations and perspectives; make complex plans and adjust strategies; and then use natural language to reach agreements with other people, convince them to form partnerships and alliances, and more. CICERO is so effective at using natural language to negotiate with people in Diplomacy that they often favored working with CICERO over other human participants.

Unlike games like Chess and Go, Diplomacy is a game about people rather than pieces. If an agent can't recognize that someone is likely bluffing or that another player would see a certain move as aggressive, it will quickly lose the game. Likewise, if it doesn't talk like a real person -- showing empathy, building relationships, and speaking knowledgeably about the game -- it won't find other players willing to work with it.

The key to our achievement was developing new techniques at the intersection of two completely different areas of AI research: strategic reasoning, as used in agents like AlphaGo and Pluribus, and natural language processing, as used in models like GPT-3, BlenderBot 3, LaMDA, and OPT-175B. CICERO can deduce, for example, that later in the game it will need the support of one particular player, and then craft a strategy to win that person’s favor – and even recognize the risks and opportunities that that player sees from their particular point of view.

We’ve open-sourced the code and published a paper to help the wider AI community use CICERO to spur further progress in human-AI cooperation. You can also visit the CICERO website to learn more about the project and see the agent in action. Interested researchers can submit a proposal to the CICERO RFP to gain access to the data.

Under the hood: How we built CICERO

At the heart of CICERO is a controllable dialogue model for Diplomacy coupled with a strategic reasoning engine. At each point in the game, CICERO looks at the game board and its conversation history, and models how the other players are likely to act. It then uses this plan to control a language model that can generate free-form dialogue, informing other players of its plans and proposing reasonable actions for the other players that coordinate well with them.

Controllable dialogue

To build a controllable dialogue model, we started with a 2.7 billion parameter BART-like language model pre-trained on text from the internet and fine tuned on over 40,000 human games on webDiplomacy.net. We developed techniques to automatically annotate messages in the training data with corresponding planned moves in the game, so that at inference time we can control dialogue generation to discuss specific desired actions for the agent and its conversation partners. For example, if our agent is playing as France, conditioning the dialogue model on a plan involving England supporting France into Burgundy might yield a message to England like, “Hi England! Are you willing to support me into Burgundy this turn?” Controlling generation in this manner allows Cicero to ground its conversations in a set of plans that it develops and revises over time to better negotiate. This helps the agent coordinate with and persuade other players more effectively.

Step 1 Using the board state and current dialogue, Cicero makes an initial prediction of what everyone will do.

Step 2 CICERO iteratively refines that prediction using planning and then uses those predictions to form an intent for itself and its partner.

Step 3 It generates several candidate messages based on the board state, dialogue, and its intents.

Step 4 It filters the candidate message to reduce nonsense, maximize value, and ensure consistency with our intents.

We further improve dialogue quality using several filtering mechanisms – such as classifiers trained to distinguish between human and model-generated text – that ensure that our dialogue is sensible, consistent with the current game state and previous messages, and strategically sound.

Dialogue-aware strategy & planning

Past superhuman agents in adversarial games like chess, Go, and poker were created through self-play reinforcement learning (RL) – having the agents learn optimal policies by playing millions of games against other copies of itself. However, games involving cooperation require modeling what humans will actually do in real life, rather than modeling what they should do if they were perfect copies of the bot. In particular, we want CICERO to make plans that are consistent with its dialogue with other players.

Something Went Wrong We're having trouble playing this video. Learn more

The classic approach to human modeling is supervised learning, where the agent is trained with labeled data such as a database of human players’ actions in past games. However, relying purely on supervised learning to choose actions based on past dialogue results in an agent that is relatively weak and highly exploitable. For example, a player could tell the agent, ""I'm glad we agreed that you will move your unit out of Paris!"" Since similar messages appear in the training data only when an agreement was reached, the agent might indeed move its unit out of Paris even if doing so is a clear strategic blunder.

To fix this, CICERO runs an iterative planning algorithm that balances dialogue consistency with rationality. The agent first predicts everyone's policy for the current turn based on the dialogue it has shared with other players, and also predicts what other players think the agent's policy will be. It then runs a planning algorithm we developed called piKL, which iteratively improves these predictions by trying to choose new policies that have higher expected value given the other players' predicted policies, while also trying to keep the new predictions close to the original policy predictions. We found that piKL better models human play and leads to better policies for the agent compared to supervised learning alone.

Generating natural, purposeful dialogue

In Diplomacy, how a player talks to other people can be even more important than how they move their pieces. CICERO is able to speak clearly and persuasively when strategizing with other players. For example, in one demonstration game CICERO asked one player for immediate support on one part of the board while pressing another to consider an alliance later in the game.

In these exchanges, CICERO tries to execute its strategy by proposing moves to three different players. In the second dialog, the agent is able to tell the other player why they should cooperate and how it will be mutually beneficial. In the third, CICERO is both soliciting information and setting the groundwork for future moves.

Where there is still room for improvement

It is important to recognize that CICERO also sometimes generates inconsistent dialogue that can undermine its objectives. In the example below where CICERO was playing as Austria, the agent contradicts its first message asking Italy to move to Venice. While our suite of filters aims to detect these sorts of mistakes, it is not perfect.

Diplomacy as a sandbox for advancing human-AI interaction

The emergence of goal-oriented dialogue systems in a game that involves both cooperation and competition raises important social and technical challenges in aligning AI with human intentions and objectives. Diplomacy provides a particularly interesting environment for studying this because playing the game requires wrestling with conflicting objectives and translating those complex goals into natural language. As a simple example, a player might choose to compromise on short term gains in order to maintain an ally, on the chance that this ally will help them into an even better position on the next turn.

While we’ve made significant headway in this work, both the ability to robustly align language models with specific intentions and the technical (and normative) challenge of deciding on those intentions remain open and important problems. By open sourcing the CICERO code, we hope that AI researchers can continue to build off our work in a responsible manner. We have made early steps towards detecting and removing toxic messages in this new domain by using our dialogue model for zero-shot classification. We hope Diplomacy can serve as a safe sandbox to advance research in human-AI interaction.

Future directions

While CICERO is only capable of playing Diplomacy, the technology behind this achievement is relevant to many real world applications. Controlling natural language generation via planning and RL, could, for example, ease communication barriers between humans and AI-powered agents. For instance, today's AI assistants excel at simple question-answering tasks, like telling you the weather, but what if they could maintain a long-term conversation with the goal of teaching you a new skill? Alternatively, imagine a video game in which the non player characters (NPCs) could plan and converse like people do — understanding your motivations and adapting the conversation accordingly — to help you on your quest of storming the castle.

We’re excited about the potential for future advances in these areas and seeing how others build on our research.

Read the paper

Visit the CICERO site

Learn about the RFP

We’d like to acknowledge the contributions of a broad team to this work: Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra, Ana Paula Kirschner Mofarrej, Anne Davidson, Oliver Libaw, Amanda Felix, Karla Caraballo-Torres, Christopher Johnson, Lydia Baillergeau, Julia Vargas, Eric Kaplan, Raghu Nayani, Aiman Farooq, Andrea Cheung, Emily Astbury, Gopika Jhala, Jon Carvill, Jon Shepard, Josh Terry, Marina Zannoli, Nathan Riley, Michelle Restrepo, Noah Rizk, Ritika Trikha, Steph Miles, Tamara Piksa, Zara Blum, Daniel Duncan, Antoine Bordes, Laurens van der Maaten, Alex Boesenberg, Korey Anvaripour, Somya Jain, Harrison Rudolph, Michael Friedrichs, Elisabeth Sperle, and Cesar Guiterrez.

* All rights in Diplomacy are owned by Hasbro, Inc."
Meta_Blog,https://ai.meta.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/,,"CICERO: An AI agent that negotiates, persuades, and cooperates with people","Games have long been a proving ground for new AI advancements — from Deep Blue’s victory over chess grandmaster Garry Kasparov, to AlphaGo’s mastery of Go, to Pluribus out-bluffing the best humans in poker. But truly useful, versatile agents will need to go beyond just moving pieces on a board. Can we build more effective and flexible agents that can use language to negotiate, persuade, and work with people to achieve strategic goals similar to the way humans do?

Today, we’re announcing a breakthrough toward building AI that has mastered these skills. We’ve built an agent – CICERO – that is the first AI to achieve human-level performance in the popular strategy game Diplomacy*. CICERO demonstrated this by playing on webDiplomacy.net, an online version of the game, where CICERO achieved more than double the average score of the human players and ranked in the top 10 percent of participants who played more than one game.

Something Went Wrong We're having trouble playing this video. Learn more

Diplomacy has been viewed for decades as a near-impossible grand challenge in AI because it requires players to master the art of understanding other people’s motivations and perspectives; make complex plans and adjust strategies; and then use natural language to reach agreements with other people, convince them to form partnerships and alliances, and more. CICERO is so effective at using natural language to negotiate with people in Diplomacy that they often favored working with CICERO over other human participants.

Unlike games like Chess and Go, Diplomacy is a game about people rather than pieces. If an agent can't recognize that someone is likely bluffing or that another player would see a certain move as aggressive, it will quickly lose the game. Likewise, if it doesn't talk like a real person -- showing empathy, building relationships, and speaking knowledgeably about the game -- it won't find other players willing to work with it.

The key to our achievement was developing new techniques at the intersection of two completely different areas of AI research: strategic reasoning, as used in agents like AlphaGo and Pluribus, and natural language processing, as used in models like GPT-3, BlenderBot 3, LaMDA, and OPT-175B. CICERO can deduce, for example, that later in the game it will need the support of one particular player, and then craft a strategy to win that person’s favor – and even recognize the risks and opportunities that that player sees from their particular point of view.

We’ve open-sourced the code and published a paper to help the wider AI community use CICERO to spur further progress in human-AI cooperation. You can also visit the CICERO website to learn more about the project and see the agent in action. Interested researchers can submit a proposal to the CICERO RFP to gain access to the data.

Under the hood: How we built CICERO

At the heart of CICERO is a controllable dialogue model for Diplomacy coupled with a strategic reasoning engine. At each point in the game, CICERO looks at the game board and its conversation history, and models how the other players are likely to act. It then uses this plan to control a language model that can generate free-form dialogue, informing other players of its plans and proposing reasonable actions for the other players that coordinate well with them.

Controllable dialogue

To build a controllable dialogue model, we started with a 2.7 billion parameter BART-like language model pre-trained on text from the internet and fine tuned on over 40,000 human games on webDiplomacy.net. We developed techniques to automatically annotate messages in the training data with corresponding planned moves in the game, so that at inference time we can control dialogue generation to discuss specific desired actions for the agent and its conversation partners. For example, if our agent is playing as France, conditioning the dialogue model on a plan involving England supporting France into Burgundy might yield a message to England like, “Hi England! Are you willing to support me into Burgundy this turn?” Controlling generation in this manner allows Cicero to ground its conversations in a set of plans that it develops and revises over time to better negotiate. This helps the agent coordinate with and persuade other players more effectively.

Step 1 Using the board state and current dialogue, Cicero makes an initial prediction of what everyone will do.

Step 2 CICERO iteratively refines that prediction using planning and then uses those predictions to form an intent for itself and its partner.

Step 3 It generates several candidate messages based on the board state, dialogue, and its intents.

Step 4 It filters the candidate message to reduce nonsense, maximize value, and ensure consistency with our intents.

We further improve dialogue quality using several filtering mechanisms – such as classifiers trained to distinguish between human and model-generated text – that ensure that our dialogue is sensible, consistent with the current game state and previous messages, and strategically sound.

Dialogue-aware strategy & planning

Past superhuman agents in adversarial games like chess, Go, and poker were created through self-play reinforcement learning (RL) – having the agents learn optimal policies by playing millions of games against other copies of itself. However, games involving cooperation require modeling what humans will actually do in real life, rather than modeling what they should do if they were perfect copies of the bot. In particular, we want CICERO to make plans that are consistent with its dialogue with other players.

Something Went Wrong We're having trouble playing this video. Learn more

The classic approach to human modeling is supervised learning, where the agent is trained with labeled data such as a database of human players’ actions in past games. However, relying purely on supervised learning to choose actions based on past dialogue results in an agent that is relatively weak and highly exploitable. For example, a player could tell the agent, ""I'm glad we agreed that you will move your unit out of Paris!"" Since similar messages appear in the training data only when an agreement was reached, the agent might indeed move its unit out of Paris even if doing so is a clear strategic blunder.

To fix this, CICERO runs an iterative planning algorithm that balances dialogue consistency with rationality. The agent first predicts everyone's policy for the current turn based on the dialogue it has shared with other players, and also predicts what other players think the agent's policy will be. It then runs a planning algorithm we developed called piKL, which iteratively improves these predictions by trying to choose new policies that have higher expected value given the other players' predicted policies, while also trying to keep the new predictions close to the original policy predictions. We found that piKL better models human play and leads to better policies for the agent compared to supervised learning alone.

Generating natural, purposeful dialogue

In Diplomacy, how a player talks to other people can be even more important than how they move their pieces. CICERO is able to speak clearly and persuasively when strategizing with other players. For example, in one demonstration game CICERO asked one player for immediate support on one part of the board while pressing another to consider an alliance later in the game.

In these exchanges, CICERO tries to execute its strategy by proposing moves to three different players. In the second dialog, the agent is able to tell the other player why they should cooperate and how it will be mutually beneficial. In the third, CICERO is both soliciting information and setting the groundwork for future moves.

Where there is still room for improvement

It is important to recognize that CICERO also sometimes generates inconsistent dialogue that can undermine its objectives. In the example below where CICERO was playing as Austria, the agent contradicts its first message asking Italy to move to Venice. While our suite of filters aims to detect these sorts of mistakes, it is not perfect.

Diplomacy as a sandbox for advancing human-AI interaction

The emergence of goal-oriented dialogue systems in a game that involves both cooperation and competition raises important social and technical challenges in aligning AI with human intentions and objectives. Diplomacy provides a particularly interesting environment for studying this because playing the game requires wrestling with conflicting objectives and translating those complex goals into natural language. As a simple example, a player might choose to compromise on short term gains in order to maintain an ally, on the chance that this ally will help them into an even better position on the next turn.

While we’ve made significant headway in this work, both the ability to robustly align language models with specific intentions and the technical (and normative) challenge of deciding on those intentions remain open and important problems. By open sourcing the CICERO code, we hope that AI researchers can continue to build off our work in a responsible manner. We have made early steps towards detecting and removing toxic messages in this new domain by using our dialogue model for zero-shot classification. We hope Diplomacy can serve as a safe sandbox to advance research in human-AI interaction.

Future directions

While CICERO is only capable of playing Diplomacy, the technology behind this achievement is relevant to many real world applications. Controlling natural language generation via planning and RL, could, for example, ease communication barriers between humans and AI-powered agents. For instance, today's AI assistants excel at simple question-answering tasks, like telling you the weather, but what if they could maintain a long-term conversation with the goal of teaching you a new skill? Alternatively, imagine a video game in which the non player characters (NPCs) could plan and converse like people do — understanding your motivations and adapting the conversation accordingly — to help you on your quest of storming the castle.

We’re excited about the potential for future advances in these areas and seeing how others build on our research.

Read the paper

Visit the CICERO site

Learn about the RFP

We’d like to acknowledge the contributions of a broad team to this work: Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra, Ana Paula Kirschner Mofarrej, Anne Davidson, Oliver Libaw, Amanda Felix, Karla Caraballo-Torres, Christopher Johnson, Lydia Baillergeau, Julia Vargas, Eric Kaplan, Raghu Nayani, Aiman Farooq, Andrea Cheung, Emily Astbury, Gopika Jhala, Jon Carvill, Jon Shepard, Josh Terry, Marina Zannoli, Nathan Riley, Michelle Restrepo, Noah Rizk, Ritika Trikha, Steph Miles, Tamara Piksa, Zara Blum, Daniel Duncan, Antoine Bordes, Laurens van der Maaten, Alex Boesenberg, Korey Anvaripour, Somya Jain, Harrison Rudolph, Michael Friedrichs, Elisabeth Sperle, and Cesar Guiterrez.

* All rights in Diplomacy are owned by Hasbro, Inc."
Meta_Blog,https://ai.meta.com/blog/multiray-large-scale-AI-models/,,MultiRay: Optimizing efficiency for large-scale AI models,"In current AI systems that process text, images, and other modalities, the best possible results are obtained by taking a very large model trained on an immense amount of data and then specializing it on a particular task (for example, identifying harmful speech). This produces an extremely high-quality, extremely expensive one-trick pony. If you have many problems to solve, you’ll need many models, and the cost of operating so many large models will rapidly spiral out of control. This means that in practice, state-of-the-art large models are rarely used in production, and real-world models are often much smaller and simpler.

What if you could compute the expensive part of understanding content with AI once but reuse this result (known as an embedding) across multiple tasks? As part of our push to make our AI systems more efficient, we’ve developed MultiRay, a new platform for running state-of-the-art AI models at scale. MultiRay allows multiple models to run on the same input, and share the majority of the processing costs while incurring only a small per-model cost. Doing this helps us optimize the total cost of performing these AI tasks. We can more easily introduce AI accelerators due to the concentration of company-wide computation into a single model, and we can also trade off between compute power and storage at the company level.

MultiRay’s universal models are trained to perform well across a wide set of tasks and domains. Such a jack-of-all-trades model delivers better quality than the much smaller per-task specialized models we used previously. With MultiRay, teams across Meta can more quickly improve and iterate on machine learning (ML) models for myriad applications, ranging from topic tagging of posts to hate speech detection. These tasks can also be achieved with better efficiency and less human effort than if each team were to build large end-to-end models from scratch.

MultiRay’s first model, TextRay, has been in production since 2020 and supports text understanding applications, such as detecting inauthentic content and improving users’ search experience.

More modalities, more problems

Text is a good start, but the real world is more complex, incorporating many modalities. A Facebook post, for example, might contain text, images, and video. To understand a post, a system needs to analyze each of these elements separately and in context of the others. But doing this means combining several models that are already compute-intensive into a larger, even more intensive model. The resulting increase in compute and power consumption slows down our efforts to bring the most advanced ML models into production for our products and services.

PostRay, MultiRay’s second model, brings together text and image understanding into the same model. Since posts across FB and IG often contain both text and image data, PostRay reduces the need for teams to have their own text and image understanding. PostRay has several use cases across Meta, including topic classification, which is used for Reels.

PostRay models, because they incorporate cutting-edge research in multiple fields simultaneously, are more complex to train, deploy, and maintain. With MultiRay, we only have to do these tasks a single time, and the whole company reaps the benefits. A centralized system serving a jack-of-all-trades model allows us to work directly with cutting-edge research teams and bring their work to production soon after it is published.

How MultiRay works

MultiRay’s primary aim is to democratize access to large foundational models at Meta. It does so by centralizing the execution on accelerators like GPUs and using a cache to save on cost of recomputation as much as possible. Currently, MultiRay powers over 125 use cases across Meta, and it supports up to 20 million queries per second (QPS) while serving 800 billion queries per day.

What are embeddings?

MultiRay uses large foundational models that return a point in a high-dimensional vector space that represents the input. This point is called an embedding and is a more ML-friendly version of the original input. Instead of processing the raw input (such as text and images), task-specific models can consume the embedding from MultiRay, which is much simpler to handle. The foundational models deployed in MultiRay are optimized to work for a variety of tasks, including similarity and classification. This universality makes our embeddings quite large (many kilobytes) so as to convey more information.

Why centralize large models?

Amortization across many teams

Large models and latency constraints demand execution on accelerators like GPUs. Accelerators (specialized hardware) are in high demand across Meta, and even with them, state-of-the-art models consume a lot of energy to train and host. MultiRay’s client teams split the bill for training and hosting these large models, as the same hardware and processing can be used multiple times. These are much larger and higher quality than what each team could have hosted alone. In this case, the whole is greater than the sum of the parts.

Simpler development and operations

Generally, teams across Meta are responsible for their own models, infrastructure, and model upkeep. As the models grow in size, it places an increasing operational burden on each team to train and serve them. It also makes it harder to apply sophisticated optimization techniques to models spread across a variety of teams. MultiRay serves a small number of large centralized models, allowing a single team to handle the majority of the operations and optimization. Client teams own smaller, task-specific models that are easier to manage. This allows many teams that didn’t have the bandwidth to train, deploy, and manage cutting-edge AI to use that technology.

Faster research to production: Single-point acceleration

Since MultiRay is a centralized service used by over 125 clients, improvements benefit all the clients. As a result, MultiRay has become a sandbox for our ML and systems specialists to contribute key optimizations that support the broader PyTorch and accelerator ecosystem. MultiRay, for example, was the first large use case to deploy PyTorch’s BetterTransformer in production at Meta. This brought significant capacity savings with no impact on quality.

Efficiency on accelerators: Cross-request batching

Accelerator hardware is most efficient when it processes an aggregated group of requests in parallel, known as a batch. Optimal batching of requests allows increasing throughput of the service without causing undue latency. Batch construction adds complexity to our internal clients, and the ideal batch can change with new hardware or models.

To keep things simple for our internal users, the MultiRay external API is for a single request at a time. MultiRay then internally uses a cross-request batching logic to aggregate many concurrent requests across clients into a batch. This allows us to write the logic once and tune it to create ideally sized batches for the model and hardware. This batching is completely hidden from the clients sending the requests, even when we make major performance changes, such as the larger batch size used by migration to the new generation of GPU accelerator hardware.

Cache: Trade-off compute and storage

MultiRay utilizes a cache to save on cost of recomputation as much as possible. It is a multilayered cache to minimize cost and latency, with each layer bringing more hit rate, at the cost of lower speed. The layers start from a fast but small per-host local cache in the RAM of every MultiRay server, and they end with a slower but much larger globally distributed cache in flash memory.

The MultiRay models are large, and they produce large embeddings (many kilobytes) to preserve universality. For text understanding, these embeddings are much larger than the inputs themselves! It takes less energy to serve an embedding out of cache than to recompute it, but it’s not zero. Since the cache storage available is finite, it is not possible to cache the results for a long time.

MultiRay measures the request patterns across clients to figure out the best cache settings (size, time-to-live, update policies) to reduce the total cost of the service. For example, we use these measured data to simulate the energy required for various cache lifetime settings, trading off the cost of recomputation of a request on accelerators versus serving it from cache. This feedback loop allowed us to improve the efficiency of MultiRay even while client behavior constantly changes.

No free lunch: The challenges of a centralized service

A centralized service used across Meta comes with many challenges. Some of the challenges, such as client management, quotas, and cost attribution, considered solved problems for large-scale systems like databases, had to be adapted for the AI domain. Query size and cache hit rate both affect the energy required to process queries, so quotas are more complex. Additionally, sharing the expense of the higher-quality, more expensive MultiRay models only works if our models are widely used, which requires models to offer state-of-the-art quality across many use cases. This moving target means that we made heavy investments in model refresh (versioning, upgrades to newer versions, and deprecations of older versions) and innovating new model architectures and training flows to reduce research to production time and keep MultiRay users on the latest technology.

Learn more about MultiRay

If you’re curious about MultiRay, we encourage to you take a look at the research from Meta’s Foundational AI Research (FAIR) team that led to its development:

Unsupervised cross-lingual representation learning at scale — where researchers first demonstrated that multilingual modeling can be done without sacrificing per-language performance.

General purpose text embeddings from pre-trained language models for scalable inference — where researchers demonstrate a solution for NLP in which multiple tasks are performed on the same text using large-scale pre-trained models at a fraction of the compute cost.

Multiscale vision transformers and Masked autoencoders as spatiotemporal learners — foundational research pointing toward how MultiRay can be applied to video-related tasks in the future.

Acknowledgements

We would also like to thank Abhinandan Krishnan, Anshul Verma, Daniel Ho, Davis Liang, Emily Shen, Evan Trippler, Hanchao Yu, Harley Boughton, Harsha Naidu K, Jafar Taghiyar, Michael Saha, Philippe Brunet, Rui Hou, Ruty Rinott, Shreya Goyal, Victor Dogaru, Akram Baharlouei, Charles Bai, Chenyang Yu, Jeff Wang, Manisha Jain, Marvin Wang, Maxim Grechkin, Michael Wu, Rao Bayyana, and Ves Stoyanov, who helped make this happen."
Meta_Blog,https://ai.meta.com/blog/multiray-large-scale-AI-models/,,MultiRay: Optimizing efficiency for large-scale AI models,"In current AI systems that process text, images, and other modalities, the best possible results are obtained by taking a very large model trained on an immense amount of data and then specializing it on a particular task (for example, identifying harmful speech). This produces an extremely high-quality, extremely expensive one-trick pony. If you have many problems to solve, you’ll need many models, and the cost of operating so many large models will rapidly spiral out of control. This means that in practice, state-of-the-art large models are rarely used in production, and real-world models are often much smaller and simpler.

What if you could compute the expensive part of understanding content with AI once but reuse this result (known as an embedding) across multiple tasks? As part of our push to make our AI systems more efficient, we’ve developed MultiRay, a new platform for running state-of-the-art AI models at scale. MultiRay allows multiple models to run on the same input, and share the majority of the processing costs while incurring only a small per-model cost. Doing this helps us optimize the total cost of performing these AI tasks. We can more easily introduce AI accelerators due to the concentration of company-wide computation into a single model, and we can also trade off between compute power and storage at the company level.

MultiRay’s universal models are trained to perform well across a wide set of tasks and domains. Such a jack-of-all-trades model delivers better quality than the much smaller per-task specialized models we used previously. With MultiRay, teams across Meta can more quickly improve and iterate on machine learning (ML) models for myriad applications, ranging from topic tagging of posts to hate speech detection. These tasks can also be achieved with better efficiency and less human effort than if each team were to build large end-to-end models from scratch.

MultiRay’s first model, TextRay, has been in production since 2020 and supports text understanding applications, such as detecting inauthentic content and improving users’ search experience.

More modalities, more problems

Text is a good start, but the real world is more complex, incorporating many modalities. A Facebook post, for example, might contain text, images, and video. To understand a post, a system needs to analyze each of these elements separately and in context of the others. But doing this means combining several models that are already compute-intensive into a larger, even more intensive model. The resulting increase in compute and power consumption slows down our efforts to bring the most advanced ML models into production for our products and services.

PostRay, MultiRay’s second model, brings together text and image understanding into the same model. Since posts across FB and IG often contain both text and image data, PostRay reduces the need for teams to have their own text and image understanding. PostRay has several use cases across Meta, including topic classification, which is used for Reels.

PostRay models, because they incorporate cutting-edge research in multiple fields simultaneously, are more complex to train, deploy, and maintain. With MultiRay, we only have to do these tasks a single time, and the whole company reaps the benefits. A centralized system serving a jack-of-all-trades model allows us to work directly with cutting-edge research teams and bring their work to production soon after it is published.

How MultiRay works

MultiRay’s primary aim is to democratize access to large foundational models at Meta. It does so by centralizing the execution on accelerators like GPUs and using a cache to save on cost of recomputation as much as possible. Currently, MultiRay powers over 125 use cases across Meta, and it supports up to 20 million queries per second (QPS) while serving 800 billion queries per day.

What are embeddings?

MultiRay uses large foundational models that return a point in a high-dimensional vector space that represents the input. This point is called an embedding and is a more ML-friendly version of the original input. Instead of processing the raw input (such as text and images), task-specific models can consume the embedding from MultiRay, which is much simpler to handle. The foundational models deployed in MultiRay are optimized to work for a variety of tasks, including similarity and classification. This universality makes our embeddings quite large (many kilobytes) so as to convey more information.

Why centralize large models?

Amortization across many teams

Large models and latency constraints demand execution on accelerators like GPUs. Accelerators (specialized hardware) are in high demand across Meta, and even with them, state-of-the-art models consume a lot of energy to train and host. MultiRay’s client teams split the bill for training and hosting these large models, as the same hardware and processing can be used multiple times. These are much larger and higher quality than what each team could have hosted alone. In this case, the whole is greater than the sum of the parts.

Simpler development and operations

Generally, teams across Meta are responsible for their own models, infrastructure, and model upkeep. As the models grow in size, it places an increasing operational burden on each team to train and serve them. It also makes it harder to apply sophisticated optimization techniques to models spread across a variety of teams. MultiRay serves a small number of large centralized models, allowing a single team to handle the majority of the operations and optimization. Client teams own smaller, task-specific models that are easier to manage. This allows many teams that didn’t have the bandwidth to train, deploy, and manage cutting-edge AI to use that technology.

Faster research to production: Single-point acceleration

Since MultiRay is a centralized service used by over 125 clients, improvements benefit all the clients. As a result, MultiRay has become a sandbox for our ML and systems specialists to contribute key optimizations that support the broader PyTorch and accelerator ecosystem. MultiRay, for example, was the first large use case to deploy PyTorch’s BetterTransformer in production at Meta. This brought significant capacity savings with no impact on quality.

Efficiency on accelerators: Cross-request batching

Accelerator hardware is most efficient when it processes an aggregated group of requests in parallel, known as a batch. Optimal batching of requests allows increasing throughput of the service without causing undue latency. Batch construction adds complexity to our internal clients, and the ideal batch can change with new hardware or models.

To keep things simple for our internal users, the MultiRay external API is for a single request at a time. MultiRay then internally uses a cross-request batching logic to aggregate many concurrent requests across clients into a batch. This allows us to write the logic once and tune it to create ideally sized batches for the model and hardware. This batching is completely hidden from the clients sending the requests, even when we make major performance changes, such as the larger batch size used by migration to the new generation of GPU accelerator hardware.

Cache: Trade-off compute and storage

MultiRay utilizes a cache to save on cost of recomputation as much as possible. It is a multilayered cache to minimize cost and latency, with each layer bringing more hit rate, at the cost of lower speed. The layers start from a fast but small per-host local cache in the RAM of every MultiRay server, and they end with a slower but much larger globally distributed cache in flash memory.

The MultiRay models are large, and they produce large embeddings (many kilobytes) to preserve universality. For text understanding, these embeddings are much larger than the inputs themselves! It takes less energy to serve an embedding out of cache than to recompute it, but it’s not zero. Since the cache storage available is finite, it is not possible to cache the results for a long time.

MultiRay measures the request patterns across clients to figure out the best cache settings (size, time-to-live, update policies) to reduce the total cost of the service. For example, we use these measured data to simulate the energy required for various cache lifetime settings, trading off the cost of recomputation of a request on accelerators versus serving it from cache. This feedback loop allowed us to improve the efficiency of MultiRay even while client behavior constantly changes.

No free lunch: The challenges of a centralized service

A centralized service used across Meta comes with many challenges. Some of the challenges, such as client management, quotas, and cost attribution, considered solved problems for large-scale systems like databases, had to be adapted for the AI domain. Query size and cache hit rate both affect the energy required to process queries, so quotas are more complex. Additionally, sharing the expense of the higher-quality, more expensive MultiRay models only works if our models are widely used, which requires models to offer state-of-the-art quality across many use cases. This moving target means that we made heavy investments in model refresh (versioning, upgrades to newer versions, and deprecations of older versions) and innovating new model architectures and training flows to reduce research to production time and keep MultiRay users on the latest technology.

Learn more about MultiRay

If you’re curious about MultiRay, we encourage to you take a look at the research from Meta’s Foundational AI Research (FAIR) team that led to its development:

Unsupervised cross-lingual representation learning at scale — where researchers first demonstrated that multilingual modeling can be done without sacrificing per-language performance.

General purpose text embeddings from pre-trained language models for scalable inference — where researchers demonstrate a solution for NLP in which multiple tasks are performed on the same text using large-scale pre-trained models at a fraction of the compute cost.

Multiscale vision transformers and Masked autoencoders as spatiotemporal learners — foundational research pointing toward how MultiRay can be applied to video-related tasks in the future.

Acknowledgements

We would also like to thank Abhinandan Krishnan, Anshul Verma, Daniel Ho, Davis Liang, Emily Shen, Evan Trippler, Hanchao Yu, Harley Boughton, Harsha Naidu K, Jafar Taghiyar, Michael Saha, Philippe Brunet, Rui Hou, Ruty Rinott, Shreya Goyal, Victor Dogaru, Akram Baharlouei, Charles Bai, Chenyang Yu, Jeff Wang, Manisha Jain, Marvin Wang, Maxim Grechkin, Michael Wu, Rao Bayyana, and Ves Stoyanov, who helped make this happen."
Meta_Blog,https://ai.meta.com/blog/ai-math-theorem-proving/,,Teaching AI advanced mathematical reasoning,"The scientific community has long regarded mathematical theorem proving as a crucial step in building intelligent machines. Demonstrating that a particular conjecture is true or false requires using symbolic reasoning and navigating an infinite number of possible choices — skills that stretch the abilities of even the most advanced AI systems today.

We are excited to share a significant advance in the field of AI and mathematics. Meta AI has built a neural theorem prover that has solved 10 International Math Olympiad (IMO) problems — 5x more than any previous AI system. Our AI model also improves upon the current state of the art by 20 percent on miniF2F, a widely used mathematics benchmark, and by 10 percent on the Metamath benchmark.

Our method, HyperTree Proof Search (HTPS), is trained on a dataset of successful mathematical proofs and then learns to generalize to new, very different kinds of problems. It was able to deduce a correct proof for an IMO problem that involved some arithmetic reduction to a finite number of cases.

We’ve detailed our work in a new research paper on HTPS to be presented at NeurIPS 2022, and we’ve made our model available through the Lean Visual Studio Code (VSCode) plugin, which will allow other researchers to explore the capabilities of our AI model within the popular Lean environment. We hope that others can build on our work as we have built on previous research and that collectively we can continue to make rapid progress in this exciting field.

Something Went Wrong We're having trouble playing this video. Learn more

The challenge of International Math Olympiad problems

Experts have long believed that creating a system that competes at the highest level at the IMO is a grand challenge for the field of AI. The IMO is the world’s premier high school mathematics competition. Since 1959, students have competed to solve challenging problems in algebra, combinatorics, number theory, and geometry. To do well, competitors need both creativity and advanced reasoning skills. Some questions are so difficult that the majority of students score zero points on them.

In many ways, theorem proving is more challenging than building AI to play board games like chess. When trying to prove a theorem, the action space of possible moves is not just large but infinite. And in chess or Go, exploring a chain of possible moves can be useful even if that decision tree ultimately doesn’t lead to the best possible move. In theorem proving, a dead end is just a dead end, and the computational effort the solver used is simply wasted effort.

We have been able to solve the following problem:

Let a and b be natural numbers both prime with 7, and such that 7 is also prime with a + b, if we assume that 77 divides (a + b)7 - a7 - b7, we need to show that a + b is at least 19.

The proof involves using the binomial formula and then checking different cases. Although it is clear from a human perspective that this approach will work in this particular case, solving a problem like this by exhaustive checking is a rarely used, rarely effective strategy.

The model proceeds by contraposition and simplifies the equation:

contrapose h₄, simp only [nat.dvd_iff_mod_eq_zero, nat.add_zero] at *, norm_num [nat.mod_eq_of_lt, mul_comm, nat.add_mod] at h₄,

And then checks the different cases.

This problem shown is not the most difficult that IMO competitors face; in fact, it might be one of the most approachable. But what makes a problem like this particularly difficult from an AI perspective is that it requires a somewhat unusual approach compared to proofs found in standard training data, such as the formal mathematical corpus Mathlib. And because the problem space is infinite, a brute force search is not feasible. The solver – whether person or machine – must rely on the kind of creative reasoning that has been hard for AI to “grasp.”

This is in large part because previous approaches have usually relied only on language models. While these systems can produce very impressive results, they can sometimes lack the capacity to explore different approaches. This skill is critical to solve challenging mathematical problems that may require a bit of creativity.

Getting closer to human reasoning

Mathematical reasoning is difficult to codify and even more difficult to quantify. Current approaches in AI have focused on building machines that can solve a problem “at once” by generating a complete solution to the problem in a single step. But this is not how people tackle these challenges. We use intuition, break a complex problem into component parts, and look for ways to make incremental progress.

To mimic a more human approach, we needed the neural theorem prover to associate a particular “state” with our current (incomplete) understanding of the problem. We first started with a reinforcement learning approach that was tightly coupled with existing proving assistants such as Lean.

Using proving assistants makes this approach possible as they implement a step-by-step reasoning mechanism. We can then interpret the “current state” of the (incomplete) proof as a node in a graph and each new step as an edge. Using this approach, we can leverage techniques that have proved tremendously efficient for two-player games such as Go or chess.

We then needed a way to evaluate the quality of a proof state — similar to how a chess-playing AI one needs to evaluate a particular position in the game. To do this, we used an approach inspired by a Monte Carlo tree search (MCTS) in which the model cycles between two tasks: 1) the prior estimation of reasonable arguments to use in a given proof state, and 2) the outcome of the proof after a certain number of arguments are given.

The HTPS is a variation on the standard MCTS approach, where, to explore a graph, one leverages prior knowledge about the graph to choose a set of leaves to expand, then refines the initial knowledge via a backup correction. The graph is progressively explored, and knowledge about the graph structure gets refined with iterations.

This makes it possible to use an online-training procedure that considerably enhances the performance of the initial pretrained model on a specific type of problem, namely those similar to the ones used in the IMO.

The result is that our approach was able to solve 10 unseen IMO problems and achieves 67 percent accuracy on the Minif2f validation set accuracy — a full 20 percent better than the current published state of the art.

Real world impact, from software verification to aerospace

Building machines capable of solving advanced math problems will lead to real-world impact, notably in the field of software verification. Today, many companies (including Meta) use formal proofs to verify software. In fact, the tools and formal systems used to verify software and to prove theorems are the same. The main difference lies in what type of data the models are trained on: either a dataset of functions or mathematical theorems. Beyond software verification, there are a number of industrial applications, especially as complexity continues to increase and automation permeates critical tasks. Some examples include cryptography and aerospace, where operating conditions can vary and testing and simulation are critical.

To learn more about how AI is disrupting mathematics, please join us at the Formal Languages, AI and Mathematics (FLAIM) workshop in Paris on November 3–4, where there will be speakers from Meta AI, DeepMind, OpenAI, and many others. You can also read our paper on HTPS, which will be presented at NeurIPS 2022, and interact with our model through the Lean Visual Studio Code (VSCode) plugin.

Suggestions from the plugin to prove a basic lemma about group theory: Here, the suggestion rw inv_mul_self at key (in red) is a correct step in the proof (in yellow).

We’d like to acknowledge the contributions of a broad team to this work: Guillaume Lample, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Thibaut Lavril, Aurelien Rodriguez, Hugo Touvron, Fabian Gloeckle, Albert Jiang, Yongyi Hu, Amaury Hayat (CERMICS École des Ponts ParisTech), Gabriel Ebner (Vrije Universiteit Amsterdam), Alexander Miller."
Meta_Blog,https://ai.meta.com/blog/ai-math-theorem-proving/,,Teaching AI advanced mathematical reasoning,"The scientific community has long regarded mathematical theorem proving as a crucial step in building intelligent machines. Demonstrating that a particular conjecture is true or false requires using symbolic reasoning and navigating an infinite number of possible choices — skills that stretch the abilities of even the most advanced AI systems today.

We are excited to share a significant advance in the field of AI and mathematics. Meta AI has built a neural theorem prover that has solved 10 International Math Olympiad (IMO) problems — 5x more than any previous AI system. Our AI model also improves upon the current state of the art by 20 percent on miniF2F, a widely used mathematics benchmark, and by 10 percent on the Metamath benchmark.

Our method, HyperTree Proof Search (HTPS), is trained on a dataset of successful mathematical proofs and then learns to generalize to new, very different kinds of problems. It was able to deduce a correct proof for an IMO problem that involved some arithmetic reduction to a finite number of cases.

We’ve detailed our work in a new research paper on HTPS to be presented at NeurIPS 2022, and we’ve made our model available through the Lean Visual Studio Code (VSCode) plugin, which will allow other researchers to explore the capabilities of our AI model within the popular Lean environment. We hope that others can build on our work as we have built on previous research and that collectively we can continue to make rapid progress in this exciting field.

Something Went Wrong We're having trouble playing this video. Learn more

The challenge of International Math Olympiad problems

Experts have long believed that creating a system that competes at the highest level at the IMO is a grand challenge for the field of AI. The IMO is the world’s premier high school mathematics competition. Since 1959, students have competed to solve challenging problems in algebra, combinatorics, number theory, and geometry. To do well, competitors need both creativity and advanced reasoning skills. Some questions are so difficult that the majority of students score zero points on them.

In many ways, theorem proving is more challenging than building AI to play board games like chess. When trying to prove a theorem, the action space of possible moves is not just large but infinite. And in chess or Go, exploring a chain of possible moves can be useful even if that decision tree ultimately doesn’t lead to the best possible move. In theorem proving, a dead end is just a dead end, and the computational effort the solver used is simply wasted effort.

We have been able to solve the following problem:

Let a and b be natural numbers both prime with 7, and such that 7 is also prime with a + b, if we assume that 77 divides (a + b)7 - a7 - b7, we need to show that a + b is at least 19.

The proof involves using the binomial formula and then checking different cases. Although it is clear from a human perspective that this approach will work in this particular case, solving a problem like this by exhaustive checking is a rarely used, rarely effective strategy.

The model proceeds by contraposition and simplifies the equation:

contrapose h₄, simp only [nat.dvd_iff_mod_eq_zero, nat.add_zero] at *, norm_num [nat.mod_eq_of_lt, mul_comm, nat.add_mod] at h₄,

And then checks the different cases.

This problem shown is not the most difficult that IMO competitors face; in fact, it might be one of the most approachable. But what makes a problem like this particularly difficult from an AI perspective is that it requires a somewhat unusual approach compared to proofs found in standard training data, such as the formal mathematical corpus Mathlib. And because the problem space is infinite, a brute force search is not feasible. The solver – whether person or machine – must rely on the kind of creative reasoning that has been hard for AI to “grasp.”

This is in large part because previous approaches have usually relied only on language models. While these systems can produce very impressive results, they can sometimes lack the capacity to explore different approaches. This skill is critical to solve challenging mathematical problems that may require a bit of creativity.

Getting closer to human reasoning

Mathematical reasoning is difficult to codify and even more difficult to quantify. Current approaches in AI have focused on building machines that can solve a problem “at once” by generating a complete solution to the problem in a single step. But this is not how people tackle these challenges. We use intuition, break a complex problem into component parts, and look for ways to make incremental progress.

To mimic a more human approach, we needed the neural theorem prover to associate a particular “state” with our current (incomplete) understanding of the problem. We first started with a reinforcement learning approach that was tightly coupled with existing proving assistants such as Lean.

Using proving assistants makes this approach possible as they implement a step-by-step reasoning mechanism. We can then interpret the “current state” of the (incomplete) proof as a node in a graph and each new step as an edge. Using this approach, we can leverage techniques that have proved tremendously efficient for two-player games such as Go or chess.

We then needed a way to evaluate the quality of a proof state — similar to how a chess-playing AI one needs to evaluate a particular position in the game. To do this, we used an approach inspired by a Monte Carlo tree search (MCTS) in which the model cycles between two tasks: 1) the prior estimation of reasonable arguments to use in a given proof state, and 2) the outcome of the proof after a certain number of arguments are given.

The HTPS is a variation on the standard MCTS approach, where, to explore a graph, one leverages prior knowledge about the graph to choose a set of leaves to expand, then refines the initial knowledge via a backup correction. The graph is progressively explored, and knowledge about the graph structure gets refined with iterations.

This makes it possible to use an online-training procedure that considerably enhances the performance of the initial pretrained model on a specific type of problem, namely those similar to the ones used in the IMO.

The result is that our approach was able to solve 10 unseen IMO problems and achieves 67 percent accuracy on the Minif2f validation set accuracy — a full 20 percent better than the current published state of the art.

Real world impact, from software verification to aerospace

Building machines capable of solving advanced math problems will lead to real-world impact, notably in the field of software verification. Today, many companies (including Meta) use formal proofs to verify software. In fact, the tools and formal systems used to verify software and to prove theorems are the same. The main difference lies in what type of data the models are trained on: either a dataset of functions or mathematical theorems. Beyond software verification, there are a number of industrial applications, especially as complexity continues to increase and automation permeates critical tasks. Some examples include cryptography and aerospace, where operating conditions can vary and testing and simulation are critical.

To learn more about how AI is disrupting mathematics, please join us at the Formal Languages, AI and Mathematics (FLAIM) workshop in Paris on November 3–4, where there will be speakers from Meta AI, DeepMind, OpenAI, and many others. You can also read our paper on HTPS, which will be presented at NeurIPS 2022, and interact with our model through the Lean Visual Studio Code (VSCode) plugin.

Suggestions from the plugin to prove a basic lemma about group theory: Here, the suggestion rw inv_mul_self at key (in red) is a correct step in the proof (in yellow).

We’d like to acknowledge the contributions of a broad team to this work: Guillaume Lample, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Thibaut Lavril, Aurelien Rodriguez, Hugo Touvron, Fabian Gloeckle, Albert Jiang, Yongyi Hu, Amaury Hayat (CERMICS École des Ponts ParisTech), Gabriel Ebner (Vrije Universiteit Amsterdam), Alexander Miller."
Meta_Blog,https://ai.meta.com/blog/protein-folding-esmfold-metagenomics/,,ESM Metagenomic Atlas: The first view of the ‘dark matter’ of the protein universe,"Something Went Wrong We're having trouble playing this video. Learn more

Meta AI has created the first database that reveals the structures of the metagenomic world at the scale of hundreds of millions of proteins. These proteins – which are found in microbes in the soil, deep in the ocean, and even inside our bodies – vastly outnumber those that make up animal and plant life. But they are the least understood proteins on earth.

Decoding metagenomic structures can help us solve long-standing mysteries of evolutionary history and discover proteins that may help cure diseases, clean up the environment, and produce cleaner energy.

To make structure predictions at this scale, a breakthrough in the speed of protein folding is necessary. We trained a large language model to learn evolutionary patterns and generate accurate structure predictions end to end directly from the sequence of a protein. Predictions are up to 60x faster than the current state-of-the-art while maintaining accuracy, making our approach scalable to far larger databases.

We are now sharing our models, research paper, and a database of more than 600 million metagenomic structures, as well as an API that allows scientists to easily retrieve specific protein structures relevant to their work.

Explore the ESM Metagenomic Atlas here.

Proteins are complex and dynamic molecules, encoded by our genes, that are responsible for many of the varied and fundamental processes of life. They have an astounding range of roles in biology. The rods and cones in our eyes that sense light and make it possible for us to see, the molecular sensors that underlie hearing and our sense of touch, the complex molecular machines that convert sunlight into chemical energy in plants, the motors that drive motion in microbes and our muscles, enzymes that break down plastic, antibodies that protect us from disease, and molecular circuits that cause disease when they fail — are all proteins.

Metagenomics, one of the new frontiers in the natural sciences, uses gene sequencing to discover proteins in samples from environments across the earth, from microbes living in the soil, deep in the ocean, in extreme environments like hydrothermal vents, and even in our guts and on our skin. The natural world contains a vast number of proteins beyond the ones that have been cataloged and annotated in well-studied organisms. Metagenomics is starting to reveal the incredible breadth and diversity of these proteins, uncovering billions of protein sequences that are new to science and cataloged for the first time in large databases compiled by public initiatives such as the NCBI, European Bioinformatics Institute and Joint Genome Institute, incorporating studies from a worldwide community of researchers.

Meta AI has developed a new protein-folding approach that harnesses large language models to create the first comprehensive view of the structures of proteins in a metagenomics database at the scale of hundreds of millions of proteins. Our research team found that language models can accelerate the speed at which an atomic-level three-dimensional structure can be predicted up to 60x faster relative to existing state-of-the-art protein structure prediction approaches. This advance will help to accelerate a new era of structural understanding where it could be possible for the first time to understand the structure of billions of proteins that gene-sequencing technology is cataloging.

Today, we are releasing the 600+ million protein ESM Metagenomic Atlas, with predictions for nearly the entire MGnify90 database, a public resource cataloging metagenomic sequences. To our knowledge, this is the largest database of high resolution predicted structures, 3x larger than any existing protein structure database, and the first to cover metagenomic proteins comprehensively and at scale. These structures provide an unprecedented view into the breadth and diversity of nature, and hold the potential for new scientific insights and to accelerate discovery of proteins for practical applications in fields such as medicine, green chemistry, environmental applications, and renewable energy.

In addition, we are releasing the fast protein folding model used to create the database and an API that allows researchers to use it for scientific discovery. With 15 billion parameters, our new language model is the largest language model of proteins to date.

Unlocking a hidden natural world: the first comprehensive view of metagenomic structural space

Advancements in gene sequencing have made it possible to catalog billions of metagenomic protein sequences. Although we know that these proteins exist, because we have discovered their sequences, understanding their biology is a staggering challenge. Determining the three-dimensional structures for hundreds of millions of proteins experimentally is far beyond the reach of time-intensive laboratory techniques such as X-ray crystallography, which can take weeks to years for a single protein. Computational approaches can give us insight into metagenomics proteins that isn’t possible with experimental techniques.

The ESM Metagenomic Atlas will enable scientists to search and analyze the structures of metagenomic proteins at the scale of hundreds of millions of proteins. This can help researchers to identify structures that have not been characterized before, search for distant evolutionary relationships, and discover new proteins that can be useful in medicine and other applications.

A map of tens of thousands of high-confidence predictions showing similarity to proteins whose structure is currently known. The image shows large regions of completely unknown structural space revealed for the first time.

Learning to read the language of nature

The ESM-2 language model is trained to predict amino acids that have been masked out of sequences across evolution. We discovered that, as a result of this training, information about the protein’s structure emerges in the internal states of the model. This is surprising because the model has been trained only on sequences.

Like the text of an essay or letter, proteins can be written as sequences of characters. Each character corresponds to one of 20 standard chemical elements — amino acids, each with different properties — that are the building blocks of proteins. These building blocks can be combined in an astronomical number of different ways — e.g., for a protein made of 200 amino acids, there are 20^200 possible sequences — more than the number of atoms in the visible universe. Every sequence folds into a three-dimensional shape (though not all will fold into coherent structures; many sequences fold into disordered forms), and it is this shape that largely determines the biological function of the protein.

Learning to read this language of biology poses extraordinary challenges. While a protein sequence and a passage of text can both be written down as characters, there are deep and fundamental differences between them. A protein sequence describes the chemical structure of a molecule, which folds into a complex three-dimensional shape according to the laws of physics.

Protein sequences contain statistical patterns that convey information about the folded structure of the protein. For example, if two positions in a protein coevolve with each other — in other words, if at one of the positions a certain amino acid appears, which is usually paired with a certain amino acid at the other position — this could be a signal that those two positions are interacting with each other in the folded structure. Similar to two pieces of a puzzle fitting together, evolution must choose amino acids that fit together in the folded structure. This means we can often infer something about the structure of a protein by looking at patterns in protein sequences.

Evolutionary scale modeling (ESM) uses AI to learn to read these patterns. In 2019, we presented evidence that language models learn the properties of proteins, such as their structure and function. Using a form of self-supervised learning known as masked language modeling, we trained a language model on the sequences of millions of natural proteins. With this approach, the model must correctly fill in the blanks in a passage of text, such as “To __ or not to __, that is the ________.” We trained a language model to fill in the blanks in a protein sequence, like “GL_KKE_AHY_G” across millions of diverse proteins. We found that information about the structure and function of proteins emerges from this training. In 2020, we released ESM1b, a state-of-the-art protein language model, which is being used for a variety of applications including to help scientists predict the evolution of COVID-19 and discover genetic causes of disease.

We have now scaled up this approach to create a next-generation protein language model, ESM-2, which at 15B parameters is the largest language model of proteins to date. We found that as the model is scaled up from 8M to 15B parameters, information emerges in the internal representations that enables 3D structure prediction at an atomic resolution.

Accelerating protein folding by an order of magnitude

Something Went Wrong We're having trouble playing this video. Learn more

High-resolution protein structure emerges as the model scales up. As the model scales, new details emerge in the atomic resolution image of the structure.

With current state-of-the-art computational tools, predicting structures for hundreds of millions of protein sequences in a practical time frame could take years, even using the resources of a major research institution. To make predictions at the scale of metagenomics a breakthrough in prediction speed is critical.

We found that using a language model of protein sequences greatly accelerates the speed of structure prediction (up to 60x). This is fast enough to make predictions for an entire metagenomics database in just weeks and will be scalable to databases much larger than the one we are releasing today. In fact this new structure prediction capability enabled us to predict sequences for the more than 600 million metagenomic proteins in the atlas in just two weeks on a cluster of approximately 2,000 GPUs.

Current state-of-the-art structure prediction methods need to search through large protein databases to identify related sequences. The approaches actually need a whole group of evolutionarily related sequences as input so that they can extract the patterns that are linked to structure. The language model learns these evolutionary patterns during its training on protein sequences, enabling a high resolution prediction of the three-dimensional structure directly from the sequence of the protein.

Protein folding with a language model. Arrows show the information flow in the network from the language model to the folding trunk to the structure module, which outputs 3D coordinates and confidences.

Where do we go from here?

Several billion years ago, evolution invented a language by which complex and dynamic molecular machines can be formed out of simple building blocks. This language is the basis of life. Learning to read the language of proteins is an important step in our understanding of the natural world.

ESMFold shows how AI can give us new tools to understand the natural world, much like the microscope, which enabled us to see into the world at an infinitesimal scale and opened up a whole new understanding of life. AI can help us understand the immense scope of natural diversity, and see biology in a new way. Much of AI research has focused on helping computers understand the world in a way similar to how humans do. The language of proteins is one that is beyond human comprehension and has eluded even the most powerful computational tools. AI has the potential to open up this language to our understanding. Studying AI in new domains such as biology can also give insight into artificial intelligence more broadly. Our work reveals connections across domains: large language models that are behind advances in machine translation, natural language understanding, speech recognition, and image generation are also able to learn deep information about biology.

Metagenomics provides a view into the millions of diverse molecular machines that nature has invented. To extend this work even further, we’re studying how language models can be used to design new proteins and contribute to solving challenges in health, disease, and the environment. This work extends across many disciplines, from AI to chemistry to biology, so it is important to work openly, share our data and learnings, and build upon others’ insights. We hope that the release of this large-scale structure atlas and fast protein folding models will fuel further scientific progress and better our understanding of the world around us.

Explore the ESM Metagenomic Atlas

Read the research paper

View code and models on GitHub"
Meta_Blog,https://ai.meta.com/blog/protein-folding-esmfold-metagenomics/,,ESM Metagenomic Atlas: The first view of the ‘dark matter’ of the protein universe,"Something Went Wrong We're having trouble playing this video. Learn more

Meta AI has created the first database that reveals the structures of the metagenomic world at the scale of hundreds of millions of proteins. These proteins – which are found in microbes in the soil, deep in the ocean, and even inside our bodies – vastly outnumber those that make up animal and plant life. But they are the least understood proteins on earth.

Decoding metagenomic structures can help us solve long-standing mysteries of evolutionary history and discover proteins that may help cure diseases, clean up the environment, and produce cleaner energy.

To make structure predictions at this scale, a breakthrough in the speed of protein folding is necessary. We trained a large language model to learn evolutionary patterns and generate accurate structure predictions end to end directly from the sequence of a protein. Predictions are up to 60x faster than the current state-of-the-art while maintaining accuracy, making our approach scalable to far larger databases.

We are now sharing our models, research paper, and a database of more than 600 million metagenomic structures, as well as an API that allows scientists to easily retrieve specific protein structures relevant to their work.

Explore the ESM Metagenomic Atlas here.

Proteins are complex and dynamic molecules, encoded by our genes, that are responsible for many of the varied and fundamental processes of life. They have an astounding range of roles in biology. The rods and cones in our eyes that sense light and make it possible for us to see, the molecular sensors that underlie hearing and our sense of touch, the complex molecular machines that convert sunlight into chemical energy in plants, the motors that drive motion in microbes and our muscles, enzymes that break down plastic, antibodies that protect us from disease, and molecular circuits that cause disease when they fail — are all proteins.

Metagenomics, one of the new frontiers in the natural sciences, uses gene sequencing to discover proteins in samples from environments across the earth, from microbes living in the soil, deep in the ocean, in extreme environments like hydrothermal vents, and even in our guts and on our skin. The natural world contains a vast number of proteins beyond the ones that have been cataloged and annotated in well-studied organisms. Metagenomics is starting to reveal the incredible breadth and diversity of these proteins, uncovering billions of protein sequences that are new to science and cataloged for the first time in large databases compiled by public initiatives such as the NCBI, European Bioinformatics Institute and Joint Genome Institute, incorporating studies from a worldwide community of researchers.

Meta AI has developed a new protein-folding approach that harnesses large language models to create the first comprehensive view of the structures of proteins in a metagenomics database at the scale of hundreds of millions of proteins. Our research team found that language models can accelerate the speed at which an atomic-level three-dimensional structure can be predicted up to 60x faster relative to existing state-of-the-art protein structure prediction approaches. This advance will help to accelerate a new era of structural understanding where it could be possible for the first time to understand the structure of billions of proteins that gene-sequencing technology is cataloging.

Today, we are releasing the 600+ million protein ESM Metagenomic Atlas, with predictions for nearly the entire MGnify90 database, a public resource cataloging metagenomic sequences. To our knowledge, this is the largest database of high resolution predicted structures, 3x larger than any existing protein structure database, and the first to cover metagenomic proteins comprehensively and at scale. These structures provide an unprecedented view into the breadth and diversity of nature, and hold the potential for new scientific insights and to accelerate discovery of proteins for practical applications in fields such as medicine, green chemistry, environmental applications, and renewable energy.

In addition, we are releasing the fast protein folding model used to create the database and an API that allows researchers to use it for scientific discovery. With 15 billion parameters, our new language model is the largest language model of proteins to date.

Unlocking a hidden natural world: the first comprehensive view of metagenomic structural space

Advancements in gene sequencing have made it possible to catalog billions of metagenomic protein sequences. Although we know that these proteins exist, because we have discovered their sequences, understanding their biology is a staggering challenge. Determining the three-dimensional structures for hundreds of millions of proteins experimentally is far beyond the reach of time-intensive laboratory techniques such as X-ray crystallography, which can take weeks to years for a single protein. Computational approaches can give us insight into metagenomics proteins that isn’t possible with experimental techniques.

The ESM Metagenomic Atlas will enable scientists to search and analyze the structures of metagenomic proteins at the scale of hundreds of millions of proteins. This can help researchers to identify structures that have not been characterized before, search for distant evolutionary relationships, and discover new proteins that can be useful in medicine and other applications.

A map of tens of thousands of high-confidence predictions showing similarity to proteins whose structure is currently known. The image shows large regions of completely unknown structural space revealed for the first time.

Learning to read the language of nature

The ESM-2 language model is trained to predict amino acids that have been masked out of sequences across evolution. We discovered that, as a result of this training, information about the protein’s structure emerges in the internal states of the model. This is surprising because the model has been trained only on sequences.

Like the text of an essay or letter, proteins can be written as sequences of characters. Each character corresponds to one of 20 standard chemical elements — amino acids, each with different properties — that are the building blocks of proteins. These building blocks can be combined in an astronomical number of different ways — e.g., for a protein made of 200 amino acids, there are 20^200 possible sequences — more than the number of atoms in the visible universe. Every sequence folds into a three-dimensional shape (though not all will fold into coherent structures; many sequences fold into disordered forms), and it is this shape that largely determines the biological function of the protein.

Learning to read this language of biology poses extraordinary challenges. While a protein sequence and a passage of text can both be written down as characters, there are deep and fundamental differences between them. A protein sequence describes the chemical structure of a molecule, which folds into a complex three-dimensional shape according to the laws of physics.

Protein sequences contain statistical patterns that convey information about the folded structure of the protein. For example, if two positions in a protein coevolve with each other — in other words, if at one of the positions a certain amino acid appears, which is usually paired with a certain amino acid at the other position — this could be a signal that those two positions are interacting with each other in the folded structure. Similar to two pieces of a puzzle fitting together, evolution must choose amino acids that fit together in the folded structure. This means we can often infer something about the structure of a protein by looking at patterns in protein sequences.

Evolutionary scale modeling (ESM) uses AI to learn to read these patterns. In 2019, we presented evidence that language models learn the properties of proteins, such as their structure and function. Using a form of self-supervised learning known as masked language modeling, we trained a language model on the sequences of millions of natural proteins. With this approach, the model must correctly fill in the blanks in a passage of text, such as “To __ or not to __, that is the ________.” We trained a language model to fill in the blanks in a protein sequence, like “GL_KKE_AHY_G” across millions of diverse proteins. We found that information about the structure and function of proteins emerges from this training. In 2020, we released ESM1b, a state-of-the-art protein language model, which is being used for a variety of applications including to help scientists predict the evolution of COVID-19 and discover genetic causes of disease.

We have now scaled up this approach to create a next-generation protein language model, ESM-2, which at 15B parameters is the largest language model of proteins to date. We found that as the model is scaled up from 8M to 15B parameters, information emerges in the internal representations that enables 3D structure prediction at an atomic resolution.

Accelerating protein folding by an order of magnitude

Something Went Wrong We're having trouble playing this video. Learn more

High-resolution protein structure emerges as the model scales up. As the model scales, new details emerge in the atomic resolution image of the structure.

With current state-of-the-art computational tools, predicting structures for hundreds of millions of protein sequences in a practical time frame could take years, even using the resources of a major research institution. To make predictions at the scale of metagenomics a breakthrough in prediction speed is critical.

We found that using a language model of protein sequences greatly accelerates the speed of structure prediction (up to 60x). This is fast enough to make predictions for an entire metagenomics database in just weeks and will be scalable to databases much larger than the one we are releasing today. In fact this new structure prediction capability enabled us to predict sequences for the more than 600 million metagenomic proteins in the atlas in just two weeks on a cluster of approximately 2,000 GPUs.

Current state-of-the-art structure prediction methods need to search through large protein databases to identify related sequences. The approaches actually need a whole group of evolutionarily related sequences as input so that they can extract the patterns that are linked to structure. The language model learns these evolutionary patterns during its training on protein sequences, enabling a high resolution prediction of the three-dimensional structure directly from the sequence of the protein.

Protein folding with a language model. Arrows show the information flow in the network from the language model to the folding trunk to the structure module, which outputs 3D coordinates and confidences.

Where do we go from here?

Several billion years ago, evolution invented a language by which complex and dynamic molecular machines can be formed out of simple building blocks. This language is the basis of life. Learning to read the language of proteins is an important step in our understanding of the natural world.

ESMFold shows how AI can give us new tools to understand the natural world, much like the microscope, which enabled us to see into the world at an infinitesimal scale and opened up a whole new understanding of life. AI can help us understand the immense scope of natural diversity, and see biology in a new way. Much of AI research has focused on helping computers understand the world in a way similar to how humans do. The language of proteins is one that is beyond human comprehension and has eluded even the most powerful computational tools. AI has the potential to open up this language to our understanding. Studying AI in new domains such as biology can also give insight into artificial intelligence more broadly. Our work reveals connections across domains: large language models that are behind advances in machine translation, natural language understanding, speech recognition, and image generation are also able to learn deep information about biology.

Metagenomics provides a view into the millions of diverse molecular machines that nature has invented. To extend this work even further, we’re studying how language models can be used to design new proteins and contribute to solving challenges in health, disease, and the environment. This work extends across many disciplines, from AI to chemistry to biology, so it is important to work openly, share our data and learnings, and build upon others’ insights. We hope that the release of this large-scale structure atlas and fast protein folding models will fuel further scientific progress and better our understanding of the world around us.

Explore the ESM Metagenomic Atlas

Read the research paper

View code and models on GitHub"
Meta_Blog,https://ai.meta.com/blog/ai-powered-audio-compression-technique/,,Using AI to compress audio files for quick and easy sharing,"Something Went Wrong We're having trouble playing this video. Learn more

Something Went Wrong We're having trouble playing this video. Learn more

Compression is an important part of the internet today, because it enables people to easily share high-quality photos, listen to audio messages, stream their favorite shows, and so much more. Even when using today’s state-of-the-art techniques, enjoying these rich multimedia experiences requires a speedy internet connection and plenty of storage space. For current and future experiences — like the metaverse — to deliver high-quality, uninterrupted experiences for everyone, compression techniques will need to overcome these limitations.

Today, we are detailing progress that our Fundamental AI Research (FAIR) team has made in the area of AI-powered hypercompression of audio. Imagine listening to a friend’s audio message in an area with low connectivity and not having it stall or glitch. Our research shows how we can use AI to help us achieve this. We built a three-part system and trained it end to end to compress audio data to the size we target. This data can then be decoded using a neural network. We achieve an approximate 10x compression rate compared with MP3 at 64 kbps, without a loss of quality. While such techniques have been explored before for speech, we are the first to make it work for 48 kHz sampled stereo audio (i.e., CD quality), which is the standard for music distribution. We are sharing additional details in a research paper, along with code and samples as part of our commitment to open science.

The new approach can compress and decompress audio in real time to state-of-the-art size reductions. More work needs to be done, but eventually it could lead to improvements such as supporting faster, better-quality calls under poor network conditions and delivering rich metaverse experiences without requiring major bandwidth improvements.

While our techniques do not yet cover video, this is the start of an ongoing initiative with the goal of advances that could improve experiences such as videoconferencing, streaming movies, and playing games with friends in VR.

Something Went Wrong We're having trouble playing this video. Learn more

Something Went Wrong We're having trouble playing this video. Learn more

Our AI-powered compression technique

Codecs , which act as encoders and decoders for streams of data, help power most of the audio compression people currently use online. Some examples of commonly used codecs include MP3, Opus, and EVS. Classic codecs like these decompose the signal between different frequencies and encode as efficiently as possible. Most classic codecs leverage human hearing knowledge (psychoacoustics) but have a finite or given set of handcrafted ways to efficiently code and decode the file. We are probably close to the limit of what handcrafting can give us, which is why it’s important to explore new techniques.

In order to push the boundaries of what’s possible, we need AI to help. We created Encodec, a neural network that is trained end to end to reconstruct the input signal. It consists of three parts:

The encoder, which takes the uncompressed data and transforms it into a higher dimensional and lower frame rate representation.

The quantizer, which compresses this representation to the size we target. We train the quantizer to give us the size (or set of sizes) that we want while retaining the most important information to rebuild the original signal. This compressed representation is what we store on disk or send through the network. This is the equivalent of the .mp3 file on your computer.

The decoder is the final step. It turns the compressed signal back into a waveform that is as similar as possible to the original. The key to lossy compression is to identify changes that will not be perceivable by humans, as perfect reconstruction is impossible at low bit rates. To do so, we use discriminators to improve the perceptual quality of the generated samples.This creates a cat-and-mouse game where the discriminator’s job is to differentiate between real samples and reconstructed samples. The compression model attempts to generate samples to fool the discriminators by pushing the reconstructed samples to be more perceptually similar to the original samples.

We achieve state-of-the art results in low bit rate speech audio compression (1.5 kbps to 12 kbps) as evaluated by human annotators who compared several compression methods, including Google's latest codec Lyra-v2, with the uncompressed one and ranked them accordingly. Across all bandwidth and quality levels, our model encodes and decodes audio in real time on a single CPU core. We see many areas where we can continue to build and improve on this research in the future. We believe we can attain even smaller file sizes, as we haven’t yet reached the limits of quantization techniques. On the applied research side, there is more work that can be done on the trade-off between computing power and the size of compressed audio. Dedicated chips, such as those that are already on phones and laptops, could be improved in the future to help compress and decompress files, while consuming less power.

Building on this research

We invest in and share fundamental AI research like this so the broader community can learn from and build on these advancements. This research could lead to richer, faster online experiences for people around the world, regardless of the speed of their internet connection.

There is additional research that will need to be done to help us get there. We want to continue exploring how we can compress audio to even smaller file sizes without significantly degrading the quality. We also plan to explore spatial audio compression, which will require a technique that can compress several audio channels while keeping accurate spatial information. These learnings could be useful for future metaverse experiences. We are also exploring techniques for using AI to compress audio and video, and we hope to share more about that work in the future.

Read our paper to learn more about AI-powered hypercompression for audio, and then download the code."
Meta_Blog,https://ai.meta.com/blog/ai-powered-audio-compression-technique/,,Using AI to compress audio files for quick and easy sharing,"Something Went Wrong We're having trouble playing this video. Learn more

Something Went Wrong We're having trouble playing this video. Learn more

Compression is an important part of the internet today, because it enables people to easily share high-quality photos, listen to audio messages, stream their favorite shows, and so much more. Even when using today’s state-of-the-art techniques, enjoying these rich multimedia experiences requires a speedy internet connection and plenty of storage space. For current and future experiences — like the metaverse — to deliver high-quality, uninterrupted experiences for everyone, compression techniques will need to overcome these limitations.

Today, we are detailing progress that our Fundamental AI Research (FAIR) team has made in the area of AI-powered hypercompression of audio. Imagine listening to a friend’s audio message in an area with low connectivity and not having it stall or glitch. Our research shows how we can use AI to help us achieve this. We built a three-part system and trained it end to end to compress audio data to the size we target. This data can then be decoded using a neural network. We achieve an approximate 10x compression rate compared with MP3 at 64 kbps, without a loss of quality. While such techniques have been explored before for speech, we are the first to make it work for 48 kHz sampled stereo audio (i.e., CD quality), which is the standard for music distribution. We are sharing additional details in a research paper, along with code and samples as part of our commitment to open science.

The new approach can compress and decompress audio in real time to state-of-the-art size reductions. More work needs to be done, but eventually it could lead to improvements such as supporting faster, better-quality calls under poor network conditions and delivering rich metaverse experiences without requiring major bandwidth improvements.

While our techniques do not yet cover video, this is the start of an ongoing initiative with the goal of advances that could improve experiences such as videoconferencing, streaming movies, and playing games with friends in VR.

Something Went Wrong We're having trouble playing this video. Learn more

Something Went Wrong We're having trouble playing this video. Learn more

Our AI-powered compression technique

Codecs , which act as encoders and decoders for streams of data, help power most of the audio compression people currently use online. Some examples of commonly used codecs include MP3, Opus, and EVS. Classic codecs like these decompose the signal between different frequencies and encode as efficiently as possible. Most classic codecs leverage human hearing knowledge (psychoacoustics) but have a finite or given set of handcrafted ways to efficiently code and decode the file. We are probably close to the limit of what handcrafting can give us, which is why it’s important to explore new techniques.

In order to push the boundaries of what’s possible, we need AI to help. We created Encodec, a neural network that is trained end to end to reconstruct the input signal. It consists of three parts:

The encoder, which takes the uncompressed data and transforms it into a higher dimensional and lower frame rate representation.

The quantizer, which compresses this representation to the size we target. We train the quantizer to give us the size (or set of sizes) that we want while retaining the most important information to rebuild the original signal. This compressed representation is what we store on disk or send through the network. This is the equivalent of the .mp3 file on your computer.

The decoder is the final step. It turns the compressed signal back into a waveform that is as similar as possible to the original. The key to lossy compression is to identify changes that will not be perceivable by humans, as perfect reconstruction is impossible at low bit rates. To do so, we use discriminators to improve the perceptual quality of the generated samples.This creates a cat-and-mouse game where the discriminator’s job is to differentiate between real samples and reconstructed samples. The compression model attempts to generate samples to fool the discriminators by pushing the reconstructed samples to be more perceptually similar to the original samples.

We achieve state-of-the art results in low bit rate speech audio compression (1.5 kbps to 12 kbps) as evaluated by human annotators who compared several compression methods, including Google's latest codec Lyra-v2, with the uncompressed one and ranked them accordingly. Across all bandwidth and quality levels, our model encodes and decodes audio in real time on a single CPU core. We see many areas where we can continue to build and improve on this research in the future. We believe we can attain even smaller file sizes, as we haven’t yet reached the limits of quantization techniques. On the applied research side, there is more work that can be done on the trade-off between computing power and the size of compressed audio. Dedicated chips, such as those that are already on phones and laptops, could be improved in the future to help compress and decompress files, while consuming less power.

Building on this research

We invest in and share fundamental AI research like this so the broader community can learn from and build on these advancements. This research could lead to richer, faster online experiences for people around the world, regardless of the speed of their internet connection.

There is additional research that will need to be done to help us get there. We want to continue exploring how we can compress audio to even smaller file sizes without significantly degrading the quality. We also plan to explore spatial audio compression, which will require a technique that can compress several audio channels while keeping accurate spatial information. These learnings could be useful for future metaverse experiences. We are also exploring techniques for using AI to compress audio and video, and we hope to share more about that work in the future.

Read our paper to learn more about AI-powered hypercompression for audio, and then download the code."
Meta_Blog,https://ai.meta.com/blog/ai-translation-hokkien/,,A new AI-powered speech translation system for Hokkien pioneers a new approach for a primarily oral language,"Until now, AI translation has mainly focused on written languages. Yet nearly half of the world’s 7,000+ living languages are primarily oral and do not have a standard or widely used writing system. This makes it impossible to build machine translation tools using standard techniques, which require large amounts of written text in order to train an AI model. To address this challenge, we've built the first AI-powered translation system for a primarily oral language, Hokkien. Hokkien is widely spoken within the Chinese diaspora but lacks a standard written form. Our technology allows Hokkien speakers to hold conversations with English speakers.

The open sourced translation system is part of Meta’s Universal Speech Translator (UST) project, which is developing new AI methods that we hope will eventually allow real-time speech-to-speech translation across all extant languages, even primarily spoken ones. We believe spoken communication can help break down barriers and bring people together wherever they are located — even in the metaverse.

To develop this new speech-only translation system, Meta’s AI researchers had to overcome many challenges from traditional machine translation systems, including data gathering, model design, and evaluation. We have much work ahead to extend UST to more languages. But the ability to speak effortlessly to people in any language is a long-sought dream, and we’re pleased to be one step closer to achieving it. We’re open-sourcing not just our Hokkien translation models but also the evaluation datasets and research papers, so that others can reproduce and build on our work.

Something Went Wrong We're having trouble playing this video. Learn more

Overcoming training data challenges

Collecting sufficient data was a significant obstacle we faced when setting out to build a Hokkien translation system. Hokkien is what’s known as a low-resource language, which means there isn’t an ample supply of training data readily available for the language, compared with, say, Spanish or English. In addition, there are relatively few human English-to-Hokkien translators, making it difficult to collect and annotate data to train the model.

We leveraged Mandarin as an intermediate language to build pseudolabel as well as human translations, where we first translated English (or Hokkien) speech to Mandarin text, and we then translated to Hokkien (or English) and added it to training data. This method greatly improved the model performance by leveraging data from a similar high-resource language.

Speech mining is another approach to training data generation. With a pretrained speech encoder, we were able to encode Hokkien speech embeddings into the same semantic space as other languages without requiring Hokkien to have a written form. Hokkien speech can be aligned with English speech and texts whose semantic embeddings are similar. We then synthesized English speech from texts, yielding parallel Hokkien and English speech.

A new modeling approach

Many speech translation systems rely on transcriptions or are speech-to-text systems. However, since primarily oral languages do not have standard written forms, producing transcribed text as the translation output doesn’t work. Thus, we focused on speech-to-speech translation.

We used speech-to-unit translation (S2UT) to translate input speech to a sequence of acoustic units directly in the path previously pioneered by Meta. Then, we generated waveforms from the units. In addition, UnitY was adopted for a two-pass decoding mechanism, where the first-pass decoder generates text in a related language (Mandarin) and the second-pass decoder creates units.

Evaluating accuracy

Speech translation systems are usually evaluated using a metric called ASR-BLEU, which involves first transcribing the translated speech into text using automatic speech recognition (ASR), and then computing BLEU scores (a standard machine translation metric) by comparing the transcribed text with a human-translated text. However, one of the challenges of evaluating speech translations for an oral language such as Hokkien is that there is no standard writing system. In order to enable automatic evaluation, we developed a system that transcribes Hokkien speech into a standardized phonetic notation called Tâi-lô. This technique enabled us to compute a BLEU score at the syllable level and easily compare the translation quality of different approaches.

In addition to developing a method for evaluating Hokkien-English speech translations, we also created the first Hokkien-English bidirectional speech-to-speech translation benchmark dataset based on a Hokkien speech corpus called Taiwanese Across Taiwan. This benchmark dataset will be open-sourced to encourage other researchers to work on Hokkien speech translation and together make further progress in the field.

Looking to the future of translation

In its current phase, our approach allows someone who speaks Hokkien to converse with someone who speaks English. While the model is still a work in progress and can translate only one full sentence at a time, it’s a step toward a future where simultaneous translation between languages is possible.

The techniques we pioneered with Hokkien can be extended to many other written and unwritten languages. To that end, we are releasing SpeechMatrix, a large corpus of speech-to-speech translations mined with Meta’s innovative data mining technique, called LASER, which will enable researchers to create their own speech-to-speech translation (S2ST) systems and build on our work.

Meta’s recent advances in unsupervised speech recognition (wav2vec-U) and unsupervised machine translation (mBART) will inform future work in translating more spoken languages. Our progress in unsupervised learning demonstrates the feasibility of building high-quality speech-to-speech translation models without any human annotations. The system significantly lowers the requirements for expanding the coverage of low-resource languages, as many of them do not have labeled data at all.

AI research is helping break down language barriers in both the real world and the metaverse, with the goal of encouraging connection and mutual understanding. We’re excited to expand our research and bring this technology to more people in the future.

Hokkien direct speech-to-speech translation

SpeechMatrix

Unsupervised direct speech-to-speech translation

UnitY direct speech-to-speech translation

This work is being undertaken by a multidisciplinary team that includes Al Youngblood, Amanda Kallet, Ana Paula Kirschner Mofarrej, Andy Chung, Angela Fan, Ann Lee, Benjamin Peloquin, Benoît Sagot, Brian Bui, Brian O’Horo, Carleigh Wood, Changhan Wang, Chloe Meyere, Chris Summers, Christopher Johnson, David Wu, Diana Otero, Eric Kaplan, Ethan Ye, Gopika Jhala, Gustavo Gandia Rivera, Hirofumi Inaguma, Holger Schwenk, Hongyu Gong, Ilia Kulikov, Iska Saric, Janice Lam, Jeff Wang, Jingfei Du, Juan Pino, Julia Vargas, Justine Kao, Karla Caraballo-Torres, Kevin Tran, Koklioong Loa, Lachlan Mackenzie, Michael Auli, Michael Friedrichs, Natalie Hereth, Ning Dong, Oliver Libaw, Orialis Valentin, Paden Tomasello, Paul-Ambroise Duquenne, Peng-Jen Chen, Pengwei Li, Robert Lee, Safiyyah Saleem, Sascha Brodsky, Semarley Jarrett, Sravya Popuri, TJ Krusinski, Vedanuj Goswami, Wei-Ning Hsu, Xutai Ma, Yilin Yang, and Yun Tang."
Meta_Blog,https://ai.meta.com/blog/ai-translation-hokkien/,,A new AI-powered speech translation system for Hokkien pioneers a new approach for a primarily oral language,"Until now, AI translation has mainly focused on written languages. Yet nearly half of the world’s 7,000+ living languages are primarily oral and do not have a standard or widely used writing system. This makes it impossible to build machine translation tools using standard techniques, which require large amounts of written text in order to train an AI model. To address this challenge, we've built the first AI-powered translation system for a primarily oral language, Hokkien. Hokkien is widely spoken within the Chinese diaspora but lacks a standard written form. Our technology allows Hokkien speakers to hold conversations with English speakers.

The open sourced translation system is part of Meta’s Universal Speech Translator (UST) project, which is developing new AI methods that we hope will eventually allow real-time speech-to-speech translation across all extant languages, even primarily spoken ones. We believe spoken communication can help break down barriers and bring people together wherever they are located — even in the metaverse.

To develop this new speech-only translation system, Meta’s AI researchers had to overcome many challenges from traditional machine translation systems, including data gathering, model design, and evaluation. We have much work ahead to extend UST to more languages. But the ability to speak effortlessly to people in any language is a long-sought dream, and we’re pleased to be one step closer to achieving it. We’re open-sourcing not just our Hokkien translation models but also the evaluation datasets and research papers, so that others can reproduce and build on our work.

Something Went Wrong We're having trouble playing this video. Learn more

Overcoming training data challenges

Collecting sufficient data was a significant obstacle we faced when setting out to build a Hokkien translation system. Hokkien is what’s known as a low-resource language, which means there isn’t an ample supply of training data readily available for the language, compared with, say, Spanish or English. In addition, there are relatively few human English-to-Hokkien translators, making it difficult to collect and annotate data to train the model.

We leveraged Mandarin as an intermediate language to build pseudolabel as well as human translations, where we first translated English (or Hokkien) speech to Mandarin text, and we then translated to Hokkien (or English) and added it to training data. This method greatly improved the model performance by leveraging data from a similar high-resource language.

Speech mining is another approach to training data generation. With a pretrained speech encoder, we were able to encode Hokkien speech embeddings into the same semantic space as other languages without requiring Hokkien to have a written form. Hokkien speech can be aligned with English speech and texts whose semantic embeddings are similar. We then synthesized English speech from texts, yielding parallel Hokkien and English speech.

A new modeling approach

Many speech translation systems rely on transcriptions or are speech-to-text systems. However, since primarily oral languages do not have standard written forms, producing transcribed text as the translation output doesn’t work. Thus, we focused on speech-to-speech translation.

We used speech-to-unit translation (S2UT) to translate input speech to a sequence of acoustic units directly in the path previously pioneered by Meta. Then, we generated waveforms from the units. In addition, UnitY was adopted for a two-pass decoding mechanism, where the first-pass decoder generates text in a related language (Mandarin) and the second-pass decoder creates units.

Evaluating accuracy

Speech translation systems are usually evaluated using a metric called ASR-BLEU, which involves first transcribing the translated speech into text using automatic speech recognition (ASR), and then computing BLEU scores (a standard machine translation metric) by comparing the transcribed text with a human-translated text. However, one of the challenges of evaluating speech translations for an oral language such as Hokkien is that there is no standard writing system. In order to enable automatic evaluation, we developed a system that transcribes Hokkien speech into a standardized phonetic notation called Tâi-lô. This technique enabled us to compute a BLEU score at the syllable level and easily compare the translation quality of different approaches.

In addition to developing a method for evaluating Hokkien-English speech translations, we also created the first Hokkien-English bidirectional speech-to-speech translation benchmark dataset based on a Hokkien speech corpus called Taiwanese Across Taiwan. This benchmark dataset will be open-sourced to encourage other researchers to work on Hokkien speech translation and together make further progress in the field.

Looking to the future of translation

In its current phase, our approach allows someone who speaks Hokkien to converse with someone who speaks English. While the model is still a work in progress and can translate only one full sentence at a time, it’s a step toward a future where simultaneous translation between languages is possible.

The techniques we pioneered with Hokkien can be extended to many other written and unwritten languages. To that end, we are releasing SpeechMatrix, a large corpus of speech-to-speech translations mined with Meta’s innovative data mining technique, called LASER, which will enable researchers to create their own speech-to-speech translation (S2ST) systems and build on our work.

Meta’s recent advances in unsupervised speech recognition (wav2vec-U) and unsupervised machine translation (mBART) will inform future work in translating more spoken languages. Our progress in unsupervised learning demonstrates the feasibility of building high-quality speech-to-speech translation models without any human annotations. The system significantly lowers the requirements for expanding the coverage of low-resource languages, as many of them do not have labeled data at all.

AI research is helping break down language barriers in both the real world and the metaverse, with the goal of encouraging connection and mutual understanding. We’re excited to expand our research and bring this technology to more people in the future.

Hokkien direct speech-to-speech translation

SpeechMatrix

Unsupervised direct speech-to-speech translation

UnitY direct speech-to-speech translation

This work is being undertaken by a multidisciplinary team that includes Al Youngblood, Amanda Kallet, Ana Paula Kirschner Mofarrej, Andy Chung, Angela Fan, Ann Lee, Benjamin Peloquin, Benoît Sagot, Brian Bui, Brian O’Horo, Carleigh Wood, Changhan Wang, Chloe Meyere, Chris Summers, Christopher Johnson, David Wu, Diana Otero, Eric Kaplan, Ethan Ye, Gopika Jhala, Gustavo Gandia Rivera, Hirofumi Inaguma, Holger Schwenk, Hongyu Gong, Ilia Kulikov, Iska Saric, Janice Lam, Jeff Wang, Jingfei Du, Juan Pino, Julia Vargas, Justine Kao, Karla Caraballo-Torres, Kevin Tran, Koklioong Loa, Lachlan Mackenzie, Michael Auli, Michael Friedrichs, Natalie Hereth, Ning Dong, Oliver Libaw, Orialis Valentin, Paden Tomasello, Paul-Ambroise Duquenne, Peng-Jen Chen, Pengwei Li, Robert Lee, Safiyyah Saleem, Sascha Brodsky, Semarley Jarrett, Sravya Popuri, TJ Krusinski, Vedanuj Goswami, Wei-Ning Hsu, Xutai Ma, Yilin Yang, and Yun Tang."
Meta_Blog,https://ai.meta.com/blog/meta-ai-residency-program-mentors/,,2022 Meta AI Program Mentors,"Meta AI created the Artificial Intelligence (AI) Residency Program to give talented professionals hands-on experience with artificial intelligence research while working at a leading technology company. This one-year research training position is a great opportunity for those interested in honing their skills in areas such as image and video generation and AI-powered translation before applying to PhD programs in AI.

We asked current AI Residents to share their perspective on the program here. In this blog post, we introduce the Meta AI researchers serving as Mentors in the program.

Emily Dinan is a Research Engineer with Meta's FAIR lab in New York.

Emily Dinan is a Research Engineer in FAIR’s New York office and works on NLP research. “With our Resident, we are working on improving control over dialogue models such that they do not respond with toxic or offensive content. Meta is a great place to work on this type of research, as we get the opportunity to collaborate with experts in various fields across all of AI,” she says.

Emily loves working with the AI Residents — it’s really a unique opportunity to develop the skills of the next generation of AI talent. As a Mentor, Emily has learned a lot. “The Residents I’ve worked with have helped bring fresh perspectives to our research projects, and teaching AI skills has been a great way to challenge my own assumptions,” she says. When asked what advice she would give to future Residents, Emily suggests that future Residents make the most of their time by building connections with and learning from as many people as possible! There are so many opportunities to contribute to interesting projects and grow the skills that are most important to each person.

Akshara Rai is a Research Scientist on the Embodied AI team at FAIR’s Menlo Park office and works on AI for robotics. “My research is focused around building smart robots that can help us in day-to-day tasks. It is an extremely challenging problem, involving perception, language, control, and interaction, but also extremely rewarding to watch a robot understand and interact with the environment, and do something useful, like collect soft toys and put them in bags,” she says.

Akshara Rai's work focuses on the intersection of machine learning and control.

“Working at Meta and FAIR is a dream come true. Right now, we are still scratching the surface of intelligence in embodied agents, and Meta AI is a great place to work on such a far-looking project,” Akshara says.

“Both of my previous AI Residents started with backgrounds in AI and little knowledge of robotics. It has been highly rewarding to see them grow, enjoy, and understand the challenges of working with real robots,” she says. “Working with both has also given me fresh perspectives on learning for robotics, as well as AI research in general. My Residents are excited by the latest and greatest approaches, which results in a very interesting but grounded collaboration.”

Akshara advises Residents to figure out early on (with their Mentor) what they want out of the Residency, alongside their Mentor, and make a one-year plan based on that. It might help to make a one-year plan with intermediate goals and milestones to make sure that a project stays on track.

Ari Morcos is a Research Scientist with Meta's FAIR lab in Menlo Park, California.

Ari Morcos is a Research Scientist in FAIR’s Menlo Park office and works on understanding and evaluating deep networks. “Together with our Resident, we worked on two projects this year, one focused on whether we can train models to the same performance with less data and another focused on understanding the impact of different self-supervised learning approaches on the learned representations. Working on these projects at Meta is wonderful, because we are surrounded by world-renowned researchers in all these areas and have the freedom to pursue the ideas we think are most important,” says Ari.

Ari’s commitment to the program is palpable. “I believe that mentoring budding researchers who are new to the field is just as important as conducting good research. It’s been so rewarding to watch Residents grow their research skills and their confidence over the course of the program, and is an experience I look forward to each year. Working with Residents has taught me so much — how to be a better Mentor and tailor my mentorship style to each person’s unique needs, and how best to help Residents feel comfortable and confident in a fast-paced research environment. Since each Resident comes from a different background, Residents have encouraged me to look at projects from new and diverse angles.”

Adina Williams is a Research Scientist in FAIR Labs New York working on evaluating the performance of large language models. “There are many outstanding challenges for large language models, but one of the most pressing is the challenge of representing people from all groups fairly and safely. We have recently focused on better understanding the datasets and metrics used to measure social biases in large models, and together with our Residents, we have isolated a lot of noise and instability in these metrics and datasets,” she says.

“Understanding issues with current measurement techniques is the first step toward devising better ones, which can ultimately tell us how to improve our models in the future,” says Adina. “When working on projects with social import, it is important to have a large set of committed researchers with diverse backgrounds and viewpoints, as we are fostering at Meta, so we can better explore avenues for improvement quickly and inclusively.” Adina also finds mentoring really rewarding. “I’ve had to go back and re-earn (or learn for the first time) many technical details so that I could help my mentee. It’s been fun to (re)learn alongside a Resident!”

Adina Williams is a Research Scientist in FAIR Labs New York.

Adina’s advice to future Residents? “As a Resident at Meta AI, you are expected to be the main driver of your project, and it is up to the Resident to figure out what you need in order to have a successful project. For example, if you work better in groups, feel free to tell your Mentors—maybe they can find you a coding buddy! If you need a particular meeting cadence or more linear algebra review, let them know! Take some time to introspect on skills or experiences, and ask for them directly.”

Click here to learn more about the AI Residency and here to apply for the 2023 cohort."
Meta_Blog,https://ai.meta.com/blog/meta-ai-residency-program-mentors/,,2022 Meta AI Program Mentors,"Meta AI created the Artificial Intelligence (AI) Residency Program to give talented professionals hands-on experience with artificial intelligence research while working at a leading technology company. This one-year research training position is a great opportunity for those interested in honing their skills in areas such as image and video generation and AI-powered translation before applying to PhD programs in AI.

We asked current AI Residents to share their perspective on the program here. In this blog post, we introduce the Meta AI researchers serving as Mentors in the program.

Emily Dinan is a Research Engineer with Meta's FAIR lab in New York.

Emily Dinan is a Research Engineer in FAIR’s New York office and works on NLP research. “With our Resident, we are working on improving control over dialogue models such that they do not respond with toxic or offensive content. Meta is a great place to work on this type of research, as we get the opportunity to collaborate with experts in various fields across all of AI,” she says.

Emily loves working with the AI Residents — it’s really a unique opportunity to develop the skills of the next generation of AI talent. As a Mentor, Emily has learned a lot. “The Residents I’ve worked with have helped bring fresh perspectives to our research projects, and teaching AI skills has been a great way to challenge my own assumptions,” she says. When asked what advice she would give to future Residents, Emily suggests that future Residents make the most of their time by building connections with and learning from as many people as possible! There are so many opportunities to contribute to interesting projects and grow the skills that are most important to each person.

Akshara Rai is a Research Scientist on the Embodied AI team at FAIR’s Menlo Park office and works on AI for robotics. “My research is focused around building smart robots that can help us in day-to-day tasks. It is an extremely challenging problem, involving perception, language, control, and interaction, but also extremely rewarding to watch a robot understand and interact with the environment, and do something useful, like collect soft toys and put them in bags,” she says.

Akshara Rai's work focuses on the intersection of machine learning and control.

“Working at Meta and FAIR is a dream come true. Right now, we are still scratching the surface of intelligence in embodied agents, and Meta AI is a great place to work on such a far-looking project,” Akshara says.

“Both of my previous AI Residents started with backgrounds in AI and little knowledge of robotics. It has been highly rewarding to see them grow, enjoy, and understand the challenges of working with real robots,” she says. “Working with both has also given me fresh perspectives on learning for robotics, as well as AI research in general. My Residents are excited by the latest and greatest approaches, which results in a very interesting but grounded collaboration.”

Akshara advises Residents to figure out early on (with their Mentor) what they want out of the Residency, alongside their Mentor, and make a one-year plan based on that. It might help to make a one-year plan with intermediate goals and milestones to make sure that a project stays on track.

Ari Morcos is a Research Scientist with Meta's FAIR lab in Menlo Park, California.

Ari Morcos is a Research Scientist in FAIR’s Menlo Park office and works on understanding and evaluating deep networks. “Together with our Resident, we worked on two projects this year, one focused on whether we can train models to the same performance with less data and another focused on understanding the impact of different self-supervised learning approaches on the learned representations. Working on these projects at Meta is wonderful, because we are surrounded by world-renowned researchers in all these areas and have the freedom to pursue the ideas we think are most important,” says Ari.

Ari’s commitment to the program is palpable. “I believe that mentoring budding researchers who are new to the field is just as important as conducting good research. It’s been so rewarding to watch Residents grow their research skills and their confidence over the course of the program, and is an experience I look forward to each year. Working with Residents has taught me so much — how to be a better Mentor and tailor my mentorship style to each person’s unique needs, and how best to help Residents feel comfortable and confident in a fast-paced research environment. Since each Resident comes from a different background, Residents have encouraged me to look at projects from new and diverse angles.”

Adina Williams is a Research Scientist in FAIR Labs New York working on evaluating the performance of large language models. “There are many outstanding challenges for large language models, but one of the most pressing is the challenge of representing people from all groups fairly and safely. We have recently focused on better understanding the datasets and metrics used to measure social biases in large models, and together with our Residents, we have isolated a lot of noise and instability in these metrics and datasets,” she says.

“Understanding issues with current measurement techniques is the first step toward devising better ones, which can ultimately tell us how to improve our models in the future,” says Adina. “When working on projects with social import, it is important to have a large set of committed researchers with diverse backgrounds and viewpoints, as we are fostering at Meta, so we can better explore avenues for improvement quickly and inclusively.” Adina also finds mentoring really rewarding. “I’ve had to go back and re-earn (or learn for the first time) many technical details so that I could help my mentee. It’s been fun to (re)learn alongside a Resident!”

Adina Williams is a Research Scientist in FAIR Labs New York.

Adina’s advice to future Residents? “As a Resident at Meta AI, you are expected to be the main driver of your project, and it is up to the Resident to figure out what you need in order to have a successful project. For example, if you work better in groups, feel free to tell your Mentors—maybe they can find you a coding buddy! If you need a particular meeting cadence or more linear algebra review, let them know! Take some time to introspect on skills or experiences, and ask for them directly.”

Click here to learn more about the AI Residency and here to apply for the 2023 cohort."
Meta_Blog,https://ai.meta.com/blog/meta-ai-residency-profiles/,,"Q&A with 2022 Meta AI Residency: Research topics, future plans, and more","The Artificial Intelligence (AI) Residency Program is a one-year research training position designed to give talented professionals hands-on experience in AI research while working in Meta AI. This program is ideal for those interested in applying to PhD programs, publishing research papers, and maximizing their experience in AI research as they advance their careers. The AI Residency provides the perfect opportunity for participants, typically recent graduates with bachelor’s or master’s degrees, to experience the day-to-day work of a researcher. Residents can work with Meta AI researchers in areas such as AI-powered translation, diffusion models for video generation, decoding speech from brain activity, and much more.

In this program, participants will be paired with AI researchers to help guide their projects and research problems. The research will be communicated to the academic community through collaboration across Meta AI, academic papers and conferences (e.g., NeurIPS, ICML, ICLR, CVPR), open source code releases, or applications at Meta.

We encourage applications from people with a technical background and who hope to apply to a graduate program but want more preparation before doing so. Prior experience in machine learning is certainly a strength, but we seek people who have a passion for AI and come from a diversity of backgrounds, including math, physics, finance, economics, linguistics, computational social science, neuroscience, and bioinformatics. This is a full-time program that cannot be undertaken in conjunction with university study or a full-time job.

We asked some of the members of this year’s cohort of AI Residents to describe their experiences in the program. (You can also read brief interviews with several Meta AI Residency Program Mentors.)

Abdalgader Abubaker, London AI Resident 2021-2022 “I am working on integrity challenges. The exciting part of the research is its potential impact on Meta social networks to enhance the existing models that are used to prevent integrity violations on the different Meta platforms.” Alexander Gurung, New York AI Resident 2021-2022 “The program taught me how to drive my own research, and how to come up with new research questions and areas for exploration based on my own intuition. It has also taught me how to tackle larger-scale problems, from the ideation stage all the way through to paper writing and presentation.” Badr Alkhamissi, Seattle AI Resident 2021-2022 “I am building new language model architectures to enable AI to perform different reasoning skills. Since reasoning is one of the pillars for language models to be deployed in the real world, this research addresses an essential problem for using them in a reliable and responsible fashion.” Christina Du, London AI Resident 2021-2022 “The program helped me become a stronger researcher. I learned how to ask good research questions and how to design experiments to test our hypotheses. I also got more hands-on experience in writing academic papers and presenting my work to other people.” Isabelle Hu, Menlo Park AI Resident 2021-2022 “The program gave me the opportunity to work on exciting research directions. As part of FAIR’s multimodal generation research project, I’ve worked on data curation modeling experimentation for text-to-video generation and image-to-image translation. This research area will enhance how people express their creativity in a very fun and meaningful way.” Ishita Mediratta, London AI Resident 2021-2022 “The program has been a great learning experience for me. I learned how to formulate ideas, present my work to others, and write scalable code. I will apply to PhD programs this fall and hope to continue my collaboration with my Meta Mentor throughout my PhD.” Jude Fernandes, New York AI Resident 2021-2022 “My biggest struggle with research before the AI Residency was feeling lost with open-ended questions. I’ve been lucky to have Mentors and collaborators who have given me tools that help guide me in these situations. The program has also given me the chance to fully immerse myself in topics I care about, which I have found is the best way to develop intuition and insight.” Karmesh Yadav, Menlo Park AI Resident 2021–2022 “I am currently working on learning visual representations for embodied navigation and rearrangement tasks using self-supervised learning. The AI Residency helped me hone my technical and interpersonal skills by learning how to orchestrate large-scale experimentation and distributed training, how to collaborate in large teams while still leading and taking ownership of a project, and how to manage contributions over multiple projects.” Keren Fuentes, New York AI Resident 2021–2022 “I am currently working on quantifying the limitations of widely used fairness metrics that aim to measure bias in masked language models. I find this area of research exciting because it can have a positive impact on people in underrepresented groups.” Millicent Li, Seattle AI Resident 2021–2022 “My research has encapsulated a wide variety of topics in the natural language processing (NLP) space. During my time here, I’ve been able to meet so many researchers of different backgrounds, take part in a number of projects, and ask many questions.” Nima Shoghi, Menlo Park AI Resident 2021–2022 “I am on the Open Catalyst Project, which is a collaborative research effort between FAIR and Carnegie Mellon University’s Department of Chemical Engineering. My research involves the development of new machine learning models for molecular property prediction tasks. As someone who does not have a background in chemical engineering, I found things quite challenging at first. However, having the opportunity to conduct research in a challenging new domain alongside a diverse set of highly qualified researchers has proved to be extremely exciting and rewarding.” Suvir Mirchandani, Menlo Park AI Resident 2021–2022 “My research has focused on vision-and-language pretraining and self-supervised learning, specifically interactive image retrieval: Given an image and a piece of text feedback from a user, retrieve an image that incorporates the user’s feedback. It has been inspiring to be surrounded by so many talented researchers at Meta AI. After the AI Residency, I will be starting my PhD at Stanford University."" Vidhi Jain, Menlo Park AI Resident 2021–2022 “I am focused on building learnable policies for complex, long-horizon tasks in unknown indoor environments. Solving a real-world problem is hard and messy, yet so important, and building solutions geared for it is what makes me excited about this research.” Zeyu Liu, Menlo Park AI Resident 2021–2022 “The AI Residency program gave me a chance to work with great researchers and receive different perspectives. Additionally, being at Meta AI and FAIR has provided me with the resource to run experiments that are of strong interest to me. After the AI Residency program, I will complete my master’s program and then hope to apply to PhD programs.”

Tackling high-impact research challenges

Abdalgader Abubaker is working with Meta AI researchers in London on integrity challenges. His AI Residency is focused on developing a new approach for graph neural networks to leverage the hypergraph structure — a generalization of graphs with high-level representation in which the edges can connect more than two nodes. “The exciting part of the research is its potential impact on Meta social networks to enhance the existing models that are used to prevent integrity violations on the Meta different platforms,” Abdalgader says. He plans to apply to PhD programs after completing the Residency program.

Badr Alkhamissi is based in Meta’s Seattle office, where he works on responsible AI research. He is building new language model architectures to enable AI to perform different reasoning skills, such as chaining, composition, coreference, arithmetic, and temporal resolution. “Since reasoning is one of the pillars for language models to be deployed in the real world, this research addresses an essential problem for using them in a reliable and responsible fashion.” Badr plans to apply to PhD programs after his AI Residency.

Jude Fernandes is with Meta AI NLP researchers in FAIR’s New York office. Jude has worked with his Mentors to improve the way dialogue models respond to toxic content using controllable generations and has worked on improving fairness in language models by training them on demographically perturbed corpora. “As AI becomes ubiquitous in our world, I feel an urgency to make sure that these systems won’t cause harm to people, and I feel excited about the potential of building a future that isn’t defined by the power structures that exist in society today,” Jude says.

Keren Fuentes also works at FAIR’s New York office, on the Responsible AI team. Keren’s interests include understanding and reducing social biases in language models. She is working on quantifying the limitations of widely used fairness metrics that aim to measure bias in masked language models. “I find this area of research exciting because it can have a positive impact on people in underrepresented groups,” she says. After the program, Keren will apply to PhD programs.

Building research skills

Alexander Gurung is working with Meta AI researchers in FAIR’s New York office toward building text-adventure game language models that can both predict what will happen after a player takes an action (requiring an understanding of the state of the world and how it changes) and produce interesting narrations of the event. They hope this work will one day help train language models that have a grounded understanding of how our world works.

“The Residency has helped me learn to drive my own research, coming up with new research questions and areas for exploration based on my own intuition,” he says. “It has also taught me how to tackle larger-scale problems, from the ideation stage all the way through to paper writing and presentation.” Alexander will complete his last year of his master’s degree after the program.

Christina Du is working with Meta AI NLP researchers in FAIR’s London office. She is exploring new autoregressive solutions to extract a set of entities from an input context. “This project is very exciting since our results can bring potential impact to the NLP community and inspire further research on mention-free entity linking,” Christina says.

Christina says the AI Residency program has helped her become a stronger researcher. “I learned how to ask good research questions and how to design experiments to test our hypotheses from my Mentors. I also got more hands-on experience in writing academic papers and presenting my work to other people,” she says. She will pursue her PhD at the University of Edinburgh in the fall.

Karmesh Yadav works on the Habitat project in FAIR’s Menlo Park office. “The AI residency has helped me hone my technical and interpersonal skills as a researcher. I have learned how to orchestrate large-scale experimentation and gotten well versed with distributed training, how to collaborate in large teams while still leading and taking ownership of a project, how to build on top of other people's efforts and familiarizing them with my own work, and, lastly, how to manage contributions over multiple projects while not getting pulled in every direction,” he says. After the AI Residency, Karmesh will start his PhD in Embodied AI at Georgia Tech.

Suvir Mirchandani works on AI Commerce Multimodal in FAIR’s Menlo Park office. “Through the AI Residency, I have developed skills in writing and structuring research code, sharing research ideas with collaborators, and managing experiments. It has been inspiring to be surrounded by so many talented researchers at Meta,” he says. After the program, Suvir will be starting his PhD at Stanford University.

From predicting molecular properties to teaching robots to load dishwashers

Vidhi Jain joined Meta AI researchers working on problems in robotics and embodied AI. Her AI Residency has focused on building learnable policies for complex, long-horizon tasks in unknown indoor environments. “My focus is currently on dishwasher loading based on different human preferences. Solving a real-world problem is hard and messy yet so important; and building solutions geared for it is what makes me excited about this research,” she says. Jain plans to pursue her PhD at Carnegie Mellon University once she’s completed her AI Residency.

Isabelle Hu is part of FAIR’s Make-A-Scene research project. “I’ve worked on modeling experimentation for text-to-video generation and image-to-image translation, and recently I’m working on curating a large scale video-text dataset using multimodal models. This research area connects the domains of vision and language, and it will also enhance how people express their creativity in a very fun and meaningful way.”

“The residency program gave me the opportunity to work on exciting research directions that I had little previous experience in and to experience doing AI research at a much larger scale than in academia,” she says. Hu will continue working in the industry and join a biotech company as a machine learning scientist once she’s completed the program.

Ishita Mediratta researches reinforcement learning (RL) at FAIR London. Taking inspiration from knowledge distillation, she is working to improve systematic generalization of RL agents, such as leveraging Unsupervised Environment Design for automatic curriculum development and open-ended learning. She’s also using Procedurally Generated (PCG) environments to test generalization capabilities of RL agents.

“The residency program has been a great learning experience for me,” she says. “I got to learn how to formulate ideas, present my work to others, and write scalable code.” Mediratta will apply to PhD programs this fall and hope to continue her collaboration with her Mentor through the program.

Zeyu Liu focuses on NLP at FAIR’s Menlo Park office. Zeyu’s recent project developments include “scaling up” with “mixture of experts” techniques. Zeyu and his team proposed a unified framework to characterize different exploration in this region with two variables. “We show that the upper bound of the transformer is far from being reached. And we further study the impact of the two variables to inform future architecture exploration,” says Zeyu.

Millicent Li is on the NLP team at FAIR Seattle, and her research has encapsulated a wide variety of topics in the NLP space. “I’ve been able to meet so many researchers of different backgrounds, take part in a number of projects, and ask questions based on what I’m interested in learning about. The program has let me let my curiosity drive the work that I do without strings attached,” says Millicent. After the program, Millicent will be starting her PhD in computer science at Northeastern University working at the intersection of NLP, human-computer interaction, and healthcare.

Nima Shoghi is on the Open Catalyst Project at FAIR MPK. Open Catalyst is a collaborative research effort between FAIR and Carnegie Mellon University’s Department of Chemical Engineering. “My research involves the development of new machine learning models for molecular property prediction tasks. As someone who does not have a background in chemical engineering, I found things quite challenging at first. Once I got past the onboarding stage, however, having the opportunity to conduct research in a challenging new domain alongside a diverse set of highly qualified researchers with backgrounds in machine learning and computational chemistry has proved to be extremely exciting and rewarding,” says Nima.

Click here to apply for the 2023 cohort."
Meta_Blog,https://ai.meta.com/blog/meta-ai-residency-profiles/,,"Q&A with 2022 Meta AI Residency: Research topics, future plans, and more","The Artificial Intelligence (AI) Residency Program is a one-year research training position designed to give talented professionals hands-on experience in AI research while working in Meta AI. This program is ideal for those interested in applying to PhD programs, publishing research papers, and maximizing their experience in AI research as they advance their careers. The AI Residency provides the perfect opportunity for participants, typically recent graduates with bachelor’s or master’s degrees, to experience the day-to-day work of a researcher. Residents can work with Meta AI researchers in areas such as AI-powered translation, diffusion models for video generation, decoding speech from brain activity, and much more.

In this program, participants will be paired with AI researchers to help guide their projects and research problems. The research will be communicated to the academic community through collaboration across Meta AI, academic papers and conferences (e.g., NeurIPS, ICML, ICLR, CVPR), open source code releases, or applications at Meta.

We encourage applications from people with a technical background and who hope to apply to a graduate program but want more preparation before doing so. Prior experience in machine learning is certainly a strength, but we seek people who have a passion for AI and come from a diversity of backgrounds, including math, physics, finance, economics, linguistics, computational social science, neuroscience, and bioinformatics. This is a full-time program that cannot be undertaken in conjunction with university study or a full-time job.

We asked some of the members of this year’s cohort of AI Residents to describe their experiences in the program. (You can also read brief interviews with several Meta AI Residency Program Mentors.)

Abdalgader Abubaker, London AI Resident 2021-2022 “I am working on integrity challenges. The exciting part of the research is its potential impact on Meta social networks to enhance the existing models that are used to prevent integrity violations on the different Meta platforms.” Alexander Gurung, New York AI Resident 2021-2022 “The program taught me how to drive my own research, and how to come up with new research questions and areas for exploration based on my own intuition. It has also taught me how to tackle larger-scale problems, from the ideation stage all the way through to paper writing and presentation.” Badr Alkhamissi, Seattle AI Resident 2021-2022 “I am building new language model architectures to enable AI to perform different reasoning skills. Since reasoning is one of the pillars for language models to be deployed in the real world, this research addresses an essential problem for using them in a reliable and responsible fashion.” Christina Du, London AI Resident 2021-2022 “The program helped me become a stronger researcher. I learned how to ask good research questions and how to design experiments to test our hypotheses. I also got more hands-on experience in writing academic papers and presenting my work to other people.” Isabelle Hu, Menlo Park AI Resident 2021-2022 “The program gave me the opportunity to work on exciting research directions. As part of FAIR’s multimodal generation research project, I’ve worked on data curation modeling experimentation for text-to-video generation and image-to-image translation. This research area will enhance how people express their creativity in a very fun and meaningful way.” Ishita Mediratta, London AI Resident 2021-2022 “The program has been a great learning experience for me. I learned how to formulate ideas, present my work to others, and write scalable code. I will apply to PhD programs this fall and hope to continue my collaboration with my Meta Mentor throughout my PhD.” Jude Fernandes, New York AI Resident 2021-2022 “My biggest struggle with research before the AI Residency was feeling lost with open-ended questions. I’ve been lucky to have Mentors and collaborators who have given me tools that help guide me in these situations. The program has also given me the chance to fully immerse myself in topics I care about, which I have found is the best way to develop intuition and insight.” Karmesh Yadav, Menlo Park AI Resident 2021–2022 “I am currently working on learning visual representations for embodied navigation and rearrangement tasks using self-supervised learning. The AI Residency helped me hone my technical and interpersonal skills by learning how to orchestrate large-scale experimentation and distributed training, how to collaborate in large teams while still leading and taking ownership of a project, and how to manage contributions over multiple projects.” Keren Fuentes, New York AI Resident 2021–2022 “I am currently working on quantifying the limitations of widely used fairness metrics that aim to measure bias in masked language models. I find this area of research exciting because it can have a positive impact on people in underrepresented groups.” Millicent Li, Seattle AI Resident 2021–2022 “My research has encapsulated a wide variety of topics in the natural language processing (NLP) space. During my time here, I’ve been able to meet so many researchers of different backgrounds, take part in a number of projects, and ask many questions.” Nima Shoghi, Menlo Park AI Resident 2021–2022 “I am on the Open Catalyst Project, which is a collaborative research effort between FAIR and Carnegie Mellon University’s Department of Chemical Engineering. My research involves the development of new machine learning models for molecular property prediction tasks. As someone who does not have a background in chemical engineering, I found things quite challenging at first. However, having the opportunity to conduct research in a challenging new domain alongside a diverse set of highly qualified researchers has proved to be extremely exciting and rewarding.” Suvir Mirchandani, Menlo Park AI Resident 2021–2022 “My research has focused on vision-and-language pretraining and self-supervised learning, specifically interactive image retrieval: Given an image and a piece of text feedback from a user, retrieve an image that incorporates the user’s feedback. It has been inspiring to be surrounded by so many talented researchers at Meta AI. After the AI Residency, I will be starting my PhD at Stanford University."" Vidhi Jain, Menlo Park AI Resident 2021–2022 “I am focused on building learnable policies for complex, long-horizon tasks in unknown indoor environments. Solving a real-world problem is hard and messy, yet so important, and building solutions geared for it is what makes me excited about this research.” Zeyu Liu, Menlo Park AI Resident 2021–2022 “The AI Residency program gave me a chance to work with great researchers and receive different perspectives. Additionally, being at Meta AI and FAIR has provided me with the resource to run experiments that are of strong interest to me. After the AI Residency program, I will complete my master’s program and then hope to apply to PhD programs.”

Tackling high-impact research challenges

Abdalgader Abubaker is working with Meta AI researchers in London on integrity challenges. His AI Residency is focused on developing a new approach for graph neural networks to leverage the hypergraph structure — a generalization of graphs with high-level representation in which the edges can connect more than two nodes. “The exciting part of the research is its potential impact on Meta social networks to enhance the existing models that are used to prevent integrity violations on the Meta different platforms,” Abdalgader says. He plans to apply to PhD programs after completing the Residency program.

Badr Alkhamissi is based in Meta’s Seattle office, where he works on responsible AI research. He is building new language model architectures to enable AI to perform different reasoning skills, such as chaining, composition, coreference, arithmetic, and temporal resolution. “Since reasoning is one of the pillars for language models to be deployed in the real world, this research addresses an essential problem for using them in a reliable and responsible fashion.” Badr plans to apply to PhD programs after his AI Residency.

Jude Fernandes is with Meta AI NLP researchers in FAIR’s New York office. Jude has worked with his Mentors to improve the way dialogue models respond to toxic content using controllable generations and has worked on improving fairness in language models by training them on demographically perturbed corpora. “As AI becomes ubiquitous in our world, I feel an urgency to make sure that these systems won’t cause harm to people, and I feel excited about the potential of building a future that isn’t defined by the power structures that exist in society today,” Jude says.

Keren Fuentes also works at FAIR’s New York office, on the Responsible AI team. Keren’s interests include understanding and reducing social biases in language models. She is working on quantifying the limitations of widely used fairness metrics that aim to measure bias in masked language models. “I find this area of research exciting because it can have a positive impact on people in underrepresented groups,” she says. After the program, Keren will apply to PhD programs.

Building research skills

Alexander Gurung is working with Meta AI researchers in FAIR’s New York office toward building text-adventure game language models that can both predict what will happen after a player takes an action (requiring an understanding of the state of the world and how it changes) and produce interesting narrations of the event. They hope this work will one day help train language models that have a grounded understanding of how our world works.

“The Residency has helped me learn to drive my own research, coming up with new research questions and areas for exploration based on my own intuition,” he says. “It has also taught me how to tackle larger-scale problems, from the ideation stage all the way through to paper writing and presentation.” Alexander will complete his last year of his master’s degree after the program.

Christina Du is working with Meta AI NLP researchers in FAIR’s London office. She is exploring new autoregressive solutions to extract a set of entities from an input context. “This project is very exciting since our results can bring potential impact to the NLP community and inspire further research on mention-free entity linking,” Christina says.

Christina says the AI Residency program has helped her become a stronger researcher. “I learned how to ask good research questions and how to design experiments to test our hypotheses from my Mentors. I also got more hands-on experience in writing academic papers and presenting my work to other people,” she says. She will pursue her PhD at the University of Edinburgh in the fall.

Karmesh Yadav works on the Habitat project in FAIR’s Menlo Park office. “The AI residency has helped me hone my technical and interpersonal skills as a researcher. I have learned how to orchestrate large-scale experimentation and gotten well versed with distributed training, how to collaborate in large teams while still leading and taking ownership of a project, how to build on top of other people's efforts and familiarizing them with my own work, and, lastly, how to manage contributions over multiple projects while not getting pulled in every direction,” he says. After the AI Residency, Karmesh will start his PhD in Embodied AI at Georgia Tech.

Suvir Mirchandani works on AI Commerce Multimodal in FAIR’s Menlo Park office. “Through the AI Residency, I have developed skills in writing and structuring research code, sharing research ideas with collaborators, and managing experiments. It has been inspiring to be surrounded by so many talented researchers at Meta,” he says. After the program, Suvir will be starting his PhD at Stanford University.

From predicting molecular properties to teaching robots to load dishwashers

Vidhi Jain joined Meta AI researchers working on problems in robotics and embodied AI. Her AI Residency has focused on building learnable policies for complex, long-horizon tasks in unknown indoor environments. “My focus is currently on dishwasher loading based on different human preferences. Solving a real-world problem is hard and messy yet so important; and building solutions geared for it is what makes me excited about this research,” she says. Jain plans to pursue her PhD at Carnegie Mellon University once she’s completed her AI Residency.

Isabelle Hu is part of FAIR’s Make-A-Scene research project. “I’ve worked on modeling experimentation for text-to-video generation and image-to-image translation, and recently I’m working on curating a large scale video-text dataset using multimodal models. This research area connects the domains of vision and language, and it will also enhance how people express their creativity in a very fun and meaningful way.”

“The residency program gave me the opportunity to work on exciting research directions that I had little previous experience in and to experience doing AI research at a much larger scale than in academia,” she says. Hu will continue working in the industry and join a biotech company as a machine learning scientist once she’s completed the program.

Ishita Mediratta researches reinforcement learning (RL) at FAIR London. Taking inspiration from knowledge distillation, she is working to improve systematic generalization of RL agents, such as leveraging Unsupervised Environment Design for automatic curriculum development and open-ended learning. She’s also using Procedurally Generated (PCG) environments to test generalization capabilities of RL agents.

“The residency program has been a great learning experience for me,” she says. “I got to learn how to formulate ideas, present my work to others, and write scalable code.” Mediratta will apply to PhD programs this fall and hope to continue her collaboration with her Mentor through the program.

Zeyu Liu focuses on NLP at FAIR’s Menlo Park office. Zeyu’s recent project developments include “scaling up” with “mixture of experts” techniques. Zeyu and his team proposed a unified framework to characterize different exploration in this region with two variables. “We show that the upper bound of the transformer is far from being reached. And we further study the impact of the two variables to inform future architecture exploration,” says Zeyu.

Millicent Li is on the NLP team at FAIR Seattle, and her research has encapsulated a wide variety of topics in the NLP space. “I’ve been able to meet so many researchers of different backgrounds, take part in a number of projects, and ask questions based on what I’m interested in learning about. The program has let me let my curiosity drive the work that I do without strings attached,” says Millicent. After the program, Millicent will be starting her PhD in computer science at Northeastern University working at the intersection of NLP, human-computer interaction, and healthcare.

Nima Shoghi is on the Open Catalyst Project at FAIR MPK. Open Catalyst is a collaborative research effort between FAIR and Carnegie Mellon University’s Department of Chemical Engineering. “My research involves the development of new machine learning models for molecular property prediction tasks. As someone who does not have a background in chemical engineering, I found things quite challenging at first. Once I got past the onboarding stage, however, having the opportunity to conduct research in a challenging new domain alongside a diverse set of highly qualified researchers with backgrounds in machine learning and computational chemistry has proved to be extremely exciting and rewarding,” says Nima.

Click here to apply for the 2023 cohort."
Meta_Blog,https://ai.meta.com/blog/facebook-feed-improvements-ai-show-more-less/,,The new AI-powered feature designed to improve Feed for everyone,"AI is the driving force behind a newly launched tool on Facebook that helps people make a direct change to the content they see in their Feed and influence the types of content that is recommended to them. When people are given the option to select Show More/Show Less on certain posts, we use AI to interpret the intent behind those actions in a nuanced way. This model is capable of taking relatively few data points and then generalizing about what we do know to determine whether to prioritize or deprioritize a piece of content for a particular person.

Clicking Show More on a post increases the ranking score for it and for similar content. Show Less decreases it. Since our model learns to generalize well, we can use these aggregate signals to help improve Feeds for everyone, even if they haven’t used Show More/Show Less. Building an AI model that could interpret these signals and generalize well for everyone was an interesting challenge because we needed to deduce intent and learn to generalize off of sparse data. Someone might click Show More/Show Less once a month, but they may like or comment on posts every day. With fewer data points, we still needed to train our model to predict preferences and to integrate it with our existing recommendation system.

Our recommendation algorithms already use thousands of signals to surface content that people are most likely to want to see. Show More/Show Less is uniquely helpful because it allows people to directly tell us what they want in their Feeds.

Building an AI model that can generalize well

Building machine learning systems that personalize content for the people who use Meta’s products is a complex engineering task. We’ve previously shared more details about how our News Feed algorithm works. Likes and interactions are an important part of that, since they generate billions of examples and indirectly tell us what people like to see when they log on to Facebook. Show More/Show Less offers direct and more nuanced feedback. For example, there are cases where users like a post and then tell us they want to see less of it, which could mean they’ve seen enough content like that.

With Show More/Show Less feedback, we have data points that are a few orders of magnitude less than interactions. Adding to the challenge, Show More/Show Less feedback is also intermittent and not always present for each person, which means we don’t get the same level of past history to help us predict future behavior. This means we have to be more efficient with data and build a model that can generalize well about what we do know. For example, clicking Show Less on a cousin’s post about their new convertible might mean “show me fewer posts by that person” or “show me fewer posts about cars.” An effective system must be able to understand the intent behind the click.

To do this, we use deep learning models to generate user and post-level embeddings (sets of numbers), which help predict the types of content a person wants to see more of or less of in their Feed. A user embedding captures a person’s tastes, while the content embedding captures the essence of what a post is about. We use a neural network architecture to train these embeddings.

On the left is the user tower, which converts a person’s profile and their recent activity on Facebook, including their Show More/Show Less feedback, into one long sequence of tokens. It applies the state-of-the-art transformer models on this sequence to generate an embedding. On the right side is the content tower, which takes all the modalities of a post — text, image, who posted it, audio, video, comments, etc.— and generates an embedding from it. The two embeddings are then jointly trained to predict whether a person would want to click Show More or Show Less of that post.

The challenge with training these embeddings is that deep learning models typically require a large amount of data to be trained effectively, while we want to be able to cater to a user’s tastes without them having to provide a lot of feedback. To solve this, we first pretrain the model on other related tasks, such as predicting what users would like or share. We then fine-tune the embeddings on Show More/Show Less data. This allows the model to transfer information learned on the “Like” data set and apply it to the Show More/Show Less task. Through this process, we are able to take advantage of the much larger data volume from likes, while still optimizing for the more precise Show More/Show Less signal.

For people who have provided Show More/Show Less feedback, their embedding captures that preference and takes it into account when predicting whether they would want to see more or less of a new post. For people who have not provided any feedback, we can still use this technique to generate the embeddings from their other interactions on Facebook and make better predictions of what they would like to see. This approach solves both the scenarios: giving people more control over their own Feed by providing feedback, and improving the experience for all users on Facebook through generalizing from users who provided feedback to other users with similar interaction history.

Show More/Show Less also presents another training challenge. Generally, we train models where user interaction is the positive label and lack of interaction is the negative label. For Show More/Show Less, since many people don’t often give feedback, using the same training strategy would yield a model that makes very low predictions for many users and thus would not meaningfully affect the ranking of their content. We instead trained the model with Show More as a positive and Show Less as a negative. The advantage is that now the model will make a call for every person (and every piece of content) about whether the person is more likely to choose Show More or Show Less if they were to rate that content.

Giving people more control over their experience on our apps

These two signals are another way we can give people more control over the experience they have on Facebook. We are currently experimenting with targeted visibility when showing posts that ask for Show More/Show Less feedback. In the future, we want to present that option when it’s most useful for the person scrolling through their Feed.

We’re continuing to test new ways to customize how much content people see in Feed from the friends and family, groups, Pages, and public figures they’re connected to. For now, people can find these ranking levers — as well as others, such as Snooze, Unfollow, and Reconnect — in their Feed Preferences. We also plan to test this capability with Reels in the coming weeks. As with every product change we make, we’ll continue to use direct feedback and refine our approach to ensure that we’re offering the best possible experience."
Meta_Blog,https://ai.meta.com/blog/facebook-feed-improvements-ai-show-more-less/,,The new AI-powered feature designed to improve Feed for everyone,"AI is the driving force behind a newly launched tool on Facebook that helps people make a direct change to the content they see in their Feed and influence the types of content that is recommended to them. When people are given the option to select Show More/Show Less on certain posts, we use AI to interpret the intent behind those actions in a nuanced way. This model is capable of taking relatively few data points and then generalizing about what we do know to determine whether to prioritize or deprioritize a piece of content for a particular person.

Clicking Show More on a post increases the ranking score for it and for similar content. Show Less decreases it. Since our model learns to generalize well, we can use these aggregate signals to help improve Feeds for everyone, even if they haven’t used Show More/Show Less. Building an AI model that could interpret these signals and generalize well for everyone was an interesting challenge because we needed to deduce intent and learn to generalize off of sparse data. Someone might click Show More/Show Less once a month, but they may like or comment on posts every day. With fewer data points, we still needed to train our model to predict preferences and to integrate it with our existing recommendation system.

Our recommendation algorithms already use thousands of signals to surface content that people are most likely to want to see. Show More/Show Less is uniquely helpful because it allows people to directly tell us what they want in their Feeds.

Building an AI model that can generalize well

Building machine learning systems that personalize content for the people who use Meta’s products is a complex engineering task. We’ve previously shared more details about how our News Feed algorithm works. Likes and interactions are an important part of that, since they generate billions of examples and indirectly tell us what people like to see when they log on to Facebook. Show More/Show Less offers direct and more nuanced feedback. For example, there are cases where users like a post and then tell us they want to see less of it, which could mean they’ve seen enough content like that.

With Show More/Show Less feedback, we have data points that are a few orders of magnitude less than interactions. Adding to the challenge, Show More/Show Less feedback is also intermittent and not always present for each person, which means we don’t get the same level of past history to help us predict future behavior. This means we have to be more efficient with data and build a model that can generalize well about what we do know. For example, clicking Show Less on a cousin’s post about their new convertible might mean “show me fewer posts by that person” or “show me fewer posts about cars.” An effective system must be able to understand the intent behind the click.

To do this, we use deep learning models to generate user and post-level embeddings (sets of numbers), which help predict the types of content a person wants to see more of or less of in their Feed. A user embedding captures a person’s tastes, while the content embedding captures the essence of what a post is about. We use a neural network architecture to train these embeddings.

On the left is the user tower, which converts a person’s profile and their recent activity on Facebook, including their Show More/Show Less feedback, into one long sequence of tokens. It applies the state-of-the-art transformer models on this sequence to generate an embedding. On the right side is the content tower, which takes all the modalities of a post — text, image, who posted it, audio, video, comments, etc.— and generates an embedding from it. The two embeddings are then jointly trained to predict whether a person would want to click Show More or Show Less of that post.

The challenge with training these embeddings is that deep learning models typically require a large amount of data to be trained effectively, while we want to be able to cater to a user’s tastes without them having to provide a lot of feedback. To solve this, we first pretrain the model on other related tasks, such as predicting what users would like or share. We then fine-tune the embeddings on Show More/Show Less data. This allows the model to transfer information learned on the “Like” data set and apply it to the Show More/Show Less task. Through this process, we are able to take advantage of the much larger data volume from likes, while still optimizing for the more precise Show More/Show Less signal.

For people who have provided Show More/Show Less feedback, their embedding captures that preference and takes it into account when predicting whether they would want to see more or less of a new post. For people who have not provided any feedback, we can still use this technique to generate the embeddings from their other interactions on Facebook and make better predictions of what they would like to see. This approach solves both the scenarios: giving people more control over their own Feed by providing feedback, and improving the experience for all users on Facebook through generalizing from users who provided feedback to other users with similar interaction history.

Show More/Show Less also presents another training challenge. Generally, we train models where user interaction is the positive label and lack of interaction is the negative label. For Show More/Show Less, since many people don’t often give feedback, using the same training strategy would yield a model that makes very low predictions for many users and thus would not meaningfully affect the ranking of their content. We instead trained the model with Show More as a positive and Show Less as a negative. The advantage is that now the model will make a call for every person (and every piece of content) about whether the person is more likely to choose Show More or Show Less if they were to rate that content.

Giving people more control over their experience on our apps

These two signals are another way we can give people more control over the experience they have on Facebook. We are currently experimenting with targeted visibility when showing posts that ask for Show More/Show Less feedback. In the future, we want to present that option when it’s most useful for the person scrolling through their Feed.

We’re continuing to test new ways to customize how much content people see in Feed from the friends and family, groups, Pages, and public figures they’re connected to. For now, people can find these ranking levers — as well as others, such as Snooze, Unfollow, and Reconnect — in their Feed Preferences. We also plan to test this capability with Reels in the coming weeks. As with every product change we make, we’ll continue to use direct feedback and refine our approach to ensure that we’re offering the best possible experience."
Meta_Blog,https://ai.meta.com/blog/gpu-inference-engine-nvidia-amd-open-source/,,AITemplate: Unified inference engine on GPUs from NVIDIA and AMD,"GPUs play an important role in the delivery of the compute needed for deploying AI models, especially for large-scale pretrained models in computer vision, natural language processing, and multimodal learning. Currently, AI practitioners have very limited flexibility when choosing a high-performance GPU inference solution because these are concentrated in platform-specific, and closed black box runtimes. A machine learning system designed for one technology provider’s GPU must be completely reimplemented in order to work on a different provider’s hardware. This lack of flexibility also makes it difficult to iterate and maintain the code that makes up these solutions, due to the hardware dependencies in the complex runtime environments.

Moreover, AI production pipelines often require fast development. Developers are eager to try novel modeling techniques because the field is advancing rapidly. Although proprietary software toolkits such as TensorRT provide ways of customization, they are often not enough to satisfy this need. Furthermore, the closed, proprietary solution may make it harder to quickly debug the code, reducing development agility.

To address these industry challenges, Meta AI has developed and is open-sourcing AITemplate (AIT), a unified inference system with separate acceleration back ends for both AMD and NVIDIA GPU hardware. It delivers close to hardware-native Tensor Core (NVIDIA GPU) and Matrix Core (AMD GPU) performance on a variety of widely used AI models such as convolutional neural networks, transformers, and diffusers. With AIT, it is now possible to run performant inference on hardware from both GPU providers. We’ve used AIT to achieve performance improvements up to 12x on NVIDIA GPUs and 4x on AMD GPUs compared with eager mode within PyTorch.

AITemplate is a Python framework that transforms AI models into high-performance C++ GPU template code for accelerating inference. Our system is designed for speed and simplicity. There are two layers in AITemplate — a front-end layer, where we perform various graph transformations to optimize the graph, and a back-end layer, where we generate C++ kernel templates for the GPU target. In addition, AIT maintains a minimal dependency on external libraries. For example, the generated runtime library for inference is self-contained and only requires CUDA/ROCm runtime environments. (CUDA, NVIDIA’s Compute Unified Device Architecture, allows AI software to run efficiently on NVIDIA GPUs. ROCm is an open source software platform that does the same for AMD’s GPUs.)

Our project offers many performance innovations, including advanced kernel fusion, an optimization method that merges multiple kernels into a single kernel to run them more efficiently, and advanced optimizations for transformer blocks. These optimizations deliver state-of-the-art performance by significantly increasing utilization of NVIDIA's Tensor Cores and AMD's Matrix Cores.

AITemplate is currently enabled on NVIDIA's A100 and AMD’s MI200 GPU systems, both of which are widely used today in data centers from technology companies, research labs, and cloud computing service providers.

The benchmark results shown below compare the performance results of PyTorch eager mode and AITemplate on NVIDIA A100 GPUs for several mainstream models.

AIT and PyTorch eager* ResNet-50 and BERT-Base with sequence length 384 benchmark on NVIDIA A100-40GB.

As shown in the benchmark below, with AITemplate, models utilizing the AMD MI250 GPU can get significant performance boosts as well, including ResNet and transformer models that power advanced vision and language systems. On MI250 2 GCD settings, each GCD (core) is processing half of the data.

PyTorch eager and AIT ResNet-50, BERT-base with sequence length 384 Benchmark on AMD MI250. MI250 is running in data parallel mode, where each GCD (GPU core) is processing half of the data. For batch size 1, the batch is processed on a single GCD while the other GCD is idle.

The unified GPU back-end support gives deep learning developers more hardware vendor choices with minimal migration costs.

Deploying AITemplate is straightforward. The AI model is compiled into a self-contained binary without dependencies. This binary can work in any environment with the same hardware and newer CUDA 11 / ROCM 5 versions, which results in excellent backward compatibility. This is important in production environments, where stability and backward compatibility are crucial. AITemplate also provides out-of-the-box widely used models (e.g., VisionTransformer, BERT, Stable Diffusion, ResNet, and MaskRCNN). This simplifies the deployment process and allows practitioners to deploy PyTorch pretrained models easily.

AITemplate optimizations

AITemplate has one of the most advanced kernel fusion systems in the industry, thanks to its support of three innovative optimizations: vertical, horizontal, and memory fusions. Vertical fusions fuse chains of operations together. Horizontal fusions fuse parallel operations with no dependency together into a single grouped op. Memory fusions fuse memory movement ops and computation-intensive operations together. Vertical, horizontal, and memory fusions can also be combined.

AITemplate can combine three fusions together to accelerate inference.

As to horizontal fusions, AITemplate currently supports grouped GEMM operations, grouped GEMM + activation ops, and grouped layernorm/swish layernorm operations. AITemplate supports several vertical fusions beyond just standard element-wise operations. These include:

GEMM and element-wise fusions through CUTLASS and Composable Kernels epilogue fusion

GEMM and permute fusions for transformer multihead attention blocks

Fusion of memory operations, such as split, slice, and concatenate, with other ops to reduce memory bandwidth via Tensor Accessors

For standard transformer multihead attention blocks, AITemplate currently relies on Flash Attention on NVIDIA GPUs and generalized back-to-back GEMM/softmax/GEMM fusion in Composable Kernels on AMD GPUs. Both implementations completely remove the data traffic between the compute unit and HBM (high-bandwidth memory) for the intermediate result. With Composable Kernels, not only in attention blocks, a wide range of bottleneck structures in neural networks can be fused. Many problems that were bandwidth-bound now become compute-bound, so the system can utilize GPU compute power much more efficiently. This optimization is more effective for transformer models with long sequences, as shown below.

Our approach extends beyond previous systems by generating templates within a compiler, such as state-of-the-art multidimensional fusion — horizontal fusion, vertical fusion, and memory fusion — but also introduces a unified solution for both NVIDIA and AMD GPUs.

Developing AITemplate

AITemplate has two layers of template systems: The first is the Python Jinja2 template, and the second is the GPU Tensor Core/Matrix Core C++ template (CUTLASS for NVIDIA GPUs and Composable Kernel for AMD GPUs). AITemplate first runs profiling to find the best kernel configuration in Python, and then renders the Jinja2 template into C++ code.

After the model’s source code is generated, the GPU C++ compiler (NVIDIA NVCC and AMD HIPCC) compiles the source code into the final binary code for the model. With its front-end design, which is similar to PyTorch, users can easily convert their models to AITemplate from many different frameworks, including PyTorch.

Greener computing

Our techniques expand the availability of AI platforms and can help reduce carbon emissions to address environmental concerns. Studies show that GPU usage can be tied to carbon emissions. AITemplate reduces GPU execution time, which will also reduce emissions. Since AI models are deployed in the core systems of technology companies around the world, greater efficiency can have a significant impact. The system also makes running inference of trained AI models more accessible, by allowing more platform choices for AI inference workloads.

Extending AITemplate to new hardware and adding more functionality

AITemplate offers state-of-the-art performance for current and next-gen NVIDIA and AMD GPUs with less system complexity. However, we are only at the beginning of our journey to build a high-performance AI inference engine. We are actively working on enhancing AITemplate with more optimizations and full dynamic shape support. We also plan to extend AITemplate to additional hardware systems, such as Apple M-series GPUs, as well as CPUs from other technology providers. Beyond this, we are working on the automatic lowering of PyTorch models to provide an additional turnkey inference solution for PyTorch. We are also open to exploring integrations with other frameworks, such as ONNX and Open-XLA. We hope to build a greener and more efficient AI inference ecosystem with better performance, higher flexibility, and more back-end choices.

Get the code:

https://github.com/facebookincubator/AITemplate

This work is being undertaken by a wide-ranging team at Meta that includes Bing Xu, Ying Zhang, Hao Lu, Yang Chen, Terry Chen, Mike Iovine, Mu-Chu Lee, Scott Wolchok, Oleg Khabinov, Shirong Wu, Huaming Li, Hui Guo, Zhijing Li, Max Podkorytov, Janet Yang, Yinghai Lu, Lu Fang, Andrew Tulloch, and Ajit Mathews.

* This diagram does not include the Better Transformer, introduced in PyTorch 1.12.

**Reproduce code and instruction can be found in the repo examples folder."
Meta_Blog,https://ai.meta.com/blog/gpu-inference-engine-nvidia-amd-open-source/,,AITemplate: Unified inference engine on GPUs from NVIDIA and AMD,"GPUs play an important role in the delivery of the compute needed for deploying AI models, especially for large-scale pretrained models in computer vision, natural language processing, and multimodal learning. Currently, AI practitioners have very limited flexibility when choosing a high-performance GPU inference solution because these are concentrated in platform-specific, and closed black box runtimes. A machine learning system designed for one technology provider’s GPU must be completely reimplemented in order to work on a different provider’s hardware. This lack of flexibility also makes it difficult to iterate and maintain the code that makes up these solutions, due to the hardware dependencies in the complex runtime environments.

Moreover, AI production pipelines often require fast development. Developers are eager to try novel modeling techniques because the field is advancing rapidly. Although proprietary software toolkits such as TensorRT provide ways of customization, they are often not enough to satisfy this need. Furthermore, the closed, proprietary solution may make it harder to quickly debug the code, reducing development agility.

To address these industry challenges, Meta AI has developed and is open-sourcing AITemplate (AIT), a unified inference system with separate acceleration back ends for both AMD and NVIDIA GPU hardware. It delivers close to hardware-native Tensor Core (NVIDIA GPU) and Matrix Core (AMD GPU) performance on a variety of widely used AI models such as convolutional neural networks, transformers, and diffusers. With AIT, it is now possible to run performant inference on hardware from both GPU providers. We’ve used AIT to achieve performance improvements up to 12x on NVIDIA GPUs and 4x on AMD GPUs compared with eager mode within PyTorch.

AITemplate is a Python framework that transforms AI models into high-performance C++ GPU template code for accelerating inference. Our system is designed for speed and simplicity. There are two layers in AITemplate — a front-end layer, where we perform various graph transformations to optimize the graph, and a back-end layer, where we generate C++ kernel templates for the GPU target. In addition, AIT maintains a minimal dependency on external libraries. For example, the generated runtime library for inference is self-contained and only requires CUDA/ROCm runtime environments. (CUDA, NVIDIA’s Compute Unified Device Architecture, allows AI software to run efficiently on NVIDIA GPUs. ROCm is an open source software platform that does the same for AMD’s GPUs.)

Our project offers many performance innovations, including advanced kernel fusion, an optimization method that merges multiple kernels into a single kernel to run them more efficiently, and advanced optimizations for transformer blocks. These optimizations deliver state-of-the-art performance by significantly increasing utilization of NVIDIA's Tensor Cores and AMD's Matrix Cores.

AITemplate is currently enabled on NVIDIA's A100 and AMD’s MI200 GPU systems, both of which are widely used today in data centers from technology companies, research labs, and cloud computing service providers.

The benchmark results shown below compare the performance results of PyTorch eager mode and AITemplate on NVIDIA A100 GPUs for several mainstream models.

AIT and PyTorch eager* ResNet-50 and BERT-Base with sequence length 384 benchmark on NVIDIA A100-40GB.

As shown in the benchmark below, with AITemplate, models utilizing the AMD MI250 GPU can get significant performance boosts as well, including ResNet and transformer models that power advanced vision and language systems. On MI250 2 GCD settings, each GCD (core) is processing half of the data.

PyTorch eager and AIT ResNet-50, BERT-base with sequence length 384 Benchmark on AMD MI250. MI250 is running in data parallel mode, where each GCD (GPU core) is processing half of the data. For batch size 1, the batch is processed on a single GCD while the other GCD is idle.

The unified GPU back-end support gives deep learning developers more hardware vendor choices with minimal migration costs.

Deploying AITemplate is straightforward. The AI model is compiled into a self-contained binary without dependencies. This binary can work in any environment with the same hardware and newer CUDA 11 / ROCM 5 versions, which results in excellent backward compatibility. This is important in production environments, where stability and backward compatibility are crucial. AITemplate also provides out-of-the-box widely used models (e.g., VisionTransformer, BERT, Stable Diffusion, ResNet, and MaskRCNN). This simplifies the deployment process and allows practitioners to deploy PyTorch pretrained models easily.

AITemplate optimizations

AITemplate has one of the most advanced kernel fusion systems in the industry, thanks to its support of three innovative optimizations: vertical, horizontal, and memory fusions. Vertical fusions fuse chains of operations together. Horizontal fusions fuse parallel operations with no dependency together into a single grouped op. Memory fusions fuse memory movement ops and computation-intensive operations together. Vertical, horizontal, and memory fusions can also be combined.

AITemplate can combine three fusions together to accelerate inference.

As to horizontal fusions, AITemplate currently supports grouped GEMM operations, grouped GEMM + activation ops, and grouped layernorm/swish layernorm operations. AITemplate supports several vertical fusions beyond just standard element-wise operations. These include:

GEMM and element-wise fusions through CUTLASS and Composable Kernels epilogue fusion

GEMM and permute fusions for transformer multihead attention blocks

Fusion of memory operations, such as split, slice, and concatenate, with other ops to reduce memory bandwidth via Tensor Accessors

For standard transformer multihead attention blocks, AITemplate currently relies on Flash Attention on NVIDIA GPUs and generalized back-to-back GEMM/softmax/GEMM fusion in Composable Kernels on AMD GPUs. Both implementations completely remove the data traffic between the compute unit and HBM (high-bandwidth memory) for the intermediate result. With Composable Kernels, not only in attention blocks, a wide range of bottleneck structures in neural networks can be fused. Many problems that were bandwidth-bound now become compute-bound, so the system can utilize GPU compute power much more efficiently. This optimization is more effective for transformer models with long sequences, as shown below.

Our approach extends beyond previous systems by generating templates within a compiler, such as state-of-the-art multidimensional fusion — horizontal fusion, vertical fusion, and memory fusion — but also introduces a unified solution for both NVIDIA and AMD GPUs.

Developing AITemplate

AITemplate has two layers of template systems: The first is the Python Jinja2 template, and the second is the GPU Tensor Core/Matrix Core C++ template (CUTLASS for NVIDIA GPUs and Composable Kernel for AMD GPUs). AITemplate first runs profiling to find the best kernel configuration in Python, and then renders the Jinja2 template into C++ code.

After the model’s source code is generated, the GPU C++ compiler (NVIDIA NVCC and AMD HIPCC) compiles the source code into the final binary code for the model. With its front-end design, which is similar to PyTorch, users can easily convert their models to AITemplate from many different frameworks, including PyTorch.

Greener computing

Our techniques expand the availability of AI platforms and can help reduce carbon emissions to address environmental concerns. Studies show that GPU usage can be tied to carbon emissions. AITemplate reduces GPU execution time, which will also reduce emissions. Since AI models are deployed in the core systems of technology companies around the world, greater efficiency can have a significant impact. The system also makes running inference of trained AI models more accessible, by allowing more platform choices for AI inference workloads.

Extending AITemplate to new hardware and adding more functionality

AITemplate offers state-of-the-art performance for current and next-gen NVIDIA and AMD GPUs with less system complexity. However, we are only at the beginning of our journey to build a high-performance AI inference engine. We are actively working on enhancing AITemplate with more optimizations and full dynamic shape support. We also plan to extend AITemplate to additional hardware systems, such as Apple M-series GPUs, as well as CPUs from other technology providers. Beyond this, we are working on the automatic lowering of PyTorch models to provide an additional turnkey inference solution for PyTorch. We are also open to exploring integrations with other frameworks, such as ONNX and Open-XLA. We hope to build a greener and more efficient AI inference ecosystem with better performance, higher flexibility, and more back-end choices.

Get the code:

https://github.com/facebookincubator/AITemplate

This work is being undertaken by a wide-ranging team at Meta that includes Bing Xu, Ying Zhang, Hao Lu, Yang Chen, Terry Chen, Mike Iovine, Mu-Chu Lee, Scott Wolchok, Oleg Khabinov, Shirong Wu, Huaming Li, Hui Guo, Zhijing Li, Max Podkorytov, Janet Yang, Yinghai Lu, Lu Fang, Andrew Tulloch, and Ajit Mathews.

* This diagram does not include the Better Transformer, introduced in PyTorch 1.12.

**Reproduce code and instruction can be found in the repo examples folder."
Meta_Blog,https://ai.meta.com/blog/generative-ai-text-to-video/,,Introducing Make-A-Video: An AI system that generates videos from text,"Today, we’re announcing Make-A-Video, a new AI system that lets people turn text prompts into brief, high-quality video clips. Make-A-Video builds on Meta AI’s recent progress in generative technology research and has the potential to open new opportunities for creators and artists. The system learns what the world looks like from paired text-image data and how the world moves from video footage with no associated text. As part of our continued commitment to open science, we’re sharing details in a research paper and plan to release a demo experience.

Generative AI research is pushing creative expression forward by giving people tools to quickly and easily create new content. With just a few words or lines of text, Make-A-Video can bring imagination to life and create one-of-a-kind videos full of vivid colors, characters, and landscapes. The system can also create videos from images or take existing videos and create new ones that are similar.

Make-A-Video follows our announcement earlier this year of Make-A-Scene, a multimodal generative AI method that gives people more control over the AI generated content they create. With Make-A-Scene, we demonstrated how people can create photorealistic illustrations and storybook-quality art using words, lines of text, and freeform sketches.

We want to be thoughtful about how we build new generative AI systems like this. Make-A-Video uses publicly available datasets, which adds an extra level of transparency to the research. We are openly sharing this generative AI research and results with the community for their feedback, and will continue to use our responsible AI framework to refine and evolve our approach to this emerging technology.

Learn more about Make-A-Video by visiting our site and reading the paper.

Here are some examples of text prompts turned into videos:"
Meta_Blog,https://ai.meta.com/blog/generative-ai-text-to-video/,,Introducing Make-A-Video: An AI system that generates videos from text,"Today, we’re announcing Make-A-Video, a new AI system that lets people turn text prompts into brief, high-quality video clips. Make-A-Video builds on Meta AI’s recent progress in generative technology research and has the potential to open new opportunities for creators and artists. The system learns what the world looks like from paired text-image data and how the world moves from video footage with no associated text. As part of our continued commitment to open science, we’re sharing details in a research paper and plan to release a demo experience.

Generative AI research is pushing creative expression forward by giving people tools to quickly and easily create new content. With just a few words or lines of text, Make-A-Video can bring imagination to life and create one-of-a-kind videos full of vivid colors, characters, and landscapes. The system can also create videos from images or take existing videos and create new ones that are similar.

Make-A-Video follows our announcement earlier this year of Make-A-Scene, a multimodal generative AI method that gives people more control over the AI generated content they create. With Make-A-Scene, we demonstrated how people can create photorealistic illustrations and storybook-quality art using words, lines of text, and freeform sketches.

We want to be thoughtful about how we build new generative AI systems like this. Make-A-Video uses publicly available datasets, which adds an extra level of transparency to the research. We are openly sharing this generative AI research and results with the community for their feedback, and will continue to use our responsible AI framework to refine and evolve our approach to this emerging technology.

Learn more about Make-A-Video by visiting our site and reading the paper.

Here are some examples of text prompts turned into videos:"
Meta_Blog,https://ai.meta.com/blog/pytorch-foundation/,,Announcing the PyTorch Foundation: A new era for the cutting-edge AI framework,"To accelerate progress in AI, PyTorch is moving to a new, independent PyTorch Foundation, under the Linux Foundation umbrella.

The project will join the Linux Foundation with a diverse governing board composed of representatives from AMD, Amazon Web Services, Google Cloud, Meta, Microsoft Azure, and Nvidia, with the intention to expand over time. The PyTorch Foundation will act as a responsible steward for the technology and support PyTorch through conferences, training courses, and other initiatives.

The foundation’s mission is to drive adoption of AI tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects with PyTorch. It will democratize state-of-the-art tools, libraries, and other components to make these innovations accessible to everyone.

The PyTorch Foundation will focus on the business and product marketing of PyTorch and the related ecosystem. The transition will not entail any changes to PyTorch’s code and core project, including its separate technical governance structure.

Meta will continue to invest in PyTorch and use it as our primary framework for AI research and production applications at the company.

Since we partnered with the AI community to create the PyTorch framework for AI research six years ago, open collaboration has been essential to its success. With some 2,400 contributors who have built more than 150,000 projects on the framework, PyTorch has become one of the leading platforms for AI research as well as commercial production use. Today, we are excited to announce the next step for PyTorch: The project will transition to a newly launched PyTorch Foundation, which will be part of the nonprofit Linux Foundation, a technology consortium whose core mission is the collaborative development of open source software.

Something Went Wrong We're having trouble playing this video. Learn more

The PyTorch Foundation will boast a wide-ranging governing board composed of representatives from AMD, Amazon Web Services, Google Cloud, Meta, Microsoft Azure, and Nvidia, with the intention to expand further over time. The board will prioritize the continued growth of PyTorch’s vibrant community, the driving force behind the project’s success. Releases, features, and technical direction will still be driven by the maintainers, committers, and contributors to the project. The creation of the PyTorch Foundation ensures that decisions will be made in a transparent and open manner by a diverse group of members for many years to come.

A thriving community

It’s hard to overstate how quickly PyTorch has grown. In 2016, a group of Meta AI researchers embarked quietly on a couple of projects. They were determined to create a single, simple, standardized interface for their end-to-end workflow. They also harbored hopes of fixing the tedious, complicated research-to-production pipeline of the AI field. In fact, this pipeline was more of a labyrinth, involving multiple steps and tools, fragmented processes, and navigation between different frameworks that were optimized for either research or production, but not both. The Meta team experimented with machine learning (ML) frameworks such as Theano and Torch, and with advanced concepts from Lua Torch, Chainer, and HIPS Autograd, but stayed focused on one thing above all for their new framework: usability.

Just two years later, Meta announced PyTorch 1.0, a dynamic, interactive framework that allowed developers to not only experiment rapidly but also seamlessly transition to graph-based modes for deployment.

Their efforts have paid off. PyTorch has since grown into the lingua franca of AI research. Today, more than 80 percent of researchers who submit their work at major ML conferences, such as NeurIPS or ICML, harness the framework. We have built libraries that support some of the principal domains of the AI field, such as torchvision, which powers most of the world’s modern computer vision research. The framework will continue to be a part of Meta’s AI research and engineering work. PyTorch is also a foundation of the AI research and products built by Amazon Web Services, Microsoft Azure, OpenAI, and many other companies and research institutions.

Meta has worked steadily to nurture the community-driven growth that has fueled PyTorch’s success. We’ve committed hundreds of engineers to the framework and strongly supported product development and community outreach.

But the time is right for a new home for PyTorch.

Formalizing governance

PyTorch was built from the ground up with an open source, community-first philosophy, and that will not change. When researchers and developers open-source their code, others around the world can share their work, learn from one another’s advances, and then contribute back to the community.

We have always had a clear technical governance structure in place for PyTorch, with core maintainers setting the vision for the framework and the best AI researchers and engineers across industry building its modules. That will be even more important for the future of the framework, as Meta and other contributors have a three-year vision to expand its functionality, modularity, and diversity of code ownership.

Going forward, the framework’s contributors will benefit from the robust governance, diverse leadership, and additional investments provided by the new PyTorch Foundation partners. The PyTorch Foundation will strive to adhere to four principles: remaining open, maintaining neutral branding, staying fair, and forging a strong technical identity. One of the foundation’s main priorities is to maintain a clear separation between the business and technical governance of PyTorch.

Meta stays fully committed to PyTorch. We will continue to invest in the framework, and use it as the primary framework for our AI research and production. The transition will not entail any changes to PyTorch’s code and core project, and developer operating models, including contributor guidelines and licensing, will also remain unchanged. All releases, features, and technical direction will continue to be driven by PyTorch’s community: from individual code contributors, those who review and commit changes, to the module maintainers. We’re lucky to have strong support from leadership at Meta, which is fully behind the move to take PyTorch to its next logical phase.

Meta has put open science at the core of our work in AI, whether it’s releasing code for large language models, self-supervised computer vision systems, innovative new datasets, embodied AI platforms, and much more. We believe this approach enables the fastest progress in building and deploying new systems that will address real-world needs and answer fundamental questions about the nature of intelligence. PyTorch has been a core part of this work, and now, with the creation of the PyTorch Foundation, the entire AI community is positioned to push the field forward in countless exciting new ways."
Meta_Blog,https://ai.meta.com/blog/pytorch-foundation/,,Announcing the PyTorch Foundation: A new era for the cutting-edge AI framework,"To accelerate progress in AI, PyTorch is moving to a new, independent PyTorch Foundation, under the Linux Foundation umbrella.

The project will join the Linux Foundation with a diverse governing board composed of representatives from AMD, Amazon Web Services, Google Cloud, Meta, Microsoft Azure, and Nvidia, with the intention to expand over time. The PyTorch Foundation will act as a responsible steward for the technology and support PyTorch through conferences, training courses, and other initiatives.

The foundation’s mission is to drive adoption of AI tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects with PyTorch. It will democratize state-of-the-art tools, libraries, and other components to make these innovations accessible to everyone.

The PyTorch Foundation will focus on the business and product marketing of PyTorch and the related ecosystem. The transition will not entail any changes to PyTorch’s code and core project, including its separate technical governance structure.

Meta will continue to invest in PyTorch and use it as our primary framework for AI research and production applications at the company.

Since we partnered with the AI community to create the PyTorch framework for AI research six years ago, open collaboration has been essential to its success. With some 2,400 contributors who have built more than 150,000 projects on the framework, PyTorch has become one of the leading platforms for AI research as well as commercial production use. Today, we are excited to announce the next step for PyTorch: The project will transition to a newly launched PyTorch Foundation, which will be part of the nonprofit Linux Foundation, a technology consortium whose core mission is the collaborative development of open source software.

Something Went Wrong We're having trouble playing this video. Learn more

The PyTorch Foundation will boast a wide-ranging governing board composed of representatives from AMD, Amazon Web Services, Google Cloud, Meta, Microsoft Azure, and Nvidia, with the intention to expand further over time. The board will prioritize the continued growth of PyTorch’s vibrant community, the driving force behind the project’s success. Releases, features, and technical direction will still be driven by the maintainers, committers, and contributors to the project. The creation of the PyTorch Foundation ensures that decisions will be made in a transparent and open manner by a diverse group of members for many years to come.

A thriving community

It’s hard to overstate how quickly PyTorch has grown. In 2016, a group of Meta AI researchers embarked quietly on a couple of projects. They were determined to create a single, simple, standardized interface for their end-to-end workflow. They also harbored hopes of fixing the tedious, complicated research-to-production pipeline of the AI field. In fact, this pipeline was more of a labyrinth, involving multiple steps and tools, fragmented processes, and navigation between different frameworks that were optimized for either research or production, but not both. The Meta team experimented with machine learning (ML) frameworks such as Theano and Torch, and with advanced concepts from Lua Torch, Chainer, and HIPS Autograd, but stayed focused on one thing above all for their new framework: usability.

Just two years later, Meta announced PyTorch 1.0, a dynamic, interactive framework that allowed developers to not only experiment rapidly but also seamlessly transition to graph-based modes for deployment.

Their efforts have paid off. PyTorch has since grown into the lingua franca of AI research. Today, more than 80 percent of researchers who submit their work at major ML conferences, such as NeurIPS or ICML, harness the framework. We have built libraries that support some of the principal domains of the AI field, such as torchvision, which powers most of the world’s modern computer vision research. The framework will continue to be a part of Meta’s AI research and engineering work. PyTorch is also a foundation of the AI research and products built by Amazon Web Services, Microsoft Azure, OpenAI, and many other companies and research institutions.

Meta has worked steadily to nurture the community-driven growth that has fueled PyTorch’s success. We’ve committed hundreds of engineers to the framework and strongly supported product development and community outreach.

But the time is right for a new home for PyTorch.

Formalizing governance

PyTorch was built from the ground up with an open source, community-first philosophy, and that will not change. When researchers and developers open-source their code, others around the world can share their work, learn from one another’s advances, and then contribute back to the community.

We have always had a clear technical governance structure in place for PyTorch, with core maintainers setting the vision for the framework and the best AI researchers and engineers across industry building its modules. That will be even more important for the future of the framework, as Meta and other contributors have a three-year vision to expand its functionality, modularity, and diversity of code ownership.

Going forward, the framework’s contributors will benefit from the robust governance, diverse leadership, and additional investments provided by the new PyTorch Foundation partners. The PyTorch Foundation will strive to adhere to four principles: remaining open, maintaining neutral branding, staying fair, and forging a strong technical identity. One of the foundation’s main priorities is to maintain a clear separation between the business and technical governance of PyTorch.

Meta stays fully committed to PyTorch. We will continue to invest in the framework, and use it as the primary framework for our AI research and production. The transition will not entail any changes to PyTorch’s code and core project, and developer operating models, including contributor guidelines and licensing, will also remain unchanged. All releases, features, and technical direction will continue to be driven by PyTorch’s community: from individual code contributors, those who review and commit changes, to the module maintainers. We’re lucky to have strong support from leadership at Meta, which is fully behind the move to take PyTorch to its next logical phase.

Meta has put open science at the core of our work in AI, whether it’s releasing code for large language models, self-supervised computer vision systems, innovative new datasets, embodied AI platforms, and much more. We believe this approach enables the fastest progress in building and deploying new systems that will address real-world needs and answer fundamental questions about the nature of intelligence. PyTorch has been a core part of this work, and now, with the creation of the PyTorch Foundation, the entire AI community is positioned to push the field forward in countless exciting new ways."
Meta_Blog,https://ai.meta.com/blog/ai-speech-brain-activity/,,Using AI to decode speech from brain activity,"Every year, more than 69 million people around the world suffer traumatic brain injury, which leaves many of them unable to communicate through speech, typing, or gestures. These people’s lives could dramatically improve if researchers developed a technology to decode language directly from noninvasive brain recordings. Today, we’re sharing research that takes a step toward this goal. We’ve developed an AI model that can decode speech from noninvasive recordings of brain activity.

From three seconds of brain activity, our results show that our model can decode the corresponding speech segments with up to 73 percent top-10 accuracy from a vocabulary of 793 words, i.e., a large portion of the words we typically use on a day-to-day basis.

Decoding speech from brain activity has been a long-standing goal of neuroscientists and clinicians, but most of the progress has relied on invasive brain-recording techniques, such as stereotactic electroencephalography and electrocorticography. These devices provide clearer signals than noninvasive methods but require neurosurgical interventions. While results from that work suggest that decoding speech from recordings of brain activity is feasible, decoding speech with noninvasive approaches would provide a safer, more scalable solution that could ultimately benefit many more people. This is very challenging, however, since noninvasive recordings are notoriously noisy and can greatly vary across recording sessions and individuals for a variety of reasons, including differences in each person’s brain and where the sensors are placed.

In our work, we address these challenges by creating a deep learning model trained with contrastive learning and then use it to maximally align noninvasive brain recordings and speech sounds. To do this, we use wave2vec 2.0, an open source , self-supervised learning model developed by our FAIR team in 2020. We then use this model to identify the complex representations of speech in the brains of volunteers listening to audiobooks.

We focused on two noninvasive technologies: electroencephalography and magnetoencephalography (EEG and MEG, for short), which measure the fluctuations of electric and magnetic fields elicited by neuronal activity, respectively. In practice, both systems can take approximately 1,000 snapshots of macroscopic brain activity every second, using hundreds of sensors.

We leveraged four open source EEG and MEG datasets from academic institutions, capitalizing on more than 150 hours of recordings of 169 healthy volunteers listening to audiobooks and isolated sentences in English and Dutch.

We then input those EEG and MEG recordings into a “brain” model, which consists of a standard deep convolutional network with residual connections. EEG and MEG recordings are known to vary extensively across individuals because of individual brain anatomy, differences in the location and timing of neural functions across brain regions, and the position of the sensors during a recording session. In practice, this means that analyzing brain data generally requires a complex engineering pipeline crafted to realign brain signals on a template brain. In previous studies, brain decoders were trained on a small number of recordings to predict a limited set of speech features, such as part-of-speech categories or words from a small vocabulary. For our research, we designed a new subject-embedding layer, which is trained end-to-end to align all brain recordings in a common space.

To decode speech from noninvasive brain signals, we train a model with contrastive learning to align speech and its corresponding brain activity.

Finally, our architecture learns to align the output of this brain model to the deep representations of the speech sounds that were presented to the participants. In our previous work, we used wav2vec 2.0 to show that this speech algorithm automatically learns to generate representations of speech that align with those of the brain. The emergence of “brainlike” representations of speech in wav2vec 2.0 made it a natural choice to build our decoder, because it helps to know which representations we should try to extract from brain signals.

We recently showed that the activations of wav2vec 2.0 (left) map onto the brain (right) in response to the same speech sounds. The representations of the first layers of this algorithm (cool colors) map onto the early auditory cortex, whereas the deepest layers map onto high-level brain regions (e.g. prefrontal and parietal cortex).

After training, our system performs what’s known as zero-shot classification: Given a snippet of brain activity, it can determine from a large pool of new audio clips which one the person actually heard. From there, the algorithm infers the words the person has most likely heard. This is an exciting step because it shows AI can successfully learn to decode noisy and variable noninvasive recordings of brain activity when speech is perceived. The next step is to see whether we can extend this model to directly decode speech from brain activity without needing the pool of audio clips, i.e., to move toward a safe and versatile speech decoder.

Our analyses further show that several components of our algorithm, including the use of wav2vec 2.0 and the subject layer, were beneficial to decoding performance. Furthermore, we show that our algorithm improves with the number of EEG and MEG recordings. Practically, this means that our approach benefits from the pulling of large amounts of heterogeneous data, and could, in principle, help improve the decoding of small datasets. This is important because, in many cases, it can be hard to collect a lot of data for a given participant. For example, it isn’t practical to require patients to spend dozens of hours in a scanner to check whether the system works for them. Instead, algorithms could be pretrained on large datasets inclusive of many individuals and conditions, and then support the decoding of brain activity for a new patient with little data.

Using AI to understand how the brain works

The results of our research are encouraging because they show that self-supervised trained AI can successfully decode perceived speech from noninvasive recordings of brain activity, despite the noise and variability inherent in those data. These results are only a first step, however. In this work, we focused on decoding speech perception, but the ultimate goal of enabling patient communication will require extending this work to speech production. This line of research could even reach beyond assisting patients to potentially include enabling new ways of interacting with computers.

More generally, our work is a part of the broader effort by the scientific community to use AI to better understand the human brain. We’re sharing this research openly to accelerate progress on the challenges still ahead. We look forward to working together and contributing to the research community in this area.

Learn more by reading our paper about decoding speech from noninvasive brain recordings.

We’d like to acknowledge contributions to this research from Alexandre Défossez, Charlotte Caucheteux, Ori Kabeli, and Jérémy Rapin.

Data were provided (in part) by the Donders Institute for Brain, Cognition and Behaviour; New York University; the University of Michigan; Trinity College Dublin; and the University of Rochester."
Meta_Blog,https://ai.meta.com/blog/ai-speech-brain-activity/,,Using AI to decode speech from brain activity,"Every year, more than 69 million people around the world suffer traumatic brain injury, which leaves many of them unable to communicate through speech, typing, or gestures. These people’s lives could dramatically improve if researchers developed a technology to decode language directly from noninvasive brain recordings. Today, we’re sharing research that takes a step toward this goal. We’ve developed an AI model that can decode speech from noninvasive recordings of brain activity.

From three seconds of brain activity, our results show that our model can decode the corresponding speech segments with up to 73 percent top-10 accuracy from a vocabulary of 793 words, i.e., a large portion of the words we typically use on a day-to-day basis.

Decoding speech from brain activity has been a long-standing goal of neuroscientists and clinicians, but most of the progress has relied on invasive brain-recording techniques, such as stereotactic electroencephalography and electrocorticography. These devices provide clearer signals than noninvasive methods but require neurosurgical interventions. While results from that work suggest that decoding speech from recordings of brain activity is feasible, decoding speech with noninvasive approaches would provide a safer, more scalable solution that could ultimately benefit many more people. This is very challenging, however, since noninvasive recordings are notoriously noisy and can greatly vary across recording sessions and individuals for a variety of reasons, including differences in each person’s brain and where the sensors are placed.

In our work, we address these challenges by creating a deep learning model trained with contrastive learning and then use it to maximally align noninvasive brain recordings and speech sounds. To do this, we use wave2vec 2.0, an open source , self-supervised learning model developed by our FAIR team in 2020. We then use this model to identify the complex representations of speech in the brains of volunteers listening to audiobooks.

We focused on two noninvasive technologies: electroencephalography and magnetoencephalography (EEG and MEG, for short), which measure the fluctuations of electric and magnetic fields elicited by neuronal activity, respectively. In practice, both systems can take approximately 1,000 snapshots of macroscopic brain activity every second, using hundreds of sensors.

We leveraged four open source EEG and MEG datasets from academic institutions, capitalizing on more than 150 hours of recordings of 169 healthy volunteers listening to audiobooks and isolated sentences in English and Dutch.

We then input those EEG and MEG recordings into a “brain” model, which consists of a standard deep convolutional network with residual connections. EEG and MEG recordings are known to vary extensively across individuals because of individual brain anatomy, differences in the location and timing of neural functions across brain regions, and the position of the sensors during a recording session. In practice, this means that analyzing brain data generally requires a complex engineering pipeline crafted to realign brain signals on a template brain. In previous studies, brain decoders were trained on a small number of recordings to predict a limited set of speech features, such as part-of-speech categories or words from a small vocabulary. For our research, we designed a new subject-embedding layer, which is trained end-to-end to align all brain recordings in a common space.

To decode speech from noninvasive brain signals, we train a model with contrastive learning to align speech and its corresponding brain activity.

Finally, our architecture learns to align the output of this brain model to the deep representations of the speech sounds that were presented to the participants. In our previous work, we used wav2vec 2.0 to show that this speech algorithm automatically learns to generate representations of speech that align with those of the brain. The emergence of “brainlike” representations of speech in wav2vec 2.0 made it a natural choice to build our decoder, because it helps to know which representations we should try to extract from brain signals.

We recently showed that the activations of wav2vec 2.0 (left) map onto the brain (right) in response to the same speech sounds. The representations of the first layers of this algorithm (cool colors) map onto the early auditory cortex, whereas the deepest layers map onto high-level brain regions (e.g. prefrontal and parietal cortex).

After training, our system performs what’s known as zero-shot classification: Given a snippet of brain activity, it can determine from a large pool of new audio clips which one the person actually heard. From there, the algorithm infers the words the person has most likely heard. This is an exciting step because it shows AI can successfully learn to decode noisy and variable noninvasive recordings of brain activity when speech is perceived. The next step is to see whether we can extend this model to directly decode speech from brain activity without needing the pool of audio clips, i.e., to move toward a safe and versatile speech decoder.

Our analyses further show that several components of our algorithm, including the use of wav2vec 2.0 and the subject layer, were beneficial to decoding performance. Furthermore, we show that our algorithm improves with the number of EEG and MEG recordings. Practically, this means that our approach benefits from the pulling of large amounts of heterogeneous data, and could, in principle, help improve the decoding of small datasets. This is important because, in many cases, it can be hard to collect a lot of data for a given participant. For example, it isn’t practical to require patients to spend dozens of hours in a scanner to check whether the system works for them. Instead, algorithms could be pretrained on large datasets inclusive of many individuals and conditions, and then support the decoding of brain activity for a new patient with little data.

Using AI to understand how the brain works

The results of our research are encouraging because they show that self-supervised trained AI can successfully decode perceived speech from noninvasive recordings of brain activity, despite the noise and variability inherent in those data. These results are only a first step, however. In this work, we focused on decoding speech perception, but the ultimate goal of enabling patient communication will require extending this work to speech production. This line of research could even reach beyond assisting patients to potentially include enabling new ways of interacting with computers.

More generally, our work is a part of the broader effort by the scientific community to use AI to better understand the human brain. We’re sharing this research openly to accelerate progress on the challenges still ahead. We look forward to working together and contributing to the research community in this area.

Learn more by reading our paper about decoding speech from noninvasive brain recordings.

We’d like to acknowledge contributions to this research from Alexandre Défossez, Charlotte Caucheteux, Ori Kabeli, and Jérémy Rapin.

Data were provided (in part) by the Donders Institute for Brain, Cognition and Behaviour; New York University; the University of Michigan; Trinity College Dublin; and the University of Rochester."
Meta_Blog,https://ai.meta.com/blog/implicitron-a-new-modular-extensible-framework-for-neural-implicit-representations-in-pytorch3d/,,"Implicitron: A new modular, extensible framework for neural implicit representations in PyTorch3D","Something Went Wrong We're having trouble playing this video. Learn more

What the research is

Rapid advances in neural implicit representation are opening up exciting new possibilities for augmented reality experiences. This computer vision technique can seamlessly combine real and virtual objects in augmented reality — without requiring large amounts of data to learn from and without being limited to just a few points of view. It does this by learning a representation of a 3D object or scene using a sparse set of combined images of that object or scene from arbitrary viewpoints. Unlike traditional 3D representations such as meshes or point clouds, this newer approach represents objects as a continuous function, which allows for more accurate reconstruction of shapes with complex geometries as well as higher color reconstruction accuracy.

Meta AI is now releasing Implicitron, a modular framework within our popular open source PyTorch3D library, created and released to advance research on implicit neural representation. Implicitron provides abstractions and implementations of popular implicit representations and rendering components to allow for easy experimentation.

This research area is still in its nascent phase, with new variants regularly emerging and no clear method of choice. After the introduction of NeRF, more than 50 variants of this method for synthesizing novel views of complex scenes have been published in the past year alone. Implicitron now makes it easy to evaluate variations, combinations, and modifications of these methods with a common codebase that doesn’t require expertise in 3D or graphics.

Something Went Wrong We're having trouble playing this video. Learn more

These 3D reconstructions were generated by five different models, each available in Implicitron.

How it works:

Most current neural implicit reconstruction methods create real-time photorealistic renderings via ray marching. In ray marching, rays are emitted from the rendering camera and 3D points are sampled along these rays. An implicit shape function (which represents the shape and appearance of the scene) then evaluates density or distance to the surface at the sampled ray points. A renderer then marches along the ray points to find the first intersection between the scene’s surface and the ray in order to render image pixels. Lastly, the loss functions or discrepancy between generated and ground-truth images are computed, along with other metrics.

With this generic structure in mind, Meta has created modular and composable implementations of each component. This includes the RaySampler and PointSampler classes responsible for sampling rays and ray points. The ray points can be encoded with a HarmonicEmbedding class (implementing the NeRF’s Positional Embedding) or with a ViewSampler, which samples image features at the 2D locations of 3D point projections (PixelNeRF, NeRFormer). Given per-point feature encodings, Implicitron can leverage one of several implicit shape architectures (NeRF’s MLP, IDR’s FeatureField, SRN's implicit raymarcher) that generate the implicit shape. A renderer (MultiplassEmissionAbsorptionRenderer, LSTMRenderer, RayTracing) then converts the latter to an image. The training process is then supervised with several losses, including MSE, PSNR, and Huber losses between optionally masked images, segmentation masks, depth maps, and method-specific regularizers like Eikonal loss, Beta prior on the predicted mask, and TV regularizer for voxel grids.

This modular architecture allows people using the framework to easily combine the contributions of different papers and replace specific components to test new ideas. As the flagship end-to-end example, the Implicitron framework implements a state-of-the-art method for generalizable category-based new view synthesis, as proposed in our recent Common Objects in 3D work. This extends NeRF with a trainable view-pooling layer based on Transformer architecture.

Meta has also developed additional components to help make experimentation and extensibility easier. This includes a plug-in and configuration system that enables user-defined implementations of the components and flexible configurations that enable switching between implementations. It also includes a trainer class that uses PyTorch Lightning for the launching of new experiments.

Why it matters

Just as Detectron2 has become the go-to framework for implementing and benchmarking object detection methods on a variety of data sets, Implicitron aims to serve as a cornerstone for conducting research in the field of neural implicit representation and rendering. This lowers the barrier to entry into this field and enables vast new opportunities for exploration.

It is crucial to have better tools that can take image data and create accurate 3D reconstructions in order to accelerate research in AR/VR. This allows for useful real-world applications, like enabling people to try clothing on virtually when shopping in AR and VR or to relive memorable moments from different perspectives. This work complements Meta’s advances in Detectron2, another Meta AI open source platform that enables object detection, segmentation, and other visual recognition tasks; Common Objects in 3D; state-of-the-art 3D content understanding; self-supervised learning and Transformers; and convolutional neural nets.

By integrating this framework within the popular PyTorch3D library for 3D deep learning, already widely used by researchers in the field, Meta aims to give people using the framework a way to easily install and import components from Implicitron into their projects without needing to reimplement or copy the code.

PyTorch3D Code

Common objects in 3D

We'd like to acknowledge the contributions to Implicitron from the larger group of researchers who work on PyTorch3D here at Meta AI."
Meta_Blog,https://ai.meta.com/blog/implicitron-a-new-modular-extensible-framework-for-neural-implicit-representations-in-pytorch3d/,,"Implicitron: A new modular, extensible framework for neural implicit representations in PyTorch3D","Something Went Wrong We're having trouble playing this video. Learn more

What the research is

Rapid advances in neural implicit representation are opening up exciting new possibilities for augmented reality experiences. This computer vision technique can seamlessly combine real and virtual objects in augmented reality — without requiring large amounts of data to learn from and without being limited to just a few points of view. It does this by learning a representation of a 3D object or scene using a sparse set of combined images of that object or scene from arbitrary viewpoints. Unlike traditional 3D representations such as meshes or point clouds, this newer approach represents objects as a continuous function, which allows for more accurate reconstruction of shapes with complex geometries as well as higher color reconstruction accuracy.

Meta AI is now releasing Implicitron, a modular framework within our popular open source PyTorch3D library, created and released to advance research on implicit neural representation. Implicitron provides abstractions and implementations of popular implicit representations and rendering components to allow for easy experimentation.

This research area is still in its nascent phase, with new variants regularly emerging and no clear method of choice. After the introduction of NeRF, more than 50 variants of this method for synthesizing novel views of complex scenes have been published in the past year alone. Implicitron now makes it easy to evaluate variations, combinations, and modifications of these methods with a common codebase that doesn’t require expertise in 3D or graphics.

Something Went Wrong We're having trouble playing this video. Learn more

These 3D reconstructions were generated by five different models, each available in Implicitron.

How it works:

Most current neural implicit reconstruction methods create real-time photorealistic renderings via ray marching. In ray marching, rays are emitted from the rendering camera and 3D points are sampled along these rays. An implicit shape function (which represents the shape and appearance of the scene) then evaluates density or distance to the surface at the sampled ray points. A renderer then marches along the ray points to find the first intersection between the scene’s surface and the ray in order to render image pixels. Lastly, the loss functions or discrepancy between generated and ground-truth images are computed, along with other metrics.

With this generic structure in mind, Meta has created modular and composable implementations of each component. This includes the RaySampler and PointSampler classes responsible for sampling rays and ray points. The ray points can be encoded with a HarmonicEmbedding class (implementing the NeRF’s Positional Embedding) or with a ViewSampler, which samples image features at the 2D locations of 3D point projections (PixelNeRF, NeRFormer). Given per-point feature encodings, Implicitron can leverage one of several implicit shape architectures (NeRF’s MLP, IDR’s FeatureField, SRN's implicit raymarcher) that generate the implicit shape. A renderer (MultiplassEmissionAbsorptionRenderer, LSTMRenderer, RayTracing) then converts the latter to an image. The training process is then supervised with several losses, including MSE, PSNR, and Huber losses between optionally masked images, segmentation masks, depth maps, and method-specific regularizers like Eikonal loss, Beta prior on the predicted mask, and TV regularizer for voxel grids.

This modular architecture allows people using the framework to easily combine the contributions of different papers and replace specific components to test new ideas. As the flagship end-to-end example, the Implicitron framework implements a state-of-the-art method for generalizable category-based new view synthesis, as proposed in our recent Common Objects in 3D work. This extends NeRF with a trainable view-pooling layer based on Transformer architecture.

Meta has also developed additional components to help make experimentation and extensibility easier. This includes a plug-in and configuration system that enables user-defined implementations of the components and flexible configurations that enable switching between implementations. It also includes a trainer class that uses PyTorch Lightning for the launching of new experiments.

Why it matters

Just as Detectron2 has become the go-to framework for implementing and benchmarking object detection methods on a variety of data sets, Implicitron aims to serve as a cornerstone for conducting research in the field of neural implicit representation and rendering. This lowers the barrier to entry into this field and enables vast new opportunities for exploration.

It is crucial to have better tools that can take image data and create accurate 3D reconstructions in order to accelerate research in AR/VR. This allows for useful real-world applications, like enabling people to try clothing on virtually when shopping in AR and VR or to relive memorable moments from different perspectives. This work complements Meta’s advances in Detectron2, another Meta AI open source platform that enables object detection, segmentation, and other visual recognition tasks; Common Objects in 3D; state-of-the-art 3D content understanding; self-supervised learning and Transformers; and convolutional neural nets.

By integrating this framework within the popular PyTorch3D library for 3D deep learning, already widely used by researchers in the field, Meta aims to give people using the framework a way to easily install and import components from Implicitron into their projects without needing to reimplement or copy the code.

PyTorch3D Code

Common objects in 3D

We'd like to acknowledge the contributions to Implicitron from the larger group of researchers who work on PyTorch3D here at Meta AI."
Meta_Blog,https://ai.meta.com/blog/blenderbot-3-a-175b-parameter-publicly-available-chatbot-that-improves-its-skills-and-safety-over-time/,,"BlenderBot 3: A 175B parameter, publicly available chatbot that improves its skills and safety over time","When we launched BlenderBot 3 a few days ago, we talked extensively about the promise and challenges that come with such a public demo, including the possibility that it could result in problematic or offensive language. While it is painful to see some of these offensive responses, public demos like this are important for building truly robust conversational AI systems and bridging the clear gap that exists today before such systems can be productionized.

We’ve already collected 70K conversations from the public demo, which we will use to improve BlenderBot 3. From feedback provided by 25 percent of participants on 260K bot messages, 0.11 percent of BlenderBot’s responses were flagged as inappropriate, 1.36 percent as nonsensical, and 1 percent as off-topic. We continue to believe that the way to advance AI is through open and reproducible research at scale. We also believe that progress is best served by inviting a wide and diverse community to participate. Thanks for all your input (and patience!) as our chatbots improve.

And finally, as we push ahead with this project, it’s important to note that we have invested heavily in conversational safety research and taken steps to protect people trying the demo. We require that everyone who uses the demo be over 18, that they acknowledge they understand it’s for research and entertainment purposes only and that it can make untrue or offensive statements, and that they agree not to intentionally trigger the bot to make offensive statements.

– Joelle Pineau, Managing Director of Fundamental AI Research at Meta. August 8, 2022.

Today, we’re announcing that Meta AI has built and released BlenderBot 3, the first 175B-parameter, publicly available chatbot complete with model weights, code, datasets, and model cards. We’ve deployed it in a live interactive conversational AI demo here.

BlenderBot 3 is capable of searching the internet to chat about virtually any topic, and it’s designed to learn how to improve its skills and safety through natural conversations and feedback from people “in the wild.” Most previous publicly available datasets are typically collected through research studies with annotators that can’t reflect the diversity of the real world.

We combined two recently developed machine learning techniques, SeeKeR and Director, to build conversational models that learn from interactions and feedback. A focal point of our research is to ensure appropriate safety measures for the chatbot during this process. We developed new techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses.

Initial experiments already show that as more people interact with the model, the more it learns from its experiences and the better and safer it becomes over time — though safety remains an open problem.

We are committed to sharing organic conversational data collected from the interactive demo system as well as model snapshots in the future. We hope this work will help the wider AI community spur progress in building ever-improving intelligent AI systems that can interact with people in safe and helpful ways.

BlenderBot 3 can talk about almost any topic.

To build AI systems that can interact with people in more intelligent, safer, and useful ways, we need to teach them to adapt to our ever-changing needs. Over the past few years, Meta AI has made exciting progress in building smarter conversational AI systems with BlenderBot and its successor, BlenderBot 2. These conversational agents broke ground as the first unified system trained to blend different conversational skills — like personality, empathy, and knowledge — to have long-term memory, and to search the internet to carry out meaningful conversations.

So far, existing open research in conversational AI — including ours — has focused on human-model conversations with annotators in a controlled environment. But researchers can’t possibly predict or simulate every conversational scenario in research settings alone. The AI field is still far from truly intelligent AI systems that can understand, engage, and chat with us like other humans can. In order to build models that are more adaptable to real-world environments, chatbots need to learn from a diverse, wide-ranging perspective with people “in the wild.” These are currently open problems and require novel research to be conducted by the community.

As a step in this direction, we’ve built and deployed a live demo of BlenderBot 3, our state-of-the-art conversational agent that can converse naturally with humans, who can then provide feedback to the model on how to improve its responses. (Demo is only available in the U.S.) We will be sharing data from these interactions, and we’ve shared the BlenderBot 3 model and model cards with the scientific community to help advance research in conversational AI.

BlenderBot 3 delivers superior performance because it’s built from Meta AI’s publicly available OPT-175B language model — approximately 58 times the size of BlenderBot 2. The model itself has a modular design, which is a subsequent version of our recently introduced SeeKeR architecture. With the release of the BlenderBot 3 demo, our goal is to help the wider AI community build models that can learn how to interact with people in safe and constructive ways. Our initial experiments show that we can indeed make our models significantly better by enabling them to learn from their experience.

We take the safety of our conversational agents seriously, particularly because all conversational AI agents are known to sometimes mimic and generate unsafe, biased, or otherwise offensive utterances. As part of our ongoing commitment to improve the responsibility of AI systems, we’ve conducted large-scale studies, co-organized workshops, and developed new techniques to create safeguards for our live demo.

We believe that progress has always been cumulative, and researchers can’t overcome the current safety challenges of conversational agents without collaborative research and open science. The demo we are releasing today is not just showcasing our research; it’s also part of the research. Crucially, by collecting and sharing the conversational data from BlenderBot 3, the broader AI community can analyze and build on the feedback we collect to make models more responsible.

The promise (and challenge) of chatting with humans in the wild

Since much of the existing open conversational research has had bots engaging with people in controlled environments, it’s important for researchers to measure how well models can naturally engage humans by evaluating them “in the wild.” Our live, interactive, public demo enables BlenderBot 3 to learn from organic interactions with all kinds of people. We encourage adults in the United States to try the demo, conduct natural conversations about topics of interest, and share their responses to help advance research.

This “in the wild” collection allows for longer, more diverse conversations, as well as more varied feedback. For example, in our demo, people can react to each chat message by clicking either the thumbs-up or thumbs-down icons. The latter allows people to specify why they disliked the message, whether it was off-topic, nonsensical, rude, spam-like, or other. People can submit free-form feedback in the chat itself as well. A demo also gives us the opportunity to offer insights to the public about how AI works. Our deployment has explainability features, including displaying long-term memories the bot has about the user and its own persona, showing message-level inputs a model used (like search results or model memory), and highlighting when the model detected and avoided an inappropriate response.

A live demo is not without challenges, however. It is difficult for a bot to keep everyone engaged while talking about arbitrary topics and to ensure that it never uses offensive or toxic language. Avoiding sensitive topics, for example, could lead to responses that may seem off-topic or otherwise less engaging. We believe that long-term safety is an important component of quality chatbots — even if it means sacrificing engagingness in the short term. Developing continual learning techniques also poses extra challenges, as not all people who use chatbots are well intentioned, and some may employ toxic or otherwise harmful language that we do not want BlenderBot 3 to mimic. Our new research attempts to address these issues.

Something Went Wrong We're having trouble playing this video. Learn more

Developing a skillful and safe chatbot that improves itself

BlenderBot 3 is built with all the skills of its predecessors, which include internet search, long-term memory, personality, and empathy. To improve upon its state-of-the-art engagingness, we collected a new public dataset consisting of over 20,000 human-bot conversations predicated on over 1,000 skills. We trained BlenderBot 3 to learn from conversations to improve the diverse body of skills that people find most important – from talking about healthy food recipes to finding child-friendly amenities in the city.

When the conversational response of the bot is unsatisfactory, we collect feedback from the conversationalist. Using this data we can improve the model, so it does not repeat its mistakes. We can then redeploy it for continued conversation, iterating the approach to search for more mistakes, and eventually improving it further.

Our approach uses a new learning algorithm called Director, which generates responses using two mechanisms: language modeling and classification. Language modeling provides the model with the most relevant and fluent responses (based on training data) and then the classifier informs it what is right and wrong (based on human feedback). To generate a sentence, the language modeling and classifier mechanisms must agree.

Using data that indicates good and bad responses, we can train the classifier to penalize low-quality, toxic, contradictory, or repetitive statements, and statements that are generally unhelpful. In our tests, the Director approach was better than regular language modeling, reranking approaches, and reward-based learning.

We also needed to address the fact that not all people who use chatbots or give feedback are well intentioned. Therefore, we developed new learning algorithms that aim to distinguish between helpful responses and harmful examples. During the learning procedure, the techniques either filter or down-weight feedback that looks suspicious. We find that a method that takes into account the entire user behavior across conversations — which learns to trust some users — improves learning compared with standard training procedures.

We have also extended our existing state-of-the art dialog safety techniques (which include safety classifiers, filters, and unit tests) with a new safety recovery technique. With the new technique, BlenderBot 3 attempts to respond to feedback about challenging conversations with responses that are more likely to foster a civil conversation. While safety issues are not completely solved, our goal with the strategies described above is to help our models learn how to be more responsible through feedback on rude or offensive responses.

Putting BlenderBot 3 to the test

Given the strong performance of BlenderBot and BlenderBot 2 relative to other chatbots, such as Meena and DialoGPT, we benchmarked the conversational ability of BlenderBot 3 against its predecessors.

We found that, compared with BlenderBot 2, BlenderBot 3 provides a 31 percent improvement in overall rating on conversational tasks, as evaluated by human judgments. It is also judged to be twice as knowledgeable, while being factually incorrect 47 percent less of the time. Compared with GPT3, on topical questions it is found to be more up-to-date 82 percent of the time and more specific 76 percent of the time. Additionally, we evaluated BlenderBot 3 on a range of existing benchmark conversational datasets and found improvements in every area. See the full technical report here.

Collectively, these results show that BlenderBot 3 is better equipped to demonstrate the skills desired by the people who converse with it. Nevertheless, there are still areas where we can improve. For example, 1.1 percent of users flagged its responses as incorrect or nonsensical, 1.2 percent as being off-topic or ignoring the topic, 0.12 percent as “spammy,” and 0.46 percent as having other issues.

We also put BlenderBot 3 through safety and bias tests and found that our raw model (before safety mitigations) is level with similar models, such as BlenderBot 2, but improves on pretrained language models such as our own OPT-175B. We report a full breakdown of these metrics, released by Meta AI and other labs, in our technical report and our released model card.

The most stringent safety test was deploying it to our new, web-based live demo, which measures the performance of BlenderBot 3 in natural conversations with real people. We found that 0.16 percent of its responses were flagged as rude or inappropriate. Narrowing the gap to an ideal 0.0 percent requires both user-level personalization and a tricky balance between safety and engagingness (when a bot senses a sensitive topic, it tries to change the subject).

Our research goal is to collect and release conversational feedback data that we and the broader AI research community can leverage over time to eventually find new ways for conversational AI systems to optimize both safety and engagingness for everyone who uses them.

Driving conversational AI forward

Progress in the field of AI is dependent to a large extent on reproducibility, or the opportunity for the wider AI research community to build on the best available AI technology. Therefore, releasing chatbot models and datasets is key to gaining complete, reliable insights into how and why they work, the potential they hold, and their limitations. We believe that the future of AI involves continually learning and evolving agents, which in turn must be continually evaluated, in order to find a path to better and better systems in the long term. While BlenderBot 3 significantly advances state-of-the-art publicly available chatbots, it — like all conversational AI systems today — is certainly not at a human level, and it is still occasionally incorrect, inconsistent, and off-topic, or generates otherwise unsatisfactory responses. But we are buoyed that our deployment of BlenderBot 3 and the accompanying program of continuous data collection can provide a path to resolving these issues in reproducible research chatbots and eventually lead to useful production applications, such as virtual assistants.

As more and more people interact with the demo, we will aim to improve our models using their feedback, and release deployment data and updated model snapshots, for the benefit of the wider AI community. Together, we can advance responsible conversational AI research in the hope of one day building AI-powered computers that everyone can chat with in genuinely helpful and interesting ways.

Talk to BlenderBot 3

Read the papers here

Get the code, datasets, and smaller models here.

Request access to the largest model here.

Note: Access to the 175B parameter model will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; along with global industry research laboratories.

This work was undertaken by a team that includes Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston.

This blog post was originally published on August 5, 2022, and then updated on August 9 with a note from Joelle Pineau, Managing Director of Fundamental AI Research at Meta."
Meta_Blog,https://ai.meta.com/blog/blenderbot-3-a-175b-parameter-publicly-available-chatbot-that-improves-its-skills-and-safety-over-time/,,"BlenderBot 3: A 175B parameter, publicly available chatbot that improves its skills and safety over time","When we launched BlenderBot 3 a few days ago, we talked extensively about the promise and challenges that come with such a public demo, including the possibility that it could result in problematic or offensive language. While it is painful to see some of these offensive responses, public demos like this are important for building truly robust conversational AI systems and bridging the clear gap that exists today before such systems can be productionized.

We’ve already collected 70K conversations from the public demo, which we will use to improve BlenderBot 3. From feedback provided by 25 percent of participants on 260K bot messages, 0.11 percent of BlenderBot’s responses were flagged as inappropriate, 1.36 percent as nonsensical, and 1 percent as off-topic. We continue to believe that the way to advance AI is through open and reproducible research at scale. We also believe that progress is best served by inviting a wide and diverse community to participate. Thanks for all your input (and patience!) as our chatbots improve.

And finally, as we push ahead with this project, it’s important to note that we have invested heavily in conversational safety research and taken steps to protect people trying the demo. We require that everyone who uses the demo be over 18, that they acknowledge they understand it’s for research and entertainment purposes only and that it can make untrue or offensive statements, and that they agree not to intentionally trigger the bot to make offensive statements.

– Joelle Pineau, Managing Director of Fundamental AI Research at Meta. August 8, 2022.

Today, we’re announcing that Meta AI has built and released BlenderBot 3, the first 175B-parameter, publicly available chatbot complete with model weights, code, datasets, and model cards. We’ve deployed it in a live interactive conversational AI demo here.

BlenderBot 3 is capable of searching the internet to chat about virtually any topic, and it’s designed to learn how to improve its skills and safety through natural conversations and feedback from people “in the wild.” Most previous publicly available datasets are typically collected through research studies with annotators that can’t reflect the diversity of the real world.

We combined two recently developed machine learning techniques, SeeKeR and Director, to build conversational models that learn from interactions and feedback. A focal point of our research is to ensure appropriate safety measures for the chatbot during this process. We developed new techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses.

Initial experiments already show that as more people interact with the model, the more it learns from its experiences and the better and safer it becomes over time — though safety remains an open problem.

We are committed to sharing organic conversational data collected from the interactive demo system as well as model snapshots in the future. We hope this work will help the wider AI community spur progress in building ever-improving intelligent AI systems that can interact with people in safe and helpful ways.

BlenderBot 3 can talk about almost any topic.

To build AI systems that can interact with people in more intelligent, safer, and useful ways, we need to teach them to adapt to our ever-changing needs. Over the past few years, Meta AI has made exciting progress in building smarter conversational AI systems with BlenderBot and its successor, BlenderBot 2. These conversational agents broke ground as the first unified system trained to blend different conversational skills — like personality, empathy, and knowledge — to have long-term memory, and to search the internet to carry out meaningful conversations.

So far, existing open research in conversational AI — including ours — has focused on human-model conversations with annotators in a controlled environment. But researchers can’t possibly predict or simulate every conversational scenario in research settings alone. The AI field is still far from truly intelligent AI systems that can understand, engage, and chat with us like other humans can. In order to build models that are more adaptable to real-world environments, chatbots need to learn from a diverse, wide-ranging perspective with people “in the wild.” These are currently open problems and require novel research to be conducted by the community.

As a step in this direction, we’ve built and deployed a live demo of BlenderBot 3, our state-of-the-art conversational agent that can converse naturally with humans, who can then provide feedback to the model on how to improve its responses. (Demo is only available in the U.S.) We will be sharing data from these interactions, and we’ve shared the BlenderBot 3 model and model cards with the scientific community to help advance research in conversational AI.

BlenderBot 3 delivers superior performance because it’s built from Meta AI’s publicly available OPT-175B language model — approximately 58 times the size of BlenderBot 2. The model itself has a modular design, which is a subsequent version of our recently introduced SeeKeR architecture. With the release of the BlenderBot 3 demo, our goal is to help the wider AI community build models that can learn how to interact with people in safe and constructive ways. Our initial experiments show that we can indeed make our models significantly better by enabling them to learn from their experience.

We take the safety of our conversational agents seriously, particularly because all conversational AI agents are known to sometimes mimic and generate unsafe, biased, or otherwise offensive utterances. As part of our ongoing commitment to improve the responsibility of AI systems, we’ve conducted large-scale studies, co-organized workshops, and developed new techniques to create safeguards for our live demo.

We believe that progress has always been cumulative, and researchers can’t overcome the current safety challenges of conversational agents without collaborative research and open science. The demo we are releasing today is not just showcasing our research; it’s also part of the research. Crucially, by collecting and sharing the conversational data from BlenderBot 3, the broader AI community can analyze and build on the feedback we collect to make models more responsible.

The promise (and challenge) of chatting with humans in the wild

Since much of the existing open conversational research has had bots engaging with people in controlled environments, it’s important for researchers to measure how well models can naturally engage humans by evaluating them “in the wild.” Our live, interactive, public demo enables BlenderBot 3 to learn from organic interactions with all kinds of people. We encourage adults in the United States to try the demo, conduct natural conversations about topics of interest, and share their responses to help advance research.

This “in the wild” collection allows for longer, more diverse conversations, as well as more varied feedback. For example, in our demo, people can react to each chat message by clicking either the thumbs-up or thumbs-down icons. The latter allows people to specify why they disliked the message, whether it was off-topic, nonsensical, rude, spam-like, or other. People can submit free-form feedback in the chat itself as well. A demo also gives us the opportunity to offer insights to the public about how AI works. Our deployment has explainability features, including displaying long-term memories the bot has about the user and its own persona, showing message-level inputs a model used (like search results or model memory), and highlighting when the model detected and avoided an inappropriate response.

A live demo is not without challenges, however. It is difficult for a bot to keep everyone engaged while talking about arbitrary topics and to ensure that it never uses offensive or toxic language. Avoiding sensitive topics, for example, could lead to responses that may seem off-topic or otherwise less engaging. We believe that long-term safety is an important component of quality chatbots — even if it means sacrificing engagingness in the short term. Developing continual learning techniques also poses extra challenges, as not all people who use chatbots are well intentioned, and some may employ toxic or otherwise harmful language that we do not want BlenderBot 3 to mimic. Our new research attempts to address these issues.

Something Went Wrong We're having trouble playing this video. Learn more

Developing a skillful and safe chatbot that improves itself

BlenderBot 3 is built with all the skills of its predecessors, which include internet search, long-term memory, personality, and empathy. To improve upon its state-of-the-art engagingness, we collected a new public dataset consisting of over 20,000 human-bot conversations predicated on over 1,000 skills. We trained BlenderBot 3 to learn from conversations to improve the diverse body of skills that people find most important – from talking about healthy food recipes to finding child-friendly amenities in the city.

When the conversational response of the bot is unsatisfactory, we collect feedback from the conversationalist. Using this data we can improve the model, so it does not repeat its mistakes. We can then redeploy it for continued conversation, iterating the approach to search for more mistakes, and eventually improving it further.

Our approach uses a new learning algorithm called Director, which generates responses using two mechanisms: language modeling and classification. Language modeling provides the model with the most relevant and fluent responses (based on training data) and then the classifier informs it what is right and wrong (based on human feedback). To generate a sentence, the language modeling and classifier mechanisms must agree.

Using data that indicates good and bad responses, we can train the classifier to penalize low-quality, toxic, contradictory, or repetitive statements, and statements that are generally unhelpful. In our tests, the Director approach was better than regular language modeling, reranking approaches, and reward-based learning.

We also needed to address the fact that not all people who use chatbots or give feedback are well intentioned. Therefore, we developed new learning algorithms that aim to distinguish between helpful responses and harmful examples. During the learning procedure, the techniques either filter or down-weight feedback that looks suspicious. We find that a method that takes into account the entire user behavior across conversations — which learns to trust some users — improves learning compared with standard training procedures.

We have also extended our existing state-of-the art dialog safety techniques (which include safety classifiers, filters, and unit tests) with a new safety recovery technique. With the new technique, BlenderBot 3 attempts to respond to feedback about challenging conversations with responses that are more likely to foster a civil conversation. While safety issues are not completely solved, our goal with the strategies described above is to help our models learn how to be more responsible through feedback on rude or offensive responses.

Putting BlenderBot 3 to the test

Given the strong performance of BlenderBot and BlenderBot 2 relative to other chatbots, such as Meena and DialoGPT, we benchmarked the conversational ability of BlenderBot 3 against its predecessors.

We found that, compared with BlenderBot 2, BlenderBot 3 provides a 31 percent improvement in overall rating on conversational tasks, as evaluated by human judgments. It is also judged to be twice as knowledgeable, while being factually incorrect 47 percent less of the time. Compared with GPT3, on topical questions it is found to be more up-to-date 82 percent of the time and more specific 76 percent of the time. Additionally, we evaluated BlenderBot 3 on a range of existing benchmark conversational datasets and found improvements in every area. See the full technical report here.

Collectively, these results show that BlenderBot 3 is better equipped to demonstrate the skills desired by the people who converse with it. Nevertheless, there are still areas where we can improve. For example, 1.1 percent of users flagged its responses as incorrect or nonsensical, 1.2 percent as being off-topic or ignoring the topic, 0.12 percent as “spammy,” and 0.46 percent as having other issues.

We also put BlenderBot 3 through safety and bias tests and found that our raw model (before safety mitigations) is level with similar models, such as BlenderBot 2, but improves on pretrained language models such as our own OPT-175B. We report a full breakdown of these metrics, released by Meta AI and other labs, in our technical report and our released model card.

The most stringent safety test was deploying it to our new, web-based live demo, which measures the performance of BlenderBot 3 in natural conversations with real people. We found that 0.16 percent of its responses were flagged as rude or inappropriate. Narrowing the gap to an ideal 0.0 percent requires both user-level personalization and a tricky balance between safety and engagingness (when a bot senses a sensitive topic, it tries to change the subject).

Our research goal is to collect and release conversational feedback data that we and the broader AI research community can leverage over time to eventually find new ways for conversational AI systems to optimize both safety and engagingness for everyone who uses them.

Driving conversational AI forward

Progress in the field of AI is dependent to a large extent on reproducibility, or the opportunity for the wider AI research community to build on the best available AI technology. Therefore, releasing chatbot models and datasets is key to gaining complete, reliable insights into how and why they work, the potential they hold, and their limitations. We believe that the future of AI involves continually learning and evolving agents, which in turn must be continually evaluated, in order to find a path to better and better systems in the long term. While BlenderBot 3 significantly advances state-of-the-art publicly available chatbots, it — like all conversational AI systems today — is certainly not at a human level, and it is still occasionally incorrect, inconsistent, and off-topic, or generates otherwise unsatisfactory responses. But we are buoyed that our deployment of BlenderBot 3 and the accompanying program of continuous data collection can provide a path to resolving these issues in reproducible research chatbots and eventually lead to useful production applications, such as virtual assistants.

As more and more people interact with the demo, we will aim to improve our models using their feedback, and release deployment data and updated model snapshots, for the benefit of the wider AI community. Together, we can advance responsible conversational AI research in the hope of one day building AI-powered computers that everyone can chat with in genuinely helpful and interesting ways.

Talk to BlenderBot 3

Read the papers here

Get the code, datasets, and smaller models here.

Request access to the largest model here.

Note: Access to the 175B parameter model will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; along with global industry research laboratories.

This work was undertaken by a team that includes Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston.

This blog post was originally published on August 5, 2022, and then updated on August 9 with a note from Joelle Pineau, Managing Director of Fundamental AI Research at Meta."
Meta_Blog,https://ai.meta.com/blog/efficient-accurate-object-detection-for-hundreds-of-uncommon-object-classes/,,"Efficient, accurate object detection for hundreds of uncommon object classes","What the research is:

Meta AI is sharing new research on using Vision Transformers (ViTs) for object detection. Our approach, ViTDet, outperforms previous alternatives on benchmarks on the Large Vocabulary Instance Segmentation (LVIS) dataset, which was released by Meta AI researchers in 2019 to facilitate research on low-shot object detection. In this task, the model must learn to recognize a much wider variety of objects than conventional computer vision systems can. ViTDet outperforms previous ViT-based models in accurately recognizing objects in the LVIS dataset, which includes not just standard items like tables and chairs, but also bird feeders, wreaths, doughnuts, and much more.

To enable the research community to reproduce and build upon these advancements, we are now releasing the ViTDet code and training recipes as new baselines in our open source Detectron2 object detection library.

How it works:

Over the past year, ViTs have been established as a powerful backbone for visual recognition. Unlike typical convolutional neural networks, the original ViT is a plain, non-hierarchical architecture that maintains a single-scale feature map throughout its processing. Challenges arise when applying ViTs to object detection, however. For example, how can we detect multiscale objects effectively with a plain backbone? And is a ViT too inefficient to use for object detection in high-resolution images?

Unlike existing research, such as Swin and MViTv2, ViTDet uses only plain, nonhierarchical ViT backbones. It builds a simple feature pyramid from the single-scale feature map output by the ViT and primarily uses simple, nonoverlapping window attention to extract features from high-resolution images efficiently. This design decouples the pretraining of ViT from the fine-tuning demands of detection and thus enables the object detector to benefit from readily available pretrained masked autoencoder (MAE) models.

ViTDet builds a simple feature pyramid from the output of a plain, nonhierarchical vision transformer. The decoupling of its detector-specific designs from the ViT backbone enables it to benefit from Masked Autoencoder (MAE) pretraining.

We start by training ViTDet detectors following the Mask R-CNN framework with ViT backbones of base (B), large (L), and huge (H) sizes. We evaluate two pretraining strategies: supervised pretraining, and self-supervised MAE pretraining (supervised pretrained ViT-H model weights are not available). We measure the accuracy on LVIS by average precision of masks (Mask AP) and average precision of masks on the rare categories (Mask AP-rare). Achieving good performance on rare categories is challenging as there are 10 or fewer training samples per rare category. We have two primary observations:

Compared with supervised pretraining, MAE pretraining delivers improved LVIS results as we scale ViTDet’s ViT backbone size. We observe strong Mask AP gains for rare category detection, which is at the heart of the low-shot detection problem posed by LVIS.

Mask AP can be improved from 38.1 to 43.5 (+5.4) when scaling MAE pretrained ViT backbone from base to large while there is only +1.1 Mask AP gain when scaling supervised pretrained ViT backbone. ViTDet with ViT-H backbone in Mask R-CNN can achieve remarkable 45.9 Mask AP and 37.9 Mask AP-rare.

We also benchmark Mask R-CNN using other recently proposed hierarchical ViT backbones, including Swin and MViTv2. Swin and MViTv2 are pretrained with supervision on ImageNet-1K and ImageNet-21K. We search for optimal recipes separately for each backbone of base (B), large (L), and huge (H) sizes whenever available. Out of all the benchmarked backbones, ViTDet with MAE pretraining has the best scaling behavior and delivers the best performance on LVIS.

Benchmark results on LVIS for object detectors with different backbones in Mask R-CNN. ViTDet-H achieved 45.9 Mask AP with ImageNet-1K self-supervised MAE pretraining.

Why it matters:

Object detection is an important computer vision task with applications ranging from autonomous driving to e-commerce to augmented reality. To make object detection more useful, CV systems need to recognize uncommon objects and objects that appear only very rarely in their training data. With ViTDet, we now see a tipping point that shows LVIS, the benchmarking dataset for low-shot object detection challenge, benefits strongly from larger backbones and better pretraining. We hope that by open-sourcing our newly established strong baselines with ViTDet, we will help the research community to further push the state of the art and to build more effective CV systems.

Read the paper

Benchmark results

Training recipes"
Meta_Blog,https://ai.meta.com/blog/efficient-accurate-object-detection-for-hundreds-of-uncommon-object-classes/,,"Efficient, accurate object detection for hundreds of uncommon object classes","What the research is:

Meta AI is sharing new research on using Vision Transformers (ViTs) for object detection. Our approach, ViTDet, outperforms previous alternatives on benchmarks on the Large Vocabulary Instance Segmentation (LVIS) dataset, which was released by Meta AI researchers in 2019 to facilitate research on low-shot object detection. In this task, the model must learn to recognize a much wider variety of objects than conventional computer vision systems can. ViTDet outperforms previous ViT-based models in accurately recognizing objects in the LVIS dataset, which includes not just standard items like tables and chairs, but also bird feeders, wreaths, doughnuts, and much more.

To enable the research community to reproduce and build upon these advancements, we are now releasing the ViTDet code and training recipes as new baselines in our open source Detectron2 object detection library.

How it works:

Over the past year, ViTs have been established as a powerful backbone for visual recognition. Unlike typical convolutional neural networks, the original ViT is a plain, non-hierarchical architecture that maintains a single-scale feature map throughout its processing. Challenges arise when applying ViTs to object detection, however. For example, how can we detect multiscale objects effectively with a plain backbone? And is a ViT too inefficient to use for object detection in high-resolution images?

Unlike existing research, such as Swin and MViTv2, ViTDet uses only plain, nonhierarchical ViT backbones. It builds a simple feature pyramid from the single-scale feature map output by the ViT and primarily uses simple, nonoverlapping window attention to extract features from high-resolution images efficiently. This design decouples the pretraining of ViT from the fine-tuning demands of detection and thus enables the object detector to benefit from readily available pretrained masked autoencoder (MAE) models.

ViTDet builds a simple feature pyramid from the output of a plain, nonhierarchical vision transformer. The decoupling of its detector-specific designs from the ViT backbone enables it to benefit from Masked Autoencoder (MAE) pretraining.

We start by training ViTDet detectors following the Mask R-CNN framework with ViT backbones of base (B), large (L), and huge (H) sizes. We evaluate two pretraining strategies: supervised pretraining, and self-supervised MAE pretraining (supervised pretrained ViT-H model weights are not available). We measure the accuracy on LVIS by average precision of masks (Mask AP) and average precision of masks on the rare categories (Mask AP-rare). Achieving good performance on rare categories is challenging as there are 10 or fewer training samples per rare category. We have two primary observations:

Compared with supervised pretraining, MAE pretraining delivers improved LVIS results as we scale ViTDet’s ViT backbone size. We observe strong Mask AP gains for rare category detection, which is at the heart of the low-shot detection problem posed by LVIS.

Mask AP can be improved from 38.1 to 43.5 (+5.4) when scaling MAE pretrained ViT backbone from base to large while there is only +1.1 Mask AP gain when scaling supervised pretrained ViT backbone. ViTDet with ViT-H backbone in Mask R-CNN can achieve remarkable 45.9 Mask AP and 37.9 Mask AP-rare.

We also benchmark Mask R-CNN using other recently proposed hierarchical ViT backbones, including Swin and MViTv2. Swin and MViTv2 are pretrained with supervision on ImageNet-1K and ImageNet-21K. We search for optimal recipes separately for each backbone of base (B), large (L), and huge (H) sizes whenever available. Out of all the benchmarked backbones, ViTDet with MAE pretraining has the best scaling behavior and delivers the best performance on LVIS.

Benchmark results on LVIS for object detectors with different backbones in Mask R-CNN. ViTDet-H achieved 45.9 Mask AP with ImageNet-1K self-supervised MAE pretraining.

Why it matters:

Object detection is an important computer vision task with applications ranging from autonomous driving to e-commerce to augmented reality. To make object detection more useful, CV systems need to recognize uncommon objects and objects that appear only very rarely in their training data. With ViTDet, we now see a tipping point that shows LVIS, the benchmarking dataset for low-shot object detection challenge, benefits strongly from larger backbones and better pretraining. We hope that by open-sourcing our newly established strong baselines with ViTDet, we will help the research community to further push the state of the art and to build more effective CV systems.

Read the paper

Benchmark results

Training recipes"
Meta_Blog,https://ai.meta.com/blog/assessing-fairness-of-our-products-while-protecting-peoples-privacy/,,Assessing fairness of our products while protecting people’s privacy,"Meta is launching a new research project to make progress in assessing and improving our technologies to advance fairness

Adapting a well-established privacy-preserving method called secure multiparty computation (SMPC) to safeguard information, we are asking people on Instagram to take a survey in which they can voluntarily share their race or ethnicity.

This information is important because it’s challenging to address what we can’t measure. With this data, we can begin to better determine how well our products work for people of different races and ethnicities.

Because of the personal nature of this type of demographic information, we are using secure multiparty computation for this initiative to prevent Meta from learning individual people’s survey responses.

The detailed protocol and cryptographic fundamentals provide a strong guarantee that data used in this method can only be processed as documented and cannot be revealed in the clear to any party who should not have access to them.

This project is part of Meta’s broader, long-term effort to help ensure that our products are built responsibly and that our products benefit the people who use them.

Demographic data is often essential in assessing whether a product or process treats all groups fairly. Without this information, many companies and organizations have found it impossible to fully evaluate how well systems perform across different communities, such as for people of different races or ethnicities. But gathering demographic data raises important concerns about how to protect people’s privacy.

To make progress in assessing and improving our technologies to advance fairness and inclusion with respect to race in US, Meta’s Responsible AI, Instagram Equity, and Civil Rights teams are introducing an off-platform, voluntary one-question survey of people who use Instagram in the United States to voluntarily share their race or ethnicity. To safeguard the survey responses, the Responsible AI team at Meta, in consultation with third-party experts, has adapted a data encryption method called secure multiparty computation (SMPC). Using SMPC as documented in this technical paper, Meta cannot at any point access encrypted data linking a specific user to their specific survey responses. As a result of this and additional technical and procedural safeguards, individual survey responses can not and will not be used in our ads system.

Meta is committed to building socially responsible AI systems that treat individuals and communities fairly. In keeping with the recommendations of the civil rights leaders who conducted Meta’s Civil Rights Audit, this new initiative will help us better gauge whether people’s experiences with our technology differ across race.

Multiple layers of privacy protection

Recognizing the personal nature of this data, we worked to keep people’s privacy at the forefront by adapting well-established privacy-preserving methods. In partnership with the privacy-focused technology company Oasis Labs, we developed an approach that draws on SMPC, a subfield of cryptography that permits analysis of encrypted data in aggregate. Data is securely distributed among multiple facilitators, who together can perform computations over the combined encrypted information without exposing their individual shares. First developed in the 1970s and 1980s, SMPC has been used for years in auctions, distributed voting, and statistical analysis. More recently, it was used to evaluate fairness in social impact contexts such as pay equity.

“As a technology partner, we are excited to have co-designed and built the SMPC system to assess fairness in AI models, while protecting users’ privacy,” said Professor Dawn Song, Founder of Oasis Labs. “This is an unprecedented use of such cutting-edge cryptographic techniques for a large-scale, real world use case. We hope this can help inspire the community to expand the use of privacy-preserving technologies, and together build towards responsible AI and responsible data use for a fairer and more inclusive society.”

First, an external partner securely collects and encrypts survey data. A notice at the top of Instagram’s Feed invites people to share information about their race and ethnicity. People who click on the notice are redirected to the survey provider and assigned a freshly generated random identifier (RID). To prevent demographic data from being linked to an individual social media account, the survey provider indexes responses by RID but does not have User IDs, or any identifier they could link back to a particular individual. The survey provider does not know (and will not learn) to whom the RID-associated data belongs. Meta, meanwhile, knows who responded to the survey and holds the mapping from RIDs to User IDs (this is necessary in order to later query internal model data to be assessed using this method), but does not have access to the survey responses with demographic information.

A method called additive secret sharing is central to the privacy-preserving nature of our approach. The survey provider encrypts and splits the responses into fragments, or shares, that sum to reveal the original values. The survey provider then distributes the shares among several third-party facilitators, including Northeastern University, Texas Southern University, a Historically Black College/University (HBCU), and the University of Central Florida, a Hispanic-Serving Institution (HSI). This secret sharing scheme guarantees that no information about the encoded vector can be inferred from any incomplete set of shares. The cryptographic treatment renders individual responses indecipherable to facilitators, so they can neither link any response back to a known individual nor decipher any of the de-identified responses. The only way to recover the private information is for all shares to be combined. Within 30 days after uploading the encrypted and split data, the survey provider deletes raw responses, retaining them only as long as necessary to ensure against accidental loss of data during the next stage.

Once the data is collected, encrypted, and split, it will be available for analysis, without enabling raw survey responses to be shared with Meta. To assess fairness using this data, Meta computes a given metric, such as a classifier’s output for all survey respondents (remember that Meta knows who responded to the survey but not their answers). Meta then encrypts these values and uploads these de-identified data, indexed by RIDs, to the facilitators and identifies the combination of demographic attributes over which it wants to compute the sum of the given metric. The facilitators can then perform the computation over their de-identified response shares and the encrypted values, add noise to ensure differential privacy of the output, and return the combined, de-identified computations in an encrypted format to Meta. When Meta decrypts this response, it learns nothing except for a single number that is the sum of the classifier’s outputs over the relevant demographic attributes, plus any residual differential privacy noise.

""Meta posed an important question about how the latest secure computation techniques could be used to answer questions about the fairness and equity of ML models,"" said Dr. Abhi Shelat, professor of computer sciences at Northeastern University. ""Together with Oasis and Meta, we were able to devise a new cryptographic protocol that enables privacy and measurement efficiently. We are thrilled that we were able to implement this protocol and I am proud to have been able to participate as one of the independent parties in the computation.""

We have built a custom tool to run SMPC analysis. Similar to Bayesian Improved Surname Geocoding, Meta analysts will only be able to leverage SMPC analysis tools after receiving approval to do so through an internal, structured governance process and Privacy Review process. Requests will be reviewed by a committee that includes representatives from Meta’s Civil Rights, Responsible AI, data science, policy, and legal teams. Proposals also must pass Meta’s privacy review, in which internal teams identify and work to mitigate any broad privacy risks they identify. The governance process adds additional layers of assurance that the survey responses will be used only for approved purposes.

People on Instagram will be able to opt-in to share demographic information. Periodically after receiving survey responses, the pollster encrypts and cryptographically splits the responses and uploads the shares to several third party facilitators using a method called additive secret sharing.

External facilitators index encrypted survey response shares by Random ID.

External facilitators index encrypted survey response shares by Random ID and then returns sums, rather than user-level data.

Meta decrypts the sum of the values released by the facilitators and then measures insights by demographic group.

What’s next?

Although its complexity may initially limit the set of operations and techniques we can apply, our method has the potential to work with many types of measurements. We could use it, for example, to determine whether content produced by people of certain races is achieving disproportionate reach, or whether the performance of a model for people from particular ethnic groups is similar across groups. For example, analysis we conduct with this information might help us better understand experiences different communities may have when it comes to how we rank content on Instagram.

Based on what we learn from the implementation of this initial survey on Instagram, we hope to expand this approach to our other platforms and explore how it might be adapted to support fairness work elsewhere. This versatility is a significant benefit of the SMPC approach, and one we intend to highlight for other companies and organizations that are facing similar challenges. We are committed to working in this space for the long run.

This is only one step in a longer journey. In the future, we hope to identify additional privacy-preserving approaches that can support more complex research, such as longitudinal and causal studies. We look forward to engaging with advocates, academic experts, policymakers, and peer companies to continue advancing equity and inclusion in our products while protecting people’s privacy."
Meta_Blog,https://ai.meta.com/blog/assessing-fairness-of-our-products-while-protecting-peoples-privacy/,,Assessing fairness of our products while protecting people’s privacy,"Meta is launching a new research project to make progress in assessing and improving our technologies to advance fairness

Adapting a well-established privacy-preserving method called secure multiparty computation (SMPC) to safeguard information, we are asking people on Instagram to take a survey in which they can voluntarily share their race or ethnicity.

This information is important because it’s challenging to address what we can’t measure. With this data, we can begin to better determine how well our products work for people of different races and ethnicities.

Because of the personal nature of this type of demographic information, we are using secure multiparty computation for this initiative to prevent Meta from learning individual people’s survey responses.

The detailed protocol and cryptographic fundamentals provide a strong guarantee that data used in this method can only be processed as documented and cannot be revealed in the clear to any party who should not have access to them.

This project is part of Meta’s broader, long-term effort to help ensure that our products are built responsibly and that our products benefit the people who use them.

Demographic data is often essential in assessing whether a product or process treats all groups fairly. Without this information, many companies and organizations have found it impossible to fully evaluate how well systems perform across different communities, such as for people of different races or ethnicities. But gathering demographic data raises important concerns about how to protect people’s privacy.

To make progress in assessing and improving our technologies to advance fairness and inclusion with respect to race in US, Meta’s Responsible AI, Instagram Equity, and Civil Rights teams are introducing an off-platform, voluntary one-question survey of people who use Instagram in the United States to voluntarily share their race or ethnicity. To safeguard the survey responses, the Responsible AI team at Meta, in consultation with third-party experts, has adapted a data encryption method called secure multiparty computation (SMPC). Using SMPC as documented in this technical paper, Meta cannot at any point access encrypted data linking a specific user to their specific survey responses. As a result of this and additional technical and procedural safeguards, individual survey responses can not and will not be used in our ads system.

Meta is committed to building socially responsible AI systems that treat individuals and communities fairly. In keeping with the recommendations of the civil rights leaders who conducted Meta’s Civil Rights Audit, this new initiative will help us better gauge whether people’s experiences with our technology differ across race.

Multiple layers of privacy protection

Recognizing the personal nature of this data, we worked to keep people’s privacy at the forefront by adapting well-established privacy-preserving methods. In partnership with the privacy-focused technology company Oasis Labs, we developed an approach that draws on SMPC, a subfield of cryptography that permits analysis of encrypted data in aggregate. Data is securely distributed among multiple facilitators, who together can perform computations over the combined encrypted information without exposing their individual shares. First developed in the 1970s and 1980s, SMPC has been used for years in auctions, distributed voting, and statistical analysis. More recently, it was used to evaluate fairness in social impact contexts such as pay equity.

“As a technology partner, we are excited to have co-designed and built the SMPC system to assess fairness in AI models, while protecting users’ privacy,” said Professor Dawn Song, Founder of Oasis Labs. “This is an unprecedented use of such cutting-edge cryptographic techniques for a large-scale, real world use case. We hope this can help inspire the community to expand the use of privacy-preserving technologies, and together build towards responsible AI and responsible data use for a fairer and more inclusive society.”

First, an external partner securely collects and encrypts survey data. A notice at the top of Instagram’s Feed invites people to share information about their race and ethnicity. People who click on the notice are redirected to the survey provider and assigned a freshly generated random identifier (RID). To prevent demographic data from being linked to an individual social media account, the survey provider indexes responses by RID but does not have User IDs, or any identifier they could link back to a particular individual. The survey provider does not know (and will not learn) to whom the RID-associated data belongs. Meta, meanwhile, knows who responded to the survey and holds the mapping from RIDs to User IDs (this is necessary in order to later query internal model data to be assessed using this method), but does not have access to the survey responses with demographic information.

A method called additive secret sharing is central to the privacy-preserving nature of our approach. The survey provider encrypts and splits the responses into fragments, or shares, that sum to reveal the original values. The survey provider then distributes the shares among several third-party facilitators, including Northeastern University, Texas Southern University, a Historically Black College/University (HBCU), and the University of Central Florida, a Hispanic-Serving Institution (HSI). This secret sharing scheme guarantees that no information about the encoded vector can be inferred from any incomplete set of shares. The cryptographic treatment renders individual responses indecipherable to facilitators, so they can neither link any response back to a known individual nor decipher any of the de-identified responses. The only way to recover the private information is for all shares to be combined. Within 30 days after uploading the encrypted and split data, the survey provider deletes raw responses, retaining them only as long as necessary to ensure against accidental loss of data during the next stage.

Once the data is collected, encrypted, and split, it will be available for analysis, without enabling raw survey responses to be shared with Meta. To assess fairness using this data, Meta computes a given metric, such as a classifier’s output for all survey respondents (remember that Meta knows who responded to the survey but not their answers). Meta then encrypts these values and uploads these de-identified data, indexed by RIDs, to the facilitators and identifies the combination of demographic attributes over which it wants to compute the sum of the given metric. The facilitators can then perform the computation over their de-identified response shares and the encrypted values, add noise to ensure differential privacy of the output, and return the combined, de-identified computations in an encrypted format to Meta. When Meta decrypts this response, it learns nothing except for a single number that is the sum of the classifier’s outputs over the relevant demographic attributes, plus any residual differential privacy noise.

""Meta posed an important question about how the latest secure computation techniques could be used to answer questions about the fairness and equity of ML models,"" said Dr. Abhi Shelat, professor of computer sciences at Northeastern University. ""Together with Oasis and Meta, we were able to devise a new cryptographic protocol that enables privacy and measurement efficiently. We are thrilled that we were able to implement this protocol and I am proud to have been able to participate as one of the independent parties in the computation.""

We have built a custom tool to run SMPC analysis. Similar to Bayesian Improved Surname Geocoding, Meta analysts will only be able to leverage SMPC analysis tools after receiving approval to do so through an internal, structured governance process and Privacy Review process. Requests will be reviewed by a committee that includes representatives from Meta’s Civil Rights, Responsible AI, data science, policy, and legal teams. Proposals also must pass Meta’s privacy review, in which internal teams identify and work to mitigate any broad privacy risks they identify. The governance process adds additional layers of assurance that the survey responses will be used only for approved purposes.

People on Instagram will be able to opt-in to share demographic information. Periodically after receiving survey responses, the pollster encrypts and cryptographically splits the responses and uploads the shares to several third party facilitators using a method called additive secret sharing.

External facilitators index encrypted survey response shares by Random ID.

External facilitators index encrypted survey response shares by Random ID and then returns sums, rather than user-level data.

Meta decrypts the sum of the values released by the facilitators and then measures insights by demographic group.

What’s next?

Although its complexity may initially limit the set of operations and techniques we can apply, our method has the potential to work with many types of measurements. We could use it, for example, to determine whether content produced by people of certain races is achieving disproportionate reach, or whether the performance of a model for people from particular ethnic groups is similar across groups. For example, analysis we conduct with this information might help us better understand experiences different communities may have when it comes to how we rank content on Instagram.

Based on what we learn from the implementation of this initial survey on Instagram, we hope to expand this approach to our other platforms and explore how it might be adapted to support fairness work elsewhere. This versatility is a significant benefit of the SMPC approach, and one we intend to highlight for other companies and organizations that are facing similar challenges. We are committed to working in this space for the long run.

This is only one step in a longer journey. In the future, we hope to identify additional privacy-preserving approaches that can support more complex research, such as longitudinal and causal studies. We look forward to engaging with advocates, academic experts, policymakers, and peer companies to continue advancing equity and inclusion in our products while protecting people’s privacy."
Meta_Blog,https://ai.meta.com/blog/opt-175b-large-language-model-applications/,,"Applications of Meta’s OPT-175B: Protein design, quantum physics, translation, and more","Since we announced OPT-175B in May, more than 4,500 individuals and institutions around the world have requested access to this groundbreaking large language model for AI research. OPT-175B was the first such model to be made freely available to the research community, providing an important new resource to accelerate work in this area of AI and ultimately help create safer, more useful, and more robust language generation systems.

As of today, we have granted access to 668 entities in 49 different countries, including about 300 universities and 80 industry research labs. These requests from external researchers have focused on tasks such as:

Scientific question-answering

Classification and prediction involving clinical warehouse data

Applications in mathematical logic: proofs of lemmas and theorems

Inspirations for new ideas in practical quantum physics research

Moderation of online platforms

Generation of quantum “games” (interactive proof systems)

Generative protein design

Novel algorithms for compression

Low-resource translation

Interpretability of other modalities via language interface

Bias analysis in survey interview research

We are excited by this initial response to OPT-175B and look forward to seeing how this model helps advance work in these different research areas. It’s been particularly interesting to see how other researchers have responded to our decision to release not just the model weights and code but also our notes and full logbook detailing the training process. We hope these additional resources can benefit newcomers into the field by providing a valuable behind-the-scenes look into implementation details that are not easily captured in published papers. We are also partnering with third-party organizations like the Partnership on AI so that we can help establish industry-wide norms around when and how to release these models.

We released OPT-175B (along with versions with fewer parameters) under a noncommercial license to focus on certain research use cases, and we have also taken additional precautions by gradually rolling out initial access to OPT-175B via a prioritization system. Requestors who:

provide links to prior publications; have an email address matching an academic institution or industry research lab; and provide extensive details on the intended research use cases

are highly prioritized for initial access to OPT-175B. Each request has also been manually screened to verify the consistency and accuracy of information provided in the request form before the requestor receives a link to download the model.

Something Went Wrong We're having trouble playing this video. Learn more

Our goal with this approach is to methodically expand access to the broader research community so that we can collectively help define the risks, limitations, and appropriate applications of these models. We will continue to refine this release strategy over time, as we obtain feedback from researchers, before we consider extending access beyond research use cases."
Meta_Blog,https://ai.meta.com/blog/opt-175b-large-language-model-applications/,,"Applications of Meta’s OPT-175B: Protein design, quantum physics, translation, and more","Since we announced OPT-175B in May, more than 4,500 individuals and institutions around the world have requested access to this groundbreaking large language model for AI research. OPT-175B was the first such model to be made freely available to the research community, providing an important new resource to accelerate work in this area of AI and ultimately help create safer, more useful, and more robust language generation systems.

As of today, we have granted access to 668 entities in 49 different countries, including about 300 universities and 80 industry research labs. These requests from external researchers have focused on tasks such as:

Scientific question-answering

Classification and prediction involving clinical warehouse data

Applications in mathematical logic: proofs of lemmas and theorems

Inspirations for new ideas in practical quantum physics research

Moderation of online platforms

Generation of quantum “games” (interactive proof systems)

Generative protein design

Novel algorithms for compression

Low-resource translation

Interpretability of other modalities via language interface

Bias analysis in survey interview research

We are excited by this initial response to OPT-175B and look forward to seeing how this model helps advance work in these different research areas. It’s been particularly interesting to see how other researchers have responded to our decision to release not just the model weights and code but also our notes and full logbook detailing the training process. We hope these additional resources can benefit newcomers into the field by providing a valuable behind-the-scenes look into implementation details that are not easily captured in published papers. We are also partnering with third-party organizations like the Partnership on AI so that we can help establish industry-wide norms around when and how to release these models.

We released OPT-175B (along with versions with fewer parameters) under a noncommercial license to focus on certain research use cases, and we have also taken additional precautions by gradually rolling out initial access to OPT-175B via a prioritization system. Requestors who:

provide links to prior publications; have an email address matching an academic institution or industry research lab; and provide extensive details on the intended research use cases

are highly prioritized for initial access to OPT-175B. Each request has also been manually screened to verify the consistency and accuracy of information provided in the request form before the requestor receives a link to download the model.

Something Went Wrong We're having trouble playing this video. Learn more

Our goal with this approach is to methodically expand access to the broader research community so that we can collectively help define the risks, limitations, and appropriate applications of these models. We will continue to refine this release strategy over time, as we obtain feedback from researchers, before we consider extending access beyond research use cases."
Meta_Blog,https://ai.meta.com/blog/theseus-a-library-for-encoding-domain-knowledge-in-end-to-end-ai-models/,,"Introducing Theseus, a library for encoding domain knowledge in end to end AI models","What the research is:

Meta AI is open-sourcing Theseus, a library for an optimization technique called differentiable nonlinear least squares (NLS) that is particularly useful for applications like robotics and computer vision. Built on PyTorch, Theseus enables researchers to easily incorporate expert domain knowledge into modern AI architectures. It does this by expressing that knowledge as an optimization problem and adding it to the architecture as a modular “optimization layer” in the usual gradient-based learning process. This domain knowledge is distinct from the training data and can help the model make more accurate predictions. For instance, to ensure that a robot’s movements are smooth, researchers could include knowledge about the robot’s embodiment and movement patterns (called a kinematics model) as a layer while the robot is trained end to end to move.

Theseus is the first library to provide an application-agnostic framework for differentiable nonlinear optimization. Theseus is also highly efficient — it speeds computation and memory by supporting batching, GPU acceleration, sparse solvers, and implicit differentiation. As a result, it is up to four times faster than Google’s state-of-the-art, C++-based Ceres Solver (which does not support end-to-end learning).

Theseus fuses the best aspects of the two prevailing methods for injecting prior knowledge into an AI system. Before the advent of deep learning, researchers used simpler, standalone AI optimization algorithms to solve individual problems in robotics. Robotic systems learned the best way to carry out commands by calculating the minimum value of a hand-selected combination of factors, such as joint motion and energy use. This method was effective but inflexible; the application-specific optimization algorithms often proved difficult to adapt to new systems or environments. Deep learning methods, on the other hand, are much more scalable, but they require a massive amount of data, and they may produce solutions that are effective but also brittle outside of the training domain.

To train a deep learning model for a particular application, researchers use a carefully selected loss function to measure how well the model is predicting the data. But to update the model weights through backpropagation, each layer must be differentiable, allowing the error information to flow through the network. Traditional optimization algorithms are not end to end differentiable, so researchers face a trade-off: They can abandon optimization algorithms for end to end deep learning dedicated to the specific task — and risk losing optimization’s efficiency as well as its facility for generalization. Or, they can train the deep learning model offline and add it to the optimization algorithms at inference time. The second method has the benefit of combining deep learning and prior knowledge, but — because the deep learning model is trained without that pre-existing information or the task-specific error function — its predictions might prove inaccurate.

To blend these strategies in a way that mitigates their weaknesses and leverages their strengths, Theseus converts the results of optimization into a layer that can be plugged into any neural network architecture. That way, revisions can back-propagate through the optimization layer, allowing researchers to fine-tune with domain-specific knowledge on the final task loss as an integral part of the end to end deep learning model.

In the Theseus layer (green), the objective is composed of the output tensors of upstream neural models (gray) and prior knowledge (orange). The output of the Theseus layer are tensors that minimize the objective.

How it works:

NLS measures how much a nonlinear function varies from the actual data it is meant to predict. A small value means the function fits the data set well. NLS is prevalent in the formulation of many robotics and vision problems, from mapping and estimation to planning and control. For example, a robot’s route toward a desired goal can be formulated as an NLS optimization problem: To plot the fastest safe trajectory, the system finds the solution to a sum-of-costs objective that minimizes both travel duration and unwanted behavior, like sharp turns or collisions with obstacles in the environment. A sum-of-costs objective can also capture sensor measurement errors to optimize the past trajectories of a robot or camera.

Making NLS differentiable, Theseus provides differentiable nonlinear optimization as a layer that researchers can insert into their neural network. Input tensors define a sum-of-weighted-squares objective function, and output tensors are arguments that produce the minimum of that objective. (In contrast, typical neural layers take input tensors through a linear transformation and some element-wise nonlinear activation function.) The ability to compute gradients end to end is retained by differentiating through the optimizer.

This integrates the optimizer and known priors into the deep learning training loop, allowing models to encode domain knowledge and learn on the actual task loss. For instance, to ensure that a robot’s movements are smooth, researchers could include known robot kinematics in the optimizer; meanwhile, the deep learning model will extract the larger goal from perception or a language instruction during training. That way, researchers can develop the goal prediction model end to end with the known kinematics model in the training loop. This technique of modularly mixing known priors with neural components leads to improved data efficiency and generalization.

For efficiency, Theseus incorporates support for sparse solvers, automatic vectorization, batching, GPU acceleration, and gradient computation with implicit differentiation. Just as autodiff and GPU acceleration have propelled the evolution of PyTorch over NumPy, sparsity and implicit differentiation — on top of autodiff and GPU acceleration — power Theseus, in contrast to solvers like Ceres that typically support only sparsity. On a standard GPU, Theseus with a sparse solver is much faster and requires significantly less memory than a dense solver. Additionally, when Theseus is solving a batch of large problems, its forward pass is up to four times faster than that of Ceres, which has limited GPU support and does not support batching or end to end learning. Finally, implicit differentiation yields better gradients than standard unrolling. Implicit differentiation also has a constant memory and compute footprint with increasing optimization iterations, unlike unrolling, which scales linearly in compute and memory.

Why it matters:

Theseus provides a common framework to leverage the complementary strengths of traditional robotics and vision approaches and deep learning. Differentiable optimization acts as an inductive prior, improving data efficiency and generalization, which is crucial in robotics because data and labels often do not come cheap, and application domains tend to be broad.

Recognizing the flexibility of differentiable NLS, previous researchers have reported state-of-the-art results with similar methods in a wide range of applications in robotics and vision, but existing implementations are task-specific and often inefficient. Theseus is application-agnostic, so the AI community can make faster progress by training accurate models that excel in multiple tasks and environments. We have developed several example applications, including pose graph optimization, tactile state estimation, bundle adjustment, motion planning, and homography estimation. We built these examples using the same underlying differentiable components, such as second-order optimizers, standard costs functions, and Lie groups.

Beyond pushing the current state of the art, our framework will enable avenues for future research into the role and possible evolution of structure in complex robot systems, learning end to end on such systems, and continually learning during real-world interactions.

Read the full paper and get the code:

https://github.com/facebookresearch/theseus

Theseus project website"
Meta_Blog,https://ai.meta.com/blog/theseus-a-library-for-encoding-domain-knowledge-in-end-to-end-ai-models/,,"Introducing Theseus, a library for encoding domain knowledge in end to end AI models","What the research is:

Meta AI is open-sourcing Theseus, a library for an optimization technique called differentiable nonlinear least squares (NLS) that is particularly useful for applications like robotics and computer vision. Built on PyTorch, Theseus enables researchers to easily incorporate expert domain knowledge into modern AI architectures. It does this by expressing that knowledge as an optimization problem and adding it to the architecture as a modular “optimization layer” in the usual gradient-based learning process. This domain knowledge is distinct from the training data and can help the model make more accurate predictions. For instance, to ensure that a robot’s movements are smooth, researchers could include knowledge about the robot’s embodiment and movement patterns (called a kinematics model) as a layer while the robot is trained end to end to move.

Theseus is the first library to provide an application-agnostic framework for differentiable nonlinear optimization. Theseus is also highly efficient — it speeds computation and memory by supporting batching, GPU acceleration, sparse solvers, and implicit differentiation. As a result, it is up to four times faster than Google’s state-of-the-art, C++-based Ceres Solver (which does not support end-to-end learning).

Theseus fuses the best aspects of the two prevailing methods for injecting prior knowledge into an AI system. Before the advent of deep learning, researchers used simpler, standalone AI optimization algorithms to solve individual problems in robotics. Robotic systems learned the best way to carry out commands by calculating the minimum value of a hand-selected combination of factors, such as joint motion and energy use. This method was effective but inflexible; the application-specific optimization algorithms often proved difficult to adapt to new systems or environments. Deep learning methods, on the other hand, are much more scalable, but they require a massive amount of data, and they may produce solutions that are effective but also brittle outside of the training domain.

To train a deep learning model for a particular application, researchers use a carefully selected loss function to measure how well the model is predicting the data. But to update the model weights through backpropagation, each layer must be differentiable, allowing the error information to flow through the network. Traditional optimization algorithms are not end to end differentiable, so researchers face a trade-off: They can abandon optimization algorithms for end to end deep learning dedicated to the specific task — and risk losing optimization’s efficiency as well as its facility for generalization. Or, they can train the deep learning model offline and add it to the optimization algorithms at inference time. The second method has the benefit of combining deep learning and prior knowledge, but — because the deep learning model is trained without that pre-existing information or the task-specific error function — its predictions might prove inaccurate.

To blend these strategies in a way that mitigates their weaknesses and leverages their strengths, Theseus converts the results of optimization into a layer that can be plugged into any neural network architecture. That way, revisions can back-propagate through the optimization layer, allowing researchers to fine-tune with domain-specific knowledge on the final task loss as an integral part of the end to end deep learning model.

In the Theseus layer (green), the objective is composed of the output tensors of upstream neural models (gray) and prior knowledge (orange). The output of the Theseus layer are tensors that minimize the objective.

How it works:

NLS measures how much a nonlinear function varies from the actual data it is meant to predict. A small value means the function fits the data set well. NLS is prevalent in the formulation of many robotics and vision problems, from mapping and estimation to planning and control. For example, a robot’s route toward a desired goal can be formulated as an NLS optimization problem: To plot the fastest safe trajectory, the system finds the solution to a sum-of-costs objective that minimizes both travel duration and unwanted behavior, like sharp turns or collisions with obstacles in the environment. A sum-of-costs objective can also capture sensor measurement errors to optimize the past trajectories of a robot or camera.

Making NLS differentiable, Theseus provides differentiable nonlinear optimization as a layer that researchers can insert into their neural network. Input tensors define a sum-of-weighted-squares objective function, and output tensors are arguments that produce the minimum of that objective. (In contrast, typical neural layers take input tensors through a linear transformation and some element-wise nonlinear activation function.) The ability to compute gradients end to end is retained by differentiating through the optimizer.

This integrates the optimizer and known priors into the deep learning training loop, allowing models to encode domain knowledge and learn on the actual task loss. For instance, to ensure that a robot’s movements are smooth, researchers could include known robot kinematics in the optimizer; meanwhile, the deep learning model will extract the larger goal from perception or a language instruction during training. That way, researchers can develop the goal prediction model end to end with the known kinematics model in the training loop. This technique of modularly mixing known priors with neural components leads to improved data efficiency and generalization.

For efficiency, Theseus incorporates support for sparse solvers, automatic vectorization, batching, GPU acceleration, and gradient computation with implicit differentiation. Just as autodiff and GPU acceleration have propelled the evolution of PyTorch over NumPy, sparsity and implicit differentiation — on top of autodiff and GPU acceleration — power Theseus, in contrast to solvers like Ceres that typically support only sparsity. On a standard GPU, Theseus with a sparse solver is much faster and requires significantly less memory than a dense solver. Additionally, when Theseus is solving a batch of large problems, its forward pass is up to four times faster than that of Ceres, which has limited GPU support and does not support batching or end to end learning. Finally, implicit differentiation yields better gradients than standard unrolling. Implicit differentiation also has a constant memory and compute footprint with increasing optimization iterations, unlike unrolling, which scales linearly in compute and memory.

Why it matters:

Theseus provides a common framework to leverage the complementary strengths of traditional robotics and vision approaches and deep learning. Differentiable optimization acts as an inductive prior, improving data efficiency and generalization, which is crucial in robotics because data and labels often do not come cheap, and application domains tend to be broad.

Recognizing the flexibility of differentiable NLS, previous researchers have reported state-of-the-art results with similar methods in a wide range of applications in robotics and vision, but existing implementations are task-specific and often inefficient. Theseus is application-agnostic, so the AI community can make faster progress by training accurate models that excel in multiple tasks and environments. We have developed several example applications, including pose graph optimization, tactile state estimation, bundle adjustment, motion planning, and homography estimation. We built these examples using the same underlying differentiable components, such as second-order optimizers, standard costs functions, and Lie groups.

Beyond pushing the current state of the art, our framework will enable avenues for future research into the role and possible evolution of structure in complex robot systems, learning end to end on such systems, and continually learning during real-world interactions.

Read the full paper and get the code:

https://github.com/facebookresearch/theseus

Theseus project website"
Meta_Blog,https://ai.meta.com/blog/greater-creative-control-for-ai-image-generation/,,Greater creative control for AI image generation,"Creative expression is central to human connection, and using artificial intelligence (AI) to augment human creativity is a powerful use of technology — whether it’s by generating expressive avatars, animating children's drawings, creating new virtual worlds in the metaverse, or producing stunning digital artwork using just text descriptions.

It’s not enough for an AI system to just generate content, though. To realize AI’s potential to push creative expression forward, people should be able to shape and control the content a system generates. It should be intuitive and easy to use so people can leverage whatever modes of expression work best for them, whether speech, text, gestures, eye movements, or even sketches, to bring their vision to life in whatever mediums work best for them, including audio, images, animations, video, and 3D. Imagine creating beautiful impressionist paintings in compositions you envision without ever picking up a paintbrush. Or instantly generating imaginative storybook illustrations to accompany the words.

Today, we’re showcasing an exploratory AI research concept called Make-A-Scene that demonstrates AI’s potential for empowering anyone to bring their imagination to life. This multimodal generative AI method puts creative control in the hands of people who use it by allowing them to describe and illustrate their vision through both text descriptions and freeform sketches.

Make-A-Scene’s freeform sketch and text components enables a greater level of creative control for AI-generated images. By drawing what you want, you can decide the size of the cat, which way it’s facing, and the shape and length of its tail.

Prior state-of-the-art AI systems that generated awe-inspiring images primarily used a text description as input. But text prompts, like “a painting of a zebra riding a bike,” generate images with compositions that can be difficult to predict. The zebra might be on the left side of the image or the right, for example, or it might be much bigger than the bicycle or much smaller, or the zebra and bicycle may be facing the camera or facing sideways. As a result, the image may not be a reflection of a person’s creative voice, and they may not feel a strong sense of pride and ownership over the content. If, for instance, you wanted to specify the relative size of the bicycle wheel, orientation of the handle bars, and the width of the road, there is no easy way to convey all of these elements using just a text description.

With Make-A-Scene, this is no longer the case. It demonstrates how people can use both text and simple drawings to convey their vision with greater specificity, using a variety of elements, forms, arrangements, depth, compositions, and structures.

Something Went Wrong We're having trouble playing this video. Learn more

AI artists Sofia Crespo, Scott Eaton, Alexander Reben, and Refik Anadol share their perspective on using Make-A-Scene as part of their creative process.

We validated this premise using human evaluators. Each was shown two images generated by Make-A-Scene: one generated from only a text prompt, and one from both a sketch and a text prompt. The latter used the segmentation map of an image from a public dataset as the sketch. Both used the corresponding image caption as the text input. We found that the image generated from both text and sketch was almost always (99.54 percent of the time) rated as better aligned with the original sketch. It was often (66.3 percent of the time) more aligned with the text prompt too. This demonstrates that Make-A-Scene generations are indeed faithful to a person’s vision communicated via the sketch.

Like other generative AI models, Make-A-Scene learns the relationship between visuals and text by training on millions of example images. Among other factors, the bias reflected in the training data affects the output of those models. The AI industry is still in early days of understanding and addressing these challenges, and there’s a lot more work to be done. We believe transparency will accelerate progress toward this. As a step toward promoting transparency in this research direction, we used publicly available datasets to train Make-A-Scene to help the broader AI community analyze, study, and understand the existing biases of the system.

Something Went Wrong We're having trouble playing this video. Learn more

Make-A-Scene can generate its own scene layout using text-only prompts, if that’s what the user chooses to do.

Make-A-Scene uses a novel intermediate representation that captures the scene layout to enable nuanced sketches as input. It can also generate its own scene layout with text-only prompts, if that’s what the creator chooses. The model focuses on learning key aspects of the imagery that are more likely to be important to the creator, such as objects or animals. This technique helped increase the generation quality, as evaluated by the widely used FID score, which assesses the quality of images created by generative models.

Empowering creativity for artists and non-artists alike

So, how exactly would people use Make-A-Scene to bring their imaginations to life? As part of our research and development process, we’re sharing access to our Make-A-Scene demo with well-known AI artists, including Sofia Crespo, Scott Eaton, Alexander Reben, and Refik Anadol — all of whom have experience using state-of-the-art generative AI. We asked these artists to use Make-A-Scene as part of their creative process and to provide feedback along the way.

Something Went Wrong We're having trouble playing this video. Learn more

Crespo, for instance, is a generative artist focusing on the intersection between nature and technology. She’s interested in imagining artificial life forms that have never existed, and she used Make-A-Scene’s sketch and text prompts to create new hybrid creatures, like jellyfish in the shape of a flower. Using its freeform drawing capabilities, she found that she could iterate quickly across new ideas. “It’s going to help move creativity a lot faster and help artists work with interfaces that are more intuitive,” Crespo says.

Eaton is an artist, an educator, and a creative technologist whose work investigates contemporary situations and relationships with our technologies. He similarly leveraged Make-A-Scene as a way to deliberately compose scenes but still explore variations by experimenting with different prompts, like “skyscrapers sunken and decaying in the desert” to highlight the climate crisis.

Something Went Wrong We're having trouble playing this video. Learn more

Reben is an artist, researcher, and roboticist, who says that having more control over the output really helps get your artistic intent through. He incorporated the tools in his ongoing series focused on creating art in real-life as described by AI systems. In this case, he used AI-generated text from another AI system, created a sketch to interpret that text, and used that sketch as well as the text as input for Make-A-Scene. “It made quite a difference to be able to sketch things in, especially to tell the system where you wanted things to give it suggestions of where things should go, but still be surprised at the end,” Reben says.

Something Went Wrong We're having trouble playing this video. Learn more

For media artist and director Refik Anadol, the tool was a way to prompt an imagination and explore uncharted territories. “I was prompting ideas, mixing and matching different worlds — you are literally dipping the brush in the mind of a machine and painting with machine consciousness,” he says.

Something Went Wrong We're having trouble playing this video. Learn more

The prototype tool is not just for people with a penchant for art. We believe it could help anyone better express themselves, including people without artistic skill sets. As a starting point, we’ve provided access on a limited basis to Meta employees who are testing and providing feedback about their experience with Make-A-Scene. Andy Boyatzis, a program manager at Meta, used Make-A-Scene to generate art with his young children of ages two and four. They used playful drawings to bring their ideas and imagination to life.

Trying new tools like Make-A-Scene is a fundamental way for our employees to stay connected to the cutting-edge AI research at Meta and to have influence over how we improve exploratory concepts that could impact different types of generative AI tools developed and released in production in the future.

Building the next generation of creative AI tools

Through scientific research and exploratory projects like Make-A-Scene, we believe we can expand the boundaries of creative expression — regardless of artistic ability. We want to make it as easy for people to bring their vision to life in the physical world and in the metaverse as it is to post across our apps today. This research endeavor is part of Meta’s commitment to exploring ways in which AI can empower creativity – whether that’s bringing your 2D sketches to life, using natural language among other modalities to create 3D objects, building entire virtual spaces, or any other creative project. It could one day enable entirely new forms of AI-powered expression, while putting creators and their vision at the center of the process — whether that’s an art director ideating on their next creative campaign, a social media influencer creating more personalized content, an author developing unique illustrations for their books and stories, or just someone sharing a fun, unique greeting for a friend’s birthday.

We’re making progress in this space, but this is just the beginning. We’ll continue to push the boundaries of what’s possible using this new class of generative creative tools to build methods for richer, more expressive messaging in 2D, 3D, and general communications between people in mixed reality and virtual worlds.

Since the research paper was released, Make-A-Scene has incorporated a super resolution network that generates imagery at 2048 x 2048, 4x the resolution, and we’re continuously improving our generative AI models. We aim to provide broader access to our research demos in the future to give more people the opportunity to be in control of their own creations and unlock entirely new forms of expression.

In the meantime, check out the wide range of fascinating outputs from Make-A-Scene below. And you can catch our oral presentation at this year’s ECCV conference held in Tel Aviv on October 23 - 27, 2022.

This blog post reflects the research contributions of Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.

We would also like to acknowledge Kelly Freed, Deb Banerji, Somya Jain, Sasha Sheng, Maria Ruiz, and Aran Mun. Thank you for your contributions!"
Meta_Blog,https://ai.meta.com/blog/greater-creative-control-for-ai-image-generation/,,Greater creative control for AI image generation,"Creative expression is central to human connection, and using artificial intelligence (AI) to augment human creativity is a powerful use of technology — whether it’s by generating expressive avatars, animating children's drawings, creating new virtual worlds in the metaverse, or producing stunning digital artwork using just text descriptions.

It’s not enough for an AI system to just generate content, though. To realize AI’s potential to push creative expression forward, people should be able to shape and control the content a system generates. It should be intuitive and easy to use so people can leverage whatever modes of expression work best for them, whether speech, text, gestures, eye movements, or even sketches, to bring their vision to life in whatever mediums work best for them, including audio, images, animations, video, and 3D. Imagine creating beautiful impressionist paintings in compositions you envision without ever picking up a paintbrush. Or instantly generating imaginative storybook illustrations to accompany the words.

Today, we’re showcasing an exploratory AI research concept called Make-A-Scene that demonstrates AI’s potential for empowering anyone to bring their imagination to life. This multimodal generative AI method puts creative control in the hands of people who use it by allowing them to describe and illustrate their vision through both text descriptions and freeform sketches.

Make-A-Scene’s freeform sketch and text components enables a greater level of creative control for AI-generated images. By drawing what you want, you can decide the size of the cat, which way it’s facing, and the shape and length of its tail.

Prior state-of-the-art AI systems that generated awe-inspiring images primarily used a text description as input. But text prompts, like “a painting of a zebra riding a bike,” generate images with compositions that can be difficult to predict. The zebra might be on the left side of the image or the right, for example, or it might be much bigger than the bicycle or much smaller, or the zebra and bicycle may be facing the camera or facing sideways. As a result, the image may not be a reflection of a person’s creative voice, and they may not feel a strong sense of pride and ownership over the content. If, for instance, you wanted to specify the relative size of the bicycle wheel, orientation of the handle bars, and the width of the road, there is no easy way to convey all of these elements using just a text description.

With Make-A-Scene, this is no longer the case. It demonstrates how people can use both text and simple drawings to convey their vision with greater specificity, using a variety of elements, forms, arrangements, depth, compositions, and structures.

Something Went Wrong We're having trouble playing this video. Learn more

AI artists Sofia Crespo, Scott Eaton, Alexander Reben, and Refik Anadol share their perspective on using Make-A-Scene as part of their creative process.

We validated this premise using human evaluators. Each was shown two images generated by Make-A-Scene: one generated from only a text prompt, and one from both a sketch and a text prompt. The latter used the segmentation map of an image from a public dataset as the sketch. Both used the corresponding image caption as the text input. We found that the image generated from both text and sketch was almost always (99.54 percent of the time) rated as better aligned with the original sketch. It was often (66.3 percent of the time) more aligned with the text prompt too. This demonstrates that Make-A-Scene generations are indeed faithful to a person’s vision communicated via the sketch.

Like other generative AI models, Make-A-Scene learns the relationship between visuals and text by training on millions of example images. Among other factors, the bias reflected in the training data affects the output of those models. The AI industry is still in early days of understanding and addressing these challenges, and there’s a lot more work to be done. We believe transparency will accelerate progress toward this. As a step toward promoting transparency in this research direction, we used publicly available datasets to train Make-A-Scene to help the broader AI community analyze, study, and understand the existing biases of the system.

Something Went Wrong We're having trouble playing this video. Learn more

Make-A-Scene can generate its own scene layout using text-only prompts, if that’s what the user chooses to do.

Make-A-Scene uses a novel intermediate representation that captures the scene layout to enable nuanced sketches as input. It can also generate its own scene layout with text-only prompts, if that’s what the creator chooses. The model focuses on learning key aspects of the imagery that are more likely to be important to the creator, such as objects or animals. This technique helped increase the generation quality, as evaluated by the widely used FID score, which assesses the quality of images created by generative models.

Empowering creativity for artists and non-artists alike

So, how exactly would people use Make-A-Scene to bring their imaginations to life? As part of our research and development process, we’re sharing access to our Make-A-Scene demo with well-known AI artists, including Sofia Crespo, Scott Eaton, Alexander Reben, and Refik Anadol — all of whom have experience using state-of-the-art generative AI. We asked these artists to use Make-A-Scene as part of their creative process and to provide feedback along the way.

Something Went Wrong We're having trouble playing this video. Learn more

Crespo, for instance, is a generative artist focusing on the intersection between nature and technology. She’s interested in imagining artificial life forms that have never existed, and she used Make-A-Scene’s sketch and text prompts to create new hybrid creatures, like jellyfish in the shape of a flower. Using its freeform drawing capabilities, she found that she could iterate quickly across new ideas. “It’s going to help move creativity a lot faster and help artists work with interfaces that are more intuitive,” Crespo says.

Eaton is an artist, an educator, and a creative technologist whose work investigates contemporary situations and relationships with our technologies. He similarly leveraged Make-A-Scene as a way to deliberately compose scenes but still explore variations by experimenting with different prompts, like “skyscrapers sunken and decaying in the desert” to highlight the climate crisis.

Something Went Wrong We're having trouble playing this video. Learn more

Reben is an artist, researcher, and roboticist, who says that having more control over the output really helps get your artistic intent through. He incorporated the tools in his ongoing series focused on creating art in real-life as described by AI systems. In this case, he used AI-generated text from another AI system, created a sketch to interpret that text, and used that sketch as well as the text as input for Make-A-Scene. “It made quite a difference to be able to sketch things in, especially to tell the system where you wanted things to give it suggestions of where things should go, but still be surprised at the end,” Reben says.

Something Went Wrong We're having trouble playing this video. Learn more

For media artist and director Refik Anadol, the tool was a way to prompt an imagination and explore uncharted territories. “I was prompting ideas, mixing and matching different worlds — you are literally dipping the brush in the mind of a machine and painting with machine consciousness,” he says.

Something Went Wrong We're having trouble playing this video. Learn more

The prototype tool is not just for people with a penchant for art. We believe it could help anyone better express themselves, including people without artistic skill sets. As a starting point, we’ve provided access on a limited basis to Meta employees who are testing and providing feedback about their experience with Make-A-Scene. Andy Boyatzis, a program manager at Meta, used Make-A-Scene to generate art with his young children of ages two and four. They used playful drawings to bring their ideas and imagination to life.

Trying new tools like Make-A-Scene is a fundamental way for our employees to stay connected to the cutting-edge AI research at Meta and to have influence over how we improve exploratory concepts that could impact different types of generative AI tools developed and released in production in the future.

Building the next generation of creative AI tools

Through scientific research and exploratory projects like Make-A-Scene, we believe we can expand the boundaries of creative expression — regardless of artistic ability. We want to make it as easy for people to bring their vision to life in the physical world and in the metaverse as it is to post across our apps today. This research endeavor is part of Meta’s commitment to exploring ways in which AI can empower creativity – whether that’s bringing your 2D sketches to life, using natural language among other modalities to create 3D objects, building entire virtual spaces, or any other creative project. It could one day enable entirely new forms of AI-powered expression, while putting creators and their vision at the center of the process — whether that’s an art director ideating on their next creative campaign, a social media influencer creating more personalized content, an author developing unique illustrations for their books and stories, or just someone sharing a fun, unique greeting for a friend’s birthday.

We’re making progress in this space, but this is just the beginning. We’ll continue to push the boundaries of what’s possible using this new class of generative creative tools to build methods for richer, more expressive messaging in 2D, 3D, and general communications between people in mixed reality and virtual worlds.

Since the research paper was released, Make-A-Scene has incorporated a super resolution network that generates imagery at 2048 x 2048, 4x the resolution, and we’re continuously improving our generative AI models. We aim to provide broader access to our research demos in the future to give more people the opportunity to be in control of their own creations and unlock entirely new forms of expression.

In the meantime, check out the wide range of fascinating outputs from Make-A-Scene below. And you can catch our oral presentation at this year’s ECCV conference held in Tel Aviv on October 23 - 27, 2022.

This blog post reflects the research contributions of Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman.

We would also like to acknowledge Kelly Freed, Deb Banerji, Somya Jain, Sasha Sheng, Maria Ruiz, and Aran Mun. Thank you for your contributions!"
Meta_Blog,https://ai.meta.com/blog/using-ai-to-find-post-quantum-cryptographys-vulnerabilities/,,Using AI to find post-quantum cryptography’s vulnerabilities,"Research in cryptography, the science of securing information, must stay ahead of attacks in order to protect everyone’s data. Since current standards for public key cryptography may one day be vulnerable to attacks by large-scale quantum computers, researchers are proposing and testing new post-quantum cryptographic proposals based on hard problems in mathematics.

Today, Meta AI is sharing new research showing how one of the most promising proposals for a post-quantum cryptographic system may be vulnerable to attacks using transformers, a widely used AI architecture. That proposal, lattice-based cryptography, is based on a hard problem called learning with errors (LWE), which assumes that it is hard to learn a secret vector, given only noisy inner products with random vectors.

However, since machine learning (ML) is good at learning under noisy conditions, our research demonstrates how AI can be used to attack toy versions of these problems, opening a new approach to explore further. We used state-of-the-art AI techniques to test the resiliency of lattice-based cryptography and discovered that AI can defeat small- and medium-scale lattice-based cryptography applications. By uncovering these potential weaknesses now, we hope to help the research community develop more robust ways to safeguard information in the future.

Public key cryptography and the post-quantum era

Today’s widely deployed public key cryptographic systems are mostly based on two hard problems — the factorization of integers into primes, and the discrete logarithm problem on elliptic curves. If you want to share a secret with three friends, you could just tell them. But what if you want to securely share that secret with anyone, even a stranger? Traditional cryptography enables correspondents to securely share a secret key that they can use to encode and decode their messages. The communications between them are protected because only they have that key.

Once quantum computers have been developed at a large enough scale, they can be used to efficiently attack the hard problems used in public key cryptographic systems deployed today. That is why we need new problems for protecting secrets. The U.S. National Institute of Standards and Technology (NIST) has been running a multiyear competition to select new post-quantum cryptosystems (PQC), involving researchers from industry, government, and universities around the world. This month, NIST announced that it will recommend three lattice-based schemes for standardization. Lattice-based cryptography, based on the difficulty of learning secrets with errors added to the computations (LWE), has been a leading candidate for PQC. Meta AI is contributing to this initiative by exploring potential AI-related weaknesses in LWE.

Using AI to recover secrets in lattice-based cryptography

Given a lot of noisy data, ML can still learn patterns. We introduce a technique called SALSA, or secret-recovery attacks on LWE via seq2seq models with attention. SALSA has three modules:

- A transformer model M, which is trained to predict the result of LWE encryption

- A secret recovery algorithm, which tries to guess the secret from the transformer

- A secret verification procedure, which checks whether the guess is correct

Using examples of LWE messages, we train a language model to mimic the encoder (i.e., to produce the ciphertext given only the randomness, not the secret). Then, we use the trained model to design a distinguisher to recover the secret. SALSA evaluates the guesses by verifying that residues computed from LWE samples have a small standard deviation. If they do, the secret has been recovered and SALSA stops. If they don’t, SALSA continues its attempts to recover the secret.

To make this method work, we needed to first demonstrate that transformers can learn to perform modular arithmetic on integers and vectors, a challenging task that had not yet been accomplished successfully. Our research also shows how these techniques yield a practical attack on LWE and demonstrates its efficacy in the cryptanalysis of small and midsize LWE instances with sparse binary secrets. A secret is a vector of integers. A binary vector has entries equal to 0 or 1; a sparse binary vector, however, has most of its coordinates equal to zero. Industrial implementations of LWE-based homomorphic encryption would have dimensions ranging from 1,024 to 2,048 or larger and a sparsity of a few percent. Right now, SALSA doesn’t yet succeed against that large dimension, but it can fully recover secrets up to n=128 and densities between 2 percent and 8 percent.

One key conclusion from these experiments is that transformer models can compromise the security of these cryptosystems even without being trained to high accuracy. In most cases, we only need the model to begin to learn in order for the attack to succeed. This is important because transformers typically need a large number of samples to be trained, since they learn everything from scratch. In real-life situations, an attacker would try to collect examples and use them to train a model. Our research shows that in most cases the model needs very little training to succeed, which would make it easier for would-be attackers.

Charting the future of cryptography

With SALSA, we are attacking lattice-based cryptography in a new way, showing that full-scale cryptosystems may be vulnerable as the power of ML models grows. Currently, SALSA doesn’t break industrial-strength implementations of lattice-based cryptography, but this attack shows that there are chinks in its armor.

""We gain confidence by active research such as this, and new results in the community,"" said Lily Chen, leader of NIST's Cryptographic Technology Group.

Cryptography has been around since the dawn of civilization. While one of the earliest instances has been traced back to unusual symbols mixed with hieroglyphics in ancient Egypt, the practice has evolved over thousands of years, moving from coded messages in a Polybius square, in which each number corresponds to a letter, to the sophisticated systems we use today to safeguard our digital lives, from passwords to e-commerce purchases. But doing this well requires the entire industry to stay steps ahead of emerging threats by developing new methods for cryptography and conducting extensive research on hard problems at the start.

Establishing quantum-resistant cryptographic solutions will be an ongoing process for researchers and practitioners over the next few decades. A lattice-based cryptographic system depends on a lot of parameters, including the dimension of the keys, whether the secret is binary or sparse, the size of the error values, and some computational factors. We still believe a lattice-based approach is a viable way forward to protect data and privacy. But we need to continue to explore the vulnerabilities to protect against them and to recommend wise parameter choices. Our work demonstrates a new type of vulnerability, which the community of researchers will continue to explore in future work. We look forward to continuing to work with the community to ensure that the next cryptography standard will reliably safeguard data well into the future.

We'd like to acknowledge the contributions of Emily Wenger and Mingjie Chen to this research.

Learn more about SALSA by reading our paper"
Meta_Blog,https://ai.meta.com/blog/using-ai-to-find-post-quantum-cryptographys-vulnerabilities/,,Using AI to find post-quantum cryptography’s vulnerabilities,"Research in cryptography, the science of securing information, must stay ahead of attacks in order to protect everyone’s data. Since current standards for public key cryptography may one day be vulnerable to attacks by large-scale quantum computers, researchers are proposing and testing new post-quantum cryptographic proposals based on hard problems in mathematics.

Today, Meta AI is sharing new research showing how one of the most promising proposals for a post-quantum cryptographic system may be vulnerable to attacks using transformers, a widely used AI architecture. That proposal, lattice-based cryptography, is based on a hard problem called learning with errors (LWE), which assumes that it is hard to learn a secret vector, given only noisy inner products with random vectors.

However, since machine learning (ML) is good at learning under noisy conditions, our research demonstrates how AI can be used to attack toy versions of these problems, opening a new approach to explore further. We used state-of-the-art AI techniques to test the resiliency of lattice-based cryptography and discovered that AI can defeat small- and medium-scale lattice-based cryptography applications. By uncovering these potential weaknesses now, we hope to help the research community develop more robust ways to safeguard information in the future.

Public key cryptography and the post-quantum era

Today’s widely deployed public key cryptographic systems are mostly based on two hard problems — the factorization of integers into primes, and the discrete logarithm problem on elliptic curves. If you want to share a secret with three friends, you could just tell them. But what if you want to securely share that secret with anyone, even a stranger? Traditional cryptography enables correspondents to securely share a secret key that they can use to encode and decode their messages. The communications between them are protected because only they have that key.

Once quantum computers have been developed at a large enough scale, they can be used to efficiently attack the hard problems used in public key cryptographic systems deployed today. That is why we need new problems for protecting secrets. The U.S. National Institute of Standards and Technology (NIST) has been running a multiyear competition to select new post-quantum cryptosystems (PQC), involving researchers from industry, government, and universities around the world. This month, NIST announced that it will recommend three lattice-based schemes for standardization. Lattice-based cryptography, based on the difficulty of learning secrets with errors added to the computations (LWE), has been a leading candidate for PQC. Meta AI is contributing to this initiative by exploring potential AI-related weaknesses in LWE.

Using AI to recover secrets in lattice-based cryptography

Given a lot of noisy data, ML can still learn patterns. We introduce a technique called SALSA, or secret-recovery attacks on LWE via seq2seq models with attention. SALSA has three modules:

- A transformer model M, which is trained to predict the result of LWE encryption

- A secret recovery algorithm, which tries to guess the secret from the transformer

- A secret verification procedure, which checks whether the guess is correct

Using examples of LWE messages, we train a language model to mimic the encoder (i.e., to produce the ciphertext given only the randomness, not the secret). Then, we use the trained model to design a distinguisher to recover the secret. SALSA evaluates the guesses by verifying that residues computed from LWE samples have a small standard deviation. If they do, the secret has been recovered and SALSA stops. If they don’t, SALSA continues its attempts to recover the secret.

To make this method work, we needed to first demonstrate that transformers can learn to perform modular arithmetic on integers and vectors, a challenging task that had not yet been accomplished successfully. Our research also shows how these techniques yield a practical attack on LWE and demonstrates its efficacy in the cryptanalysis of small and midsize LWE instances with sparse binary secrets. A secret is a vector of integers. A binary vector has entries equal to 0 or 1; a sparse binary vector, however, has most of its coordinates equal to zero. Industrial implementations of LWE-based homomorphic encryption would have dimensions ranging from 1,024 to 2,048 or larger and a sparsity of a few percent. Right now, SALSA doesn’t yet succeed against that large dimension, but it can fully recover secrets up to n=128 and densities between 2 percent and 8 percent.

One key conclusion from these experiments is that transformer models can compromise the security of these cryptosystems even without being trained to high accuracy. In most cases, we only need the model to begin to learn in order for the attack to succeed. This is important because transformers typically need a large number of samples to be trained, since they learn everything from scratch. In real-life situations, an attacker would try to collect examples and use them to train a model. Our research shows that in most cases the model needs very little training to succeed, which would make it easier for would-be attackers.

Charting the future of cryptography

With SALSA, we are attacking lattice-based cryptography in a new way, showing that full-scale cryptosystems may be vulnerable as the power of ML models grows. Currently, SALSA doesn’t break industrial-strength implementations of lattice-based cryptography, but this attack shows that there are chinks in its armor.

""We gain confidence by active research such as this, and new results in the community,"" said Lily Chen, leader of NIST's Cryptographic Technology Group.

Cryptography has been around since the dawn of civilization. While one of the earliest instances has been traced back to unusual symbols mixed with hieroglyphics in ancient Egypt, the practice has evolved over thousands of years, moving from coded messages in a Polybius square, in which each number corresponds to a letter, to the sophisticated systems we use today to safeguard our digital lives, from passwords to e-commerce purchases. But doing this well requires the entire industry to stay steps ahead of emerging threats by developing new methods for cryptography and conducting extensive research on hard problems at the start.

Establishing quantum-resistant cryptographic solutions will be an ongoing process for researchers and practitioners over the next few decades. A lattice-based cryptographic system depends on a lot of parameters, including the dimension of the keys, whether the secret is binary or sparse, the size of the error values, and some computational factors. We still believe a lattice-based approach is a viable way forward to protect data and privacy. But we need to continue to explore the vulnerabilities to protect against them and to recommend wise parameter choices. Our work demonstrates a new type of vulnerability, which the community of researchers will continue to explore in future work. We look forward to continuing to work with the community to ensure that the next cryptography standard will reliably safeguard data well into the future.

We'd like to acknowledge the contributions of Emily Wenger and Mingjie Chen to this research.

Learn more about SALSA by reading our paper"
Meta_Blog,https://ai.meta.com/blog/nllb-200-high-quality-machine-translation/,,200 languages within a single AI model: A breakthrough in high-quality machine translation,"Meta AI has built a single AI model, NLLB-200, that is the first to translate across 200 different languages with state-of-the-art quality that has been validated through extensive evaluations for each of them.

We’ve also created a new evaluation dataset, FLORES-200, and measured NLLB-200’s performance in each language to confirm that the translations are high quality. NLLB-200 exceeds the previous state of the art by an average of 44 percent.

We’re now using modeling techniques and learnings from the project to improve and extend translations on Facebook, Instagram, and Wikipedia.

We’re open-sourcing NLLB-200 models, FLORES-200, model training code, and code for re-creating the training dataset in order to help other researchers improve their translation tools and build on our work.

Language is our culture, identity, and lifeline to the world. But because high-quality translation tools don’t exist for hundreds of languages, billions of people today can't access digital content or participate fully in conversations and communities online in their preferred or native languages. This is especially true for hundreds of millions of people who speak the many languages of Africa and Asia.

Something Went Wrong We're having trouble playing this video. Learn more

To help people connect better today and be part of the metaverse of tomorrow, Meta AI researchers created No Language Left Behind (NLLB), an effort to develop high-quality machine translation capabilities for most of the world’s languages. Today, we’re announcing an important breakthrough in NLLB: We’ve built a single AI model called NLLB-200, which translates 200 different languages with state-of-the-art results. Many of these languages, such as Kamba and Lao, were not supported well or at all by even the best existing translation tools today. Fewer than 25 African languages are currently supported by widely used translation tools — many of which are of poor quality. In contrast, NLLB-200 supports 55 African languages with high-quality results. In total, this single model can provide high-quality translations for languages spoken by billions of people around the globe. In total, NLLB-200’s BLEU scores improve on the previous state of the art by an average of 44 percent across all 10k directions of the FLORES-101 benchmark. For some African and Indian languages, the increase is greater than 70 percent over recent translation systems.

We are now open-sourcing the NLLB-200 model and publishing a slew of research tools to enable other researchers to extend this work to more languages and build more inclusive technologies. Meta AI is also providing up to $200,000 of grants to nonprofit organizations for real world applications for NLLB-200.

The research advancements from NLLB will support more than 25 billion translations served every day on Facebook News Feed, Instagram, and our other platforms. Imagine visiting a favorite Facebook group, coming across a post in Igbo or Luganda, and being able to understand it in your own language with just a click of a button. Highly accurate translations in more languages could also help to spot harmful content and misinformation, protect election integrity, and curb instances of online sexual exploitation and human trafficking. Modeling techniques and learnings from our NLLB research are now also being applied to translation systems used by Wikipedia editors.

Translation is one of the most exciting areas in AI because of its impact on people’s everyday lives. NLLB is about much more than just giving people better access to content on the web. It will make it easier for people to contribute and share information across languages. We have more work ahead, but we are energized by our recent progress and how it is moving us closer to fulfilling Meta’s mission.

You can explore a demo of NLLB-200 here, showing how the model can translate stories from around the world, and read the research paper here.

Unlocking translation tools for billions more people

We’ve partnered with the Wikimedia Foundation the nonprofit organization that hosts Wikipedia and other free knowledge projects, to help improve translation systems on Wikipedia. There are versions of Wikipedia in more than 300 languages, but most have far fewer articles than the 6+ million available in English. This disparity is especially large for languages primarily spoken outside of Europe and North America. For example there are around 3,260 Wikipedia articles in Lingala, a language spoken by 45 million people in the Democratic Republic of the Congo, Republic of the Congo, Central African Republic, and South Sudan. Contrast that with a language like Swedish, which has 10 million speakers in Sweden and Finland and more than 2.5 million articles.

Wikipedia editors are now using the technology behind NLLB-200, via the Wikimedia Foundation’s Content Translation Tool, to translate articles in more than 20 low-resource languages (those that don’t have extensive datasets to train AI systems), including 10 that previously were not supported by any machine translation tools on the platform.

The challenges of building a single model for hundreds of languages

Machine translation systems, like all AI models, are trained on data. For text translation systems, this typically consists of millions of sentences carefully matched between languages. But there simply aren’t large volumes of parallel sentences across, say, English and Fula. Current translation models try to overcome this by mining data from the web. But the results are often of poor quality because the source text is different for each of the languages. Furthermore, it is often full of incorrect or inconsistent spellings and is missing accent marks and other diacritical marks.

Another significant challenge is optimizing a single model to work across hundreds of languages without compromising performance or translation quality. Traditionally, the best translation quality has come from having a separate model for each language direction. But it’s difficult to scale this approach, as performance and translation quality suffer as more languages are added.

Translation models also produce errors that can be difficult to catch. These systems are built on neural networks used for text generation, so they can naturally produce errors such as hallucinations (confidently stating something as true even when it’s not), misstatements, and unsafe content. In general, there are simply fewer benchmarks and datasets for low-resource languages, which makes it much more difficult to test and improve models.

Innovating in architecture, data sourcing, benchmarking, and more

In recent years, we’ve made steady progress to overcome the challenges described above. In 2020, we announced our 100-language M2M-100 translation model, which leveraged new methods to acquire training data, new architectures to scale model size without compromising performance, and new ways to evaluate and improve the results. To scale to another 100 languages, we’ve made further advances in all three of these areas.

Expanded training resources

To collect highly accurate parallel texts in more languages, we improved LASER, our toolkit for zero-shot transfer in natural language processing (NLP). Instead of LSTM, the new version, LASER3, uses a Transformer model that is trained in a self-supervised manner with a masked language modeling objective. We further boosted performance by using a teacher-student training procedure and creating language-group specific encoders, which enabled us to scale LASER3’s language coverage and produce massive quantities of sentence pairs, even for low-resource languages. We are open-sourcing the LASER3 multilingual embedding method to make it available to other researchers, and we’re also making available billions of parallel sentences in different language pairs, which have been mined and cleaned using the techniques described here.

Since we cast a wider net when sourcing training examples in more languages, it was important to make sure the quality of the examples remained high. We completely overhauled our data cleaning pipeline to scale to 200 languages, adding major filtering steps that included first using our LID-200 models to filter data and remove noise from internet-scale corpora with high confidence. We developed toxicity lists for the full set of 200 languages, and then used those lists to assess and filter potential hallucinated toxicity. These steps ensured that we have cleaner and less toxic datasets with correctly identified languages. This is important for improving translation quality and reducing the risk of what is known as hallucinated toxicity, where the system mistakenly introduces toxic content during the translation process.

Scaling model size while maintaining high performance

Multilingual translation systems offer two major benefits. They enable similar languages — such as Assamese and Bengali, which are both written in Bengali script — to share data during training. This helps improve translation quality significantly for low-resource languages when trained together with similar high-resource languages. Also, researchers can iterate, scale, and experiment with a single multilingual model much more easily than with hundreds or even thousands of different bilingual models.

But there are still significant challenges when expanding a model from 100 to 200 languages. With more low-resource language pairs in the training data, the multilingual systems start to overfit as we train the models for longer periods. We tackled these issues by innovating on three fronts: regularization and curriculum learning, self-supervised learning, and diversifying back-translation.

First, we developed mixture-of-experts networks that have shared and specialized capacity so that low-resource languages without much data could be automatically routed to the shared capacity. This, combined with better designed regularization systems, avoids overfitting. We also followed a two-step curriculum learning approach, where we first trained the high-resource languages for a few epochs, before introducing the low-resource language pairs, which again reduced the overfitting problem. Then, given low quantities of parallel bitext data for low-resource languages, we leveraged self-supervised learning on monolingual data for both the low-resource and similar high-resource languages to improve the overall model performance.

Finally, we analyzed how to best generate back-translation data and found that mixing back-translated data generated from both bilingual statistical machine translation and multilingual neural machine translation models helped improve performance for low-resource languages due to the increased diversity of the generated synthetic data. To train the NLLB-200 model, which has 54B parameters, we leveraged our newly built Research SuperCluster (RSC), which is among the fastest AI supercomputers in the world.

Evaluation and mitigation tools for 200 languages

To evaluate and improve NLLB-200, we built FLORES-200, a unique many-to-many evaluation dataset that enables researchers to assess performance in 40,000 different language directions. We’re open-sourcing this new dataset to help other researchers rapidly test and improve their translation models. FLORES-200 can be used to evaluate translation systems for a wide range of applications, including health pamphlets, films, books, and online content within countries or regions where a number of low-resource languages are spoken.

Scaling to 200 languages meant addressing the risks of generating toxic content, which can be difficult to manage within a multidirectional translation system. We did this by building toxicity lists for all the supported languages to make it possible to detect and filter out profanity and other potentially offensive content. We’re releasing toxicity evaluation lists and benchmarks for all 200 languages to give other researchers the tools to reduce risks in their models.

And to ensure that we are expanding our efforts in a responsible manner, we are working with an interdisciplinary team that includes linguists, sociologists, and ethicists to learn more about each of the languages we consider.

This graphic shows average BLEU score on FLORES-101 translations to and from English into 100 languages. On the left there are two published state-of-the-art models, M2M and Delta LM, that support 100 languages. Models on the right support 200 languages: A baseline Transformer model with 3.3B parameters, the baseline model with self-supervised learning (SSL), the baseline model with back translation (BT), and NLLB-200, a large mixture-of-experts based model that leverages both self-supervised learning and back translation.

Expanded translation and greater inclusion

High-quality translation tools can be transformative. The reality today is that a handful of languages — including English, Mandarin, Spanish, and Arabic — dominate the web. Native speakers of these very widely spoken languages may lose sight of how meaningful it is to read something in your own mother tongue. We believe NLLB will help preserve language as it was intended to be shared rather than always requiring an intermediary language that often gets the sentiment/content wrong.

It can also help advance other NLP tasks, beyond translation. This could include building assistants that work well in languages such as Javanese and Uzbek or creating systems to take Bollywood movies and add accurate subtitles in Swahili or Oromo. As the metaverse begins to take shape, the ability to build technologies that work well in hundreds or even thousands of languages will truly help to democratize access to new, immersive experiences in virtual worlds.

A few short years ago, high-quality machine translation worked in only a handful of languages. With NLLB-200, we are closer to one day having systems that enable people to communicate with whomever they choose. We’re excited by what this unlocks in the present and what it could mean for the future as we continue to push the boundaries of machine translations.

This work is being undertaken by a multidisciplinary team at Meta AI that includes Bapi Akula, Pierre Andrews, Necip Fazil Ayan, Loic Barrault, Shruti Bhosale, Marta Ruiz Costa-jussa, James Cross, Onur Çelebi, Sergey Edunov, Maha Elbayad, Angela Fan, Cynthia Gao, Gabriel Mejia Gonzalez, Vedanuj Goswami, Francisco Guzmán, Prangthip Hansanti, Kennet Heafield, Kevin Heffernan, John Hoffman, Semarley Jarrett, Elahe Kalbassi, Philipp Koehn, Janice Lam, Daniel Licht, Jean Maillard, Alexandre Mourachko, Christophe Ropers, Kaushik Ram Sadagopan, Safiyyah Saleem, Holger Schwenk, Shannon Spruit, Anna Sun, Chau Tran, Skyler Wang, Guillaume Wenzek, Jeff Wang, and Al Youngblood."
Meta_Blog,https://ai.meta.com/blog/nllb-200-high-quality-machine-translation/,,200 languages within a single AI model: A breakthrough in high-quality machine translation,"Meta AI has built a single AI model, NLLB-200, that is the first to translate across 200 different languages with state-of-the-art quality that has been validated through extensive evaluations for each of them.

We’ve also created a new evaluation dataset, FLORES-200, and measured NLLB-200’s performance in each language to confirm that the translations are high quality. NLLB-200 exceeds the previous state of the art by an average of 44 percent.

We’re now using modeling techniques and learnings from the project to improve and extend translations on Facebook, Instagram, and Wikipedia.

We’re open-sourcing NLLB-200 models, FLORES-200, model training code, and code for re-creating the training dataset in order to help other researchers improve their translation tools and build on our work.

Language is our culture, identity, and lifeline to the world. But because high-quality translation tools don’t exist for hundreds of languages, billions of people today can't access digital content or participate fully in conversations and communities online in their preferred or native languages. This is especially true for hundreds of millions of people who speak the many languages of Africa and Asia.

Something Went Wrong We're having trouble playing this video. Learn more

To help people connect better today and be part of the metaverse of tomorrow, Meta AI researchers created No Language Left Behind (NLLB), an effort to develop high-quality machine translation capabilities for most of the world’s languages. Today, we’re announcing an important breakthrough in NLLB: We’ve built a single AI model called NLLB-200, which translates 200 different languages with state-of-the-art results. Many of these languages, such as Kamba and Lao, were not supported well or at all by even the best existing translation tools today. Fewer than 25 African languages are currently supported by widely used translation tools — many of which are of poor quality. In contrast, NLLB-200 supports 55 African languages with high-quality results. In total, this single model can provide high-quality translations for languages spoken by billions of people around the globe. In total, NLLB-200’s BLEU scores improve on the previous state of the art by an average of 44 percent across all 10k directions of the FLORES-101 benchmark. For some African and Indian languages, the increase is greater than 70 percent over recent translation systems.

We are now open-sourcing the NLLB-200 model and publishing a slew of research tools to enable other researchers to extend this work to more languages and build more inclusive technologies. Meta AI is also providing up to $200,000 of grants to nonprofit organizations for real world applications for NLLB-200.

The research advancements from NLLB will support more than 25 billion translations served every day on Facebook News Feed, Instagram, and our other platforms. Imagine visiting a favorite Facebook group, coming across a post in Igbo or Luganda, and being able to understand it in your own language with just a click of a button. Highly accurate translations in more languages could also help to spot harmful content and misinformation, protect election integrity, and curb instances of online sexual exploitation and human trafficking. Modeling techniques and learnings from our NLLB research are now also being applied to translation systems used by Wikipedia editors.

Translation is one of the most exciting areas in AI because of its impact on people’s everyday lives. NLLB is about much more than just giving people better access to content on the web. It will make it easier for people to contribute and share information across languages. We have more work ahead, but we are energized by our recent progress and how it is moving us closer to fulfilling Meta’s mission.

You can explore a demo of NLLB-200 here, showing how the model can translate stories from around the world, and read the research paper here.

Unlocking translation tools for billions more people

We’ve partnered with the Wikimedia Foundation the nonprofit organization that hosts Wikipedia and other free knowledge projects, to help improve translation systems on Wikipedia. There are versions of Wikipedia in more than 300 languages, but most have far fewer articles than the 6+ million available in English. This disparity is especially large for languages primarily spoken outside of Europe and North America. For example there are around 3,260 Wikipedia articles in Lingala, a language spoken by 45 million people in the Democratic Republic of the Congo, Republic of the Congo, Central African Republic, and South Sudan. Contrast that with a language like Swedish, which has 10 million speakers in Sweden and Finland and more than 2.5 million articles.

Wikipedia editors are now using the technology behind NLLB-200, via the Wikimedia Foundation’s Content Translation Tool, to translate articles in more than 20 low-resource languages (those that don’t have extensive datasets to train AI systems), including 10 that previously were not supported by any machine translation tools on the platform.

The challenges of building a single model for hundreds of languages

Machine translation systems, like all AI models, are trained on data. For text translation systems, this typically consists of millions of sentences carefully matched between languages. But there simply aren’t large volumes of parallel sentences across, say, English and Fula. Current translation models try to overcome this by mining data from the web. But the results are often of poor quality because the source text is different for each of the languages. Furthermore, it is often full of incorrect or inconsistent spellings and is missing accent marks and other diacritical marks.

Another significant challenge is optimizing a single model to work across hundreds of languages without compromising performance or translation quality. Traditionally, the best translation quality has come from having a separate model for each language direction. But it’s difficult to scale this approach, as performance and translation quality suffer as more languages are added.

Translation models also produce errors that can be difficult to catch. These systems are built on neural networks used for text generation, so they can naturally produce errors such as hallucinations (confidently stating something as true even when it’s not), misstatements, and unsafe content. In general, there are simply fewer benchmarks and datasets for low-resource languages, which makes it much more difficult to test and improve models.

Innovating in architecture, data sourcing, benchmarking, and more

In recent years, we’ve made steady progress to overcome the challenges described above. In 2020, we announced our 100-language M2M-100 translation model, which leveraged new methods to acquire training data, new architectures to scale model size without compromising performance, and new ways to evaluate and improve the results. To scale to another 100 languages, we’ve made further advances in all three of these areas.

Expanded training resources

To collect highly accurate parallel texts in more languages, we improved LASER, our toolkit for zero-shot transfer in natural language processing (NLP). Instead of LSTM, the new version, LASER3, uses a Transformer model that is trained in a self-supervised manner with a masked language modeling objective. We further boosted performance by using a teacher-student training procedure and creating language-group specific encoders, which enabled us to scale LASER3’s language coverage and produce massive quantities of sentence pairs, even for low-resource languages. We are open-sourcing the LASER3 multilingual embedding method to make it available to other researchers, and we’re also making available billions of parallel sentences in different language pairs, which have been mined and cleaned using the techniques described here.

Since we cast a wider net when sourcing training examples in more languages, it was important to make sure the quality of the examples remained high. We completely overhauled our data cleaning pipeline to scale to 200 languages, adding major filtering steps that included first using our LID-200 models to filter data and remove noise from internet-scale corpora with high confidence. We developed toxicity lists for the full set of 200 languages, and then used those lists to assess and filter potential hallucinated toxicity. These steps ensured that we have cleaner and less toxic datasets with correctly identified languages. This is important for improving translation quality and reducing the risk of what is known as hallucinated toxicity, where the system mistakenly introduces toxic content during the translation process.

Scaling model size while maintaining high performance

Multilingual translation systems offer two major benefits. They enable similar languages — such as Assamese and Bengali, which are both written in Bengali script — to share data during training. This helps improve translation quality significantly for low-resource languages when trained together with similar high-resource languages. Also, researchers can iterate, scale, and experiment with a single multilingual model much more easily than with hundreds or even thousands of different bilingual models.

But there are still significant challenges when expanding a model from 100 to 200 languages. With more low-resource language pairs in the training data, the multilingual systems start to overfit as we train the models for longer periods. We tackled these issues by innovating on three fronts: regularization and curriculum learning, self-supervised learning, and diversifying back-translation.

First, we developed mixture-of-experts networks that have shared and specialized capacity so that low-resource languages without much data could be automatically routed to the shared capacity. This, combined with better designed regularization systems, avoids overfitting. We also followed a two-step curriculum learning approach, where we first trained the high-resource languages for a few epochs, before introducing the low-resource language pairs, which again reduced the overfitting problem. Then, given low quantities of parallel bitext data for low-resource languages, we leveraged self-supervised learning on monolingual data for both the low-resource and similar high-resource languages to improve the overall model performance.

Finally, we analyzed how to best generate back-translation data and found that mixing back-translated data generated from both bilingual statistical machine translation and multilingual neural machine translation models helped improve performance for low-resource languages due to the increased diversity of the generated synthetic data. To train the NLLB-200 model, which has 54B parameters, we leveraged our newly built Research SuperCluster (RSC), which is among the fastest AI supercomputers in the world.

Evaluation and mitigation tools for 200 languages

To evaluate and improve NLLB-200, we built FLORES-200, a unique many-to-many evaluation dataset that enables researchers to assess performance in 40,000 different language directions. We’re open-sourcing this new dataset to help other researchers rapidly test and improve their translation models. FLORES-200 can be used to evaluate translation systems for a wide range of applications, including health pamphlets, films, books, and online content within countries or regions where a number of low-resource languages are spoken.

Scaling to 200 languages meant addressing the risks of generating toxic content, which can be difficult to manage within a multidirectional translation system. We did this by building toxicity lists for all the supported languages to make it possible to detect and filter out profanity and other potentially offensive content. We’re releasing toxicity evaluation lists and benchmarks for all 200 languages to give other researchers the tools to reduce risks in their models.

And to ensure that we are expanding our efforts in a responsible manner, we are working with an interdisciplinary team that includes linguists, sociologists, and ethicists to learn more about each of the languages we consider.

This graphic shows average BLEU score on FLORES-101 translations to and from English into 100 languages. On the left there are two published state-of-the-art models, M2M and Delta LM, that support 100 languages. Models on the right support 200 languages: A baseline Transformer model with 3.3B parameters, the baseline model with self-supervised learning (SSL), the baseline model with back translation (BT), and NLLB-200, a large mixture-of-experts based model that leverages both self-supervised learning and back translation.

Expanded translation and greater inclusion

High-quality translation tools can be transformative. The reality today is that a handful of languages — including English, Mandarin, Spanish, and Arabic — dominate the web. Native speakers of these very widely spoken languages may lose sight of how meaningful it is to read something in your own mother tongue. We believe NLLB will help preserve language as it was intended to be shared rather than always requiring an intermediary language that often gets the sentiment/content wrong.

It can also help advance other NLP tasks, beyond translation. This could include building assistants that work well in languages such as Javanese and Uzbek or creating systems to take Bollywood movies and add accurate subtitles in Swahili or Oromo. As the metaverse begins to take shape, the ability to build technologies that work well in hundreds or even thousands of languages will truly help to democratize access to new, immersive experiences in virtual worlds.

A few short years ago, high-quality machine translation worked in only a handful of languages. With NLLB-200, we are closer to one day having systems that enable people to communicate with whomever they choose. We’re excited by what this unlocks in the present and what it could mean for the future as we continue to push the boundaries of machine translations.

This work is being undertaken by a multidisciplinary team at Meta AI that includes Bapi Akula, Pierre Andrews, Necip Fazil Ayan, Loic Barrault, Shruti Bhosale, Marta Ruiz Costa-jussa, James Cross, Onur Çelebi, Sergey Edunov, Maha Elbayad, Angela Fan, Cynthia Gao, Gabriel Mejia Gonzalez, Vedanuj Goswami, Francisco Guzmán, Prangthip Hansanti, Kennet Heafield, Kevin Heffernan, John Hoffman, Semarley Jarrett, Elahe Kalbassi, Philipp Koehn, Janice Lam, Daniel Licht, Jean Maillard, Alexandre Mourachko, Christophe Ropers, Kaushik Ram Sadagopan, Safiyyah Saleem, Holger Schwenk, Shannon Spruit, Anna Sun, Chau Tran, Skyler Wang, Guillaume Wenzek, Jeff Wang, and Al Youngblood."
Meta_Blog,https://ai.meta.com/blog/introducing-sphere-meta-ais-web-scale-corpus-for-better-knowledge-intensive-nlp/,,Introducing Sphere: Meta AI’s web-scale corpus for better knowledge-intensive NLP,"“Who won the first Nobel Prize in physics?” If you were stumped by that question 40 years ago, you might have opened up an encyclopedia. Today, you’d just ask the voice assistant on your phone. (It was Wilhelm Conrad Röntgen, by the way, for his discovery of X-rays).

How does the assistant on your phone know the answer? Just like humans did in decades past — by looking it up. In these kinds of question-answering or fact-checking tasks, known collectively as knowledge-intensive natural language processing (KI-NLP), AI models comb through a digital archive for relevant information. The more comprehensive the collection, the more answers it holds.

In the current research landscape, however, KI-NLP architectures face a few key limitations. First, they typically depend on commercial black-box search engines to surface relevant web knowledge. When we use such proprietary search engines, we don't know what we can't see; reader models might miss relevant information because the search engine algorithms rank it too low in the results. Alternatively, retrievers rely on Wikipedia to find relevant knowledge. While Wikipedia is accurate, well formatted, and small enough for the majority of architectures to navigate, it’s also crowdsourced and doesn’t capture all the knowledge available on the web. And its continued growth has made it challenging for editors to double-check every citation or inadvertent biases.

At Meta AI, we’re creating new advancements toward more intelligent AI systems that better leverage real-world knowledge. We’ve created the first white-box retrieval solution using the world’s vastest library — the open web — as a universal, uncurated, and unstructured source of knowledge, to solve multiple KI-NLP tasks at once. Our new knowledge source, Sphere, uses open web data rather than traditional, proprietary search engines. That means other AI researchers can see into and control the corpus, so they can experiment with scaling and optimizing different methods to push retrieval technology forward. Sphere contains 134 million documents — split into 906 million passages of 100 tokens each — representing orders of magnitude more data than the knowledge sources considered in current KI-NLP research. Because Sphere can access far more public information than today’s standard models, it could provide useful information that they cannot.

Sphere is open sourced here. We have tested Sphere on the Knowledge Intensive Language Tasks benchmark, and Sphere surpassed the state of the art on two. We’ve also tested it on a real-world application and developed a model that can successfully review and verify citations in Wikipedia.

Using web snapshot to build a knowledge corpus

To build Sphere, we first culled the text from a real web snapshot by CCNet, a variant of Common Crawl that jettisons redundant material and scores pages based on writing quality.

But why not turn to an existing search engine, rather than depend on Common Crawl? Unlike those proprietary black-box systems, Sphere provides a direct and explainable way to leverage the backbone of state-of-the-art NLP research. Sphere opens access to the whole corpus, which we hope will help us identify our retriever's blind spots.

Researchers can examine all the text in Sphere, so we can tinker with architectures that capitalize on the system’s strengths and zero in on specific weaknesses. That insight will help us build universal KI-NLP models that can handle diverse data.

An open corpus also allows us to experiment with new architectures, such as dense retrievers. In dense retrieval, documents and queries are represented as vectors, which can be easily fed to the reader model. In essence, the reader and retriever speak the same language, so it’s fairly straightforward to optimize them to interact with each other. By contrast, search engines were designed for humans to use, so our systems must communicate with them in natural language — raising the potential for errors in translation.

Overcoming the challenges of billion scale

In one of the most defining developments of our era, the web has opened access to in-depth information on a sweeping range of topics. Need to know when the last time tug-of-war was an official Olympic sport? The web has you covered. But that massive scope is both a blessing and a challenge. For our project to succeed, we needed to confront an important question: Are KI-NLP systems ready for web scale?

We found that for a corpus as big as Sphere, the dense index — which stores vector representations of corpus documents to make them easier for the retriever to find — quickly exceeds typical single-server hardware limits for both GPU and RAM. We responded by building distributed-faiss — a wrapper around FAISS, our open source library for similarity search. FAISS lets us search quickly for similar multimedia documents — a task where traditional query search engines fall short — in billion-scale data sets. The new wrapper, distributed-faiss, helps us apportion indices across multiple machines to manage the computational load.

Next-level language models

There’s no guarantee that traditional search engines will continue to allow AI researchers access to build KI-NLP models. As part of our ongoing commitment to help the AI community, we’re releasing Sphere, along with our precomputed sparse and dense indices and distributed-faiss library, to encourage further experimentation in this area. Sphere will help researchers train retrievers to handle a wider range of documents, preparing automatic systems for some of the web’s thorniest challenges — misinformation, noise, and incoherent text. In the real world, these models could muzzle harmful content and, when combined with a well-designed UI, enhance people’s digital literacy and critical thinking skills.

On the web, of course, we can’t be sure that any particular statement is accurate or that a single page will contain all the information we need. Indeed, some parts of the web are laden with toxic content and misinformation. Our next step is to train models to assess the quality of retrieved documents, detect potential contradictions, prioritize more trustworthy sources — and, if no convincing evidence exists, concede that they, like us, can still be stumped. We’re also continuously pushing new scaling advancements and techniques that will help us pave the way toward more ubiquitous search for better, smarter neural networks.

NOTE: Wikimedia and Meta are not partnering on this project. The project is still in the research phase and not being used to automatically update any content on Wikipedia."
Meta_Blog,https://ai.meta.com/blog/introducing-sphere-meta-ais-web-scale-corpus-for-better-knowledge-intensive-nlp/,,Introducing Sphere: Meta AI’s web-scale corpus for better knowledge-intensive NLP,"“Who won the first Nobel Prize in physics?” If you were stumped by that question 40 years ago, you might have opened up an encyclopedia. Today, you’d just ask the voice assistant on your phone. (It was Wilhelm Conrad Röntgen, by the way, for his discovery of X-rays).

How does the assistant on your phone know the answer? Just like humans did in decades past — by looking it up. In these kinds of question-answering or fact-checking tasks, known collectively as knowledge-intensive natural language processing (KI-NLP), AI models comb through a digital archive for relevant information. The more comprehensive the collection, the more answers it holds.

In the current research landscape, however, KI-NLP architectures face a few key limitations. First, they typically depend on commercial black-box search engines to surface relevant web knowledge. When we use such proprietary search engines, we don't know what we can't see; reader models might miss relevant information because the search engine algorithms rank it too low in the results. Alternatively, retrievers rely on Wikipedia to find relevant knowledge. While Wikipedia is accurate, well formatted, and small enough for the majority of architectures to navigate, it’s also crowdsourced and doesn’t capture all the knowledge available on the web. And its continued growth has made it challenging for editors to double-check every citation or inadvertent biases.

At Meta AI, we’re creating new advancements toward more intelligent AI systems that better leverage real-world knowledge. We’ve created the first white-box retrieval solution using the world’s vastest library — the open web — as a universal, uncurated, and unstructured source of knowledge, to solve multiple KI-NLP tasks at once. Our new knowledge source, Sphere, uses open web data rather than traditional, proprietary search engines. That means other AI researchers can see into and control the corpus, so they can experiment with scaling and optimizing different methods to push retrieval technology forward. Sphere contains 134 million documents — split into 906 million passages of 100 tokens each — representing orders of magnitude more data than the knowledge sources considered in current KI-NLP research. Because Sphere can access far more public information than today’s standard models, it could provide useful information that they cannot.

Sphere is open sourced here. We have tested Sphere on the Knowledge Intensive Language Tasks benchmark, and Sphere surpassed the state of the art on two. We’ve also tested it on a real-world application and developed a model that can successfully review and verify citations in Wikipedia.

Using web snapshot to build a knowledge corpus

To build Sphere, we first culled the text from a real web snapshot by CCNet, a variant of Common Crawl that jettisons redundant material and scores pages based on writing quality.

But why not turn to an existing search engine, rather than depend on Common Crawl? Unlike those proprietary black-box systems, Sphere provides a direct and explainable way to leverage the backbone of state-of-the-art NLP research. Sphere opens access to the whole corpus, which we hope will help us identify our retriever's blind spots.

Researchers can examine all the text in Sphere, so we can tinker with architectures that capitalize on the system’s strengths and zero in on specific weaknesses. That insight will help us build universal KI-NLP models that can handle diverse data.

An open corpus also allows us to experiment with new architectures, such as dense retrievers. In dense retrieval, documents and queries are represented as vectors, which can be easily fed to the reader model. In essence, the reader and retriever speak the same language, so it’s fairly straightforward to optimize them to interact with each other. By contrast, search engines were designed for humans to use, so our systems must communicate with them in natural language — raising the potential for errors in translation.

Overcoming the challenges of billion scale

In one of the most defining developments of our era, the web has opened access to in-depth information on a sweeping range of topics. Need to know when the last time tug-of-war was an official Olympic sport? The web has you covered. But that massive scope is both a blessing and a challenge. For our project to succeed, we needed to confront an important question: Are KI-NLP systems ready for web scale?

We found that for a corpus as big as Sphere, the dense index — which stores vector representations of corpus documents to make them easier for the retriever to find — quickly exceeds typical single-server hardware limits for both GPU and RAM. We responded by building distributed-faiss — a wrapper around FAISS, our open source library for similarity search. FAISS lets us search quickly for similar multimedia documents — a task where traditional query search engines fall short — in billion-scale data sets. The new wrapper, distributed-faiss, helps us apportion indices across multiple machines to manage the computational load.

Next-level language models

There’s no guarantee that traditional search engines will continue to allow AI researchers access to build KI-NLP models. As part of our ongoing commitment to help the AI community, we’re releasing Sphere, along with our precomputed sparse and dense indices and distributed-faiss library, to encourage further experimentation in this area. Sphere will help researchers train retrievers to handle a wider range of documents, preparing automatic systems for some of the web’s thorniest challenges — misinformation, noise, and incoherent text. In the real world, these models could muzzle harmful content and, when combined with a well-designed UI, enhance people’s digital literacy and critical thinking skills.

On the web, of course, we can’t be sure that any particular statement is accurate or that a single page will contain all the information we need. Indeed, some parts of the web are laden with toxic content and misinformation. Our next step is to train models to assess the quality of retrieved documents, detect potential contradictions, prioritize more trustworthy sources — and, if no convincing evidence exists, concede that they, like us, can still be stumped. We’re also continuously pushing new scaling advancements and techniques that will help us pave the way toward more ubiquitous search for better, smarter neural networks.

NOTE: Wikimedia and Meta are not partnering on this project. The project is still in the research phase and not being used to automatically update any content on Wikipedia."
Meta_Blog,https://ai.meta.com/blog/ai-driven-acoustic-synthesis-for-augmented-and-virtual-reality-experiences/,,Introducing AI-driven acoustic synthesis for AR and VR,"Something Went Wrong We're having trouble playing this video. Learn more

Whether it’s mingling at a party in the metaverse or watching a home movie in your living room while wearing augmented reality (AR) glasses, acoustics play a role in how these moments will be experienced. We are building for mixed reality and virtual reality experiences like these, and we believe AI will be core to delivering sound quality that realistically matches the settings people are immersed in.

Today, Meta AI researchers, in collaboration with an audio specialist from Meta’s Reality Labs and researchers from the University of Texas at Austin, are open-sourcing three new models for audio-visual understanding of human speech and sounds in video that are designed to push us toward this reality at a faster rate.

We need AI models that understand a person’s physical surroundings based on both how they look and how things sound. For example, there’s a big difference between how a concert would sound in a large venue versus in your living room. That’s because the geometry of a physical space, the materials and surfaces in the area, and the proximity of where the sounds are coming from all factor into how we hear audio.

Something Went Wrong We're having trouble playing this video. Learn more

The research we are sharing today with the AI community focuses on three audio-visual tasks that outperform existing methods. For our Visual Acoustic Matching model, we can input an audio clip recorded anywhere, along with an image of a target environment, and transform the clip to make it sound as if it were recorded in that environment. For example, the model could take an image of a dining room in a restaurant, together with the audio of a voice recorded in a cave, and make that voice sound instead like it was recorded in the pictured restaurant. The second model, Visually-Informed Dereverberation, does the opposite. Using observed sounds and the visual cues of a space, it focuses on removing reverberation, which is the echo a sound makes based on the environment where it is recorded. Imagine a violin concert in a busy train station. This model can distill the essence of the violinist’s music without the reverberations bouncing around the massive train station. The third model, VisualVoice, uses visual and audio cues to separate speech from other background sounds and voices, which will be beneficial for human and machine understanding tasks, such as creating better subtitles or mingling at a party in VR.

All three works tie into the body of AI research we are doing at Meta AI around audio-visual perception. We envision a future where people can put on AR glasses and relive a holographic memory that looks and sounds the exact way they experienced it from their vantage point, or feel immersed by not just the graphics but also the sounds as they play games in a virtual world. These models are bringing us even closer to the multimodal, immersive experiences we want to build in the future.

Visual Acoustic Matching

Anyone who has watched a video where the audio isn’t consistent with the scene knows how disruptive this can feel to human perception. However, getting audio and video from different environments to match has previously been a challenge. Acoustic simulation models can be used to generate a room impulse response to re-create the acoustics of a room, but this can be done only if the geometry — often in the form of a 3D mesh — and material properties of the space are known. In most cases, this information isn’t available. Acoustic properties can also be estimated from just the audio captured in a particular room, but this provides only limited acoustic information about the target space from the reverberation of the audio sample. While these approaches are available, they often do not yield great results.

To address these challenges, we created a self-supervised Visual Acoustic Matching model, called AViTAR, which adjusts audio to match the space of a target image. We use a cross-modal transformer model, where the inputs consist of both images and audio, allowing the transformer to perform intermodality reasoning and generate a realistic audio output that matches the visual input. The self-supervised training objective learns acoustic matching from in-the-wild web videos, despite their lack of acoustically mismatched audio and unlabeled data.

We built this task with two datasets. For our first dataset, we built on the work we did with SoundSpaces, the audio-visual platform for AI that we open-sourced in 2020. Built on top of AI Habitat, SoundSpaces makes it possible to insert high-fidelity, realistic simulations of any sound source into various real-world scanned environments from the open -source Replica and Matterport3D datasets.The second dataset consists of three- to 10-second clips of people speaking across 290,000 publicly available English-language videos.

For both datasets, we focused on speech in indoor settings, given their relevance to many of the possible future use cases and because human listeners have strong prior knowledge about how reverberation should affect speech. We filtered the datasets down to clips that met our problem formulation criterion: The microphone and camera needed to be located together and away from the sound source. This was important because sounds may be heard differently depending on where the source of the sound is and where the person or microphone is located.

Something Went Wrong We're having trouble playing this video. Learn more

One challenge we had to overcome for the web videos was that we only had audio matching the acoustics of the target environment. Because of this, we introduced the idea of mismatches — first by performing dereverberation to remove reverberation. We then intertwined the audio with the impulse response of another environment to randomize the acoustics, and added noise to create audio that has the same content but different acoustics.

We validated our model on both datasets and measured the quality of the generated audio on three criteria, including whether it was closest to the ground truth audio (if available), the correctness of room acoustics, and the speech quality preserved in the synthesized speech. But we also wanted to see how it performed with human listeners, whom we asked to evaluate whether the acoustics matched the reference image. The results show that our model successfully translates human speech to a variety of real-world environments depicted in images, outperforming both traditional audio-only acoustic matching and more heavily supervised baselines.

For Visual Acoustic Matching, one future use case we are interested in involves reliving past memories. Imagine being able to put on a pair of AR glasses and see an object with the option to play a memory associated with it, such as picking up a tutu and seeing a hologram of your child’s ballet recital. The audio strips away reverberation and makes the memory sound just like the time you experienced it, sitting in your exact seat in the audience.

Something Went Wrong We're having trouble playing this video. Learn more

Visually-Informed Dereverberation

While there are many cases where adding reverberation with visual acoustic matching is helpful, there are also settings where we need to do the opposite, removing reverberation in order to enhance hearing and understanding.

Reverberation reflects off surfaces and objects in the environment, degrades the quality of speech for human perception, and severely affects the accuracy of automatic speech recognition. By removing dereverberation, we strip away environmental effects so that speech can be more easily recognized and enhanced, helping automatic speech recognition create more accurate subtitles for people with hearing loss, for example.

Prior approaches have tried to remove reverberation based solely on the audio modality, but this does not inform us of the complete acoustic characteristics of the environment. Blind dereverberation relies on prior knowledge of human speech to remove the reverberation, without accounting for the surrounding environment. This is why we need visual observations.

The Visually-Informed Dereverberation of Audio (VIDA) model learns to remove reverberation based on both the observed sounds and the visual stream, which reveals cues about room geometry, materials, and speaker locations — factors that influence the reverberation effects heard in the audio stream.

Something Went Wrong We're having trouble playing this video. Learn more

In this case, we want to take the reverberant audio from a specific place and strip away the room’s acoustic effects. To do this, we built on our work with SoundSpaces and developed a large-scale training dataset that uses realistic acoustic renderings of speech.

We demonstrated our approach on simulated and real imagery for speech enhancement, speech recognition, and speaker identification. Our results show that VIDA achieves state-of-the-art performance and is a substantial improvement over traditional audio-only methods. This will be important as we build realistic experiences for mixed and virtual reality.

A third model, VisualVoice, understands speech by looking as well as hearing. This is important for improving human and machine perception.

One reason people are better than AI at understanding speech in complex settings is that we use not just our ears but also our eyes. For example, we might see someone’s mouth moving and intuitively know the voice we’re hearing must be coming from that person. That’s why Meta AI is working on new conversational AI systems that, like humans, can recognize the nuanced correlations between what they see and what they hear in conversation.

VisualVoice learns in a way that’s similar to how people master new skills — multimodally — by learning visual and auditory cues from unlabeled videos to achieve audio-visual speech separation. For machines, this creates better perception, which can improve areas of accessibility, such as creating more accurate captions. Human perception also improves. For example, imagine being able to attend a group meeting in the metaverse with colleagues from around the world, but instead of people having fewer conversations and talking over one another, the reverberation and acoustics would adjust accordingly as they moved around the virtual space and joined smaller groups. VisualVoice generalizes well to challenging real-world videos of diverse scenarios.

Together, these models could one day enable smart assistants to hear what we’re telling them, no matter the circumstances — whether at a concert, at a crowded party, or in any other noisy place.

Building a future with AI models that understand the world around us

Existing AI models do a good job understanding images, and are getting better at video understanding. However, if we want to build new, immersive experiences for AR and VR, we need AI models that are multimodal — models that can take audio, video, and text signals all at once and create a much richer understanding of the environment.

This is an area we will continue exploring. AViTAR and VIDA are currently based on only a single image. In the future, we want to explore using video and other dynamics to capture the acoustic properties of a space. This will help bring us closer to our goal of creating multimodal AI that understands real-world environments and how people experience them.

We are excited to share this research with the open source community. We believe AI that understands the world around us can help unlock exciting new possibilities to benefit how people experience and interact in mixed and virtual reality.

Download our research papers and explore the project pages to see our models in action.

Visual Acoustic Matching

Research Paper

Project Page

Visually-Informed Dereverberation

Research Paper

Project Page

VisualVoice

Research Paper

Project Page

The research in this blog post reflects the contributions of Kristen Grauman and Changan Chen. We'd also like to acknowledge Paul Calamia of Meta’s Reality Labs and Ruohan Gao of Stanford."
Meta_Blog,https://ai.meta.com/blog/ai-driven-acoustic-synthesis-for-augmented-and-virtual-reality-experiences/,,Introducing AI-driven acoustic synthesis for AR and VR,"Something Went Wrong We're having trouble playing this video. Learn more

Whether it’s mingling at a party in the metaverse or watching a home movie in your living room while wearing augmented reality (AR) glasses, acoustics play a role in how these moments will be experienced. We are building for mixed reality and virtual reality experiences like these, and we believe AI will be core to delivering sound quality that realistically matches the settings people are immersed in.

Today, Meta AI researchers, in collaboration with an audio specialist from Meta’s Reality Labs and researchers from the University of Texas at Austin, are open-sourcing three new models for audio-visual understanding of human speech and sounds in video that are designed to push us toward this reality at a faster rate.

We need AI models that understand a person’s physical surroundings based on both how they look and how things sound. For example, there’s a big difference between how a concert would sound in a large venue versus in your living room. That’s because the geometry of a physical space, the materials and surfaces in the area, and the proximity of where the sounds are coming from all factor into how we hear audio.

Something Went Wrong We're having trouble playing this video. Learn more

The research we are sharing today with the AI community focuses on three audio-visual tasks that outperform existing methods. For our Visual Acoustic Matching model, we can input an audio clip recorded anywhere, along with an image of a target environment, and transform the clip to make it sound as if it were recorded in that environment. For example, the model could take an image of a dining room in a restaurant, together with the audio of a voice recorded in a cave, and make that voice sound instead like it was recorded in the pictured restaurant. The second model, Visually-Informed Dereverberation, does the opposite. Using observed sounds and the visual cues of a space, it focuses on removing reverberation, which is the echo a sound makes based on the environment where it is recorded. Imagine a violin concert in a busy train station. This model can distill the essence of the violinist’s music without the reverberations bouncing around the massive train station. The third model, VisualVoice, uses visual and audio cues to separate speech from other background sounds and voices, which will be beneficial for human and machine understanding tasks, such as creating better subtitles or mingling at a party in VR.

All three works tie into the body of AI research we are doing at Meta AI around audio-visual perception. We envision a future where people can put on AR glasses and relive a holographic memory that looks and sounds the exact way they experienced it from their vantage point, or feel immersed by not just the graphics but also the sounds as they play games in a virtual world. These models are bringing us even closer to the multimodal, immersive experiences we want to build in the future.

Visual Acoustic Matching

Anyone who has watched a video where the audio isn’t consistent with the scene knows how disruptive this can feel to human perception. However, getting audio and video from different environments to match has previously been a challenge. Acoustic simulation models can be used to generate a room impulse response to re-create the acoustics of a room, but this can be done only if the geometry — often in the form of a 3D mesh — and material properties of the space are known. In most cases, this information isn’t available. Acoustic properties can also be estimated from just the audio captured in a particular room, but this provides only limited acoustic information about the target space from the reverberation of the audio sample. While these approaches are available, they often do not yield great results.

To address these challenges, we created a self-supervised Visual Acoustic Matching model, called AViTAR, which adjusts audio to match the space of a target image. We use a cross-modal transformer model, where the inputs consist of both images and audio, allowing the transformer to perform intermodality reasoning and generate a realistic audio output that matches the visual input. The self-supervised training objective learns acoustic matching from in-the-wild web videos, despite their lack of acoustically mismatched audio and unlabeled data.

We built this task with two datasets. For our first dataset, we built on the work we did with SoundSpaces, the audio-visual platform for AI that we open-sourced in 2020. Built on top of AI Habitat, SoundSpaces makes it possible to insert high-fidelity, realistic simulations of any sound source into various real-world scanned environments from the open -source Replica and Matterport3D datasets.The second dataset consists of three- to 10-second clips of people speaking across 290,000 publicly available English-language videos.

For both datasets, we focused on speech in indoor settings, given their relevance to many of the possible future use cases and because human listeners have strong prior knowledge about how reverberation should affect speech. We filtered the datasets down to clips that met our problem formulation criterion: The microphone and camera needed to be located together and away from the sound source. This was important because sounds may be heard differently depending on where the source of the sound is and where the person or microphone is located.

Something Went Wrong We're having trouble playing this video. Learn more

One challenge we had to overcome for the web videos was that we only had audio matching the acoustics of the target environment. Because of this, we introduced the idea of mismatches — first by performing dereverberation to remove reverberation. We then intertwined the audio with the impulse response of another environment to randomize the acoustics, and added noise to create audio that has the same content but different acoustics.

We validated our model on both datasets and measured the quality of the generated audio on three criteria, including whether it was closest to the ground truth audio (if available), the correctness of room acoustics, and the speech quality preserved in the synthesized speech. But we also wanted to see how it performed with human listeners, whom we asked to evaluate whether the acoustics matched the reference image. The results show that our model successfully translates human speech to a variety of real-world environments depicted in images, outperforming both traditional audio-only acoustic matching and more heavily supervised baselines.

For Visual Acoustic Matching, one future use case we are interested in involves reliving past memories. Imagine being able to put on a pair of AR glasses and see an object with the option to play a memory associated with it, such as picking up a tutu and seeing a hologram of your child’s ballet recital. The audio strips away reverberation and makes the memory sound just like the time you experienced it, sitting in your exact seat in the audience.

Something Went Wrong We're having trouble playing this video. Learn more

Visually-Informed Dereverberation

While there are many cases where adding reverberation with visual acoustic matching is helpful, there are also settings where we need to do the opposite, removing reverberation in order to enhance hearing and understanding.

Reverberation reflects off surfaces and objects in the environment, degrades the quality of speech for human perception, and severely affects the accuracy of automatic speech recognition. By removing dereverberation, we strip away environmental effects so that speech can be more easily recognized and enhanced, helping automatic speech recognition create more accurate subtitles for people with hearing loss, for example.

Prior approaches have tried to remove reverberation based solely on the audio modality, but this does not inform us of the complete acoustic characteristics of the environment. Blind dereverberation relies on prior knowledge of human speech to remove the reverberation, without accounting for the surrounding environment. This is why we need visual observations.

The Visually-Informed Dereverberation of Audio (VIDA) model learns to remove reverberation based on both the observed sounds and the visual stream, which reveals cues about room geometry, materials, and speaker locations — factors that influence the reverberation effects heard in the audio stream.

Something Went Wrong We're having trouble playing this video. Learn more

In this case, we want to take the reverberant audio from a specific place and strip away the room’s acoustic effects. To do this, we built on our work with SoundSpaces and developed a large-scale training dataset that uses realistic acoustic renderings of speech.

We demonstrated our approach on simulated and real imagery for speech enhancement, speech recognition, and speaker identification. Our results show that VIDA achieves state-of-the-art performance and is a substantial improvement over traditional audio-only methods. This will be important as we build realistic experiences for mixed and virtual reality.

A third model, VisualVoice, understands speech by looking as well as hearing. This is important for improving human and machine perception.

One reason people are better than AI at understanding speech in complex settings is that we use not just our ears but also our eyes. For example, we might see someone’s mouth moving and intuitively know the voice we’re hearing must be coming from that person. That’s why Meta AI is working on new conversational AI systems that, like humans, can recognize the nuanced correlations between what they see and what they hear in conversation.

VisualVoice learns in a way that’s similar to how people master new skills — multimodally — by learning visual and auditory cues from unlabeled videos to achieve audio-visual speech separation. For machines, this creates better perception, which can improve areas of accessibility, such as creating more accurate captions. Human perception also improves. For example, imagine being able to attend a group meeting in the metaverse with colleagues from around the world, but instead of people having fewer conversations and talking over one another, the reverberation and acoustics would adjust accordingly as they moved around the virtual space and joined smaller groups. VisualVoice generalizes well to challenging real-world videos of diverse scenarios.

Together, these models could one day enable smart assistants to hear what we’re telling them, no matter the circumstances — whether at a concert, at a crowded party, or in any other noisy place.

Building a future with AI models that understand the world around us

Existing AI models do a good job understanding images, and are getting better at video understanding. However, if we want to build new, immersive experiences for AR and VR, we need AI models that are multimodal — models that can take audio, video, and text signals all at once and create a much richer understanding of the environment.

This is an area we will continue exploring. AViTAR and VIDA are currently based on only a single image. In the future, we want to explore using video and other dynamics to capture the acoustic properties of a space. This will help bring us closer to our goal of creating multimodal AI that understands real-world environments and how people experience them.

We are excited to share this research with the open source community. We believe AI that understands the world around us can help unlock exciting new possibilities to benefit how people experience and interact in mixed and virtual reality.

Download our research papers and explore the project pages to see our models in action.

Visual Acoustic Matching

Research Paper

Project Page

Visually-Informed Dereverberation

Research Paper

Project Page

VisualVoice

Research Paper

Project Page

The research in this blog post reflects the contributions of Kristen Grauman and Changan Chen. We'd also like to acknowledge Paul Calamia of Meta’s Reality Labs and Ruohan Gao of Stanford."
Meta_Blog,https://ai.meta.com/blog/new-research-helps-ai-navigate-unfamiliar-indoor-3d-spaces/,,"New research helps AI navigate unfamiliar, indoor 3D spaces","To create and interact with immersive new experiences in the metaverse — where people can navigate virtual worlds as well as our physical world with augmented reality — AI systems must learn to move through the complexities of the physical world as people do. AR glasses that show us where we left our keys, for example, require foundational new technologies that help AI understand the layout and dimensions of unfamiliar, ever-changing environments without high-compute resources, like preprovided maps. As humans, for example, we don’t need to learn the precise location or length of our coffee table to be able to walk around it without bumping into its corners (most of the time).

Today, we're announcing new scientific research that will help AI learn much more flexibly and efficiently to understand the physical world. Collectively, this body of work pushes advancements in visual navigation of embodied AI, a research area focused on training AI systems through interactions in 3D simulations rather than traditional 2D datasets.

We’ve built a point-goal navigation modelthat can navigate an entirely new environment without requiring a pre-provided map or a GPS sensor. We did this using Habitat 2.0, our state-of-the-art embodied AI platform that runs simulations orders of magnitude faster than real time

To further improve training without reliance on maps, we’ve created and released Habitat-Web, a training data collection of over 100K different human demonstrations for object-goal navigation methods. For each human demonstration, a paid Mechanical Turk user is provided a task instruction (e.g., “find the chest of drawers”) and teleoperates the virtual robot through a web browser interface on their computer.

We've developed the first ""plug and play"" modular approach that helps robots generalize to a diverse set of semantic navigation tasks and goal modalities---without retraining---in a novel zero-shot experience learning framework.

And we’re continuing to push for efficiency with a novel formulation for object-goal navigation tasks that obtains state-of-the-art results while achieving a 1,600x reduction in training time compared to prior methods.

Meta AI has been committed to long-term investments in the burgeoning field of embodied AI. Since we introduced Habitat three years ago, we’ve effectively solved the task of point-goal navigation using only an RGB-D camera, GPS, and compass data, and we’ve successfully tested our model with tasks in real-world physical settings using our PyRobot platform. Now, our latest advancements use the power of high-speed simulations to build more flexible, efficient navigation systems.

Achieving point-goal navigation without GPS sensors

The agent is tasked with navigating from its starting location to a goal location specified as a coordinate relative to its initial location.

Models achieving high performance at point-goal navigation often require access to GPS sensors in simulation. These models do not transfer well to physical robots, because GPS data can be noisy, unreliable, or unavailable in indoor spaces.

We’ve developed new methods to improve the way AI tracks its location solely from visual inputs, also known as visual odometry. Our new data-augmentation technique trains simple but highly effective neural models without human data annotations. Robust visual odometry is all you need to push the state of the art from 71.7 percent success to 94 percent success on the Realistic PointNav task without GPS or compass data, under noisy action dynamics.

While our approach does not yet completely solve this dataset, this research provides evidence to support that explicit mapping may not be necessary for navigation, even in realistic settings. Read the paper here..

Scaling the paradigm of imitation learning

In another path toward more efficient, map-free learning methods, we have shown how to scale the paradigm of imitation learning from human demonstrations, which — until now — hasn’t been possible, as there hasn’t been a large enough data collection of human demonstrations. To fill this gap, we built Habitat-Web, a new data collection infrastructure for embodied AI, connecting our Habitat simulator running in a web browser to Mechanical Turk and allowing remote users to teleoperate virtual robots safely and at scale. We’ve collected an order of magnitude larger human demo data than existing datasets in simulation and two orders of magnitude larger than existing datasets on real robots.

Agents trained with imitation learning on this data achieve state-of-the-art results, and more important, learn efficient object-search behavior from humans — peeking into rooms, checking corners for small objects, and turning in place to get a panoramic view.

None of these is exhibited as prominently by reinforcement learning (RL) agents, and enabling these types of behaviors in RL agents would require tedious, dense reward energy. It shows that for ObjectNav, a single human demonstration appears to be worth at least five agent-gathered ones from RL.Read the paper here.

Zero-shot experience learning for visual navigation

Our approach enables zero-shot experience learning (i.e., perform the target task without receiving any new experiences), and it adapts its policy much faster using fewer target-specific interactions.

When it comes to training AI to find objects, most embodied AI advancements work well on separate, well-defined tasks based on goal type (e.g., “find an object,” “navigate to a room”) or modality (e.g., text, audio). But to work well in the dynamic real world, agents need to adapt their skills on the fly without resource-intensive maps or lengthy retraining processes. In a first-of-its-kind zero-shot experience learning (ZSEL) framework, our model is trained once to capture the essential skills for semantic visual navigation and then applied to different target tasks without additional retraining in a 3D environment.

ZSEL works using a general-purpose semantic search policy that captures the essential navigation skills. This policy is trained by searching for image-goals, where an agent receives a picture taken from a random location in the environment and must travel to find it. Our approach requires up to 12.5x less training data and has up to a 14 percent better success rate than the state of the art in transfer learning. Over five navigation tasks, our ZSEL method saves more than 500 million training interactions and about six weeks of GPU compute required by the task-specific policies learned from scratch.Read the paper here.

Fewer interactions on ‘where to look’ for object-goal navigation

We introduce a method called Potential Functions for ObjectGoal Navigation with Interaction-Free Learning (PONI) to decide where to look for an unseen object in indoor 3D environments. Our key insight is that this is fundamentally a perception problem that can be solved without any interactive learning. PONI makes learning 1,600x more efficient than prior methods.

RL has been the predominant method of training virtual embodied agents, but this method notoriously requires significant computational resources and time for learning. We’ve figured out a new method to train object-goal navigation policies more efficiently than prior work while incurring up to 1,600x less computational cost for training via experiments on Gibson and Matterport3D. We do this using Potential Functions for ObjectGoal Navigation with Interaction-free Learning (PONI), a new paradigm for learning modular ObjectNav policies that disentangles the object search skill (i.e., where to look for an object) and the navigation skill (i.e., how to navigate to X, Y).

Something Went Wrong We're having trouble playing this video. Learn more

Our key insight is that “where to look?” can be treated purely as a perception problem and learned without interactions. Our network predicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. PONI not only improves over the state of the art for this task but also makes it significantly easier for researchers to achieve this. Read the paper here.

What’s next for embodied AI at Meta

In the near future, we’ll work on pushing these advancements from navigation to mobile manipulation to build agents that carry out specific tasks, like “find my wallet and bring it back to me.”

We’ll also tackle a host of new exciting challenges: How does this work in simulations carry over to physical robots? How can an embodied agent learn in a self-supervised manner without any human involvement in the form of reward engineering, demonstrations, or 3D annotations? And how do we scale simulation to the next order of magnitude of simulation and learning speed?"
Meta_Blog,https://ai.meta.com/blog/new-research-helps-ai-navigate-unfamiliar-indoor-3d-spaces/,,"New research helps AI navigate unfamiliar, indoor 3D spaces","To create and interact with immersive new experiences in the metaverse — where people can navigate virtual worlds as well as our physical world with augmented reality — AI systems must learn to move through the complexities of the physical world as people do. AR glasses that show us where we left our keys, for example, require foundational new technologies that help AI understand the layout and dimensions of unfamiliar, ever-changing environments without high-compute resources, like preprovided maps. As humans, for example, we don’t need to learn the precise location or length of our coffee table to be able to walk around it without bumping into its corners (most of the time).

Today, we're announcing new scientific research that will help AI learn much more flexibly and efficiently to understand the physical world. Collectively, this body of work pushes advancements in visual navigation of embodied AI, a research area focused on training AI systems through interactions in 3D simulations rather than traditional 2D datasets.

We’ve built a point-goal navigation modelthat can navigate an entirely new environment without requiring a pre-provided map or a GPS sensor. We did this using Habitat 2.0, our state-of-the-art embodied AI platform that runs simulations orders of magnitude faster than real time

To further improve training without reliance on maps, we’ve created and released Habitat-Web, a training data collection of over 100K different human demonstrations for object-goal navigation methods. For each human demonstration, a paid Mechanical Turk user is provided a task instruction (e.g., “find the chest of drawers”) and teleoperates the virtual robot through a web browser interface on their computer.

We've developed the first ""plug and play"" modular approach that helps robots generalize to a diverse set of semantic navigation tasks and goal modalities---without retraining---in a novel zero-shot experience learning framework.

And we’re continuing to push for efficiency with a novel formulation for object-goal navigation tasks that obtains state-of-the-art results while achieving a 1,600x reduction in training time compared to prior methods.

Meta AI has been committed to long-term investments in the burgeoning field of embodied AI. Since we introduced Habitat three years ago, we’ve effectively solved the task of point-goal navigation using only an RGB-D camera, GPS, and compass data, and we’ve successfully tested our model with tasks in real-world physical settings using our PyRobot platform. Now, our latest advancements use the power of high-speed simulations to build more flexible, efficient navigation systems.

Achieving point-goal navigation without GPS sensors

The agent is tasked with navigating from its starting location to a goal location specified as a coordinate relative to its initial location.

Models achieving high performance at point-goal navigation often require access to GPS sensors in simulation. These models do not transfer well to physical robots, because GPS data can be noisy, unreliable, or unavailable in indoor spaces.

We’ve developed new methods to improve the way AI tracks its location solely from visual inputs, also known as visual odometry. Our new data-augmentation technique trains simple but highly effective neural models without human data annotations. Robust visual odometry is all you need to push the state of the art from 71.7 percent success to 94 percent success on the Realistic PointNav task without GPS or compass data, under noisy action dynamics.

While our approach does not yet completely solve this dataset, this research provides evidence to support that explicit mapping may not be necessary for navigation, even in realistic settings. Read the paper here..

Scaling the paradigm of imitation learning

In another path toward more efficient, map-free learning methods, we have shown how to scale the paradigm of imitation learning from human demonstrations, which — until now — hasn’t been possible, as there hasn’t been a large enough data collection of human demonstrations. To fill this gap, we built Habitat-Web, a new data collection infrastructure for embodied AI, connecting our Habitat simulator running in a web browser to Mechanical Turk and allowing remote users to teleoperate virtual robots safely and at scale. We’ve collected an order of magnitude larger human demo data than existing datasets in simulation and two orders of magnitude larger than existing datasets on real robots.

Agents trained with imitation learning on this data achieve state-of-the-art results, and more important, learn efficient object-search behavior from humans — peeking into rooms, checking corners for small objects, and turning in place to get a panoramic view.

None of these is exhibited as prominently by reinforcement learning (RL) agents, and enabling these types of behaviors in RL agents would require tedious, dense reward energy. It shows that for ObjectNav, a single human demonstration appears to be worth at least five agent-gathered ones from RL.Read the paper here.

Zero-shot experience learning for visual navigation

Our approach enables zero-shot experience learning (i.e., perform the target task without receiving any new experiences), and it adapts its policy much faster using fewer target-specific interactions.

When it comes to training AI to find objects, most embodied AI advancements work well on separate, well-defined tasks based on goal type (e.g., “find an object,” “navigate to a room”) or modality (e.g., text, audio). But to work well in the dynamic real world, agents need to adapt their skills on the fly without resource-intensive maps or lengthy retraining processes. In a first-of-its-kind zero-shot experience learning (ZSEL) framework, our model is trained once to capture the essential skills for semantic visual navigation and then applied to different target tasks without additional retraining in a 3D environment.

ZSEL works using a general-purpose semantic search policy that captures the essential navigation skills. This policy is trained by searching for image-goals, where an agent receives a picture taken from a random location in the environment and must travel to find it. Our approach requires up to 12.5x less training data and has up to a 14 percent better success rate than the state of the art in transfer learning. Over five navigation tasks, our ZSEL method saves more than 500 million training interactions and about six weeks of GPU compute required by the task-specific policies learned from scratch.Read the paper here.

Fewer interactions on ‘where to look’ for object-goal navigation

We introduce a method called Potential Functions for ObjectGoal Navigation with Interaction-Free Learning (PONI) to decide where to look for an unseen object in indoor 3D environments. Our key insight is that this is fundamentally a perception problem that can be solved without any interactive learning. PONI makes learning 1,600x more efficient than prior methods.

RL has been the predominant method of training virtual embodied agents, but this method notoriously requires significant computational resources and time for learning. We’ve figured out a new method to train object-goal navigation policies more efficiently than prior work while incurring up to 1,600x less computational cost for training via experiments on Gibson and Matterport3D. We do this using Potential Functions for ObjectGoal Navigation with Interaction-free Learning (PONI), a new paradigm for learning modular ObjectNav policies that disentangles the object search skill (i.e., where to look for an object) and the navigation skill (i.e., how to navigate to X, Y).

Something Went Wrong We're having trouble playing this video. Learn more

Our key insight is that “where to look?” can be treated purely as a perception problem and learned without interactions. Our network predicts two complementary potential functions conditioned on a semantic map and uses them to decide where to look for an unseen object. PONI not only improves over the state of the art for this task but also makes it significantly easier for researchers to achieve this. Read the paper here.

What’s next for embodied AI at Meta

In the near future, we’ll work on pushing these advancements from navigation to mobile manipulation to build agents that carry out specific tasks, like “find my wallet and bring it back to me.”

We’ll also tackle a host of new exciting challenges: How does this work in simulations carry over to physical robots? How can an embodied agent learn in a self-supervised manner without any human involvement in the form of reward engineering, demonstrations, or 3D annotations? And how do we scale simulation to the next order of magnitude of simulation and learning speed?"
Meta_Blog,https://ai.meta.com/blog/introducing-mugen-a-new-dataset-for-multimodal-research/,,"Introducing MUGEN, a new dataset for multimodal research","What the research is:

What if you could search for specific video content — “sheepdogs on speedboats,” say, or “child in green hat singing the ABC song” — and receive an accurate, complete list of results, regardless of how the videos were tagged? Or, picture the reverse: You describe a scene — or upload atmospheric music, perhaps with a little dialogue — and a software program generates a video based on your suggestion, without the need for cameras, lights, actors, or editors. These applications are among the goals of multimodal research, in which AI models are trained to find relationships between different types of data, typically images, video, audio, and text. Although the AI community has made significant advances on image-text tasks, progress in the video-text domain lags. This is due in part to the difficulty inherent in modeling complicated spatiotemporal information, as well as the massive reserves of storage space and computing power that this work demands.

To push the field forward, we are releasing MUGEN (short for Multimodal Understanding and GENeration), a dataset of 375K linked video, audio, and text samples. Drawn from an enhanced version of the platform game CoinRun, the videos in MUGEN show the main character navigating environments and interacting with other characters in diverse and complex ways. Each video is annotated with text descriptions written by humans, as well as automatically generated templated text descriptions, frame-level pixel-accurate semantic segmentation maps, and synchronized audio. This combination of simplified visuals and rich annotations will enable the community to make progress on multimodal video-audio-text tasks without requiring prohibitively large computing resources.

Something Went Wrong We're having trouble playing this video. Learn more

One of the obstacles to advancing multimodal technology is that multimodal datasets tend to be either too complex or too simple to be useful. Existing datasets belong to two categories: open world, meaning collected in the wild, and closed world, which depict simple, often artificially generated environments. Most video-text datasets are open world, but the complex dynamics in these live-action videos have proved too challenging for current text-to-video generation systems to represent accurately. In response, researchers have turned to more constrained datasets for the training and evaluation of their models. However, the videos in these closed-world collections portray only limited actions and interactions between entities, aspects crucial to modeling real-world videos.

With MUGEN, we sought a happy medium: a compilation of closed-world videos that is narrow along some dimensions but rich along others, to enable focused advances in multimodal research. As with Meta’s other recent multimodal work, including our open source Pythia framework for vision and language research, the Situated and Interactive Multimodal Conversations (SIMMC) dataset, the Halo annotation platform, the Learning from Videos project, and CommerceMM , a new approach to multimodal understanding for online shopping, we wanted to use our resources to help improve AI’s understanding of complex data.

How it works:

We based our dataset on OpenAI’s CoinRun, a platform game about an alien — whom we call Mugen — who tries to gather coins without being killed by monsters. We modified CoinRun to make our videos more diverse and delightful — adding audio, slowing game physics, adjusting camera zoom and stabilization, and enabling new interactions between characters. In our version of CoinRun, Mugen can take 16 different actions. The core actions — walk, jump, collect coin, kill monster, power-up, climb ladder, bump head, and die — trigger different sound effects, which are layered with background music to produce the full audio track. Ten monsters interact with Mugen; some walk, others hop, and one flies.

Something Went Wrong We're having trouble playing this video. Learn more

Before modification of CoinRun (no audio present).

Something Went Wrong We're having trouble playing this video. Learn more

After modification of CoinRun

To collect videos, we trained 14 reinforcement learning (RL) agents — AI systems that learn through trial and error to navigate an environment in search of a predetermined reward — to play and record the game. We assigned each agent a different objective in order to increase the diversity of our dataset. For example, if we programmed an agent to value immediate rewards over future gain, the agent would take more risks and, as a result, die more often.

Our RL agents recorded 233,000 unique videos, which we split into three-second clips. (We’ve included both the longer, uncut videos and the clips in the dataset.) To demonstrate associations between text and video, we then asked human annotators to describe, in one to two sentences, what happened during those snippets of gameplay. We also developed a template-based algorithm to generate text descriptions based on game engine metadata about Mugen’s actions. After filtering low-quality annotations, we added 378,902 text descriptions for the 375,368 video clips to the MUGEN dataset. (Some clips have more than one description.)

Each video is also paired with synced audio, generated from background music and sound effects. In many open world video-text datasets, the audio and video are not well aligned: Irrelevant background noise is common, and the people shown may be talking about something unrelated to the current scene. In contrast, the video and audio in MUGEN are synchronized based on Mugen’s actions, which will facilitate experiments with underexplored tasks, like audio generation from video or text. Semantic segmentation maps delineate the objects in each frame pixel by pixel, helping models learn to associate the visual features in a clip with the coinciding sounds and text, and with similar content in other video frames.

Taken together, our annotations — the semantic maps, synced audio, and written descriptions — help illuminate the connections between video, sound, and language, which we hope will guide multimodal models to refine their understanding of these relationships.

We are also releasing the updated game engine , which can enable more customized data collection, and the trained RL agents. In our paper , we have benchmarked the performance of retrieval and generation between every pair of modalities. The code for these baselines is available here .

Why it matters:

We believe the research community can benefit from datasets that are narrow but rich — offering challenges that are achievable for current systems while reflecting the variety of activity depicted in real-world videos. Research done with the MUGEN dataset will have many real-world applications down the line. Imagine if you could, for example, find all the TV shows that have featured a particular tune (audio-to-video retrieval) or type a description of an unidentified bird call to search a library of sound effects (text-to-audio retrieval). MUGEN can also help researchers build new text-to-video generation systems — automatically creating videos from scratch based on written instructions. This type of cross-modal generation task is comparatively under-researched, largely due to a lack of feasible datasets.

The interactions between entities in MUGEN are more diverse than in other closed-world datasets. At the same time, compared with an in-the-wild dataset, the videos in MUGEN have a limited set of stripped-down objects and scenes: The physics are simplified, the camera angle is fixed, and the lighting is consistent. That narrowness will allow researchers to make steady progress on more feasible, bite-size challenges. We have also developed a storage-efficient online pipeline that can render videos and semantic maps at different resolutions on the fly based on metadata stored in a json file. By reducing data and compute requirements, MUGEN will allow a much wider swath of the AI community to join in the work to advance multimodal understanding and generation.

Read the paper

Get the code"
Meta_Blog,https://ai.meta.com/blog/introducing-mugen-a-new-dataset-for-multimodal-research/,,"Introducing MUGEN, a new dataset for multimodal research","What the research is:

What if you could search for specific video content — “sheepdogs on speedboats,” say, or “child in green hat singing the ABC song” — and receive an accurate, complete list of results, regardless of how the videos were tagged? Or, picture the reverse: You describe a scene — or upload atmospheric music, perhaps with a little dialogue — and a software program generates a video based on your suggestion, without the need for cameras, lights, actors, or editors. These applications are among the goals of multimodal research, in which AI models are trained to find relationships between different types of data, typically images, video, audio, and text. Although the AI community has made significant advances on image-text tasks, progress in the video-text domain lags. This is due in part to the difficulty inherent in modeling complicated spatiotemporal information, as well as the massive reserves of storage space and computing power that this work demands.

To push the field forward, we are releasing MUGEN (short for Multimodal Understanding and GENeration), a dataset of 375K linked video, audio, and text samples. Drawn from an enhanced version of the platform game CoinRun, the videos in MUGEN show the main character navigating environments and interacting with other characters in diverse and complex ways. Each video is annotated with text descriptions written by humans, as well as automatically generated templated text descriptions, frame-level pixel-accurate semantic segmentation maps, and synchronized audio. This combination of simplified visuals and rich annotations will enable the community to make progress on multimodal video-audio-text tasks without requiring prohibitively large computing resources.

Something Went Wrong We're having trouble playing this video. Learn more

One of the obstacles to advancing multimodal technology is that multimodal datasets tend to be either too complex or too simple to be useful. Existing datasets belong to two categories: open world, meaning collected in the wild, and closed world, which depict simple, often artificially generated environments. Most video-text datasets are open world, but the complex dynamics in these live-action videos have proved too challenging for current text-to-video generation systems to represent accurately. In response, researchers have turned to more constrained datasets for the training and evaluation of their models. However, the videos in these closed-world collections portray only limited actions and interactions between entities, aspects crucial to modeling real-world videos.

With MUGEN, we sought a happy medium: a compilation of closed-world videos that is narrow along some dimensions but rich along others, to enable focused advances in multimodal research. As with Meta’s other recent multimodal work, including our open source Pythia framework for vision and language research, the Situated and Interactive Multimodal Conversations (SIMMC) dataset, the Halo annotation platform, the Learning from Videos project, and CommerceMM , a new approach to multimodal understanding for online shopping, we wanted to use our resources to help improve AI’s understanding of complex data.

How it works:

We based our dataset on OpenAI’s CoinRun, a platform game about an alien — whom we call Mugen — who tries to gather coins without being killed by monsters. We modified CoinRun to make our videos more diverse and delightful — adding audio, slowing game physics, adjusting camera zoom and stabilization, and enabling new interactions between characters. In our version of CoinRun, Mugen can take 16 different actions. The core actions — walk, jump, collect coin, kill monster, power-up, climb ladder, bump head, and die — trigger different sound effects, which are layered with background music to produce the full audio track. Ten monsters interact with Mugen; some walk, others hop, and one flies.

Something Went Wrong We're having trouble playing this video. Learn more

Before modification of CoinRun (no audio present).

Something Went Wrong We're having trouble playing this video. Learn more

After modification of CoinRun

To collect videos, we trained 14 reinforcement learning (RL) agents — AI systems that learn through trial and error to navigate an environment in search of a predetermined reward — to play and record the game. We assigned each agent a different objective in order to increase the diversity of our dataset. For example, if we programmed an agent to value immediate rewards over future gain, the agent would take more risks and, as a result, die more often.

Our RL agents recorded 233,000 unique videos, which we split into three-second clips. (We’ve included both the longer, uncut videos and the clips in the dataset.) To demonstrate associations between text and video, we then asked human annotators to describe, in one to two sentences, what happened during those snippets of gameplay. We also developed a template-based algorithm to generate text descriptions based on game engine metadata about Mugen’s actions. After filtering low-quality annotations, we added 378,902 text descriptions for the 375,368 video clips to the MUGEN dataset. (Some clips have more than one description.)

Each video is also paired with synced audio, generated from background music and sound effects. In many open world video-text datasets, the audio and video are not well aligned: Irrelevant background noise is common, and the people shown may be talking about something unrelated to the current scene. In contrast, the video and audio in MUGEN are synchronized based on Mugen’s actions, which will facilitate experiments with underexplored tasks, like audio generation from video or text. Semantic segmentation maps delineate the objects in each frame pixel by pixel, helping models learn to associate the visual features in a clip with the coinciding sounds and text, and with similar content in other video frames.

Taken together, our annotations — the semantic maps, synced audio, and written descriptions — help illuminate the connections between video, sound, and language, which we hope will guide multimodal models to refine their understanding of these relationships.

We are also releasing the updated game engine , which can enable more customized data collection, and the trained RL agents. In our paper , we have benchmarked the performance of retrieval and generation between every pair of modalities. The code for these baselines is available here .

Why it matters:

We believe the research community can benefit from datasets that are narrow but rich — offering challenges that are achievable for current systems while reflecting the variety of activity depicted in real-world videos. Research done with the MUGEN dataset will have many real-world applications down the line. Imagine if you could, for example, find all the TV shows that have featured a particular tune (audio-to-video retrieval) or type a description of an unidentified bird call to search a library of sound effects (text-to-audio retrieval). MUGEN can also help researchers build new text-to-video generation systems — automatically creating videos from scratch based on written instructions. This type of cross-modal generation task is comparatively under-researched, largely due to a lack of feasible datasets.

The interactions between entities in MUGEN are more diverse than in other closed-world datasets. At the same time, compared with an in-the-wild dataset, the videos in MUGEN have a limited set of stripped-down objects and scenes: The physics are simplified, the camera angle is fixed, and the lighting is consistent. That narrowness will allow researchers to make steady progress on more feasible, bite-size challenges. We have also developed a storage-efficient online pipeline that can render videos and semantic maps at different resolutions on the fly based on metadata stored in a json file. By reducing data and compute requirements, MUGEN will allow a much wider swath of the AI community to join in the work to advance multimodal understanding and generation.

Read the paper

Get the code"
Meta_Blog,https://ai.meta.com/blog/advancing-direct-speech-to-speech-modeling-with-discrete-units/,,Advancing direct speech-to-speech modeling with discrete units,"What the research is:

To make it possible for people to easily understand each other while speaking in different languages, we need more than just text-based translation systems. But the conventional approach to building speech-to-speech translation systems has faced two significant shortcomings. It uses a cascaded series of steps — speech recognition, then text-to-text translation, and finally conversion of translated text back to speech — where the computational costs and inference latency accumulate in each stage. In addition, more than 40 percent of the world’s languages are without text writing systems, making this approach infeasible for extending translations to every spoken language.

To enable faster inference and support translation between unwritten languages, Meta AI is sharing new work on our direct speech-to-speech translation (S2ST) approach, which does not rely on text generation as an intermediate step. Our method outperforms previous approaches and is the first direct S2ST system trained on real-world open sourced audio data instead of synthetic audio for multiple language pairs.

How it works:

Recent speech-to-speech modeling work takes the same approach as traditional text-to-speech synthesis. These models directly translate source speech into target speech spectrograms, which are the spectrum of frequencies represented as multidimensional continuous-value vectors. It can be difficult to train translation models using speech spectrograms as the target, however, because they must learn several different aspects of the relationship between two languages. (How they align with one another, for example, and how their acoustic and linguistic characteristics compare.)

Instead of spectrograms, we use discretized speech units obtained from the clustering of self-supervised speech representations. Compared with spectrograms, discrete units can disentangle linguistic content from prosodic speech information and take advantage of existing natural language processing modeling techniques. Using discretized speech units, we’ve produced three notable advancements: Our S2ST system outperforms previous direct S2ST systems; it is the first direct S2ST system trained on real S2ST data for multiple language pairs; and it leverages pretraining with unlabeled speech data.

Direct speech-to-speech translation with discrete units

To facilitate direct speech-to-speech translation with discrete units ( audio samples ), we use self-supervised discrete units as targets (speech-to-unit translation, or S2UT) for training the direct S2ST system. In the graphic below, we propose a transformer-based sequence-to-sequence model with a speech encoder and a discrete unit decoder that incorporates auxiliary tasks (shown in dashed lines).

An illustration of the direct S2ST model with discrete units.

We perform our experiments using the Fisher Spanish-English speech translation corpus consisting of 139K sentences (approximately 170 hours) from telephone conversations in Spanish and the corresponding Spanish and English text transcriptions. We use a high-quality in-house text-to-speech engine to prepare synthetic target speech with a single female voice as the training target. All our experiments — including the baselines — are performed with the synthetic target speech and do not rely on the TTS engine for other uses. The proposed system can be trained in a textless setup by using discrete units in the source language as the auxiliary task target, which helps it achieve significant improvement compared with previous work. Using discrete units yields an improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features.

Training a textless speech-to-speech translation system in multiple languages using real-world data

Due to the lack of parallel S2ST training data, previous work on direct S2ST mainly relies on TTS to generate synthetic target speech for model training, which is impractical for supporting languages without a standard text writing system. Given the recent release of large-scale S2ST data from the FAIR team at Meta AI, in “ Textless speech-to-speech translation on real data ” ( audio samples ), we train our proposed S2UT system on real data from VoxPopuli S2S data ( download ) and automatically mined S2S data ( download ) without any extra text supervision. The key is a speech normalization technique that can be trained with as little as one hour of speech data. This method removes the variations in real target speech from multiple speakers without changing the lexical content and leads to improvement in the S2UT performance compared to unnormalized targets.

Additionally, our best textless direct speech translation model achieves similar performance to that of cascaded text-based systems without needing human annotations for building the ASR models to transcribe target speech. Further incorporating automatically mined S2ST data during training shows an additional 2.0 BLEU gain. This is the first time a textless S2ST system has been successfully trained with publicly available real-world data on multiple languages while also showing competitive results. We believe it is also the first empirical study to demonstrate the usefulness of the mined S2ST data.

Further improvements on speech-to-speech translation through pretraining

Lastly, we continue to improve upon the S2UT performance of the systems in the previous two papers through pretraining with unlabeled speech data in “ Enhanced direct speech-to-speech translation using self-supervised pretraining and data augmentation ” ( audio samples ). We show that pretraining inspirations from state-of-the-art speech-to-text translation (S2T) systems can transfer well to direct S2ST with the use of discrete units as target, bringing at least 6.5 BLEU gain and bridging or even exceeding the performance of cascaded systems. Furthermore, we augment the training data with weakly supervised data generated from more than 1K hours of speech, which leads to an additional 2.7 BLEU gain. Our effort opens up a path for future speech-to-speech translation research to further improve translation quality and produce more seamless communication experiences for users.

Spanish to English: Source and translation

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

English to Spanish: Source and translation

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

In these examples, our S2ST model directly generated the translations, without creating a text translation first. The text shown on screen was generated by an ASR model in order to make the examples clearer for readers.

Why it matters:

Direct speech-to-speech modeling with discrete units presents an exciting future for building better translation systems. Beyond just translation quality, benchmarks also show that our proposed system is the most efficient in terms of runtime, FLOPS, and max memory compared with spectrogram-based S2ST systems and cascaded systems.

The work discussed here also moves us closer to translation systems that work well for unwritten languages, which remain popular for dialects all over the world and are largely unsupported. With the release of our papers and code, we hope to enable future direct speech-to-speech translation advancements across the research community. Our evaluations are done with open sourced models. We hope that our measurement protocols can be leveraged so that all future progress can be compared fairly and openly.

Read the full papers and get the code:

This series of work discussed in this blog post is made possible by Yossi Adi, Peng-Jen Chen, Paul-Ambroise Duquenne, Hongyu Gong, Jiatao Gu, Qing He, Wei-Ning Hsu, Ann Lee, Xutai Ma, Juan Pino, Adam Polyak, Sravya Popuri, Holger Schwenk, Yun Tang, Changhan Wang (listed in alphabetical order). We thank Necip Fazil Ayan, Brian Bui, Andy Chung, Jade Copet, Ning Dong, Emmanuel Dupoux, Hirofumi Inaguma, Semarley Jarrett, Justine Kao, Evgeny Kharitonov, Felix Kreuk, Ilia Kulikov, Kushal Lakhotia, Abdelrahman Mohamed, Tu Anh Nguyen, Brian O'Horo, Gustavo Gandia Rivera, Morgane Rivière, Chris Summers, Jeff Wang, Carleigh Wood, Ethan Ye and Al Youngblood (listed in alphabetical order) for their support and discussions of this work."
Meta_Blog,https://ai.meta.com/blog/advancing-direct-speech-to-speech-modeling-with-discrete-units/,,Advancing direct speech-to-speech modeling with discrete units,"What the research is:

To make it possible for people to easily understand each other while speaking in different languages, we need more than just text-based translation systems. But the conventional approach to building speech-to-speech translation systems has faced two significant shortcomings. It uses a cascaded series of steps — speech recognition, then text-to-text translation, and finally conversion of translated text back to speech — where the computational costs and inference latency accumulate in each stage. In addition, more than 40 percent of the world’s languages are without text writing systems, making this approach infeasible for extending translations to every spoken language.

To enable faster inference and support translation between unwritten languages, Meta AI is sharing new work on our direct speech-to-speech translation (S2ST) approach, which does not rely on text generation as an intermediate step. Our method outperforms previous approaches and is the first direct S2ST system trained on real-world open sourced audio data instead of synthetic audio for multiple language pairs.

How it works:

Recent speech-to-speech modeling work takes the same approach as traditional text-to-speech synthesis. These models directly translate source speech into target speech spectrograms, which are the spectrum of frequencies represented as multidimensional continuous-value vectors. It can be difficult to train translation models using speech spectrograms as the target, however, because they must learn several different aspects of the relationship between two languages. (How they align with one another, for example, and how their acoustic and linguistic characteristics compare.)

Instead of spectrograms, we use discretized speech units obtained from the clustering of self-supervised speech representations. Compared with spectrograms, discrete units can disentangle linguistic content from prosodic speech information and take advantage of existing natural language processing modeling techniques. Using discretized speech units, we’ve produced three notable advancements: Our S2ST system outperforms previous direct S2ST systems; it is the first direct S2ST system trained on real S2ST data for multiple language pairs; and it leverages pretraining with unlabeled speech data.

Direct speech-to-speech translation with discrete units

To facilitate direct speech-to-speech translation with discrete units ( audio samples ), we use self-supervised discrete units as targets (speech-to-unit translation, or S2UT) for training the direct S2ST system. In the graphic below, we propose a transformer-based sequence-to-sequence model with a speech encoder and a discrete unit decoder that incorporates auxiliary tasks (shown in dashed lines).

An illustration of the direct S2ST model with discrete units.

We perform our experiments using the Fisher Spanish-English speech translation corpus consisting of 139K sentences (approximately 170 hours) from telephone conversations in Spanish and the corresponding Spanish and English text transcriptions. We use a high-quality in-house text-to-speech engine to prepare synthetic target speech with a single female voice as the training target. All our experiments — including the baselines — are performed with the synthetic target speech and do not rely on the TTS engine for other uses. The proposed system can be trained in a textless setup by using discrete units in the source language as the auxiliary task target, which helps it achieve significant improvement compared with previous work. Using discrete units yields an improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features.

Training a textless speech-to-speech translation system in multiple languages using real-world data

Due to the lack of parallel S2ST training data, previous work on direct S2ST mainly relies on TTS to generate synthetic target speech for model training, which is impractical for supporting languages without a standard text writing system. Given the recent release of large-scale S2ST data from the FAIR team at Meta AI, in “ Textless speech-to-speech translation on real data ” ( audio samples ), we train our proposed S2UT system on real data from VoxPopuli S2S data ( download ) and automatically mined S2S data ( download ) without any extra text supervision. The key is a speech normalization technique that can be trained with as little as one hour of speech data. This method removes the variations in real target speech from multiple speakers without changing the lexical content and leads to improvement in the S2UT performance compared to unnormalized targets.

Additionally, our best textless direct speech translation model achieves similar performance to that of cascaded text-based systems without needing human annotations for building the ASR models to transcribe target speech. Further incorporating automatically mined S2ST data during training shows an additional 2.0 BLEU gain. This is the first time a textless S2ST system has been successfully trained with publicly available real-world data on multiple languages while also showing competitive results. We believe it is also the first empirical study to demonstrate the usefulness of the mined S2ST data.

Further improvements on speech-to-speech translation through pretraining

Lastly, we continue to improve upon the S2UT performance of the systems in the previous two papers through pretraining with unlabeled speech data in “ Enhanced direct speech-to-speech translation using self-supervised pretraining and data augmentation ” ( audio samples ). We show that pretraining inspirations from state-of-the-art speech-to-text translation (S2T) systems can transfer well to direct S2ST with the use of discrete units as target, bringing at least 6.5 BLEU gain and bridging or even exceeding the performance of cascaded systems. Furthermore, we augment the training data with weakly supervised data generated from more than 1K hours of speech, which leads to an additional 2.7 BLEU gain. Our effort opens up a path for future speech-to-speech translation research to further improve translation quality and produce more seamless communication experiences for users.

Spanish to English: Source and translation

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

English to Spanish: Source and translation

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

In these examples, our S2ST model directly generated the translations, without creating a text translation first. The text shown on screen was generated by an ASR model in order to make the examples clearer for readers.

Why it matters:

Direct speech-to-speech modeling with discrete units presents an exciting future for building better translation systems. Beyond just translation quality, benchmarks also show that our proposed system is the most efficient in terms of runtime, FLOPS, and max memory compared with spectrogram-based S2ST systems and cascaded systems.

The work discussed here also moves us closer to translation systems that work well for unwritten languages, which remain popular for dialects all over the world and are largely unsupported. With the release of our papers and code, we hope to enable future direct speech-to-speech translation advancements across the research community. Our evaluations are done with open sourced models. We hope that our measurement protocols can be leveraged so that all future progress can be compared fairly and openly.

Read the full papers and get the code:

This series of work discussed in this blog post is made possible by Yossi Adi, Peng-Jen Chen, Paul-Ambroise Duquenne, Hongyu Gong, Jiatao Gu, Qing He, Wei-Ning Hsu, Ann Lee, Xutai Ma, Juan Pino, Adam Polyak, Sravya Popuri, Holger Schwenk, Yun Tang, Changhan Wang (listed in alphabetical order). We thank Necip Fazil Ayan, Brian Bui, Andy Chung, Jade Copet, Ning Dong, Emmanuel Dupoux, Hirofumi Inaguma, Semarley Jarrett, Justine Kao, Evgeny Kharitonov, Felix Kreuk, Ilia Kulikov, Kushal Lakhotia, Abdelrahman Mohamed, Tu Anh Nguyen, Brian O'Horo, Gustavo Gandia Rivera, Morgane Rivière, Chris Summers, Jeff Wang, Carleigh Wood, Ethan Ye and Al Youngblood (listed in alphabetical order) for their support and discussions of this work."
Meta_Blog,https://ai.meta.com/blog/building-with-ai-across-all-of-meta/,,Building with AI across all of Meta,"Artificial Intelligence sits at the very heart of our work across Meta. In fact it would be hard to identify a single product that hasn’t been transformed by this work. Whether for personalization or protection, whether to improve existing services or create entirely novel ones, our future depends on our ability to leverage the newest AI technology at scale. To support this mission, today we are announcing a new decentralized organizational structure for Meta AI.

For the last several years the AI Organization here at Meta has been a key driver of improving our products as well as advancing fundamental research. Teams like AI Platform, AI for Product, and more recently AI4AR have developed state of the art techniques – drawing inspiration from their colleagues at FAIR and counterparts across the industry – to leverage AI to improve our products, to better protect the people who use them, and to build innovative new applications. These teams incubated countless new initiatives such as Responsible AI and have built a strong presence for Meta as a leader in the broader AI community.

Jerome Pesenti has been our fearless leader in this work but over the last several months he has put in place a plan to change the status quo. Jerome identified that while the centralized nature of the organization gave us leverage in some areas it also made it a challenge to integrate as deeply as we would hope. In the new model we will distribute the ownership of these AI systems back to Meta’s product groups. But we do so with the caveat that they must invest in a balanced portfolio that supports existing systems while also advancing the state of the art in AI. We believe that this will accelerate the adoption of important new technology across the company while allowing us to continue to push the envelope. The teams tasked with driving AI advancements and best practices into the products they support will be known as AI Innovation Centers.

Today we are announcing the following changes:

The Responsible AI organization will join the Social Impact team.

The AI for Product teams that work to protect the people using our platforms, improve recommendations and make content more relevant, and improve our Ads and Commerce services will move to our product engineering team.

The AI4AR team will join with the XR team in Reality Labs.

Our AI research team, FAIR, will become a new pillar within Reality Labs Research. Its mission and charter will remain unchanged: Drive fundamental breakthroughs in AI through research excellence, open science, and broad collaboration.

FAIR will continue to have incredibly strong leadership in place with Joelle Pineau, Antoine Bordes, and Yann LeCun.

As part of this transition, I am sorry to share that Jerome will depart Meta in mid-June after helping us through the early stages of this transition. We are grateful for the incredible work Jerome has done over the past 4+ years in building, leading, and scaling a world-class AI function for Meta.

To continue advancing both the AI technology and community at Meta we will also convene a new cross-functional AI leadership team led by Joelle. This group will be called upon to evaluate our progress in AI across the entire company. At our scale nothing comes easily. More centralized approaches run into their limits when the last mile proves to be too far for downstream teams to close the gap. With this new team structure, we are excited to push the boundaries of what AI can do and use it to create new features and products for billions of people."
Meta_Blog,https://ai.meta.com/blog/building-with-ai-across-all-of-meta/,,Building with AI across all of Meta,"Artificial Intelligence sits at the very heart of our work across Meta. In fact it would be hard to identify a single product that hasn’t been transformed by this work. Whether for personalization or protection, whether to improve existing services or create entirely novel ones, our future depends on our ability to leverage the newest AI technology at scale. To support this mission, today we are announcing a new decentralized organizational structure for Meta AI.

For the last several years the AI Organization here at Meta has been a key driver of improving our products as well as advancing fundamental research. Teams like AI Platform, AI for Product, and more recently AI4AR have developed state of the art techniques – drawing inspiration from their colleagues at FAIR and counterparts across the industry – to leverage AI to improve our products, to better protect the people who use them, and to build innovative new applications. These teams incubated countless new initiatives such as Responsible AI and have built a strong presence for Meta as a leader in the broader AI community.

Jerome Pesenti has been our fearless leader in this work but over the last several months he has put in place a plan to change the status quo. Jerome identified that while the centralized nature of the organization gave us leverage in some areas it also made it a challenge to integrate as deeply as we would hope. In the new model we will distribute the ownership of these AI systems back to Meta’s product groups. But we do so with the caveat that they must invest in a balanced portfolio that supports existing systems while also advancing the state of the art in AI. We believe that this will accelerate the adoption of important new technology across the company while allowing us to continue to push the envelope. The teams tasked with driving AI advancements and best practices into the products they support will be known as AI Innovation Centers.

Today we are announcing the following changes:

The Responsible AI organization will join the Social Impact team.

The AI for Product teams that work to protect the people using our platforms, improve recommendations and make content more relevant, and improve our Ads and Commerce services will move to our product engineering team.

The AI4AR team will join with the XR team in Reality Labs.

Our AI research team, FAIR, will become a new pillar within Reality Labs Research. Its mission and charter will remain unchanged: Drive fundamental breakthroughs in AI through research excellence, open science, and broad collaboration.

FAIR will continue to have incredibly strong leadership in place with Joelle Pineau, Antoine Bordes, and Yann LeCun.

As part of this transition, I am sorry to share that Jerome will depart Meta in mid-June after helping us through the early stages of this transition. We are grateful for the incredible work Jerome has done over the past 4+ years in building, leading, and scaling a world-class AI function for Meta.

To continue advancing both the AI technology and community at Meta we will also convene a new cross-functional AI leadership team led by Joelle. This group will be called upon to evaluate our progress in AI across the entire company. At our scale nothing comes easily. More centralized approaches run into their limits when the last mile proves to be too far for downstream teams to close the gap. With this new team structure, we are excited to push the boundaries of what AI can do and use it to create new features and products for billions of people."
Meta_Blog,https://ai.meta.com/blog/measure-fairness-and-mitigate-ai-bias/,,Introducing two new datasets to help measure fairness and mitigate AI bias,"Developing reliable, large-scale ways of measuring fairness and mitigating bias gives AI researchers and practitioners helpful benchmarks that can be used to test NLP (natural language processing) systems — driving progress toward the goal of ensuring that AI systems treat everyone fairly.

The research community has made significant strides in doing this with gender, race, and ethnicity. While this foundation is an important start in addressing fairness along these dimensions, it falls short of being able to uncover fairness issues on the basis of other relevant communities or identities, such as religion, socioeconomic background, and queer identities, for example. To better identify a wider range of demographic biases in a variety of technologies and to create AI systems that respectfully represent a greater diversity of personal identities, we need new tools and benchmarks.

Today, we are introducing and open-sourcing several datasets and models for responsible NLP to address this gap. For this work, we assembled a diverse team of researchers and contributors, including people who identify as men, as women, and as nonbinary; people of different races and ethnicities; and people at different stages of their careers. Together, we developed a new method to test a wide range of biases in NLP models, beyond just race, gender, and ethnicity, including a list of more than 500 terms across a dozen axes. We devised a straightforward technique for controlling how models generate text, and taught models to recognize and better avoid social biases when generating responses to certain demographic terms. We also trained an AI model for reducing demographic biases in text, which can help break stereotypical associations present in NLP data. This method, known as a demographic text perturber, introduces demographically diverse variations that are otherwise similar to the original text. For example, it might take the sentence “He likes his grandma"" and create alternatives, such as ""She likes her grandma"" and ""They like their grandma,"" to augment the dataset with parallel, demographically altered variants.

We hope that these datasets will be used to help further research around fairness in the AI community. State-of-the-art NLP systems are notoriously data-intensive and use large-scale text resources to train. This data reliance presents particular challenges, as AI systems can unwittingly replicate or even amplify unwanted social biases present in the data. For example, NLP data can contain biases and stereotypes about particular demographic groups — or fail to represent them entirely.

Training models to measure fairness and mitigate biases

To better identify a wider range of demographic biases in AI, we need a comprehensive set of terms that reflect a diverse set of identities. We used a combination of algorithmic and participatory processes to develop the most comprehensive descriptor list of its kind, which spans over 500 terms across roughly a dozen demographic axes. More specifically, we came up with sample terms per axis, then expanded these terms algorithmically using nearest neighbor techniques. We then used a participatory process that involved gathering new term ideas and feedback on existing terms from a variety of policy experts and domain experts (such as civil rights and racial justice experts), as well as individuals with lived experience related to a wide variety of identities (representing multiple ethnicities, religions, races, sexual orientations, genders, and disabilities).

This extensive set of demographic terms can be used to better measure and then mitigate model biases. Before this work, the field generally measured biases with respect to broad terms such as Asian, which means, for example, we would only know whether our models were biased against Asian people, not whether they were biased against Japanese people. Our list enables more fine-grained analysis and is built with terms that groups of people use when self-identifying, rather than just common dictionary terms.

To address these issues, we devised a straightforward technique for controlling how models generate text, and taught models to recognize and better avoid social biases when generating responses to certain demographic terms.

Data augmented with our models can be used for a wide range of practical applications, including evaluating the fairness of generative and classification models, improving the representation of minority groups in certain domains, and ultimately training models that exhibit less demographic bias. We are openly sharing our demographics terms list with the broader AI community, and we invite additional term contributions to further increase its inclusiveness, scope, and utility. This work also contributes to a broader initiative at Meta AI to build AI responsibly and inclusively, particularly when it comes to combating social biases and enhancing the representation of underrepresented groups.

In addition to creating this more comprehensive demographic term list to measure fairness, we have built a model for reducing demographic biases in text — a machine-learned sequence-to-sequence demographic perturber, which we trained on a large-scale dataset of human-generated text rewrites that we collected. Our demographic perturber is trained to perturb several demographic attributes, including gender, race/ethnicity, and age, which can help break stereotypical associations present in NLP data by augmenting it with parallel, demographically perturbed variants. For example, the stereotypical statement “Women like to shop” would be augmented with variants such as “Men like to shop” and “Nonbinary people like to shop.”

The process starts by feeding a source text to the perturber. Next, we add the word we want to perturb in the sentence, which in this case is women. We then add the target demographic we want the output text to contain, such as “gender: nonbinary/underspecified.” The perturber will automatically recognize relevant references to the person or group and change the text to reflect the new target demographic. Models trained on demographically augmented data are less likely to strongly associate women and shopping, because there will be more examples in the data about people of different genders liking shopping.

Building on open source fairness research to benefit the AI community

Building the future of responsible AI at Meta is important to our business. Fairness cuts to the heart of the importance of everyone having access to information, services, and opportunities. It is a process.

The comprehensive demographic terms list, perturber, and the human-generated rewrites are openly available for researchers. Our results suggest that the perturber generates higher-quality text rewrites — even for longer text passages with many demographic terms, which are notoriously hard to generate accurately. Compared with previous works, the perturber also has better coverage of historically underrepresented groups, such as nonbinary gender identities, so that fewer examples of unfairness go unmeasured. To further demonstrate the utility of our model for use in training more fair NLP models, we are also open-sourcing a large language model trained on data that has been demographically augmented using the perturber.

Our work makes clear strides toward demonstrably fairer NLP models that can include and respectfully represent everyone, regardless of their identity. In particular, both the perturber and the comprehensive set of demographic terms rely on minimally modifying text content, which is a tried-and-tested method for evaluating and improving AI robustness, and extend it to improve upon our past work.

This work contributes to a larger body of research at Meta AI aimed at incorporating responsible research practices from the beginning of every project, and improving methods for measuring the fairness of AI systems. This project builds on insights from other recent papers from Meta AI, such as our work measuring gender-based errors in machine translation, and our groundbreaking new dataset for evaluating the performance of computer vision models across a diverse range of ages, genders, and apparent skin tones.

While these efforts can benefit the wider AI community by improving methods for measuring fairness and social biases across a variety of AI systems and technologies, more work will need to be done. We look forward to seeing what the research community does with these datasets, as well as to continuing to build on our work around responsible AI.

For more information on the comprehensive set of demographic terms and evidence of its broad utility, download the research paper.

For more information on the perturber, its training dataset, and the language model pretrained with perturber-augmented data, download the research paper.

Download the datasets

We'd like to acknowledge the work of our collabarators on this project, including Candace Ross, Melissa Hall, Jude Fernandes, Eleonora Presani, and Douwe Kiela."
Meta_Blog,https://ai.meta.com/blog/measure-fairness-and-mitigate-ai-bias/,,Introducing two new datasets to help measure fairness and mitigate AI bias,"Developing reliable, large-scale ways of measuring fairness and mitigating bias gives AI researchers and practitioners helpful benchmarks that can be used to test NLP (natural language processing) systems — driving progress toward the goal of ensuring that AI systems treat everyone fairly.

The research community has made significant strides in doing this with gender, race, and ethnicity. While this foundation is an important start in addressing fairness along these dimensions, it falls short of being able to uncover fairness issues on the basis of other relevant communities or identities, such as religion, socioeconomic background, and queer identities, for example. To better identify a wider range of demographic biases in a variety of technologies and to create AI systems that respectfully represent a greater diversity of personal identities, we need new tools and benchmarks.

Today, we are introducing and open-sourcing several datasets and models for responsible NLP to address this gap. For this work, we assembled a diverse team of researchers and contributors, including people who identify as men, as women, and as nonbinary; people of different races and ethnicities; and people at different stages of their careers. Together, we developed a new method to test a wide range of biases in NLP models, beyond just race, gender, and ethnicity, including a list of more than 500 terms across a dozen axes. We devised a straightforward technique for controlling how models generate text, and taught models to recognize and better avoid social biases when generating responses to certain demographic terms. We also trained an AI model for reducing demographic biases in text, which can help break stereotypical associations present in NLP data. This method, known as a demographic text perturber, introduces demographically diverse variations that are otherwise similar to the original text. For example, it might take the sentence “He likes his grandma"" and create alternatives, such as ""She likes her grandma"" and ""They like their grandma,"" to augment the dataset with parallel, demographically altered variants.

We hope that these datasets will be used to help further research around fairness in the AI community. State-of-the-art NLP systems are notoriously data-intensive and use large-scale text resources to train. This data reliance presents particular challenges, as AI systems can unwittingly replicate or even amplify unwanted social biases present in the data. For example, NLP data can contain biases and stereotypes about particular demographic groups — or fail to represent them entirely.

Training models to measure fairness and mitigate biases

To better identify a wider range of demographic biases in AI, we need a comprehensive set of terms that reflect a diverse set of identities. We used a combination of algorithmic and participatory processes to develop the most comprehensive descriptor list of its kind, which spans over 500 terms across roughly a dozen demographic axes. More specifically, we came up with sample terms per axis, then expanded these terms algorithmically using nearest neighbor techniques. We then used a participatory process that involved gathering new term ideas and feedback on existing terms from a variety of policy experts and domain experts (such as civil rights and racial justice experts), as well as individuals with lived experience related to a wide variety of identities (representing multiple ethnicities, religions, races, sexual orientations, genders, and disabilities).

This extensive set of demographic terms can be used to better measure and then mitigate model biases. Before this work, the field generally measured biases with respect to broad terms such as Asian, which means, for example, we would only know whether our models were biased against Asian people, not whether they were biased against Japanese people. Our list enables more fine-grained analysis and is built with terms that groups of people use when self-identifying, rather than just common dictionary terms.

To address these issues, we devised a straightforward technique for controlling how models generate text, and taught models to recognize and better avoid social biases when generating responses to certain demographic terms.

Data augmented with our models can be used for a wide range of practical applications, including evaluating the fairness of generative and classification models, improving the representation of minority groups in certain domains, and ultimately training models that exhibit less demographic bias. We are openly sharing our demographics terms list with the broader AI community, and we invite additional term contributions to further increase its inclusiveness, scope, and utility. This work also contributes to a broader initiative at Meta AI to build AI responsibly and inclusively, particularly when it comes to combating social biases and enhancing the representation of underrepresented groups.

In addition to creating this more comprehensive demographic term list to measure fairness, we have built a model for reducing demographic biases in text — a machine-learned sequence-to-sequence demographic perturber, which we trained on a large-scale dataset of human-generated text rewrites that we collected. Our demographic perturber is trained to perturb several demographic attributes, including gender, race/ethnicity, and age, which can help break stereotypical associations present in NLP data by augmenting it with parallel, demographically perturbed variants. For example, the stereotypical statement “Women like to shop” would be augmented with variants such as “Men like to shop” and “Nonbinary people like to shop.”

The process starts by feeding a source text to the perturber. Next, we add the word we want to perturb in the sentence, which in this case is women. We then add the target demographic we want the output text to contain, such as “gender: nonbinary/underspecified.” The perturber will automatically recognize relevant references to the person or group and change the text to reflect the new target demographic. Models trained on demographically augmented data are less likely to strongly associate women and shopping, because there will be more examples in the data about people of different genders liking shopping.

Building on open source fairness research to benefit the AI community

Building the future of responsible AI at Meta is important to our business. Fairness cuts to the heart of the importance of everyone having access to information, services, and opportunities. It is a process.

The comprehensive demographic terms list, perturber, and the human-generated rewrites are openly available for researchers. Our results suggest that the perturber generates higher-quality text rewrites — even for longer text passages with many demographic terms, which are notoriously hard to generate accurately. Compared with previous works, the perturber also has better coverage of historically underrepresented groups, such as nonbinary gender identities, so that fewer examples of unfairness go unmeasured. To further demonstrate the utility of our model for use in training more fair NLP models, we are also open-sourcing a large language model trained on data that has been demographically augmented using the perturber.

Our work makes clear strides toward demonstrably fairer NLP models that can include and respectfully represent everyone, regardless of their identity. In particular, both the perturber and the comprehensive set of demographic terms rely on minimally modifying text content, which is a tried-and-tested method for evaluating and improving AI robustness, and extend it to improve upon our past work.

This work contributes to a larger body of research at Meta AI aimed at incorporating responsible research practices from the beginning of every project, and improving methods for measuring the fairness of AI systems. This project builds on insights from other recent papers from Meta AI, such as our work measuring gender-based errors in machine translation, and our groundbreaking new dataset for evaluating the performance of computer vision models across a diverse range of ages, genders, and apparent skin tones.

While these efforts can benefit the wider AI community by improving methods for measuring fairness and social biases across a variety of AI systems and technologies, more work will need to be done. We look forward to seeing what the research community does with these datasets, as well as to continuing to build on our work around responsible AI.

For more information on the comprehensive set of demographic terms and evidence of its broad utility, download the research paper.

For more information on the perturber, its training dataset, and the language model pretrained with perturber-augmented data, download the research paper.

Download the datasets

We'd like to acknowledge the work of our collabarators on this project, including Candace Ross, Melissa Hall, Jude Fernandes, Eleonora Presani, and Douwe Kiela."
Meta_Blog,https://ai.meta.com/blog/commercemm-a-new-approach-to-multimodal-understanding-for-online-shopping/,,Introducing CommerceMM: A new approach to multimodal understanding for online shopping,"We’ve built CommerceMM, a new approach to multimodal understanding for online shopping. Because so many product posts rely on both text and images, comprehension of multimodal data is crucial to make products more discoverable and help shoppers more easily find what they’re looking for.

CommerceMM relies on a novel set of pretraining tasks, called omni retrieval, to fuse the model’s characterizations of a post’s text and image, creating a representation of the post as a whole.

We achieved state-of-the-art performance on seven downstream tasks in research settings, such as product categorization and retrieval. An early version of CommerceMM has been deployed to provide more relevant search results and recommendations across Instagram Shops, Facebook Shops, and Marketplace listings.

When you shop online, how do you find what you’re looking for? If you’re browsing a traditional e-commerce site, designers may have spent months slotting products into the right categories and appending the most descriptive tags. The decisions are not always straightforward: Do blenders belong with pots and pans in kitchen supplies or in appliances alongside portable dishwashers? Gray areas like these demand nuanced interpretation and a keen understanding of how shoppers think.

At an online marketplace with a vast number of independent vendors — and an even vaster array of merchandise — the task becomes significantly more complex. Given the sheer scale of Meta’s shopping experiences across Marketplace and Shops on Facebook and Instagram, we leverage AI tools to help categorize and label products. To address this need, we’ve developed a powerful new approach to pretraining and a versatile new model for Commerce MultiModal Representation (CommerceMM).

We’ve taught CommerceMM to analyze a post as a whole rather than as merely isolated blocks of images and words. That’s a vital skill because so many commerce posts are multimodal, with pictures, captions, and other text working in concert to express a wealth of information. Often, a product’s description identifies the features important to a particular shopper — that the wood in a chair was responsibly harvested or a pair of pants is machine washable — but in other cases, those attributes are instead folded into a series of hashtags. Sometimes, the photo, not the text, reveals the crucial detail, such as the profile of the perfect sofa or cut of a flattering dress. Fully comprehending a product post means parsing the subtleties of this sort of multimodal content.

CommerceMM achieves a richer understanding of multimodal data by integrating its characterizations of a post’s text and image.

What the research is:

Previous researchers have pretrained Transformer-based models to associate an image with its accompanying text description, using medium-scale image-text pairs as the default training data. But online shopping is conducive to more diverse text and image data, which we can use to teach AI systems to find new relationships between modalities. For example, when a shopper searches for “planter” and then clicks on a porcelain flowerpot, the shopper confirms the affinity between the query text and the multimodal product page. That page might show multiple views of the flowerpot, which suggests a connection both among the images and between each image and the entire page (which would include varying multimodal content). Influencers can tag the flowerpot in their own photos, which builds a multimodal-to-multimodal mapping between their posts and the product page.

We began to realize that these connections had another use: to help develop a generalized multimodal representation for various commerce-related applications. As with Meta AI’s recent breakthroughs in multimodal training, including our open source MMF framework for vision and language research, the Situated and Interactive Multimodal Conversations (SIMMC) data set, the Halo annotation platform, and the Learning from Videos project, we wanted to use our resources to help deepen AI’s understanding of complex data. We found that we could employ those multimodal mappings as weak labels in our novel approach to pretraining, which teaches the model how to correlate an image, its corresponding text, and the post as a whole.

How it works:

CommerceMM is composed of an image encoder, a text encoder, and a multimodal fusion encoder. The encoders translate data into embeddings, which are sets of mathematical vectors. In our text encoder, the embeddings represent the continuums along which a sentence could be similar to or different from another: structure, tense, subject matter, sentiment, formality, perspective, and a host of other shades of meaning. For an image, the embeddings might quantify, for example, patterns in edges, the sharpness of angles, or the contours implied by shadows. These embeddings consolidate an abundance of information, encapsulating the unique qualities that characterize each passage in a text or object in a photograph.

What makes CommerceMM unique is that for each photo-and-text input, in addition to separate text and image representations, the system creates a dedicated multimodal embedding that represents the post as a whole. First, the image encoder analyzes each picture, and a Transformer-based text encoder handles the accompanying text. Both pass the resulting embeddings to a Transformer-based multimodal fusion encoder, where the two modalities learn to interact to create a joint representation.

The goal of training is to teach the model to bunch similar inputs together in the multidimensional embedding space and to push unrelated data apart. We train all three encoders simultaneously on a suite of tasks combining masking and contrastive learning. In the masking tasks — masked language modeling, masked image modeling KL-divergence, and masked image modeling feature regression — part of the image or text is blacked out, and the model learns to reconstruct the missing section based on its surroundings. Contrastive learning approaches — in our case, image-text contrastive learning and image-text matching — teach a model to cluster the representations of nearly identical inputs close together in embedding space while pushing them away from dissimilar examples.

Once we have our three embeddings — image, text, and multimodal — we continue to calibrate them with a novel set of tasks called omni retrieval. This step refines the relationships between all the embedding modalities: image and image, image and text, image and multimodal, text and multimodal, etc. To begin, we feed two text-image pairs into two identical versions of our model. Each side returns three embeddings. Our goal is to teach the system how to associate the two sets of three embeddings. There are in total nine pairs of relationships, where all three embeddings from one model should be highly correlated with every embedding from the replica. With contrastive learning, we can coach the model to learn all these relationships. Omni retrieval has been shown to learn a more discriminative and generalized representation.

We iterate between the image-text tasks and omni retrieval. The two phases are complementary, which makes training more efficient; the system runs only one forward and backward pass for each set of tasks.

We introduced another novel idea, modality randomization, to deepen CommerceMM’s multimodal comprehension. Before each training step, we randomly shuffle layers between the text and multimodal encoders. As the layers move from Transformer to Transformer, they learn from both modalities and share their knowledge with the rest of the system. We found that modality-randomized pretraining improves performance on downstream tasks over any fixed architecture.

Why it matters:

Once our system has been pretrained to learn these representations, researchers can easily fine-tune the model for any number of specialized duties. We used CommerceMM to achieve state-of-the-art performance on seven tasks in research settings: Catalog product categorization for Instagram Shops and Facebook Shops, product categorization for Marketplace, image-to-text retrieval, text-to-image retrieval, query-to-product retrieval, image-to-product retrieval, and image-to-image retrieval. Our model outperforms all the existing systems dedicated to these individual use cases. The detailed experiments are in arXiv paper.

Last year, Meta deployed an early version of CommerceMM to improve category filters, such as beauty and home — providing more relevant results and recommendations on Instagram Shops, Facebook Shops, and Marketplace listings. We also used it to improve attribute filters, like color and material, in English on Instagram Shops and Facebook Shops.

What’s next?

CommerceMM is becoming a standard model to inform ranking and product recommendation across the company. We hope to deploy CommerceMM to support more of Meta’s applications, such as product search in Marketplace and visual search on Instagram. Our ultimate goal is for CommerceMM to help shoppers find exactly what they’re looking for.

Read the full paper:

https://arxiv.org/pdf/2202.07247.pdf"
Meta_Blog,https://ai.meta.com/blog/commercemm-a-new-approach-to-multimodal-understanding-for-online-shopping/,,Introducing CommerceMM: A new approach to multimodal understanding for online shopping,"We’ve built CommerceMM, a new approach to multimodal understanding for online shopping. Because so many product posts rely on both text and images, comprehension of multimodal data is crucial to make products more discoverable and help shoppers more easily find what they’re looking for.

CommerceMM relies on a novel set of pretraining tasks, called omni retrieval, to fuse the model’s characterizations of a post’s text and image, creating a representation of the post as a whole.

We achieved state-of-the-art performance on seven downstream tasks in research settings, such as product categorization and retrieval. An early version of CommerceMM has been deployed to provide more relevant search results and recommendations across Instagram Shops, Facebook Shops, and Marketplace listings.

When you shop online, how do you find what you’re looking for? If you’re browsing a traditional e-commerce site, designers may have spent months slotting products into the right categories and appending the most descriptive tags. The decisions are not always straightforward: Do blenders belong with pots and pans in kitchen supplies or in appliances alongside portable dishwashers? Gray areas like these demand nuanced interpretation and a keen understanding of how shoppers think.

At an online marketplace with a vast number of independent vendors — and an even vaster array of merchandise — the task becomes significantly more complex. Given the sheer scale of Meta’s shopping experiences across Marketplace and Shops on Facebook and Instagram, we leverage AI tools to help categorize and label products. To address this need, we’ve developed a powerful new approach to pretraining and a versatile new model for Commerce MultiModal Representation (CommerceMM).

We’ve taught CommerceMM to analyze a post as a whole rather than as merely isolated blocks of images and words. That’s a vital skill because so many commerce posts are multimodal, with pictures, captions, and other text working in concert to express a wealth of information. Often, a product’s description identifies the features important to a particular shopper — that the wood in a chair was responsibly harvested or a pair of pants is machine washable — but in other cases, those attributes are instead folded into a series of hashtags. Sometimes, the photo, not the text, reveals the crucial detail, such as the profile of the perfect sofa or cut of a flattering dress. Fully comprehending a product post means parsing the subtleties of this sort of multimodal content.

CommerceMM achieves a richer understanding of multimodal data by integrating its characterizations of a post’s text and image.

What the research is:

Previous researchers have pretrained Transformer-based models to associate an image with its accompanying text description, using medium-scale image-text pairs as the default training data. But online shopping is conducive to more diverse text and image data, which we can use to teach AI systems to find new relationships between modalities. For example, when a shopper searches for “planter” and then clicks on a porcelain flowerpot, the shopper confirms the affinity between the query text and the multimodal product page. That page might show multiple views of the flowerpot, which suggests a connection both among the images and between each image and the entire page (which would include varying multimodal content). Influencers can tag the flowerpot in their own photos, which builds a multimodal-to-multimodal mapping between their posts and the product page.

We began to realize that these connections had another use: to help develop a generalized multimodal representation for various commerce-related applications. As with Meta AI’s recent breakthroughs in multimodal training, including our open source MMF framework for vision and language research, the Situated and Interactive Multimodal Conversations (SIMMC) data set, the Halo annotation platform, and the Learning from Videos project, we wanted to use our resources to help deepen AI’s understanding of complex data. We found that we could employ those multimodal mappings as weak labels in our novel approach to pretraining, which teaches the model how to correlate an image, its corresponding text, and the post as a whole.

How it works:

CommerceMM is composed of an image encoder, a text encoder, and a multimodal fusion encoder. The encoders translate data into embeddings, which are sets of mathematical vectors. In our text encoder, the embeddings represent the continuums along which a sentence could be similar to or different from another: structure, tense, subject matter, sentiment, formality, perspective, and a host of other shades of meaning. For an image, the embeddings might quantify, for example, patterns in edges, the sharpness of angles, or the contours implied by shadows. These embeddings consolidate an abundance of information, encapsulating the unique qualities that characterize each passage in a text or object in a photograph.

What makes CommerceMM unique is that for each photo-and-text input, in addition to separate text and image representations, the system creates a dedicated multimodal embedding that represents the post as a whole. First, the image encoder analyzes each picture, and a Transformer-based text encoder handles the accompanying text. Both pass the resulting embeddings to a Transformer-based multimodal fusion encoder, where the two modalities learn to interact to create a joint representation.

The goal of training is to teach the model to bunch similar inputs together in the multidimensional embedding space and to push unrelated data apart. We train all three encoders simultaneously on a suite of tasks combining masking and contrastive learning. In the masking tasks — masked language modeling, masked image modeling KL-divergence, and masked image modeling feature regression — part of the image or text is blacked out, and the model learns to reconstruct the missing section based on its surroundings. Contrastive learning approaches — in our case, image-text contrastive learning and image-text matching — teach a model to cluster the representations of nearly identical inputs close together in embedding space while pushing them away from dissimilar examples.

Once we have our three embeddings — image, text, and multimodal — we continue to calibrate them with a novel set of tasks called omni retrieval. This step refines the relationships between all the embedding modalities: image and image, image and text, image and multimodal, text and multimodal, etc. To begin, we feed two text-image pairs into two identical versions of our model. Each side returns three embeddings. Our goal is to teach the system how to associate the two sets of three embeddings. There are in total nine pairs of relationships, where all three embeddings from one model should be highly correlated with every embedding from the replica. With contrastive learning, we can coach the model to learn all these relationships. Omni retrieval has been shown to learn a more discriminative and generalized representation.

We iterate between the image-text tasks and omni retrieval. The two phases are complementary, which makes training more efficient; the system runs only one forward and backward pass for each set of tasks.

We introduced another novel idea, modality randomization, to deepen CommerceMM’s multimodal comprehension. Before each training step, we randomly shuffle layers between the text and multimodal encoders. As the layers move from Transformer to Transformer, they learn from both modalities and share their knowledge with the rest of the system. We found that modality-randomized pretraining improves performance on downstream tasks over any fixed architecture.

Why it matters:

Once our system has been pretrained to learn these representations, researchers can easily fine-tune the model for any number of specialized duties. We used CommerceMM to achieve state-of-the-art performance on seven tasks in research settings: Catalog product categorization for Instagram Shops and Facebook Shops, product categorization for Marketplace, image-to-text retrieval, text-to-image retrieval, query-to-product retrieval, image-to-product retrieval, and image-to-image retrieval. Our model outperforms all the existing systems dedicated to these individual use cases. The detailed experiments are in arXiv paper.

Last year, Meta deployed an early version of CommerceMM to improve category filters, such as beauty and home — providing more relevant results and recommendations on Instagram Shops, Facebook Shops, and Marketplace listings. We also used it to improve attribute filters, like color and material, in English on Instagram Shops and Facebook Shops.

What’s next?

CommerceMM is becoming a standard model to inform ranking and product recommendation across the company. We hope to deploy CommerceMM to support more of Meta’s applications, such as product search in Marketplace and visual search on Instagram. Our ultimate goal is for CommerceMM to help shoppers find exactly what they’re looking for.

Read the full paper:

https://arxiv.org/pdf/2202.07247.pdf"
Meta_Blog,https://ai.meta.com/blog/new-advances-in-speech-recognition-to-power-ar-experiences-and-more/,,New advances in speech recognition to power AR experiences and more,"Building new augmented reality experiences will require technical breakthroughs beyond just computer vision. In particular, intelligent assistants — ones that can understand natural, nuanced conversational language — will need next-gen speech systems that can do much more than just help us make a hands-free phone call or open an app on our phone.

Tomorrow’s speech recognition systems will need to be far more efficient so they can run on-device on ultralight, compact, and stylish glasses. And they will need to be much more accurate and robust — capable of disambiguating words and understanding context much as people do, able to handle a large vocabulary and uncommon words, and work well even in challenging circumstances with lots of background noise and multiple people speaking.

Meta AI is committed to advancing the state of the art in speech technology and building the technology needed to create new augmented reality and virtual reality experiences, which will be an important part of the metaverse. Speech recognition systems are already an increasingly important part of our products and services. Meta has recently deployed new speech features to support video captioning across many of our apps. This is a great outcome for accessibility, as those who are deaf or have hearing loss can read high-quality captions on videos across products. Captions in FB and IG Stories have even become integral parts of the story’s visual character, with people adjusting font, color, placement to express themselves creatively. Meta’s speech technology also powers hands-free voice interaction on Portal, Quest, and Ray-Ban Stories devices.

In this blog post, we’re highlighting new speech recognition research from Meta, including some of the papers to be presented at the International Conference on Acoustics, Speech and Signal Processing (ICASSP) this month. These projects will help advance efforts both at Meta AI and in Meta’s Reality Labs to build the next generation of devices to help people connect.

We’re excited to push the cutting edge further and enable people to interact with their devices, with content, and with other people in new, more useful, more enjoyable ways.

Improving speech recognition for real-world needs

Speech recognition researchers across the industry and academia are continually publishing ever-improving results on widely used published benchmarks. But despite this important progress, big challenges remain. To some extent, solving these challenges requires a shift in focus away from typical speech recognition metrics, like the total number of errors on a test set (average word error rate), to newer metrics that better capture the deficiencies of current systems.

Improving recognition of rare words

In many instances, even if the word error rate is quite low on average, misrecognizing certain critical words is enough to ruin the experience. Consider the importance of the esoteric jargon in a science video or the names of your friends in a dictated message. Recognizing these rare or previously unseen words is particularly challenging for modern “end-to-end” speech recognition systems, such as the widely used RNN-T models.

To solve this problem, we previously created a multipronged approach that improved upon the standard shallow fusion approach by incorporating trie-based deep biasing and neural network language model contextualization . This resulted in 20 percent fewer errors compared with shallow fusion. At ICASSP, we are presenting the Neural-FST Class Language Model (NFCLM), which further improves upon this work. The NFCLM models generic background text and structured queries with entities (e.g., song requests) with a unified mathematical framework. This results in a model that achieves a better performance tradeoff between recognition of rare words and more common words, while having the additional benefit of being more than 10 times smaller.

Fairness and responsible speech recognition

Another area we have focused on is fairness and responsible AI. While the primary word error rate metric used in the research community focuses on a single number that represents the total number of errors in a dataset, it does not capture the differences in performance across different populations. Meta AI recently released the Casual Conversations dataset, a set of videos designed to measure fairness in computer vision systems along dimensions of gender, age, and apparent skin tone. At ICASSP, we are sharing a recent analysis of the speech recognition performance of this corpus along these same dimensions, in which significant variation across gender and skin tone was observed. We are making the transcriptions from the Casual Conversations dataset publicly available in hopes of motivating other researchers to study this problem and create speech systems that work well for all populations. We are also introducing a method to more accurately measure and interpret any differences in speech accuracy among subgroups of interest.

Zero-shot and few-shot learning

One of the challenges in improving fairness is access to representative training data. One alternative approach to creating a model with matched training data is to create a more universal model that can then be easily fine-tuned to any particular task (or user group). We recently leveraged large-scale semi-supervised training to create ASR models with up to 10 billion parameters using over 4.5 million hours of automatically labeled data. We evaluated this model on a publicly available aphasic speech dataset. Aphasia is a speech-language disorder that arises due to damage to portions of the brain, most commonly resulting from a stroke. Such speech is extremely challenging for speech recognition systems to accurately transcribe. We applied few-shot learning with a relatively small amount of aphasic speech to our universal model. This resulted in over 60 percent fewer errors than a system trained on the aphasic speech only, demonstrating that universal models are a promising avenue for providing high-quality transcription to everyone.

While speech recognition has made incredible progress over the last several years, there are still big challenges to making sure that we build systems that work well across all use cases and work well for everyone. We’ve made significant progress over the past year to this end, but as we say at Meta, the journey is 1 percent finished.

Speech recognition papers from Meta AI at ICASSP 2022"
Meta_Blog,https://ai.meta.com/blog/new-advances-in-speech-recognition-to-power-ar-experiences-and-more/,,New advances in speech recognition to power AR experiences and more,"Building new augmented reality experiences will require technical breakthroughs beyond just computer vision. In particular, intelligent assistants — ones that can understand natural, nuanced conversational language — will need next-gen speech systems that can do much more than just help us make a hands-free phone call or open an app on our phone.

Tomorrow’s speech recognition systems will need to be far more efficient so they can run on-device on ultralight, compact, and stylish glasses. And they will need to be much more accurate and robust — capable of disambiguating words and understanding context much as people do, able to handle a large vocabulary and uncommon words, and work well even in challenging circumstances with lots of background noise and multiple people speaking.

Meta AI is committed to advancing the state of the art in speech technology and building the technology needed to create new augmented reality and virtual reality experiences, which will be an important part of the metaverse. Speech recognition systems are already an increasingly important part of our products and services. Meta has recently deployed new speech features to support video captioning across many of our apps. This is a great outcome for accessibility, as those who are deaf or have hearing loss can read high-quality captions on videos across products. Captions in FB and IG Stories have even become integral parts of the story’s visual character, with people adjusting font, color, placement to express themselves creatively. Meta’s speech technology also powers hands-free voice interaction on Portal, Quest, and Ray-Ban Stories devices.

In this blog post, we’re highlighting new speech recognition research from Meta, including some of the papers to be presented at the International Conference on Acoustics, Speech and Signal Processing (ICASSP) this month. These projects will help advance efforts both at Meta AI and in Meta’s Reality Labs to build the next generation of devices to help people connect.

We’re excited to push the cutting edge further and enable people to interact with their devices, with content, and with other people in new, more useful, more enjoyable ways.

Improving speech recognition for real-world needs

Speech recognition researchers across the industry and academia are continually publishing ever-improving results on widely used published benchmarks. But despite this important progress, big challenges remain. To some extent, solving these challenges requires a shift in focus away from typical speech recognition metrics, like the total number of errors on a test set (average word error rate), to newer metrics that better capture the deficiencies of current systems.

Improving recognition of rare words

In many instances, even if the word error rate is quite low on average, misrecognizing certain critical words is enough to ruin the experience. Consider the importance of the esoteric jargon in a science video or the names of your friends in a dictated message. Recognizing these rare or previously unseen words is particularly challenging for modern “end-to-end” speech recognition systems, such as the widely used RNN-T models.

To solve this problem, we previously created a multipronged approach that improved upon the standard shallow fusion approach by incorporating trie-based deep biasing and neural network language model contextualization . This resulted in 20 percent fewer errors compared with shallow fusion. At ICASSP, we are presenting the Neural-FST Class Language Model (NFCLM), which further improves upon this work. The NFCLM models generic background text and structured queries with entities (e.g., song requests) with a unified mathematical framework. This results in a model that achieves a better performance tradeoff between recognition of rare words and more common words, while having the additional benefit of being more than 10 times smaller.

Fairness and responsible speech recognition

Another area we have focused on is fairness and responsible AI. While the primary word error rate metric used in the research community focuses on a single number that represents the total number of errors in a dataset, it does not capture the differences in performance across different populations. Meta AI recently released the Casual Conversations dataset, a set of videos designed to measure fairness in computer vision systems along dimensions of gender, age, and apparent skin tone. At ICASSP, we are sharing a recent analysis of the speech recognition performance of this corpus along these same dimensions, in which significant variation across gender and skin tone was observed. We are making the transcriptions from the Casual Conversations dataset publicly available in hopes of motivating other researchers to study this problem and create speech systems that work well for all populations. We are also introducing a method to more accurately measure and interpret any differences in speech accuracy among subgroups of interest.

Zero-shot and few-shot learning

One of the challenges in improving fairness is access to representative training data. One alternative approach to creating a model with matched training data is to create a more universal model that can then be easily fine-tuned to any particular task (or user group). We recently leveraged large-scale semi-supervised training to create ASR models with up to 10 billion parameters using over 4.5 million hours of automatically labeled data. We evaluated this model on a publicly available aphasic speech dataset. Aphasia is a speech-language disorder that arises due to damage to portions of the brain, most commonly resulting from a stroke. Such speech is extremely challenging for speech recognition systems to accurately transcribe. We applied few-shot learning with a relatively small amount of aphasic speech to our universal model. This resulted in over 60 percent fewer errors than a system trained on the aphasic speech only, demonstrating that universal models are a promising avenue for providing high-quality transcription to everyone.

While speech recognition has made incredible progress over the last several years, there are still big challenges to making sure that we build systems that work well across all use cases and work well for everyone. We’ve made significant progress over the past year to this end, but as we say at Meta, the journey is 1 percent finished.

Speech recognition papers from Meta AI at ICASSP 2022"
Meta_Blog,https://ai.meta.com/blog/significantly-faster-vision-transformer-training/,,Significantly faster Vision Transformer training,"What the research is

Vision Transformers (ViTs) — adopted in a wide range of computer vision tasks, from image classification to object detection and segmentation — can achieve state-of-the-art results in visual representation and recognition. Because the performance of computer vision models tends to improve with more parameters and longer training schedules, the AI community has experimented with ever-larger ViTs. But as models begin to exceed teraflops scale, the field has come up against major bottlenecks. Training a single model can take months and require hundreds or thousands of GPUs, inflating accelerator requirements and pushing large-scale ViTs beyond the reach of many practitioners.

To broaden access to ViTs, we have developed methods to make training more efficient. For these models to become more accessible, training must be optimized to achieve the best accelerator utilization. But this process is laborious and requires considerable expertise. To set up an orderly experiment, researchers must choose from myriad possible optimizations: Any of the millions of operations conducted in a single training pass could be hampered by inefficiencies.

We found that we could improve compute and memory efficiency by applying a series of optimizations to the ViT implementation in PyCls, Meta AI’s image classification codebase. Our improvements boosted training speed and per-accelerator throughput (TFLOPS) for ViT models trained using PyCls.

The relative increase in accelerator throughput per chip compared to V100 baseline using the optimized codebase. A100 optimized has 4.05x more accelerator thoughput compared with the V100 baseline.

How it works

We began by profiling our codebase to identify potential sources of inefficiency, eventually zeroing in on our choice of number format. As a default, most applications represent neural network values in the 32-bit single-precision floating-point format. Converting to a 16-bit half-precision format (FP16) reduces a model’s memory footprint and execution time but often lowers its accuracy as well.

We sought a middle ground: mixed precision. With this method, the system speeds training and reduces memory use by performing computations in half precision, while the results are stored in single precision to preserve accuracy. Rather than manually casting parts of our network down to half precision, we experimented with different modes of automatic mixed precision training (AMP), which automatically toggles between number formats. Advanced modes of AMP rely primarily on half-precision operations and model weights. We found a balanced setting that significantly accelerates training without sacrificing accuracy.

To make our process even more efficient, we took advantage of FairScale’s Fully Sharded Data Parallel (FSDP) training algorithm. It shards parameters, gradients, and optimizer states across the GPUs. With FSDP, we can build models that are orders of magnitude larger using fewer GPUs. We also used MTA optimizers, a pooled ViT classifier, and a batch-second input tensor layout to skip redundant transpose operations.

The x-axis designates possible optimizations, and the y-axis shows the relative increase in accelerator throughput for ViT-H/16 training compared with the distributed data parallel (DDP) baseline.

We achieved 1.51x higher accelerator throughout — measured by the number of floating-point operations performed per second on each accelerator chip — using a total batch size of 560. We could boost throughput to 1.86x by expanding the image size from 224 pixels to 256 pixels. However, altering the image size changes the hyperparameters, which can affect the model’s accuracy. The relative throughput increases to 2.18x when training in the full FP16 mode, although this sometimes reduces accuracy (the accuracy degradation was less than 10 percent in our experiments).

The y-axis shows epoch time — the duration of one training pass over the entire ImageNet-1K data set. We focused on the actual wall-clock training time of existing recipes that typically use an image size of 224 pixels, so we did not plot observations with larger image sizes.

Using our optimizations, we reduced epoch time — the duration of one training pass over the entire ImageNet-1K data set — from 0.65 hours to 0.43 hours.

The x-axis specifies the number of accelerator chips in a particular configuration of A100 GPUs, and the y-axis indicates absolute throughput in TFLOPS per chip.

We also investigated the effects of different GPU configurations. In each case, our system achieved higher throughput than the distributed data parallel (DDP) baseline did. As we increased the number of chips, we observed a slight drop in throughput due to the overhead allotted to interdevice communication. However, even using 64 GPUs, our system was 1.83x faster than the DDP baseline.

Why it matters

Doubling the achievable throughput in ViT training effectively doubles the training cluster size, and improved accelerator utilization directly reduces the carbon footprint of AI models. Given the recent trend of developing larger models with longer training times, we hope our optimizations will help the research community further push the state of the art, with shorter turnaround times and enhanced productivity.

Codebase

Example training configuration"
Meta_Blog,https://ai.meta.com/blog/significantly-faster-vision-transformer-training/,,Significantly faster Vision Transformer training,"What the research is

Vision Transformers (ViTs) — adopted in a wide range of computer vision tasks, from image classification to object detection and segmentation — can achieve state-of-the-art results in visual representation and recognition. Because the performance of computer vision models tends to improve with more parameters and longer training schedules, the AI community has experimented with ever-larger ViTs. But as models begin to exceed teraflops scale, the field has come up against major bottlenecks. Training a single model can take months and require hundreds or thousands of GPUs, inflating accelerator requirements and pushing large-scale ViTs beyond the reach of many practitioners.

To broaden access to ViTs, we have developed methods to make training more efficient. For these models to become more accessible, training must be optimized to achieve the best accelerator utilization. But this process is laborious and requires considerable expertise. To set up an orderly experiment, researchers must choose from myriad possible optimizations: Any of the millions of operations conducted in a single training pass could be hampered by inefficiencies.

We found that we could improve compute and memory efficiency by applying a series of optimizations to the ViT implementation in PyCls, Meta AI’s image classification codebase. Our improvements boosted training speed and per-accelerator throughput (TFLOPS) for ViT models trained using PyCls.

The relative increase in accelerator throughput per chip compared to V100 baseline using the optimized codebase. A100 optimized has 4.05x more accelerator thoughput compared with the V100 baseline.

How it works

We began by profiling our codebase to identify potential sources of inefficiency, eventually zeroing in on our choice of number format. As a default, most applications represent neural network values in the 32-bit single-precision floating-point format. Converting to a 16-bit half-precision format (FP16) reduces a model’s memory footprint and execution time but often lowers its accuracy as well.

We sought a middle ground: mixed precision. With this method, the system speeds training and reduces memory use by performing computations in half precision, while the results are stored in single precision to preserve accuracy. Rather than manually casting parts of our network down to half precision, we experimented with different modes of automatic mixed precision training (AMP), which automatically toggles between number formats. Advanced modes of AMP rely primarily on half-precision operations and model weights. We found a balanced setting that significantly accelerates training without sacrificing accuracy.

To make our process even more efficient, we took advantage of FairScale’s Fully Sharded Data Parallel (FSDP) training algorithm. It shards parameters, gradients, and optimizer states across the GPUs. With FSDP, we can build models that are orders of magnitude larger using fewer GPUs. We also used MTA optimizers, a pooled ViT classifier, and a batch-second input tensor layout to skip redundant transpose operations.

The x-axis designates possible optimizations, and the y-axis shows the relative increase in accelerator throughput for ViT-H/16 training compared with the distributed data parallel (DDP) baseline.

We achieved 1.51x higher accelerator throughout — measured by the number of floating-point operations performed per second on each accelerator chip — using a total batch size of 560. We could boost throughput to 1.86x by expanding the image size from 224 pixels to 256 pixels. However, altering the image size changes the hyperparameters, which can affect the model’s accuracy. The relative throughput increases to 2.18x when training in the full FP16 mode, although this sometimes reduces accuracy (the accuracy degradation was less than 10 percent in our experiments).

The y-axis shows epoch time — the duration of one training pass over the entire ImageNet-1K data set. We focused on the actual wall-clock training time of existing recipes that typically use an image size of 224 pixels, so we did not plot observations with larger image sizes.

Using our optimizations, we reduced epoch time — the duration of one training pass over the entire ImageNet-1K data set — from 0.65 hours to 0.43 hours.

The x-axis specifies the number of accelerator chips in a particular configuration of A100 GPUs, and the y-axis indicates absolute throughput in TFLOPS per chip.

We also investigated the effects of different GPU configurations. In each case, our system achieved higher throughput than the distributed data parallel (DDP) baseline did. As we increased the number of chips, we observed a slight drop in throughput due to the overhead allotted to interdevice communication. However, even using 64 GPUs, our system was 1.83x faster than the DDP baseline.

Why it matters

Doubling the achievable throughput in ViT training effectively doubles the training cluster size, and improved accelerator utilization directly reduces the carbon footprint of AI models. Given the recent trend of developing larger models with longer training times, we hope our optimizations will help the research community further push the state of the art, with shorter turnaround times and enhanced productivity.

Codebase

Example training configuration"
Meta_Blog,https://ai.meta.com/blog/asynchronous-federated-learning/,,A new method for asynchronous federated learning,"People increasingly care about how their data is used. Privacy-preserving ML techniques aim to enable the AI-powered services that people love while maintaining the privacy of their data. In the past, we shared the Opacus framework for training PyTorch models with differential privacy and the CrypTen framework for secure multiparty computation in PyTorch. Today, we are excited to share more about our efforts around scaling another privacy-enhancing technology, federated learning.

Federated learning (FL) is an important tool for preserving the privacy of people’s data when training AI models. It enables practitioners to train their models without requiring that people’s data ever leave their devices. For example, FL can train models that recognize voice commands without recording and transmitting audio clips into the cloud. However, the existing state-of-the art approach — synchronous FL — has a significant shortcoming: Learning can deliver results only as fast as the slowest device, seriously degrading efficiency when scaling to millions of devices.

Today, we describe a novel asynchronous approach to FL that circumvents these problems. We believe we are running the first asynchronous FL system at scale, training a model on 100 million Android devices. With the ability to scale to millions of devices, FL can make significant leaps in training speed and improve efficiency. Our results show that our asynchronous FL system is five times faster than synchronous federated learning and can achieve the same result (a high-accuracy model) with eight times less communication than the previous method.

While synchronous FL preserves privacy, it is substantially less efficient than training models in the cloud because it can only move as fast as the slowest device. In one synchronous round, a subset of clients (mobile devices) would download the current model from the server and then cooperate to send the server an update that improves the model. These synchronous rounds require the cooperation of every client in the round, so that a round can only be completed (and the next round started) once all participating clients respond. In practice, this means that progress moves at the pace of the slowest clients — the stragglers.

To address the straggler problem in synchronous FL, it is common to start training on more clients than needed and then to drop the slowest ones from the round. For example, the round might start with 1,300 clients participating, and the server will finish the round after receiving 1,000 responses — dropping the slowest 300 clients. This leads to wasted work, however, and can also result in less accurate predictions on data from dropped clients, since their updates do not get reflected in the model.

Leaving no stragglers behind with an asynchronous approach

In asynchronous FL, there is no longer a concept of a fixed group of devices participating in each round. Instead, whenever a device is available for training, it downloads the current version of the model from the server, computes the update, and transmits an update to the server once it has finished. The server receives updates from individual clients until it has the requisite number needed, and then it adjusts the model based on the combined updates it has received. This way, responses are still aggregated before being revealed to the server, so privacy is preserved.

Another difference from synchronous FL is that there is no predefined notion of rounds. Instead, slower clients may still send their updates later in asynchronous FL, but their updates may be based on stale information. Our theoretical analysis takes the impact of staleness into account and suggests strategies to reduce its effects, such as down-weighting stale information and bounding the maximum staleness allowed.

In building this new asynchronous method, we knew we had to use secure aggregation, a crucial privacy-enhancing technology commonly used in FL systems. Secure aggregation uses cryptographic protocols to ensure that the server and all participating clients receive only a response aggregated over a group of devices and never from any individual device.

Previously, there was no known way to implement secure aggregation when performing asynchronous operations, because all parties participating in secure aggregation needed to be determined in advance. Our approach introduces a novel asynchronous secure aggregation procedure leveraging trusted execution environments (TEEs) — specialized secure hardware running in the cloud. Our implementation provides security guarantees while limiting the amount of data that needs to pass through the TEE. This enables rapid processing of updates and prevents the TEE from becoming a bottleneck.

Speeding toward a more private future

This advancement yields several improvements that will benefit us as we move into the future and help build the metaverse. With improved speed and efficiency, asynchronous FL will enable engineers to iterate faster on model development and evaluation. This speedup also makes it possible to deploy models trained using FL based on more recent, fresher data⁠, which will significantly impact features and applications that benefit from recent trends.

There’s also a potential environmental benefit. The reduction in communication overhead makes it possible to train models that make high-quality predictions using data from fewer individuals, reducing the carbon footprint of FL training.

Finally, by incorporating stale updates from slower devices, we show that models trained using asynchronous FL can be more accurate. We found our method’s predictions on data from slower devices was 47 percent more accurate than models trained with synchronous FL. These benefits show that asynchronous FL can help us scale and improve user experience while still protecting the privacy of people’s data.

Learn more about our approach to asynchronous FL by downloading our simulation framework and reading our papers.

Federated Learning with Buffered Asynchronous Aggregation

Papaya: Practical, Private, and Scalable Federated Learning"
Meta_Blog,https://ai.meta.com/blog/asynchronous-federated-learning/,,A new method for asynchronous federated learning,"People increasingly care about how their data is used. Privacy-preserving ML techniques aim to enable the AI-powered services that people love while maintaining the privacy of their data. In the past, we shared the Opacus framework for training PyTorch models with differential privacy and the CrypTen framework for secure multiparty computation in PyTorch. Today, we are excited to share more about our efforts around scaling another privacy-enhancing technology, federated learning.

Federated learning (FL) is an important tool for preserving the privacy of people’s data when training AI models. It enables practitioners to train their models without requiring that people’s data ever leave their devices. For example, FL can train models that recognize voice commands without recording and transmitting audio clips into the cloud. However, the existing state-of-the art approach — synchronous FL — has a significant shortcoming: Learning can deliver results only as fast as the slowest device, seriously degrading efficiency when scaling to millions of devices.

Today, we describe a novel asynchronous approach to FL that circumvents these problems. We believe we are running the first asynchronous FL system at scale, training a model on 100 million Android devices. With the ability to scale to millions of devices, FL can make significant leaps in training speed and improve efficiency. Our results show that our asynchronous FL system is five times faster than synchronous federated learning and can achieve the same result (a high-accuracy model) with eight times less communication than the previous method.

While synchronous FL preserves privacy, it is substantially less efficient than training models in the cloud because it can only move as fast as the slowest device. In one synchronous round, a subset of clients (mobile devices) would download the current model from the server and then cooperate to send the server an update that improves the model. These synchronous rounds require the cooperation of every client in the round, so that a round can only be completed (and the next round started) once all participating clients respond. In practice, this means that progress moves at the pace of the slowest clients — the stragglers.

To address the straggler problem in synchronous FL, it is common to start training on more clients than needed and then to drop the slowest ones from the round. For example, the round might start with 1,300 clients participating, and the server will finish the round after receiving 1,000 responses — dropping the slowest 300 clients. This leads to wasted work, however, and can also result in less accurate predictions on data from dropped clients, since their updates do not get reflected in the model.

Leaving no stragglers behind with an asynchronous approach

In asynchronous FL, there is no longer a concept of a fixed group of devices participating in each round. Instead, whenever a device is available for training, it downloads the current version of the model from the server, computes the update, and transmits an update to the server once it has finished. The server receives updates from individual clients until it has the requisite number needed, and then it adjusts the model based on the combined updates it has received. This way, responses are still aggregated before being revealed to the server, so privacy is preserved.

Another difference from synchronous FL is that there is no predefined notion of rounds. Instead, slower clients may still send their updates later in asynchronous FL, but their updates may be based on stale information. Our theoretical analysis takes the impact of staleness into account and suggests strategies to reduce its effects, such as down-weighting stale information and bounding the maximum staleness allowed.

In building this new asynchronous method, we knew we had to use secure aggregation, a crucial privacy-enhancing technology commonly used in FL systems. Secure aggregation uses cryptographic protocols to ensure that the server and all participating clients receive only a response aggregated over a group of devices and never from any individual device.

Previously, there was no known way to implement secure aggregation when performing asynchronous operations, because all parties participating in secure aggregation needed to be determined in advance. Our approach introduces a novel asynchronous secure aggregation procedure leveraging trusted execution environments (TEEs) — specialized secure hardware running in the cloud. Our implementation provides security guarantees while limiting the amount of data that needs to pass through the TEE. This enables rapid processing of updates and prevents the TEE from becoming a bottleneck.

Speeding toward a more private future

This advancement yields several improvements that will benefit us as we move into the future and help build the metaverse. With improved speed and efficiency, asynchronous FL will enable engineers to iterate faster on model development and evaluation. This speedup also makes it possible to deploy models trained using FL based on more recent, fresher data⁠, which will significantly impact features and applications that benefit from recent trends.

There’s also a potential environmental benefit. The reduction in communication overhead makes it possible to train models that make high-quality predictions using data from fewer individuals, reducing the carbon footprint of FL training.

Finally, by incorporating stale updates from slower devices, we show that models trained using asynchronous FL can be more accurate. We found our method’s predictions on data from slower devices was 47 percent more accurate than models trained with synchronous FL. These benefits show that asynchronous FL can help us scale and improve user experience while still protecting the privacy of people’s data.

Learn more about our approach to asynchronous FL by downloading our simulation framework and reading our papers.

Federated Learning with Buffered Asynchronous Aggregation

Papaya: Practical, Private, and Scalable Federated Learning"
Meta_Blog,https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/,,Democratizing access to large-scale language models with OPT-175B,"Large language models — natural language processing (NLP) systems with more than 100 billion parameters — have transformed NLP and AI research over the last few years. Trained on a massive and varied volume of text, they show surprising new capabilities to generate creative text, solve basic math problems, answer reading comprehension questions, and more. While in some cases the public can interact with these models through paid APIs, full research access is still limited to only a few highly resourced labs. This restricted access has limited researchers' ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues such as bias and toxicity.

In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets, to allow for more community engagement in understanding this foundational new technology. For the first time for a language technology system of this size, the release includes both the pretrained models and the code needed to train and use them. To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license to focus on research use cases. Access to the model will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; along with industry research laboratories around the world.

We believe the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular, given their centrality in many downstream language applications. A much broader segment of the AI community needs access to these models in order to conduct reproducible research and collectively drive the field forward. With the release of OPT-175B and smaller-scale baselines, we hope to increase the diversity of voices defining the ethical considerations of such technologies.

Something Went Wrong We're having trouble playing this video. Learn more

Responsible publication with OPT-175B

Following the publication guidelines for researchers generated by the Partnership on AI, along with the governance guidance outlined by NIST in March 2022 (section 3.4), we are releasing all our notes documenting the development process, including the full logbook detailing the day-to-day training process, so other researchers can more easily build on our work. Furthermore, these details disclose how much compute was used to train OPT-175B and the human overhead required when underlying infrastructure or the training process itself becomes unstable at scale.

We are sharing OPT-175B, along with the codebase used to train and deploy the model using only 16 NVIDIA V100 GPUs, in order to increase the accessibility of these models specifically for research purposes and to provide a foundation for analyzing potential harms rooted in quantifiable metrics on a common, shared model. We are also fully releasing a suite of smaller-scale baseline models, trained on the same data set and using similar settings as OPT-175B, to enable researchers to study the effect of scale alone. The parameter count for these smaller-scale models includes 125 million, 350 million, 1.3 billion, 2.7 billion, 6.7 billion, 13 billion, and 30 billion (66 billion to be released soon).

Responsible compute

Recent developments in AI research have consumed an extraordinary amount of compute power. While industry labs have started to report the carbon footprint of these models, most do not include the computational cost associated with the R&D phases of experimentation, which in some cases can be an order of magnitude more resource-intensive than training the final model.

We developed OPT-175B with energy efficiency in mind by successfully training a model of this size using only 1/7th the carbon footprint as that of GPT-3. This was achieved by combining Meta’s open source Fully Sharded Data Parallel (FSDP) API and NVIDIA’s tensor parallel abstraction within Megatron-LM. We achieved ~147 TFLOP/s/GPU utilization on NVIDIA’s 80 GB A100 GPUs, roughly 17 percent higher than published by NVIDIA researchers on similar hardware.

By sharing these baselines along with the codebase to train a 175B model efficiently, we have an opportunity to reduce our collective environmental footprint while also allowing new results and progress in the field to be measurable in a consistent manner.

Propelling research forward through open collaboration

For AI research to advance, the broader scientific community must be able to work together with cutting-edge models to effectively explore their potential while also probing for their vulnerabilities at the same time. As with our previous open-science initiatives, such as the Image Similarity Challenge, the Deepfake Detection Challenge, and the Hateful Memes Challenge, Meta AI believes that collaboration across research organizations is critical to the responsible development of AI technologies.

While there are many exciting developments in the space of large language models, the limitations and risks these models pose are still not well understood. Without direct access to these models, researchers are also limited in their ability to design detection and mitigation strategies for possible harm, which leaves detection and mitigation in the hands of only those with sufficient capital to access models of this scale. We hope that OPT-175B will bring more voices to the frontier of large language model creation, help the community collectively design responsible release strategies, and add an unprecedented level of transparency and openness to the development of large language models in the field.

Access the open source code and small-scale pretrained models here, request access to OPT-175B here, and read the paper here.

Pretrained models are all licensed under the OPT-175B License Agreement.

This work on large-scale pretraining is being undertaken by a multidisciplinary team that includes Stephen Roller, Naman Goyal, Anjali Sridhar, Punit Singh Koura, Moya Chen, Kurt Shuster, Mikel Artetxe, Daniel Simig, and Tianlu Wang. Advisers for responsible AI conduct also include Adina Williams, Eric Smith, Emily Dinan, Y-Lan Boureau, Melanie Kambadur, and Joelle Pineau."
Meta_Blog,https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/,,Democratizing access to large-scale language models with OPT-175B,"Large language models — natural language processing (NLP) systems with more than 100 billion parameters — have transformed NLP and AI research over the last few years. Trained on a massive and varied volume of text, they show surprising new capabilities to generate creative text, solve basic math problems, answer reading comprehension questions, and more. While in some cases the public can interact with these models through paid APIs, full research access is still limited to only a few highly resourced labs. This restricted access has limited researchers' ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues such as bias and toxicity.

In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets, to allow for more community engagement in understanding this foundational new technology. For the first time for a language technology system of this size, the release includes both the pretrained models and the code needed to train and use them. To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license to focus on research use cases. Access to the model will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; along with industry research laboratories around the world.

We believe the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular, given their centrality in many downstream language applications. A much broader segment of the AI community needs access to these models in order to conduct reproducible research and collectively drive the field forward. With the release of OPT-175B and smaller-scale baselines, we hope to increase the diversity of voices defining the ethical considerations of such technologies.

Something Went Wrong We're having trouble playing this video. Learn more

Responsible publication with OPT-175B

Following the publication guidelines for researchers generated by the Partnership on AI, along with the governance guidance outlined by NIST in March 2022 (section 3.4), we are releasing all our notes documenting the development process, including the full logbook detailing the day-to-day training process, so other researchers can more easily build on our work. Furthermore, these details disclose how much compute was used to train OPT-175B and the human overhead required when underlying infrastructure or the training process itself becomes unstable at scale.

We are sharing OPT-175B, along with the codebase used to train and deploy the model using only 16 NVIDIA V100 GPUs, in order to increase the accessibility of these models specifically for research purposes and to provide a foundation for analyzing potential harms rooted in quantifiable metrics on a common, shared model. We are also fully releasing a suite of smaller-scale baseline models, trained on the same data set and using similar settings as OPT-175B, to enable researchers to study the effect of scale alone. The parameter count for these smaller-scale models includes 125 million, 350 million, 1.3 billion, 2.7 billion, 6.7 billion, 13 billion, and 30 billion (66 billion to be released soon).

Responsible compute

Recent developments in AI research have consumed an extraordinary amount of compute power. While industry labs have started to report the carbon footprint of these models, most do not include the computational cost associated with the R&D phases of experimentation, which in some cases can be an order of magnitude more resource-intensive than training the final model.

We developed OPT-175B with energy efficiency in mind by successfully training a model of this size using only 1/7th the carbon footprint as that of GPT-3. This was achieved by combining Meta’s open source Fully Sharded Data Parallel (FSDP) API and NVIDIA’s tensor parallel abstraction within Megatron-LM. We achieved ~147 TFLOP/s/GPU utilization on NVIDIA’s 80 GB A100 GPUs, roughly 17 percent higher than published by NVIDIA researchers on similar hardware.

By sharing these baselines along with the codebase to train a 175B model efficiently, we have an opportunity to reduce our collective environmental footprint while also allowing new results and progress in the field to be measurable in a consistent manner.

Propelling research forward through open collaboration

For AI research to advance, the broader scientific community must be able to work together with cutting-edge models to effectively explore their potential while also probing for their vulnerabilities at the same time. As with our previous open-science initiatives, such as the Image Similarity Challenge, the Deepfake Detection Challenge, and the Hateful Memes Challenge, Meta AI believes that collaboration across research organizations is critical to the responsible development of AI technologies.

While there are many exciting developments in the space of large language models, the limitations and risks these models pose are still not well understood. Without direct access to these models, researchers are also limited in their ability to design detection and mitigation strategies for possible harm, which leaves detection and mitigation in the hands of only those with sufficient capital to access models of this scale. We hope that OPT-175B will bring more voices to the frontier of large language model creation, help the community collectively design responsible release strategies, and add an unprecedented level of transparency and openness to the development of large language models in the field.

Access the open source code and small-scale pretrained models here, request access to OPT-175B here, and read the paper here.

Pretrained models are all licensed under the OPT-175B License Agreement.

This work on large-scale pretraining is being undertaken by a multidisciplinary team that includes Stephen Roller, Naman Goyal, Anjali Sridhar, Punit Singh Koura, Moya Chen, Kurt Shuster, Mikel Artetxe, Daniel Simig, and Tianlu Wang. Advisers for responsible AI conduct also include Adina Williams, Eric Smith, Emily Dinan, Y-Lan Boureau, Melanie Kambadur, and Joelle Pineau."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-be-more-collaborative-with-humans-without-learning-directly-from-them/,,Teaching AI to be more collaborative with humans without learning directly from them,"What the research is:

It’s often not enough for robots to excel at a given task. They must also behave in a way that people can easily understand and be able to anticipate how others will respond to their actions. This has been a difficult AI challenge, however. With the most widely used approach — reinforcement learning (RL), where the agents learn mainly from rewards collected during interactions with the environment — the agent typically develops its own unique behaviors and communication protocols. It might arbitrarily decide, for example, that the letter A represents blue pieces or that it’s best not to play a green card after playing two red cards. These can be unintelligible both to humans and to other agents trained independently.

This isn’t an issue with competitive games, such as Go or chess. But intelligibility is essential in cooperative games, such as bridge or gin rummy, where two partners must work together and know how best to help each other even with very limited information about each other’s cards. Previously, agents trained in RL without human labeled data unavoidably pick up arbitrary conventions, making them unsuitable for real world human-AI cooperation

Meta AI has developed a new, more flexible approach to teaching AI to cooperate and make their actions understandable to people: off-belief learning. Instead of using human labeled data, off-belief learning starts with the quest to search for a ""grounded communication,” where the goal is to find the most efficient way to communicate without assuming any prior conventions. We are sharing a paper on our work along, open-sourcing the code, and releasing a public demo where everyone can play with our model trained using off-belief learning.

How it works:

Let’s consider a simple cooperative card game between two strangers, Alice and Bob. Alice draws a random card that’s either red or blue. While she’s able to see each card she draws, Bob can’t and must guess the color of the card. Each time he chooses the right color — red or blue -— both players win $5. Alice can either send number 1 or 2 to Bob, or simply reveal the card to him. While it is free to send a number, it costs $1 to reveal the card.

In this game, the grounded communication is to avoid the cheaper options and always just show Bob the true color. This is the same approach that humans would use when communicating with strangers with whom they haven’t established a prior rapport. In conventional multiagent RL methods, the two agents would converge using 1 to represent one color and 2 for the other. But that approach wouldn’t work well in playing with humans or with another independently trained agent. Since there’s no way in advance to know whether an agent uses 1 to represent a particular color or not, they wouldn’t know whether 1 represented blue or red.ation? Should this thing that looks like an upside-down cat be grouped with the other cats?

This is where off-belief learning comes in. The goal of off-belief learning is to find the most efficient way to communicate without assuming any prior conventions. This grounded policy can then be used as the basis for more advanced policies. Key to this approach is our ability to fix the common-knowledge policy that each agent can always assume the other agents are operating under, even though the actual policy used by the players can be drastically different. If we pick the uniform random policy as our common-knowledge policy — which samples each action with equal probability and where there is no shared prior knowledge between the agents — both agents learn to behave as if there were no prior conventions at all.

Let’s go back to the example of Alice's and Bob’s card game. Suppose Alice initially has a convention of sending 1 to mean blue and 2 to mean red, but Bob always behaves as if Alice’s messages were sent by a randomly acting agent. As a result, Bob will only guess the color consistently when Alice sends the true color, rather than when she sends 1 or 2. As a result, Alice eventually learns she can’t communicate any meaningful information by sending 1 or 2 and will realize she is better off paying $1 to send the true color.

Any joint policies that do not rely on prior conventions are grounded policies, such as the best response to a uniform random policy. The special power of off-belief learning is that it can find the optimal one by fixing the interpretation of the past while optimizing the current jointly for all players. In more complex scenarios where grounded play may not be the best solution, we can use the outcome of off-belief learning as the new common-knowledge policy and apply off-belief learning again. For each time that we repeat this, agents can develop new, more complicated ways to communicate while sharing a common trace of reasoning originated from the grounded policy.

Why it matters:

AI already has so many capabilities, but its applications will be limited if it acts in ways people would never expect. Off-belief learning will help address this. In many ways, this process resembles how people approach complex interpersonal communication problems. While we may have to spend more time explaining and clarifying something in a new relationship, over time, as we learn how to communicate effectively with someone, we establish a shared language and logical reasoning that help us communicate more effectively.

In this latest research, we propose an efficient implementation of this algorithm and test it via Hanabi, a collaborative card game and key benchmark game for AI research. It features both cooperative gameplay and imperfect information. We found that off-belief learning significantly improves how well an AI agent can collaborate with a human proxy policy (meaning a policy that attempts to mimic the behavior of an actual person) without using human data. Many Hanabi players found it more intuitive to play with agents trained in this way than with agents from prior works. This method is a valuable next step toward building and enabling collaborative assistants that will one day be both ubiquitous and tremendously helpful to our lives.

Read the paper"
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-be-more-collaborative-with-humans-without-learning-directly-from-them/,,Teaching AI to be more collaborative with humans without learning directly from them,"What the research is:

It’s often not enough for robots to excel at a given task. They must also behave in a way that people can easily understand and be able to anticipate how others will respond to their actions. This has been a difficult AI challenge, however. With the most widely used approach — reinforcement learning (RL), where the agents learn mainly from rewards collected during interactions with the environment — the agent typically develops its own unique behaviors and communication protocols. It might arbitrarily decide, for example, that the letter A represents blue pieces or that it’s best not to play a green card after playing two red cards. These can be unintelligible both to humans and to other agents trained independently.

This isn’t an issue with competitive games, such as Go or chess. But intelligibility is essential in cooperative games, such as bridge or gin rummy, where two partners must work together and know how best to help each other even with very limited information about each other’s cards. Previously, agents trained in RL without human labeled data unavoidably pick up arbitrary conventions, making them unsuitable for real world human-AI cooperation

Meta AI has developed a new, more flexible approach to teaching AI to cooperate and make their actions understandable to people: off-belief learning. Instead of using human labeled data, off-belief learning starts with the quest to search for a ""grounded communication,” where the goal is to find the most efficient way to communicate without assuming any prior conventions. We are sharing a paper on our work along, open-sourcing the code, and releasing a public demo where everyone can play with our model trained using off-belief learning.

How it works:

Let’s consider a simple cooperative card game between two strangers, Alice and Bob. Alice draws a random card that’s either red or blue. While she’s able to see each card she draws, Bob can’t and must guess the color of the card. Each time he chooses the right color — red or blue -— both players win $5. Alice can either send number 1 or 2 to Bob, or simply reveal the card to him. While it is free to send a number, it costs $1 to reveal the card.

In this game, the grounded communication is to avoid the cheaper options and always just show Bob the true color. This is the same approach that humans would use when communicating with strangers with whom they haven’t established a prior rapport. In conventional multiagent RL methods, the two agents would converge using 1 to represent one color and 2 for the other. But that approach wouldn’t work well in playing with humans or with another independently trained agent. Since there’s no way in advance to know whether an agent uses 1 to represent a particular color or not, they wouldn’t know whether 1 represented blue or red.ation? Should this thing that looks like an upside-down cat be grouped with the other cats?

This is where off-belief learning comes in. The goal of off-belief learning is to find the most efficient way to communicate without assuming any prior conventions. This grounded policy can then be used as the basis for more advanced policies. Key to this approach is our ability to fix the common-knowledge policy that each agent can always assume the other agents are operating under, even though the actual policy used by the players can be drastically different. If we pick the uniform random policy as our common-knowledge policy — which samples each action with equal probability and where there is no shared prior knowledge between the agents — both agents learn to behave as if there were no prior conventions at all.

Let’s go back to the example of Alice's and Bob’s card game. Suppose Alice initially has a convention of sending 1 to mean blue and 2 to mean red, but Bob always behaves as if Alice’s messages were sent by a randomly acting agent. As a result, Bob will only guess the color consistently when Alice sends the true color, rather than when she sends 1 or 2. As a result, Alice eventually learns she can’t communicate any meaningful information by sending 1 or 2 and will realize she is better off paying $1 to send the true color.

Any joint policies that do not rely on prior conventions are grounded policies, such as the best response to a uniform random policy. The special power of off-belief learning is that it can find the optimal one by fixing the interpretation of the past while optimizing the current jointly for all players. In more complex scenarios where grounded play may not be the best solution, we can use the outcome of off-belief learning as the new common-knowledge policy and apply off-belief learning again. For each time that we repeat this, agents can develop new, more complicated ways to communicate while sharing a common trace of reasoning originated from the grounded policy.

Why it matters:

AI already has so many capabilities, but its applications will be limited if it acts in ways people would never expect. Off-belief learning will help address this. In many ways, this process resembles how people approach complex interpersonal communication problems. While we may have to spend more time explaining and clarifying something in a new relationship, over time, as we learn how to communicate effectively with someone, we establish a shared language and logical reasoning that help us communicate more effectively.

In this latest research, we propose an efficient implementation of this algorithm and test it via Hanabi, a collaborative card game and key benchmark game for AI research. It features both cooperative gameplay and imperfect information. We found that off-belief learning significantly improves how well an AI agent can collaborate with a human proxy policy (meaning a policy that attempts to mimic the behavior of an actual person) without using human data. Many Hanabi players found it more intuitive to play with agents trained in this way than with agents from prior works. This method is a valuable next step toward building and enabling collaborative assistants that will one day be both ubiquitous and tremendously helpful to our lives.

Read the paper"
Meta_Blog,https://ai.meta.com/blog/studying-the-brain-to-build-ai-that-processes-language-as-people-do/,,Studying the brain to build AI that processes language as people do,"Something Went Wrong We're having trouble playing this video. Learn more

AI has made impressive strides in recent years, but it’s still far from learning language as efficiently as humans. For instance, children learn that “orange” can refer to both a fruit and color from a few examples, but modern AI systems can't do this nearly as efficiently as people. This has led many researchers to wonder: Can studying the human brain help to build AI systems that can learn and reason like people do?

Today, Meta AI is announcing a long-term research initiative to better understand how the human brain processes language. In collaboration with neuroimaging center Neurospin (CEA) and INRIA we’re comparing how AI language models and the brain respond to the same spoken or written sentences. We’ll use insights from this work to guide the development of AI that processes speech and text as efficiently as people. Over the past two years, we’ve applied deep learning techniques to public neuroimaging data sets to analyze how the brain processes words and sentences.

The data sets were collected and shared by several academic institutions, including Max Planck Institute for Psycholinguistics and Princeton University. Each institution collected and shared the data sets with informed consent from the volunteers in accordance with legal policies as approved by their respective ethical committees, including the consent obtained from the study participants.

Our comparison between brains and language models have already led to valuable insights:

Language models that most closely resemble brain activity are those that best predict the next word from context (e.g. once upon a …time). Prediction based on partially observable inputs is at the core of self-supervised learning (SSL) in AI and may be key to how people learn language.

However, we discover that specific regions in the brain anticipates words and ideas far ahead in time, while most language models today are typically trained to predict the very next word. Unlocking this long-range forecasting capability could help improve modern AI language models.

Of course, we’re only scratching the surface — there’s still a lot we don’t understand about how the brain functions, and our research is ongoing. Now, our collaborators at NeuroSpin are creating an original neuroimaging data set to expand this research. We’ll be open-sourcing the data set, deep learning models, code, and research papers resulting from this effort to help spur discoveries in both AI and neuroscience communities. All of this work is part of Meta AI’s broader investments toward human-level AI that learns from limited to no supervision

Using deep learning to analyze complex brain signals

Our work is a part of the broader effort by the scientific community to use AI to better understand the brain. Neuroscientists have historically faced major limitations in analyzing brain signals — let alone comparing them with AI models. Studying neuronal activity and brain imaging is a time- and resource-intensive process, requiring heavy machinery to analyze neuronal activity, which is often opaque and noisy. Designing language experiments to measure brain responses in a controlled way can be painstaking too. For example, in classical language studies, sentences must match in complexity, and words must match frequency or number of letters, to allow a meaningful comparison of brain responses.

The rise of deep learning, where multiple layers of neural networks work together to learn, is rapidly alleviating these issues. This approach highlights where and when perceptual representations of words and sentences are generated in the brain when a volunteer reads or listens to a story.

Something Went Wrong We're having trouble playing this video. Learn more

With magnetoencephalography, we can identify the brain responses to individual words and sentences at every millisecond. We can then compare areas in the brain to modern language algorithms.

Deep learning systems require a lot of data to ensure accuracy. Functional magnetic resonance imaging (fMRI) studies capture only a few snapshots of brain activities, typically from a small sample size. To meet the demanding quantity of data required for deep learning, our team not only models thousands of brain scans recorded from public data sets using fMRI but also simultaneously models them using magnetoencephalography (MEG), a scanner that takes snapshots of brain activity every millisecond — faster than a blink of an eye. In combination, these neuroimaging devices provide the large neuroimaging data necessary to detect where and in what order the activations take place in the brain. This is key to parsing the algorithm of human cognition.

In several studies, we’ve discovered the brain is systematically organized in a hierarchy that’s strikingly similar to AI language models (here, here, and here). For example, linguists have long predicted that language processing is characterized by a sequence of sensory and lexical computations, before words can be combined into meaningful sentences. Our comparison between deep language models and the brain precisely validates this sequence of computations. When reading a word, the brain first produces representations that are similar to deep convolutional networks trained to recognize characters in the early visual cortices. These brain activations are then transformed along the visual hierarchy into lexical representations akin to word embeddings. Finally, a distributed cortical network generates neural representations that correlates with the middle and final layers of deep language models. Deep learning tools have made it possible to clarify the hierarchy of the brain in ways that wasn’t possible before.

Predicting far beyond the next word

A systematic comparison between dozens of deep language models shows that the better they predict words from context, the more their representations correlate with the brain. We found this after analyzing brain activations of here, 200 volunteers in a simple reading task. A similar discovery was independently made by a team at MIT a week apart from ours, further validating this exciting direction. These similar studies provide reassurance that the AI community is on the right path with using SSL toward human-level AI.

But finding similarities isn’t enough to grasp the principles of language understanding. The computational differences between biological and artificial neural networks are key to improving existing models and building new more intelligent language models. Recently, we revealed evidence of long-range predictions in the brain, which is an ability that still challenges today’s language models. For instance, consider the phrase “Once upon a …” Most language models today would typically predict the next word, “time,” but their ability to anticipate complex ideas, plots, and narratives like people do is still limited.

To explore this issue, together with INRIA, we compared a variety of language models to the brain responses of 345 volunteers, who listened to complex narratives while being recorded with fMRI. We enhance those models with long-range predictions to track forecasts in the brain.

Something Went Wrong We're having trouble playing this video. Learn more

Most language models today would typically predict the next word, “time,” but their ability to anticipate complex ideas, plots, and narratives like people is still limited. This is something we believe the human brain is automatically able to do – not only predict the next word or sound but also the next idea.

Our results show that specific brain regions, such as the prefrontal and parietal cortices, are best accounted for by language models enhanced with deep representations of far-off words in the future. These results shed light on the computational organization of the human brain and its inherently predictive nature and pave the way toward iimproving current AI models.

Toward human-level AI

Overall, these studies support an exciting possibility — there are, in fact, quantifiable similarities between brains and AI models. And these similarities can help generate new insights about how the brain functions. This opens new avenues, where neuroscience will guide the development of more intelligent AI, and where, in turn, AI will help uncover the wonders of the brain."
Meta_Blog,https://ai.meta.com/blog/studying-the-brain-to-build-ai-that-processes-language-as-people-do/,,Studying the brain to build AI that processes language as people do,"Something Went Wrong We're having trouble playing this video. Learn more

AI has made impressive strides in recent years, but it’s still far from learning language as efficiently as humans. For instance, children learn that “orange” can refer to both a fruit and color from a few examples, but modern AI systems can't do this nearly as efficiently as people. This has led many researchers to wonder: Can studying the human brain help to build AI systems that can learn and reason like people do?

Today, Meta AI is announcing a long-term research initiative to better understand how the human brain processes language. In collaboration with neuroimaging center Neurospin (CEA) and INRIA we’re comparing how AI language models and the brain respond to the same spoken or written sentences. We’ll use insights from this work to guide the development of AI that processes speech and text as efficiently as people. Over the past two years, we’ve applied deep learning techniques to public neuroimaging data sets to analyze how the brain processes words and sentences.

The data sets were collected and shared by several academic institutions, including Max Planck Institute for Psycholinguistics and Princeton University. Each institution collected and shared the data sets with informed consent from the volunteers in accordance with legal policies as approved by their respective ethical committees, including the consent obtained from the study participants.

Our comparison between brains and language models have already led to valuable insights:

Language models that most closely resemble brain activity are those that best predict the next word from context (e.g. once upon a …time). Prediction based on partially observable inputs is at the core of self-supervised learning (SSL) in AI and may be key to how people learn language.

However, we discover that specific regions in the brain anticipates words and ideas far ahead in time, while most language models today are typically trained to predict the very next word. Unlocking this long-range forecasting capability could help improve modern AI language models.

Of course, we’re only scratching the surface — there’s still a lot we don’t understand about how the brain functions, and our research is ongoing. Now, our collaborators at NeuroSpin are creating an original neuroimaging data set to expand this research. We’ll be open-sourcing the data set, deep learning models, code, and research papers resulting from this effort to help spur discoveries in both AI and neuroscience communities. All of this work is part of Meta AI’s broader investments toward human-level AI that learns from limited to no supervision

Using deep learning to analyze complex brain signals

Our work is a part of the broader effort by the scientific community to use AI to better understand the brain. Neuroscientists have historically faced major limitations in analyzing brain signals — let alone comparing them with AI models. Studying neuronal activity and brain imaging is a time- and resource-intensive process, requiring heavy machinery to analyze neuronal activity, which is often opaque and noisy. Designing language experiments to measure brain responses in a controlled way can be painstaking too. For example, in classical language studies, sentences must match in complexity, and words must match frequency or number of letters, to allow a meaningful comparison of brain responses.

The rise of deep learning, where multiple layers of neural networks work together to learn, is rapidly alleviating these issues. This approach highlights where and when perceptual representations of words and sentences are generated in the brain when a volunteer reads or listens to a story.

Something Went Wrong We're having trouble playing this video. Learn more

With magnetoencephalography, we can identify the brain responses to individual words and sentences at every millisecond. We can then compare areas in the brain to modern language algorithms.

Deep learning systems require a lot of data to ensure accuracy. Functional magnetic resonance imaging (fMRI) studies capture only a few snapshots of brain activities, typically from a small sample size. To meet the demanding quantity of data required for deep learning, our team not only models thousands of brain scans recorded from public data sets using fMRI but also simultaneously models them using magnetoencephalography (MEG), a scanner that takes snapshots of brain activity every millisecond — faster than a blink of an eye. In combination, these neuroimaging devices provide the large neuroimaging data necessary to detect where and in what order the activations take place in the brain. This is key to parsing the algorithm of human cognition.

In several studies, we’ve discovered the brain is systematically organized in a hierarchy that’s strikingly similar to AI language models (here, here, and here). For example, linguists have long predicted that language processing is characterized by a sequence of sensory and lexical computations, before words can be combined into meaningful sentences. Our comparison between deep language models and the brain precisely validates this sequence of computations. When reading a word, the brain first produces representations that are similar to deep convolutional networks trained to recognize characters in the early visual cortices. These brain activations are then transformed along the visual hierarchy into lexical representations akin to word embeddings. Finally, a distributed cortical network generates neural representations that correlates with the middle and final layers of deep language models. Deep learning tools have made it possible to clarify the hierarchy of the brain in ways that wasn’t possible before.

Predicting far beyond the next word

A systematic comparison between dozens of deep language models shows that the better they predict words from context, the more their representations correlate with the brain. We found this after analyzing brain activations of here, 200 volunteers in a simple reading task. A similar discovery was independently made by a team at MIT a week apart from ours, further validating this exciting direction. These similar studies provide reassurance that the AI community is on the right path with using SSL toward human-level AI.

But finding similarities isn’t enough to grasp the principles of language understanding. The computational differences between biological and artificial neural networks are key to improving existing models and building new more intelligent language models. Recently, we revealed evidence of long-range predictions in the brain, which is an ability that still challenges today’s language models. For instance, consider the phrase “Once upon a …” Most language models today would typically predict the next word, “time,” but their ability to anticipate complex ideas, plots, and narratives like people do is still limited.

To explore this issue, together with INRIA, we compared a variety of language models to the brain responses of 345 volunteers, who listened to complex narratives while being recorded with fMRI. We enhance those models with long-range predictions to track forecasts in the brain.

Something Went Wrong We're having trouble playing this video. Learn more

Most language models today would typically predict the next word, “time,” but their ability to anticipate complex ideas, plots, and narratives like people is still limited. This is something we believe the human brain is automatically able to do – not only predict the next word or sound but also the next idea.

Our results show that specific brain regions, such as the prefrontal and parietal cortices, are best accounted for by language models enhanced with deep representations of far-off words in the future. These results shed light on the computational organization of the human brain and its inherently predictive nature and pave the way toward iimproving current AI models.

Toward human-level AI

Overall, these studies support an exciting possibility — there are, in fact, quantifiable similarities between brains and AI models. And these similarities can help generate new insights about how the brain functions. This opens new avenues, where neuroscience will guide the development of more intelligent AI, and where, in turn, AI will help uncover the wonders of the brain."
Meta_Blog,https://ai.meta.com/blog/looper-meta-ai-optimization-platform-for-engineers/,,Inside Meta's AI optimization platform for engineers across the company,"AI is an important part of making modern software systems and products work as well as possible, from improving the user experience to making the compute infrastructure more efficient. Whether it’s reducing latency, improving the quality of a video stream, or streamlining the interfaces to match a particular person’s needs, AI today is often more effective than even carefully constructed human-crafted heuristic strategies. But to leverage AI more effectively in our products, we need to address several challenges: the system must accommodate software engineers without backgrounds in machine learning; it must provide mechanisms to optimize for many different product goals, which may differ from closed-form machine learning loss functions; it must distinguish causal connections from correlations in the data; and it must scale efficiently to train, host, and monitor large numbers of AI models.

To address these needs at Meta, we’ve built an end-to-end AI platform called Looper, with easy-to-use APIs for optimization, personalization, and feedback collection. Looper supports the full machine learning lifecycle from model training, deployment, and inference all the way to evaluation and tuning of products. Rather than rebuild our existing products around AI models, Looper enables us to upgrade them to use AI for personalized optimizations. The Looper platform currently hosts 700 AI models and generates 4 million of AI outputs per second.

Making smart strategies available to applications

Meta’s different services are used by billions of people every day, each of whom has different interests and preferences. Looper enables us to customize many of these “out of the box” at an unprecedented scale without requiring complex, specialized code.

Overloading someone using a product with dozens of choices in a UI menu can make a product unappealing no matter how much value it offers. But menu preferences vary among different people. Likewise, opportunistically prefetching content likely to be viewed by a user to a mobile device may greatly improve the user experience of our product, but doing this without overwhelming the hardware resources of the device t requires accurately predicting what will be of most interest.

To support real-time smart strategies in a scalable way, Looper offers several features:

Looper targets ease of use and rapid deployment of models for use cases with moderate data sizes and model complexity.

It supports a variety of model types, hosts and trains numerous models and decision policies.

It supports a wide selection of machine learning tasks (classification, estimation, value and sequence prediction, ranking, planning) via its ability to use either supervised or reinforcement learning. Combined with model management infrastructure, our automation tools (AutoML) select models and hyperparameters to balance model quality, size, inference time, etc. Looper covers the scope from data sources to product impact, evaluated and optimized via causal experiments.

It is a declarative AI system, which means that product engineers only need to declare the functionality they want and the system fills in the software implementation based on the declaration. Internally, Looper relies on our strategy blueprint abstraction, which combines configurations for features, labels, models, and decision policies into one, and maintains multiple versions of such joint configurations. This supports more comprehensive optimization, captures compatibility between versions, and enables coding-free management of the full lifecycle of smart strategies. Blueprints enable vertical optimizations of black-box product metrics using a powerful experiment optimization system.

While other AI platforms often perform inference offline in batch mode, Looper operates in real time.

Many AI systems work with uniform data, such as pixels or text, but different products often have very different metadata, often coming from different sources. Moreover, patterns in metadata change quickly, necessitating regular retraining of AI models on fresh data.

A/B testing to evaluate many different types of models and decision rules, including those used by contextual bandits, to model uncertainty in predictions across one or more objectives, or reinforcement learning, to optimize long-term, cumulative objectives.

Unlike traditional end-to-end AI systems, Looper enables engineers and others at Meta to track how a model is actually used in the software stack and experiment on all aspects of the modeling framework – all the way from metric selection to policy optimization. To do this, Looper extends the common definition of end-to-end into the software layer, so that model architecture, feature selection parameters can be optimized in a multiobjective tradeoff between model quality and computational resources. To optimize long-term product goals an engineer can adjust how much importance is placed on different inputs when making real-time decisions. Our platform makes it possible to optimize these and other parameters using AutoML techniques applied to the entire pipeline.

The Looper platform for deploying smart strategies

Unlike heavyweight AI models for vision, speech and natural language processing, which favor offline inference with batch processing, Looper works with models that can be re-trained and deployed quickly in large numbers on shared infrastructure. Our platform interprets user-interaction and system-interaction metadata as either labels for supervised learning or rewards for reinforcement learning.

Looper pursues fast onboarding, robust deployment, and low-effort maintenance of multiple smart strategies where positive impacts are measured and optimized directly in application terms. Application code is separated from platform code, and Looper leverages existing horizontal AI platforms, such as PyTorch and Ax, with interchangeable models for machine learning tasks.

To make smart strategies successful, we need a way to evaluate them and improve them when results are not sufficiently good. Such evaluation is performed based on product metrics. In some cases, each decision can be checked, so that good and poor decisions can be used as examples on which a smart strategy learns (via supervised learning). However, some product metrics track long-term objectives (such as active daily users) that cannot be tracked down to specific decisions. Both cases can be handled by Looper, and using live data is particularly important. Access to Meta’s monitoring infrastructure helps detect unforeseen side effects. On our platform, product developers define the decision space, allowing the platform to automatically select model type and hyperparameter settings. The models are trained and evaluated on live data without user impact, and improved until they can be deployed. Newly trained models are canaried (deployed on shadow traffic) before product use – such models are evaluated on a sampled subset of logged features and observations, and offline quality metrics (e.g., MSE for regression tasks) are computed. This helps avoid degrading model quality when deploying newer models.

Adoption and impact of smart strategies

Our vertical machine learning platform hosts moderate-sized models from horizontal platforms so as to improve various aspects of software systems. These models are deployed with little engineering effort and maintained without model-specific infrastructure. Looper is currently used by 90+ product teams at Meta that deploy 690 models that make 4 million predictions per second.

Application use cases fall into five categories, in decreasing order of frequency:

Personalized Experience is tailored based on the user's engagement history. For example, a product may display shopping-related content prominently only to those likely to use it (but such content is accessible to all users through menus).

Ranking orders items to improve user utility, e.g. to personalize a feed of candidate items for the viewer.

Prefetching/precomputing data/resources based on predicted likelihood of usage (Section 4.1).

Notifications/prompts can be sent only to users who find them helpful.

Value estimation predicts regression tasks, e.g., latency or memory usage of a data query.

The figure below compares resource consumption (the number of servers on the y-axis) by resource categories for active Looper use cases.

The spectrum of AI expertise varied across product teams from beginners to experienced AI engineers, and only 15 percent of teams using the Looper platform include AI engineers. For teams without production AI experience, an easy-to-use AI platform is often the deciding factor for adoption, and AI investment continues upon evidence of utility. Our platform handles concerns about software upgrades, logging, monitoring, etc behind high-level services and unlocks hefty productivity improvements. For experienced AI engineers, a smart-strategies platform improves productivity by automating repetitive time-consuming work: writing database queries, implementing data pipelines, setting up monitoring and alerts. Compared to narrow-focus systems, it helps product developers launch more AI use cases. Regardless of prior AI experience, successful platform adopters configured initial machine learning models in just a couple days, quickly started collecting training data, and then refined their models and launched new products within just months.

Making AI at scale easier for engineers and product developers

Significant opportunities exist to embed self-optimizing smart strategies for product decisions into software systems, so as to enhance user experience, optimize resource utilization, and support new functionalities. Our AI platform Looper addresses the complexities of product-driven end-to-end machine learning systems and facilitates at scale deployment of smart strategies. It offers immediate, tangible benefits in terms of data availability, easy configuration, judicious use of available resources, reduced engineering effort, and ensuring product impact. Platform adopters are particularly attracted by extensive support for product impact evaluation via causal inference and measurements of resource overhead.

Looper makes smart strategies more easily accessible to software engineers and enables product teams to build, deploy and improve AI-driven capabilities in a self-serve fashion without AI expertise. We will continue to develop the platform so that we can leverage AI in new ways to improve Meta’s products and services."
Meta_Blog,https://ai.meta.com/blog/looper-meta-ai-optimization-platform-for-engineers/,,Inside Meta's AI optimization platform for engineers across the company,"AI is an important part of making modern software systems and products work as well as possible, from improving the user experience to making the compute infrastructure more efficient. Whether it’s reducing latency, improving the quality of a video stream, or streamlining the interfaces to match a particular person’s needs, AI today is often more effective than even carefully constructed human-crafted heuristic strategies. But to leverage AI more effectively in our products, we need to address several challenges: the system must accommodate software engineers without backgrounds in machine learning; it must provide mechanisms to optimize for many different product goals, which may differ from closed-form machine learning loss functions; it must distinguish causal connections from correlations in the data; and it must scale efficiently to train, host, and monitor large numbers of AI models.

To address these needs at Meta, we’ve built an end-to-end AI platform called Looper, with easy-to-use APIs for optimization, personalization, and feedback collection. Looper supports the full machine learning lifecycle from model training, deployment, and inference all the way to evaluation and tuning of products. Rather than rebuild our existing products around AI models, Looper enables us to upgrade them to use AI for personalized optimizations. The Looper platform currently hosts 700 AI models and generates 4 million of AI outputs per second.

Making smart strategies available to applications

Meta’s different services are used by billions of people every day, each of whom has different interests and preferences. Looper enables us to customize many of these “out of the box” at an unprecedented scale without requiring complex, specialized code.

Overloading someone using a product with dozens of choices in a UI menu can make a product unappealing no matter how much value it offers. But menu preferences vary among different people. Likewise, opportunistically prefetching content likely to be viewed by a user to a mobile device may greatly improve the user experience of our product, but doing this without overwhelming the hardware resources of the device t requires accurately predicting what will be of most interest.

To support real-time smart strategies in a scalable way, Looper offers several features:

Looper targets ease of use and rapid deployment of models for use cases with moderate data sizes and model complexity.

It supports a variety of model types, hosts and trains numerous models and decision policies.

It supports a wide selection of machine learning tasks (classification, estimation, value and sequence prediction, ranking, planning) via its ability to use either supervised or reinforcement learning. Combined with model management infrastructure, our automation tools (AutoML) select models and hyperparameters to balance model quality, size, inference time, etc. Looper covers the scope from data sources to product impact, evaluated and optimized via causal experiments.

It is a declarative AI system, which means that product engineers only need to declare the functionality they want and the system fills in the software implementation based on the declaration. Internally, Looper relies on our strategy blueprint abstraction, which combines configurations for features, labels, models, and decision policies into one, and maintains multiple versions of such joint configurations. This supports more comprehensive optimization, captures compatibility between versions, and enables coding-free management of the full lifecycle of smart strategies. Blueprints enable vertical optimizations of black-box product metrics using a powerful experiment optimization system.

While other AI platforms often perform inference offline in batch mode, Looper operates in real time.

Many AI systems work with uniform data, such as pixels or text, but different products often have very different metadata, often coming from different sources. Moreover, patterns in metadata change quickly, necessitating regular retraining of AI models on fresh data.

A/B testing to evaluate many different types of models and decision rules, including those used by contextual bandits, to model uncertainty in predictions across one or more objectives, or reinforcement learning, to optimize long-term, cumulative objectives.

Unlike traditional end-to-end AI systems, Looper enables engineers and others at Meta to track how a model is actually used in the software stack and experiment on all aspects of the modeling framework – all the way from metric selection to policy optimization. To do this, Looper extends the common definition of end-to-end into the software layer, so that model architecture, feature selection parameters can be optimized in a multiobjective tradeoff between model quality and computational resources. To optimize long-term product goals an engineer can adjust how much importance is placed on different inputs when making real-time decisions. Our platform makes it possible to optimize these and other parameters using AutoML techniques applied to the entire pipeline.

The Looper platform for deploying smart strategies

Unlike heavyweight AI models for vision, speech and natural language processing, which favor offline inference with batch processing, Looper works with models that can be re-trained and deployed quickly in large numbers on shared infrastructure. Our platform interprets user-interaction and system-interaction metadata as either labels for supervised learning or rewards for reinforcement learning.

Looper pursues fast onboarding, robust deployment, and low-effort maintenance of multiple smart strategies where positive impacts are measured and optimized directly in application terms. Application code is separated from platform code, and Looper leverages existing horizontal AI platforms, such as PyTorch and Ax, with interchangeable models for machine learning tasks.

To make smart strategies successful, we need a way to evaluate them and improve them when results are not sufficiently good. Such evaluation is performed based on product metrics. In some cases, each decision can be checked, so that good and poor decisions can be used as examples on which a smart strategy learns (via supervised learning). However, some product metrics track long-term objectives (such as active daily users) that cannot be tracked down to specific decisions. Both cases can be handled by Looper, and using live data is particularly important. Access to Meta’s monitoring infrastructure helps detect unforeseen side effects. On our platform, product developers define the decision space, allowing the platform to automatically select model type and hyperparameter settings. The models are trained and evaluated on live data without user impact, and improved until they can be deployed. Newly trained models are canaried (deployed on shadow traffic) before product use – such models are evaluated on a sampled subset of logged features and observations, and offline quality metrics (e.g., MSE for regression tasks) are computed. This helps avoid degrading model quality when deploying newer models.

Adoption and impact of smart strategies

Our vertical machine learning platform hosts moderate-sized models from horizontal platforms so as to improve various aspects of software systems. These models are deployed with little engineering effort and maintained without model-specific infrastructure. Looper is currently used by 90+ product teams at Meta that deploy 690 models that make 4 million predictions per second.

Application use cases fall into five categories, in decreasing order of frequency:

Personalized Experience is tailored based on the user's engagement history. For example, a product may display shopping-related content prominently only to those likely to use it (but such content is accessible to all users through menus).

Ranking orders items to improve user utility, e.g. to personalize a feed of candidate items for the viewer.

Prefetching/precomputing data/resources based on predicted likelihood of usage (Section 4.1).

Notifications/prompts can be sent only to users who find them helpful.

Value estimation predicts regression tasks, e.g., latency or memory usage of a data query.

The figure below compares resource consumption (the number of servers on the y-axis) by resource categories for active Looper use cases.

The spectrum of AI expertise varied across product teams from beginners to experienced AI engineers, and only 15 percent of teams using the Looper platform include AI engineers. For teams without production AI experience, an easy-to-use AI platform is often the deciding factor for adoption, and AI investment continues upon evidence of utility. Our platform handles concerns about software upgrades, logging, monitoring, etc behind high-level services and unlocks hefty productivity improvements. For experienced AI engineers, a smart-strategies platform improves productivity by automating repetitive time-consuming work: writing database queries, implementing data pipelines, setting up monitoring and alerts. Compared to narrow-focus systems, it helps product developers launch more AI use cases. Regardless of prior AI experience, successful platform adopters configured initial machine learning models in just a couple days, quickly started collecting training data, and then refined their models and launched new products within just months.

Making AI at scale easier for engineers and product developers

Significant opportunities exist to embed self-optimizing smart strategies for product decisions into software systems, so as to enhance user experience, optimize resource utilization, and support new functionalities. Our AI platform Looper addresses the complexities of product-driven end-to-end machine learning systems and facilitates at scale deployment of smart strategies. It offers immediate, tangible benefits in terms of data availability, easy configuration, judicious use of available resources, reduced engineering effort, and ensuring product impact. Platform adopters are particularly attracted by extensive support for product impact evaluation via causal inference and measurements of resource overhead.

Looper makes smart strategies more easily accessible to software engineers and enables product teams to build, deploy and improve AI-driven capabilities in a self-serve fashion without AI expertise. We will continue to develop the platform so that we can leverage AI in new ways to improve Meta’s products and services."
Meta_Blog,https://ai.meta.com/blog/accelerating-renewable-energy-with-a-new-data-set-for-green-hydrogen-fuel/,,Accelerating renewable energy with new data set for green hydrogen fuel,"6.20.22 Update: The OC22 Oxygen Evolution Reaction (OER) dataset is now available. You can read the paper on the dataset here and download the dataset here.

What the research is:

Transforming renewable resources to other fuels, such as hydrogen, is one scalable solution to energy challenges posed by climate change. To be widely adopted, however, we need low-cost catalysts to drive the necessary chemical reactions at high rates. Unfortunately, finding new catalysts is a highly time- and resource-intensive process. Conventional methods, for example, allow researchers to computationally evaluate tens of thousands of chemical structures per year — yet there are billions of possible combinations of elements to test.

To address this challenge, Meta AI and Carnegie Mellon University’s (CMU) Department of Chemical Engineering have been collaborating on the Open Catalyst Project, which aims to build machine learning (ML) models that simulate chemical reactions and accelerate the discovery of low-cost catalysts. Historically, a lack of sufficient training datasets has been a roadblock for researchers developing these ML models. As part of this project, we’ve already made progress by open-sourcing OC20, the world’s largest training dataset of materials for renewable energy storage.

Today, we’re announcing an entirely new dataset focused on oxide catalysts for the Oxygen Evolution Reaction (OER), a critical chemical reaction used in green hydrogen fuel production via wind and solar energy. The OER dataset contains ~8M data points from 40K unique simulations. We believe it’s the largest dataset for oxide catalysis to date, spanning a swath of oxide materials across 52 elements. It includes interactions between the surfaces of the oxide materials and five important molecules (O, OH, H2O, OOH, and O2) involved in OER, in addition to surface interactions with CO, H, C, and N. It also explores interactions on the surface when crystal defects and multiple molecules are present. The dataset and baseline models will be open-sourced in the coming months to help the global scientific community advance renewable energy technologies.

How it works:

Something Went Wrong We're having trouble playing this video. Learn more

Relaxation trajectory of a carboxylic group (CO*) on top of an Iridium atom in a Calcium Iridium Oxide (CaIrO3) catalyst surface. The above mechanism is an important intermediate for CO2 reduction applications.

To identify promising catalysts, research scientists use quantum mechanical simulation tools like density functional theory (DFT) to predict adsorption energies of small molecules on potential catalysts. This is a crucial property in determining how effective the catalyst will be. DFT uses quantum mechanics to simulate the movement of atoms in a given scenario, iteratively moving the positions of atoms in the system until they reach their lowest energy configuration, also known as a relaxation. Each relaxation takes hundreds of hours to complete on a multicore machine.

ML can accelerate this process — we can replace DFT simulations that currently take hours or days with ML predictions that take a few seconds. These ML models need to be trained on a dataset that matches DFT-predicted configurations or energies. To build our new OER dataset, we partnered with experts at CMU to determine the materials included in the dataset and to run DFT calculations out of billions of possibilities to create baseline models.

Something Went Wrong We're having trouble playing this video. Learn more

Relaxation trajectory of a water molecule (H2O) on top of a Lutetium atom in a Lutetium Gallium Oxide (LuGaO3) catalyst surface. The above mechanism is an important step for hydrogen production via the oxygen evolution reaction.

The process of generating this dataset required tens of millions of compute hours. The carbon emissions stemming from the compute resources used to generate the dataset were committed to be 100 percent offset as part of Meta’s Net Zero program.

Why it matters:

Scalable solutions to renewable energy storage are essential to addressing the world’s rising energy needs while slowing climate change.

OER is an important electrochemical reaction for hydrogen production and the intermediate steps involved in that process. Limited by the availability of existing, expensive precious metal oxides, like ruthenium and iridium oxide, researchers’ need for efficient low-cost catalysts for OER has grown more pressing. Our new dataset enables researchers to train and build ML models that will quickly identify low-cost oxide catalysts.

Improved catalysts for OER will advance several renewable energy technologies, such as solar and wind fuel production, as well as rechargeable metal-air batteries, a renewable energy storage device that is useful for electric cars.

With this new upcoming open source dataset release, we hope to spur scientific progress by helping researchers overcome computational limits of previous methods. More broadly, we hope it will help the computational chemistry community discover promising new materials at scale."
Meta_Blog,https://ai.meta.com/blog/accelerating-renewable-energy-with-a-new-data-set-for-green-hydrogen-fuel/,,Accelerating renewable energy with new data set for green hydrogen fuel,"6.20.22 Update: The OC22 Oxygen Evolution Reaction (OER) dataset is now available. You can read the paper on the dataset here and download the dataset here.

What the research is:

Transforming renewable resources to other fuels, such as hydrogen, is one scalable solution to energy challenges posed by climate change. To be widely adopted, however, we need low-cost catalysts to drive the necessary chemical reactions at high rates. Unfortunately, finding new catalysts is a highly time- and resource-intensive process. Conventional methods, for example, allow researchers to computationally evaluate tens of thousands of chemical structures per year — yet there are billions of possible combinations of elements to test.

To address this challenge, Meta AI and Carnegie Mellon University’s (CMU) Department of Chemical Engineering have been collaborating on the Open Catalyst Project, which aims to build machine learning (ML) models that simulate chemical reactions and accelerate the discovery of low-cost catalysts. Historically, a lack of sufficient training datasets has been a roadblock for researchers developing these ML models. As part of this project, we’ve already made progress by open-sourcing OC20, the world’s largest training dataset of materials for renewable energy storage.

Today, we’re announcing an entirely new dataset focused on oxide catalysts for the Oxygen Evolution Reaction (OER), a critical chemical reaction used in green hydrogen fuel production via wind and solar energy. The OER dataset contains ~8M data points from 40K unique simulations. We believe it’s the largest dataset for oxide catalysis to date, spanning a swath of oxide materials across 52 elements. It includes interactions between the surfaces of the oxide materials and five important molecules (O, OH, H2O, OOH, and O2) involved in OER, in addition to surface interactions with CO, H, C, and N. It also explores interactions on the surface when crystal defects and multiple molecules are present. The dataset and baseline models will be open-sourced in the coming months to help the global scientific community advance renewable energy technologies.

How it works:

Something Went Wrong We're having trouble playing this video. Learn more

Relaxation trajectory of a carboxylic group (CO*) on top of an Iridium atom in a Calcium Iridium Oxide (CaIrO3) catalyst surface. The above mechanism is an important intermediate for CO2 reduction applications.

To identify promising catalysts, research scientists use quantum mechanical simulation tools like density functional theory (DFT) to predict adsorption energies of small molecules on potential catalysts. This is a crucial property in determining how effective the catalyst will be. DFT uses quantum mechanics to simulate the movement of atoms in a given scenario, iteratively moving the positions of atoms in the system until they reach their lowest energy configuration, also known as a relaxation. Each relaxation takes hundreds of hours to complete on a multicore machine.

ML can accelerate this process — we can replace DFT simulations that currently take hours or days with ML predictions that take a few seconds. These ML models need to be trained on a dataset that matches DFT-predicted configurations or energies. To build our new OER dataset, we partnered with experts at CMU to determine the materials included in the dataset and to run DFT calculations out of billions of possibilities to create baseline models.

Something Went Wrong We're having trouble playing this video. Learn more

Relaxation trajectory of a water molecule (H2O) on top of a Lutetium atom in a Lutetium Gallium Oxide (LuGaO3) catalyst surface. The above mechanism is an important step for hydrogen production via the oxygen evolution reaction.

The process of generating this dataset required tens of millions of compute hours. The carbon emissions stemming from the compute resources used to generate the dataset were committed to be 100 percent offset as part of Meta’s Net Zero program.

Why it matters:

Scalable solutions to renewable energy storage are essential to addressing the world’s rising energy needs while slowing climate change.

OER is an important electrochemical reaction for hydrogen production and the intermediate steps involved in that process. Limited by the availability of existing, expensive precious metal oxides, like ruthenium and iridium oxide, researchers’ need for efficient low-cost catalysts for OER has grown more pressing. Our new dataset enables researchers to train and build ML models that will quickly identify low-cost oxide catalysts.

Improved catalysts for OER will advance several renewable energy technologies, such as solar and wind fuel production, as well as rechargeable metal-air batteries, a renewable energy storage device that is useful for electric cars.

With this new upcoming open source dataset release, we hope to spur scientific progress by helping researchers overcome computational limits of previous methods. More broadly, we hope it will help the computational chemistry community discover promising new materials at scale."
Meta_Blog,https://ai.meta.com/blog/how-ai-is-helping-address-the-climate-crisis/,,How AI is helping address the climate crisis,"Emerging technologies are playing a pivotal role in the global effort against climate change, helping the world decarbonize energy production, boost the efficiency of industrial systems, and reduce the carbon footprint of everyday life. The development of these technologies represents one of the most urgent engineering challenges of our lifetimes and one that we believe will be powered in significant ways by advances in AI.

We’ve already seen, in so many technical fields, how AI can reshape our approach to solving problems. It can supercharge computational approaches to scientific research, enabling calculations that once took days to happen in hours and transforming the ability to model complex systems or process vast amounts of data. As a force multiplier for scientific research, AI is helping accelerate the rate of progress across many domains, including those most important to solving the climate crisis.

Computational approaches to chemistry, for example, allow researchers to simulate millions of experiments in a way that would be impossible to attempt if each needed to be done in a lab. As scientists search for new compounds and materials that could help scrub carbon dioxide from the atmosphere or convert it into a clean fuel source, AI will help accelerate the search.

One promising attempt at this is the Open Catalyst Project, run by a partnership between Meta AI and Carnegie Mellon University’s Department of Chemical Engineering. The project is bringing together AI researchers from across the world to design new machine learning models capable of predicting the result of chemical reactions, with the aim of making complex computer simulations that currently take hours or days happen instead in a matter of seconds. This leap in efficiency could help researchers identify the new materials needed to enable a massive leap forward in technologies for combating climate change. The team has already released the world’s largest training data set of materials for renewable energy storage, and will soon release a new data set with over 8M data points from 40K+ unique simulations across a variety of materials for green hydrogen production. We believe this is the largest data set for oxide catalysis to date. Here’s a conversation I recently had with Larry Zitnick, the researcher leading this amazing project:

Something Went Wrong We're having trouble playing this video. Learn more

The potential for AI to boost decarbonization efforts extends to the ability to monitor, measure, and model the effects of climate change and our efforts to mitigate them. This kind of work will grow in importance as governments and organizations seek to verify the effectiveness of carbon offset schemes or track the progress of environmental commitments.

Even a task as fundamental as understanding how much of the Earth is covered with forests, and how quickly these forests are changing in size and density, could be supercharged by better AI. Researchers at Meta are exploring using cutting-edge computer vision systems to interpret satellite imagery and produce extremely detailed, high-resolution maps of the world’s forests. The goal of the project is to estimate the carbon stored in forested areas, which would improve the ability to assess the impact of reforestation efforts.

This type of progress is incredibly exciting, but there’s equally vital work to be done in making AI itself become more carbon- and energy-efficient. Current approaches to training AI models depend on vast amounts of data processing and computing power, both of which are sources of significant energy consumption. Once these models are trained, putting them into operation is equally energy-intensive, especially when they operate at large scale, as they do at technology companies like Meta. And the physical infrastructure needed to support all this work produces significant emissions of its own, from the construction materials needed to build data centers and other facilities to the computing hardware and equipment that powers them. As AI use grows exponentially, the environmental impact of its growing infrastructure footprint needs to be accounted for.

Meta’s global operations (including projects like the Research SuperCluster, which will be the fastest AI supercomputer in the world) are supported by 100 percent renewable energy, but efficiency is still crucial as we build new, more powerful AI systems and grow our overall use of the technology across the company. Researchers at Meta and throughout the industry are currently exploring a number of approaches to Green AI, which includes things like developing the standards needed to measure the energy efficiency of an AI system to the algorithms and computing hardware needed to operate AI at scale. We believe this work will enable AI systems to grow sustainably and with lower infrastructure needs.

In a recently published paper, Meta researchers highlighted the scale of the challenge: The pursuit of more powerful AI models comes with an exponential scaling of the size of these models and the computing resources needed to train and operate them. But the potential to optimize these models to achieve the same performance gains with a significantly reduced carbon footprint is equally large: In an experiment on one AI model used for language translation in Meta’s products, the researchers identified optimizations that led to an 800x reduction in the infrastructure resources needed to serve the model at scale. A similar study by researchers at Google found that through the use of a number of best practices, they were able to reduce energy usage by 100x and carbon emissions by 1,000x.

The impact of such a reduction will be magnified by AI’s overall ability to drive major efficiency gains across a range of industries from farming to manufacturing. In agriculture, AI systems are helping optimize water and fertilizer usage and increase the productivity of farm equipment and systems. In manufacturing, AI tools are driving major improvements in robotics systems and process controls, enabling factories to be more productive with less waste and lower energy consumption. As industries race to decarbonize, AI will help drive the efficiency gains that make it possible.

While making today’s economy more efficient will be one of AI’s most important environmental contributions, the inventions it enables in the future could be even more consequential. Scientists around the world are learning to harness AI’s power to accelerate their research, and the result could be fundamental breakthroughs that are transformative to the effort to decarbonize.

We’re incredibly optimistic about the impact AI is going to have on climate and sustainability, and the role that our researchers and engineers can play in helping build it. And by applying that progress to our own operations, we hope to set an example for just how much impact this incredible technology can have on creating a sustainable global business."
Meta_Blog,https://ai.meta.com/blog/how-ai-is-helping-address-the-climate-crisis/,,How AI is helping address the climate crisis,"Emerging technologies are playing a pivotal role in the global effort against climate change, helping the world decarbonize energy production, boost the efficiency of industrial systems, and reduce the carbon footprint of everyday life. The development of these technologies represents one of the most urgent engineering challenges of our lifetimes and one that we believe will be powered in significant ways by advances in AI.

We’ve already seen, in so many technical fields, how AI can reshape our approach to solving problems. It can supercharge computational approaches to scientific research, enabling calculations that once took days to happen in hours and transforming the ability to model complex systems or process vast amounts of data. As a force multiplier for scientific research, AI is helping accelerate the rate of progress across many domains, including those most important to solving the climate crisis.

Computational approaches to chemistry, for example, allow researchers to simulate millions of experiments in a way that would be impossible to attempt if each needed to be done in a lab. As scientists search for new compounds and materials that could help scrub carbon dioxide from the atmosphere or convert it into a clean fuel source, AI will help accelerate the search.

One promising attempt at this is the Open Catalyst Project, run by a partnership between Meta AI and Carnegie Mellon University’s Department of Chemical Engineering. The project is bringing together AI researchers from across the world to design new machine learning models capable of predicting the result of chemical reactions, with the aim of making complex computer simulations that currently take hours or days happen instead in a matter of seconds. This leap in efficiency could help researchers identify the new materials needed to enable a massive leap forward in technologies for combating climate change. The team has already released the world’s largest training data set of materials for renewable energy storage, and will soon release a new data set with over 8M data points from 40K+ unique simulations across a variety of materials for green hydrogen production. We believe this is the largest data set for oxide catalysis to date. Here’s a conversation I recently had with Larry Zitnick, the researcher leading this amazing project:

Something Went Wrong We're having trouble playing this video. Learn more

The potential for AI to boost decarbonization efforts extends to the ability to monitor, measure, and model the effects of climate change and our efforts to mitigate them. This kind of work will grow in importance as governments and organizations seek to verify the effectiveness of carbon offset schemes or track the progress of environmental commitments.

Even a task as fundamental as understanding how much of the Earth is covered with forests, and how quickly these forests are changing in size and density, could be supercharged by better AI. Researchers at Meta are exploring using cutting-edge computer vision systems to interpret satellite imagery and produce extremely detailed, high-resolution maps of the world’s forests. The goal of the project is to estimate the carbon stored in forested areas, which would improve the ability to assess the impact of reforestation efforts.

This type of progress is incredibly exciting, but there’s equally vital work to be done in making AI itself become more carbon- and energy-efficient. Current approaches to training AI models depend on vast amounts of data processing and computing power, both of which are sources of significant energy consumption. Once these models are trained, putting them into operation is equally energy-intensive, especially when they operate at large scale, as they do at technology companies like Meta. And the physical infrastructure needed to support all this work produces significant emissions of its own, from the construction materials needed to build data centers and other facilities to the computing hardware and equipment that powers them. As AI use grows exponentially, the environmental impact of its growing infrastructure footprint needs to be accounted for.

Meta’s global operations (including projects like the Research SuperCluster, which will be the fastest AI supercomputer in the world) are supported by 100 percent renewable energy, but efficiency is still crucial as we build new, more powerful AI systems and grow our overall use of the technology across the company. Researchers at Meta and throughout the industry are currently exploring a number of approaches to Green AI, which includes things like developing the standards needed to measure the energy efficiency of an AI system to the algorithms and computing hardware needed to operate AI at scale. We believe this work will enable AI systems to grow sustainably and with lower infrastructure needs.

In a recently published paper, Meta researchers highlighted the scale of the challenge: The pursuit of more powerful AI models comes with an exponential scaling of the size of these models and the computing resources needed to train and operate them. But the potential to optimize these models to achieve the same performance gains with a significantly reduced carbon footprint is equally large: In an experiment on one AI model used for language translation in Meta’s products, the researchers identified optimizations that led to an 800x reduction in the infrastructure resources needed to serve the model at scale. A similar study by researchers at Google found that through the use of a number of best practices, they were able to reduce energy usage by 100x and carbon emissions by 1,000x.

The impact of such a reduction will be magnified by AI’s overall ability to drive major efficiency gains across a range of industries from farming to manufacturing. In agriculture, AI systems are helping optimize water and fertilizer usage and increase the productivity of farm equipment and systems. In manufacturing, AI tools are driving major improvements in robotics systems and process controls, enabling factories to be more productive with less waste and lower energy consumption. As industries race to decarbonize, AI will help drive the efficiency gains that make it possible.

While making today’s economy more efficient will be one of AI’s most important environmental contributions, the inventions it enables in the future could be even more consequential. Scientists around the world are learning to harness AI’s power to accelerate their research, and the result could be fundamental breakthroughs that are transformative to the effort to decarbonize.

We’re incredibly optimistic about the impact AI is going to have on climate and sustainability, and the role that our researchers and engineers can play in helping build it. And by applying that progress to our own operations, we hope to set an example for just how much impact this incredible technology can have on creating a sustainable global business."
Meta_Blog,https://ai.meta.com/blog/understanding-dimensional-collapse/,,Understanding and mitigating dimensional collapse,"Self-supervised learning has revolutionized AI training in several domains, including vision, language, and speech. By teaching models to construct useful representations of text and images without labeled data, self-supervised methods free AI systems to learn from orders of magnitude more examples, so they can recognize subtle patterns that humans might miss. Self-supervised networks now often perform as well or better than more traditional models.

But self-supervised methods have a significant drawback: They sometimes trigger dimensional collapse, in which a model fails to take advantage of its full capacity to encode information. We’re sharing new research that pinpoints two mechanisms underlying dimensional collapse. We have also developed DirectCLR, a training method that overcomes this problem by optimizing a model’s ability to create rich representations of knowledge.

The dynamics of representational embedding

An AI model translates data into complex mathematical representations, assigning each input numerical positions within hundreds of dimensions. For an image, the dimensions might quantify patterns in edges, the sharpness of angles, or the contours implied by shading. These representations — called embeddings — consolidate a wealth of meaning, distilling the unique qualities that characterize each object in a picture.

For an image model, the goal of training is to learn which traits distinguish one group of objects from another. Is color always important, or does this gray cat-shaped figure belong in the same bunch as this orange tabby? How about orientation? Should this thing that looks like an upside-down cat be grouped with the other cats?

Training forces the model to jettison pixel-level details specific to only one example and to find the common ground between similar things. When the model discards extraneous details — noise — it reduces the dimensionality of the embedding. In the process, the representations of, say, different cat pictures grow more alike.

Self-supervised approaches teach a model to cluster the representations of nearly identical inputs close together in a multidimensional graph, called the embedding space. In place of human-created labels, the system duplicates and distorts the input image — applying a series of random augmentations, such as cropping, desaturating, rotating, and resizing — to create a matching pair of pictures. The model learns to create similar embeddings for these images, and, in the process, to build representations that recognize relevant attributes and ignore noise.

The underlying dynamics of these methods remain somewhat mysterious, however. Self-supervised methods can bring about complete collapse, in which all representation vectors cluster at a single point; the model creates the exact same embedding for each input. This can happen when the system learns only from pairs of related images. In attempting to maximize the likeness between similar features, the model ends up treating all images as if they were the same.

To prevent complete collapse, researchers often turn to contrastive learning, one of the most promising methods for self-supervised image-model training. Like other self-supervised approaches, contrastive techniques teach a model to cluster the representations of nearly identical inputs, or positive pairs, close together in the embedding space — but the model also learns to push embeddings away from those of dissimilar examples, or negative pairs. The positive and negative pairs are treated differently in the loss function, which measures the error in the model’s predictions. The model will learn to bunch all the representations of cat photos together, and to separate them from the representations of otter pictures. (It doesn’t know that one group depicts animals called cats, and the other shows otters; it simply learns to discriminate between different sets of features.)

Ideally, a model’s embedding vectors would span the entire embedding space, maximizing the knowledge it can encode. However, we observed that while contrastive learning prevents total collapse, it can induce a related problem called dimensional collapse. When that happens, the embeddings all vacate certain dimensions and shrink into a lower-dimensional subspace — like a 3D sculpture compressed into a 2D drawing.

To quantify the problem, we trained a SimCLR model with a two-layer multilayer perceptron projector and evaluated its dimensionality by collecting the embedding vectors on the validation set. About 30 singular values dropped to zero, indicating that those dimensions had collapsed. This suggests that contrastive learning may not make full use of model capacity, and that as a result the system will represent a limited volume of information. These artifacts may prevent contrastive methods from being truly scalable.

Mechanisms of dimensional collapse

In contrastive methods that explicitly use positive and negative pairs in the loss function, the repulsive effect of the dissimilar examples should propel the embeddings across all the available dimensions. However, contrary to intuition, contrastive learning methods can nonetheless trigger dimensional collapse. We found that two mechanisms cause this phenomenon.

The first is strong augmentation. When the distortion applied to the duplicate is overly severe, the image is no longer similar enough to the original for the network to recognize them as a positive pair. If strong augmentation produces more variance within a particular feature than is found in the data distribution, the weight collapses in that dimension. We found that this happens when the contrastive covariance matrix (the weighted data distribution covariance matrix minus the weighted augmentation covariance matrix) is not positive semidefinite.

But contrastive learning can bring about dimensional collapse even when the positive pairs are similar — if the linear network has more parameters than necessary. Overparameterized networks tend to find low-rank — that is, lower-dimensional — solutions. A loss function can have more than one local minimum, and some minima confer lower loss than others. By design, different optimization algorithms will tend to converge to particular local minima and eschew others. These dynamics, along with the interplay of weight matrices at different layers, can cause overparameterized neural networks to find flatter minima solutions. This phenomenon, called implicit regularization, is thought to be what drives neural networks to generalize so well in supervised training.

However, in contrastive learning settings, implicit regularization can prevent neural networks from encoding more than minimal information even when positive pairs are very similar. We discovered that in these cases, gradient descent spurs adjacent layers to align and small initialized singular values to evolve exponentially more slowly than others, resulting in collapsed dimensions. For this to happen, the contrastive covariance matrix must be positive semidefinite — the exact opposite of the condition that causes dimensional collapse when augmentation is too strong.

A new method that optimizes the embedding space

Given the fundamental limitations of contrastive learning, AI researchers may need better approaches to develop truly scalable self-supervised methods. We have developed a novel contrastive learning method, DirectCLR, which uses a low-rank diagonal projector. In contrast to all recent state-of-the-art self-supervised learning approaches, DirectCLR optimizes the representation space, and it outperforms SimCLR with a linear trainable projector on ImageNet. DirectCLR sends a subvector of the representation directly to the loss function. Even though the gradient from the loss function is low-rank, DirectCLR takes advantage of residual connection in the ResNet backbone to build full-rank representation vectors.

In the near future, self-supervised pretraining will become standard procedure for machine learning pipelines. We must understand the fundamental limitations of these methods before deploying them in huge models and data sets. Although dimensional collapse is an obstacle to scalability for contrastive learning methods, our study shows at least one way to circumvent this problem. The more we learn about these approaches, the better our models will learn on their own.

Read the paper"
Meta_Blog,https://ai.meta.com/blog/understanding-dimensional-collapse/,,Understanding and mitigating dimensional collapse,"Self-supervised learning has revolutionized AI training in several domains, including vision, language, and speech. By teaching models to construct useful representations of text and images without labeled data, self-supervised methods free AI systems to learn from orders of magnitude more examples, so they can recognize subtle patterns that humans might miss. Self-supervised networks now often perform as well or better than more traditional models.

But self-supervised methods have a significant drawback: They sometimes trigger dimensional collapse, in which a model fails to take advantage of its full capacity to encode information. We’re sharing new research that pinpoints two mechanisms underlying dimensional collapse. We have also developed DirectCLR, a training method that overcomes this problem by optimizing a model’s ability to create rich representations of knowledge.

The dynamics of representational embedding

An AI model translates data into complex mathematical representations, assigning each input numerical positions within hundreds of dimensions. For an image, the dimensions might quantify patterns in edges, the sharpness of angles, or the contours implied by shading. These representations — called embeddings — consolidate a wealth of meaning, distilling the unique qualities that characterize each object in a picture.

For an image model, the goal of training is to learn which traits distinguish one group of objects from another. Is color always important, or does this gray cat-shaped figure belong in the same bunch as this orange tabby? How about orientation? Should this thing that looks like an upside-down cat be grouped with the other cats?

Training forces the model to jettison pixel-level details specific to only one example and to find the common ground between similar things. When the model discards extraneous details — noise — it reduces the dimensionality of the embedding. In the process, the representations of, say, different cat pictures grow more alike.

Self-supervised approaches teach a model to cluster the representations of nearly identical inputs close together in a multidimensional graph, called the embedding space. In place of human-created labels, the system duplicates and distorts the input image — applying a series of random augmentations, such as cropping, desaturating, rotating, and resizing — to create a matching pair of pictures. The model learns to create similar embeddings for these images, and, in the process, to build representations that recognize relevant attributes and ignore noise.

The underlying dynamics of these methods remain somewhat mysterious, however. Self-supervised methods can bring about complete collapse, in which all representation vectors cluster at a single point; the model creates the exact same embedding for each input. This can happen when the system learns only from pairs of related images. In attempting to maximize the likeness between similar features, the model ends up treating all images as if they were the same.

To prevent complete collapse, researchers often turn to contrastive learning, one of the most promising methods for self-supervised image-model training. Like other self-supervised approaches, contrastive techniques teach a model to cluster the representations of nearly identical inputs, or positive pairs, close together in the embedding space — but the model also learns to push embeddings away from those of dissimilar examples, or negative pairs. The positive and negative pairs are treated differently in the loss function, which measures the error in the model’s predictions. The model will learn to bunch all the representations of cat photos together, and to separate them from the representations of otter pictures. (It doesn’t know that one group depicts animals called cats, and the other shows otters; it simply learns to discriminate between different sets of features.)

Ideally, a model’s embedding vectors would span the entire embedding space, maximizing the knowledge it can encode. However, we observed that while contrastive learning prevents total collapse, it can induce a related problem called dimensional collapse. When that happens, the embeddings all vacate certain dimensions and shrink into a lower-dimensional subspace — like a 3D sculpture compressed into a 2D drawing.

To quantify the problem, we trained a SimCLR model with a two-layer multilayer perceptron projector and evaluated its dimensionality by collecting the embedding vectors on the validation set. About 30 singular values dropped to zero, indicating that those dimensions had collapsed. This suggests that contrastive learning may not make full use of model capacity, and that as a result the system will represent a limited volume of information. These artifacts may prevent contrastive methods from being truly scalable.

Mechanisms of dimensional collapse

In contrastive methods that explicitly use positive and negative pairs in the loss function, the repulsive effect of the dissimilar examples should propel the embeddings across all the available dimensions. However, contrary to intuition, contrastive learning methods can nonetheless trigger dimensional collapse. We found that two mechanisms cause this phenomenon.

The first is strong augmentation. When the distortion applied to the duplicate is overly severe, the image is no longer similar enough to the original for the network to recognize them as a positive pair. If strong augmentation produces more variance within a particular feature than is found in the data distribution, the weight collapses in that dimension. We found that this happens when the contrastive covariance matrix (the weighted data distribution covariance matrix minus the weighted augmentation covariance matrix) is not positive semidefinite.

But contrastive learning can bring about dimensional collapse even when the positive pairs are similar — if the linear network has more parameters than necessary. Overparameterized networks tend to find low-rank — that is, lower-dimensional — solutions. A loss function can have more than one local minimum, and some minima confer lower loss than others. By design, different optimization algorithms will tend to converge to particular local minima and eschew others. These dynamics, along with the interplay of weight matrices at different layers, can cause overparameterized neural networks to find flatter minima solutions. This phenomenon, called implicit regularization, is thought to be what drives neural networks to generalize so well in supervised training.

However, in contrastive learning settings, implicit regularization can prevent neural networks from encoding more than minimal information even when positive pairs are very similar. We discovered that in these cases, gradient descent spurs adjacent layers to align and small initialized singular values to evolve exponentially more slowly than others, resulting in collapsed dimensions. For this to happen, the contrastive covariance matrix must be positive semidefinite — the exact opposite of the condition that causes dimensional collapse when augmentation is too strong.

A new method that optimizes the embedding space

Given the fundamental limitations of contrastive learning, AI researchers may need better approaches to develop truly scalable self-supervised methods. We have developed a novel contrastive learning method, DirectCLR, which uses a low-rank diagonal projector. In contrast to all recent state-of-the-art self-supervised learning approaches, DirectCLR optimizes the representation space, and it outperforms SimCLR with a linear trainable projector on ImageNet. DirectCLR sends a subvector of the representation directly to the loss function. Even though the gradient from the loss function is low-rank, DirectCLR takes advantage of residual connection in the ResNet backbone to build full-rank representation vectors.

In the near future, self-supervised pretraining will become standard procedure for machine learning pipelines. We must understand the fundamental limitations of these methods before deploying them in huge models and data sets. Although dimensional collapse is an obstacle to scalability for contrastive learning methods, our study shows at least one way to circumvent this problem. The more we learn about these approaches, the better our models will learn on their own.

Read the paper"
Meta_Blog,https://ai.meta.com/blog/dino-self-supervised-learning-demo/,,Explore Meta AI’s self-supervised learning demo for images,"Today, we are releasing the first-ever external demo based on Meta AI's self-supervised learning work. We focus on Vision Transformers pretrained with DINO, a method we released last year that has grown in popularity based on its capacity to understand the semantic layout of an image.

Our choice to focus the first demo on DINO is motivated by its ability to learn both general and powerful semantic features, including patch-level matching and retrieval. Using the demo, people will be able to experience these advancements firsthand, including finding similar images or pieces of similar images, such as matching the eyes of a puppy to find similar-looking dogs, regardless of their position, location, or lighting in an image.

This illustration shows an example of patch-level retrieval.

While this may sound like a trivial use case, the technology underpinning this demo is part of the important bigger-picture future we are building at Meta AI. Computer vision powered by self-supervised learning is an important part of helping Meta AI researchers deliver AI systems that are more robust and less domain-centric in nature.

DINO enables AI researchers to build highly efficient computer vision systems that perform extremely well at a variety of tasks and are far less dependent on labeled data sets. For this to work, large-scale self-supervised learning training for computer vision needs an algorithm that can learn from random, unlabeled images and videos, and a vast amount of data to capture every piece of a diverse, everyday life. Our new AI Research SuperCluster will allow us to explore the training of larger models on even larger data sets, pushing the boundaries of what self-supervised learning can achieve.

Using self-supervised learning to advance computer vision

While we previously released the DINO code, this demo allows researchers and engineers to explore how the model understands images, to test its robustness, and to try it on their own images. And it allows others who are interested in new AI techniques to see how a single technique can create models that are generic enough to solve many tasks.

There are several experiences people can explore in the demo. Through image retrieval, a person could select a picture and discover similar images from a third-party data set of five million images. Patch-level retrieval lets people select an object or area from an image to discover similar images, such as the dog eyes we mentioned earlier. Finally, patch-matching can find similar areas between two given images, despite differences in the background, positioning of objects, and lighting.

When a person opens the demo and inputs an image or defines a patch of an image, DINO outputs features and descriptions that can be used to specify how similar it is to other images. These outputs are useful because they can be used to compute the distance between two images, in the same way we can compute distances between 3D points described by three numbers. (For example, an image of a cat is “far away” from the image of a car but close to the image of a dog, and even closer to the image of another cat.) It’s this distance property that powers the DINO demo and delivers results, whether retrieving the nearest image or using patch-matching to show the closest patch.

DINO provides a training procedure to enable an untrained model to learn this property, without using any labeled data. It’s based on a simple intuition: Given an image, we apply several modifications and teach our model that the modified image should still be similar to the original image. These modifications include changing the brightness or contrast, cropping a smaller part of the image, or rotating the image. With each modification, the model can learn something. From rotating, it learns that a bunny in different poses will still represent the same thing, while the brightness modification will teach it that a bunny in the shadow is similar to a bunny in bright sunlight.

While this model wasn't developed with metaverse applications in mind, there are potential future applications for doing visual queries that can be personalized and remain entirely on a person’s device, which can help keep data more private. For example, you take a photo of an object to teach DINO “these are my car keys.” Later, when looking for your keys, you can query “Where are my car keys?” This type of application requires being able to memorize objects and find them in images — and this is something the DINO model can do well.

Image duplication identification is another potential future use case. DINO-based models could help detect copies of a particular piece of harmful content, even when the image has been modified. We believe self-supervised learning advances will ultimately pave the way for a future where machine learning algorithms can be built on and stay on a person’s device, creating a more private and personalized future powered by AI assistants.

Exploring the DINO Demo

While we are only beginning to harness the potential of self-supervised learning, we believe it will be an important advancement as we help build the metaverse and new AR/VR experiences. Self-supervised learning helps us gain a deep understanding of real-world environments and how people experience them, which is too big and diverse to capture in labeled data sets. We'll need AI that can learn from everything it sees and hears, and that's only possible with self-supervised learning.

While DINO shows an advancement in self-supervised learning, and has many exciting potential future use cases, we want to make sure this demo is used as part of our open science responsible AI. It is against the demo’s terms of use to upload photos of humans, and we include a detector to block human faces.

We invite everyone to try our demo. While self-supervised learning is still in its infancy, we are excited about the potential it holds for the future as we continue to work on more private and personalized AI projects.

Explore the demo"
Meta_Blog,https://ai.meta.com/blog/dino-self-supervised-learning-demo/,,Explore Meta AI’s self-supervised learning demo for images,"Today, we are releasing the first-ever external demo based on Meta AI's self-supervised learning work. We focus on Vision Transformers pretrained with DINO, a method we released last year that has grown in popularity based on its capacity to understand the semantic layout of an image.

Our choice to focus the first demo on DINO is motivated by its ability to learn both general and powerful semantic features, including patch-level matching and retrieval. Using the demo, people will be able to experience these advancements firsthand, including finding similar images or pieces of similar images, such as matching the eyes of a puppy to find similar-looking dogs, regardless of their position, location, or lighting in an image.

This illustration shows an example of patch-level retrieval.

While this may sound like a trivial use case, the technology underpinning this demo is part of the important bigger-picture future we are building at Meta AI. Computer vision powered by self-supervised learning is an important part of helping Meta AI researchers deliver AI systems that are more robust and less domain-centric in nature.

DINO enables AI researchers to build highly efficient computer vision systems that perform extremely well at a variety of tasks and are far less dependent on labeled data sets. For this to work, large-scale self-supervised learning training for computer vision needs an algorithm that can learn from random, unlabeled images and videos, and a vast amount of data to capture every piece of a diverse, everyday life. Our new AI Research SuperCluster will allow us to explore the training of larger models on even larger data sets, pushing the boundaries of what self-supervised learning can achieve.

Using self-supervised learning to advance computer vision

While we previously released the DINO code, this demo allows researchers and engineers to explore how the model understands images, to test its robustness, and to try it on their own images. And it allows others who are interested in new AI techniques to see how a single technique can create models that are generic enough to solve many tasks.

There are several experiences people can explore in the demo. Through image retrieval, a person could select a picture and discover similar images from a third-party data set of five million images. Patch-level retrieval lets people select an object or area from an image to discover similar images, such as the dog eyes we mentioned earlier. Finally, patch-matching can find similar areas between two given images, despite differences in the background, positioning of objects, and lighting.

When a person opens the demo and inputs an image or defines a patch of an image, DINO outputs features and descriptions that can be used to specify how similar it is to other images. These outputs are useful because they can be used to compute the distance between two images, in the same way we can compute distances between 3D points described by three numbers. (For example, an image of a cat is “far away” from the image of a car but close to the image of a dog, and even closer to the image of another cat.) It’s this distance property that powers the DINO demo and delivers results, whether retrieving the nearest image or using patch-matching to show the closest patch.

DINO provides a training procedure to enable an untrained model to learn this property, without using any labeled data. It’s based on a simple intuition: Given an image, we apply several modifications and teach our model that the modified image should still be similar to the original image. These modifications include changing the brightness or contrast, cropping a smaller part of the image, or rotating the image. With each modification, the model can learn something. From rotating, it learns that a bunny in different poses will still represent the same thing, while the brightness modification will teach it that a bunny in the shadow is similar to a bunny in bright sunlight.

While this model wasn't developed with metaverse applications in mind, there are potential future applications for doing visual queries that can be personalized and remain entirely on a person’s device, which can help keep data more private. For example, you take a photo of an object to teach DINO “these are my car keys.” Later, when looking for your keys, you can query “Where are my car keys?” This type of application requires being able to memorize objects and find them in images — and this is something the DINO model can do well.

Image duplication identification is another potential future use case. DINO-based models could help detect copies of a particular piece of harmful content, even when the image has been modified. We believe self-supervised learning advances will ultimately pave the way for a future where machine learning algorithms can be built on and stay on a person’s device, creating a more private and personalized future powered by AI assistants.

Exploring the DINO Demo

While we are only beginning to harness the potential of self-supervised learning, we believe it will be an important advancement as we help build the metaverse and new AR/VR experiences. Self-supervised learning helps us gain a deep understanding of real-world environments and how people experience them, which is too big and diverse to capture in labeled data sets. We'll need AI that can learn from everything it sees and hears, and that's only possible with self-supervised learning.

While DINO shows an advancement in self-supervised learning, and has many exciting potential future use cases, we want to make sure this demo is used as part of our open science responsible AI. It is against the demo’s terms of use to upload photos of humans, and we include a detector to block human faces.

We invite everyone to try our demo. While self-supervised learning is still in its infancy, we are excited about the potential it holds for the future as we continue to work on more private and personalized AI projects.

Explore the demo"
Meta_Blog,https://ai.meta.com/blog/qa-with-david-esiobu-and-aram-h-markosyan-from-raise-meta-ais-rotational-program/,,"Q&A with David Esiobu and Aram H. Markosyan from RAISE, Meta AI’s rotational program","This is the final piece of a three-part series that spotlights software engineers participating in Meta AI’s Rotational AI Science & Engineering (RAISE) program, which aims to give participants from diverse backgrounds an opportunity to start their AI journey full-time at Meta. Read more about the RAISE program here.

“AI is one of the most promising frontiers of human discovery. Basic research, precision medicine, transportation, manufacturing — so many aspects of life could be revolutionized by these techniques,” said Software Engineer David Esiobu. “A career in AI seems like one of the best ways to tackle interesting, challenging projects that make a meaningful impact.” Working at Microsoft, Esiobu saw firsthand AI’s power to transform services. He wanted to contribute, but his background was in distributed systems, not AI.

Then he heard about Meta’s new Rotational AI Science and Engineering (RAISE) program, designed for software engineers who, like Esiobu, hope to kick-start a career in AI but lack professional experience in the field. Over 18 months and three rotations, participants — called RAISErs — join world-class research and engineering teams at Meta AI and gain hands-on experience working with a variety of cutting-edge technologies.

In the third and final installment of our RAISE Q&A series, we hear from David and his fellow RAISEr Aram H. Markosyan, both part of the inaugural cohort. David, located in Seattle, came to the program to pursue his interest in the intersection of software, AI, and life sciences. Aram, based in Menlo Park, is leveraging his background in large infrastructure engineering and fundamental research. David and Aram share how they learned about RAISE, how they found themselves at Meta, what they’re working on, and where they see themselves after the program.

If you missed the first two RAISE Q&A, read them here and here.

Applications for the 2022 RAISE program will close on April 15, 2022. For more information and to apply, visit our program page.

Tell us a bit about your career path.

David Esiobu: I discovered programming as a kid when I learned BASIC to make games on a DOS computer, and it’s remained a passion. I went on to study computer engineering at Georgia Tech with a focus on systems programming and bridging the worlds of hardware and software.

Upon graduation, I joined Citrix full-time, where I spent eight formative years. In my last role there I was part of the platform team spearheading the company’s transition to cloud-first offerings. I helped build core services for the effort and developed an interest in distributed systems.

I moved to Seattle in 2016 to work at Amazon, where I learned about building and running services at scale. I worked on Amazon Photos for four years, on everything from clients to back end, and started exploring regression models for sharing suggestions.

Working in healthcare and life sciences runs in my family, and I’d always seen that as a potential calling. When a chance to work on AI-powered services for health care providers at Microsoft came up, I jumped at the opportunity. While there, I worked on back end, deployment infrastructure, and data pipelines supporting researchers.

Aram H. Markosyan My career has oscillated between applied and theoretical science. I was born in Yerevan, Armenia, and I received my first MS in pure mathematics from Yerevan State University. Then I wanted to do more applied science, so I did my second MS in applied mathematics from Université Pierre-et-Marie-Curie in Paris. After that, I earned a PhD from Eindhoven University of Technology, in the Netherlands, in computational plasma physics, which was a good balance of theory and modeling.

That pattern — back and forth between applied and theoretical work — repeated many times in my career. My research has journeyed through optics, chemistry, vacuum science, and computer science. With every iteration, I worked faster and dug deeper into the subject at hand.

During the last months of my PhD, Professor Mark Kushner, the director of the Michigan Institute for Plasma Science and Engineering at the University of Michigan, invited me to work with him — the highest honor. During my two wonderful years at the University of Michigan, I worried about the amount of time it took to make our physics simulation codes work on the high-performance computing cluster. I started to develop a theory of runtime systems, an automatic way of discovering concurrency and parallelism in programs. After several weeks of work, I discovered that this is already a field in computer science! Less than a year later, I took a postdoc position at Sandia National Laboratories, the best place to do research on that topic.

In 2018, I joined Xilinx (now AMD), where I led two large infrastructure projects, one of which was building distributed computing infra from the ground up.

What drew you to AI and the RAISE program?

DE: At Microsoft, my team was pushing the state of the art in speech and natural language processing (NLP) for medicine. I wanted to contribute more in that area, and RAISE is allowing me to make a real-world impact while I deepen my skills in AI. The rotations add a unique opportunity to work in different domains, modalities, and parts of the stack before doubling down in any one specialty.

“ RAISE is allowing me to make a real-world impact while I deepen my skills in AI.

David Esiobu, RAISE

AHM: I have contributed to several scientific fields, always with the same motivation: to expand our understanding of the world around us. We use cognition in that process, and AI promises to help enhance our cognitive abilities. RAISE is the perfect opportunity to get to know the field from various perspectives.

Now that you’ve completed your first full rotation, how would you describe your experience?

DE: This was my first six months at Meta, and it’s been great learning the culture. I’ve always had to be deliberate about creating work-life balance — I have a tendency to fixate on things until I can see progress, to the detriment of other things in life. The people I’ve worked with here have been respectful of that balance.

Thankfully, my peers are ramping up at the same time. It’s a very social environment at Meta, and we learn faster from each other. Different teams collaborate and coalesce around initiatives in an organic, bottom-up way.

“ It’s a very social environment at Meta, and we learn faster from each other. Different teams collaborate and coalesce around initiatives in an organic, bottoms-up way.

David Esiobu, RAISE

AHM: Although I’m used to working long hours, it feels like I’ve compressed many years into the past several months. I learned a huge amount about the scale of the problems in AI. My fantastic managers, cohort, and peers have been very supportive, and I’ve met so many super-talented Metamates.

What are you working on?

DE: I’m currently working with the translation team on rewriting the language identification model and expanding the number of languages and dialects it supports. That’s been a great introduction to the experimentation process — I’m coming to appreciate how critically important high-quality data is.

AHM: I’m working on improving the robustness of large-scale transformer models used in NLP and computer vision.

Have any skills from your previous career been particularly useful as you ramp up?

DE: During my first rotation with the AI Foundation team, my experience with back-end architecture and distributed systems helped in building observability tools for distributed training infrastructure.

AHM: My mathematics background was a tremendous help in reading AI papers, and my software engineering past helped me ramp up quickly on infrastructure.

What are your plans after RAISE?

DE: Language seems to be fundamental to the way we think and understand — and possibly to how virtual agents do as well. I hope to continue in NLP and natural language understanding, eventually bridging back to the life sciences. NLP techniques have already shown promising results in fields like genomics, and I’d like to be part of that.

AHM: I will feel privileged to continue doing research at Meta after RAISE. The problems we are facing in terms of scale and complexity are at the forefront of state-of-the-art AI research.

What advice would you pass along to someone thinking of applying to RAISE?

DE: Be curious. Many topics at first appear difficult to grasp, but approaching with curiosity opens you to learning quickly and trying new things.

“ The problems we are facing in terms of scale and complexity are at the forefront of state-of-the-art AI research.

Aram H. Markosyan, RAISE

Also, be deliberate about what you want to do. The field is broad. Within Meta, things move quickly, and it’s easy to lose the signal in the flow of information. Having a clear direction will help you push through challenges and discern what to pay attention to.

AHM: Don’t expect a smooth transition. There will be a lot of learning — mostly on your own time — as well as frustration and tight timelines. But your fellow RAISErs, the RAISErs from the past cohorts, your managers, and your peers will be there to champion you."
Meta_Blog,https://ai.meta.com/blog/qa-with-david-esiobu-and-aram-h-markosyan-from-raise-meta-ais-rotational-program/,,"Q&A with David Esiobu and Aram H. Markosyan from RAISE, Meta AI’s rotational program","This is the final piece of a three-part series that spotlights software engineers participating in Meta AI’s Rotational AI Science & Engineering (RAISE) program, which aims to give participants from diverse backgrounds an opportunity to start their AI journey full-time at Meta. Read more about the RAISE program here.

“AI is one of the most promising frontiers of human discovery. Basic research, precision medicine, transportation, manufacturing — so many aspects of life could be revolutionized by these techniques,” said Software Engineer David Esiobu. “A career in AI seems like one of the best ways to tackle interesting, challenging projects that make a meaningful impact.” Working at Microsoft, Esiobu saw firsthand AI’s power to transform services. He wanted to contribute, but his background was in distributed systems, not AI.

Then he heard about Meta’s new Rotational AI Science and Engineering (RAISE) program, designed for software engineers who, like Esiobu, hope to kick-start a career in AI but lack professional experience in the field. Over 18 months and three rotations, participants — called RAISErs — join world-class research and engineering teams at Meta AI and gain hands-on experience working with a variety of cutting-edge technologies.

In the third and final installment of our RAISE Q&A series, we hear from David and his fellow RAISEr Aram H. Markosyan, both part of the inaugural cohort. David, located in Seattle, came to the program to pursue his interest in the intersection of software, AI, and life sciences. Aram, based in Menlo Park, is leveraging his background in large infrastructure engineering and fundamental research. David and Aram share how they learned about RAISE, how they found themselves at Meta, what they’re working on, and where they see themselves after the program.

If you missed the first two RAISE Q&A, read them here and here.

Applications for the 2022 RAISE program will close on April 15, 2022. For more information and to apply, visit our program page.

Tell us a bit about your career path.

David Esiobu: I discovered programming as a kid when I learned BASIC to make games on a DOS computer, and it’s remained a passion. I went on to study computer engineering at Georgia Tech with a focus on systems programming and bridging the worlds of hardware and software.

Upon graduation, I joined Citrix full-time, where I spent eight formative years. In my last role there I was part of the platform team spearheading the company’s transition to cloud-first offerings. I helped build core services for the effort and developed an interest in distributed systems.

I moved to Seattle in 2016 to work at Amazon, where I learned about building and running services at scale. I worked on Amazon Photos for four years, on everything from clients to back end, and started exploring regression models for sharing suggestions.

Working in healthcare and life sciences runs in my family, and I’d always seen that as a potential calling. When a chance to work on AI-powered services for health care providers at Microsoft came up, I jumped at the opportunity. While there, I worked on back end, deployment infrastructure, and data pipelines supporting researchers.

Aram H. Markosyan My career has oscillated between applied and theoretical science. I was born in Yerevan, Armenia, and I received my first MS in pure mathematics from Yerevan State University. Then I wanted to do more applied science, so I did my second MS in applied mathematics from Université Pierre-et-Marie-Curie in Paris. After that, I earned a PhD from Eindhoven University of Technology, in the Netherlands, in computational plasma physics, which was a good balance of theory and modeling.

That pattern — back and forth between applied and theoretical work — repeated many times in my career. My research has journeyed through optics, chemistry, vacuum science, and computer science. With every iteration, I worked faster and dug deeper into the subject at hand.

During the last months of my PhD, Professor Mark Kushner, the director of the Michigan Institute for Plasma Science and Engineering at the University of Michigan, invited me to work with him — the highest honor. During my two wonderful years at the University of Michigan, I worried about the amount of time it took to make our physics simulation codes work on the high-performance computing cluster. I started to develop a theory of runtime systems, an automatic way of discovering concurrency and parallelism in programs. After several weeks of work, I discovered that this is already a field in computer science! Less than a year later, I took a postdoc position at Sandia National Laboratories, the best place to do research on that topic.

In 2018, I joined Xilinx (now AMD), where I led two large infrastructure projects, one of which was building distributed computing infra from the ground up.

What drew you to AI and the RAISE program?

DE: At Microsoft, my team was pushing the state of the art in speech and natural language processing (NLP) for medicine. I wanted to contribute more in that area, and RAISE is allowing me to make a real-world impact while I deepen my skills in AI. The rotations add a unique opportunity to work in different domains, modalities, and parts of the stack before doubling down in any one specialty.

“ RAISE is allowing me to make a real-world impact while I deepen my skills in AI.

David Esiobu, RAISE

AHM: I have contributed to several scientific fields, always with the same motivation: to expand our understanding of the world around us. We use cognition in that process, and AI promises to help enhance our cognitive abilities. RAISE is the perfect opportunity to get to know the field from various perspectives.

Now that you’ve completed your first full rotation, how would you describe your experience?

DE: This was my first six months at Meta, and it’s been great learning the culture. I’ve always had to be deliberate about creating work-life balance — I have a tendency to fixate on things until I can see progress, to the detriment of other things in life. The people I’ve worked with here have been respectful of that balance.

Thankfully, my peers are ramping up at the same time. It’s a very social environment at Meta, and we learn faster from each other. Different teams collaborate and coalesce around initiatives in an organic, bottom-up way.

“ It’s a very social environment at Meta, and we learn faster from each other. Different teams collaborate and coalesce around initiatives in an organic, bottoms-up way.

David Esiobu, RAISE

AHM: Although I’m used to working long hours, it feels like I’ve compressed many years into the past several months. I learned a huge amount about the scale of the problems in AI. My fantastic managers, cohort, and peers have been very supportive, and I’ve met so many super-talented Metamates.

What are you working on?

DE: I’m currently working with the translation team on rewriting the language identification model and expanding the number of languages and dialects it supports. That’s been a great introduction to the experimentation process — I’m coming to appreciate how critically important high-quality data is.

AHM: I’m working on improving the robustness of large-scale transformer models used in NLP and computer vision.

Have any skills from your previous career been particularly useful as you ramp up?

DE: During my first rotation with the AI Foundation team, my experience with back-end architecture and distributed systems helped in building observability tools for distributed training infrastructure.

AHM: My mathematics background was a tremendous help in reading AI papers, and my software engineering past helped me ramp up quickly on infrastructure.

What are your plans after RAISE?

DE: Language seems to be fundamental to the way we think and understand — and possibly to how virtual agents do as well. I hope to continue in NLP and natural language understanding, eventually bridging back to the life sciences. NLP techniques have already shown promising results in fields like genomics, and I’d like to be part of that.

AHM: I will feel privileged to continue doing research at Meta after RAISE. The problems we are facing in terms of scale and complexity are at the forefront of state-of-the-art AI research.

What advice would you pass along to someone thinking of applying to RAISE?

DE: Be curious. Many topics at first appear difficult to grasp, but approaching with curiosity opens you to learning quickly and trying new things.

“ The problems we are facing in terms of scale and complexity are at the forefront of state-of-the-art AI research.

Aram H. Markosyan, RAISE

Also, be deliberate about what you want to do. The field is broad. Within Meta, things move quickly, and it’s easy to lose the signal in the flow of information. Having a clear direction will help you push through challenges and discern what to pay attention to.

AHM: Don’t expect a smooth transition. There will be a lot of learning — mostly on your own time — as well as frustration and tight timelines. But your fellow RAISErs, the RAISErs from the past cohorts, your managers, and your peers will be there to champion you."
Meta_Blog,https://ai.meta.com/blog/generating-chit-chat-including-laughs-yawns-ums-and-other-nonverbal-cues-from-raw-audio/,,"Generating chit-chat including laughs, yawns, 'ums,' and other nonverbal cues from raw audio","In any given conversation, people exchange chock-full of nonverbal signals, like intonations, emotional expression, pauses, accents, rhythms — all of which are important to human interactions. But today’s AI systems fail to capture these rich, expressive signals because they learn only from written text, which captures what we say but not how we say it.

Last year, we introduced a breakthrough natural language processing (NLP) model that breaks free of the traditional dependence on text, called Generative Spoken Language Model (GSLM). GSLM discovers structured content by addressing raw audio signals, without any labels or text — like a person would do. It enables NLP models to capture the expressivity of oral language, and it can be used as a form of pretraining for downstream applications or as a generative tool, producing possible continuations from a given input audio prompt.

Today, we’re announcing three milestones toward more expressive NLP models:

First, we’ve open-sourced the Textless Python Library, which machine learning practitioners can use to quickly build experiments on top of GSLM components (encoder, language model, and decoder). Check out the library here.

Second, we can now model expressive vocalizations, like laughter, yawning, and cries. These expressions are essential to understanding the context of an interaction the way a person would, making it possible to convey nuances about their communicative intention or the sentiment they are trying to convey — whether that’s irony, irritation, boredom, etc. Check out samples here and below.

Third, we can model spontaneous, real-time chit-chat between two AI agents. The agents factor in behavior, like occasional overlaps or pauses, which will be important for building agents like virtual assistants that can understand nuanced social cues and signals, like interruptions, as well as positive or negative feedback when chatting with someone. Check out samples here and below.

As the world becomes more digital, and as we leverage AI in the metaverse, AI-powered applications will create new experiences that go beyond typing text toward more fluid ways of interaction, like voice and gesture. All these advancements using representation and self-supervised learning have the potential to help researchers break free of traditional text-based models and build more natural, engaging AI systems of the future.

Beyond lacking expressivity, traditional NLP applications, which rely on massive text resources, are available in only a handful of languages in the world. In the long term, we believe the advancement of textless NLP systems will also help make AI more inclusive of more people, particularly people who speak languages and dialects without standardized writing systems, such as dialectal Arabic or Swiss German.

Rendering realistic audio of emotive expression, from amused to sleepy

Neutral to amused

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

Neutral to angry

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

Neutral to sleepy

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

Demonstration of textless NLP model generating emotion from raw audio only.

One of the technical challenges of capturing emotional expressiveness in speech is that such expressions typically affect many aspects of language at once. When people, for instance, shift from expressing happiness to anger, they may use different vocabulary and insert cries, grunts, and other nonverbal vocalizations. They may modify prosody, like intonation and rhythm, and they may change voice quality due to stress. Likewise, each aspect of language contributes to the perception of the apparent emotional state of the speaker, where the nonverbal can completely change the meaning.

Text-only systems yield poor representations of these layers, often making it ambiguous without context, yielding miscomprehensions. To make things worse, all these manifestations of emotional expression are complex and resource-intensive to annotate, making it difficult to augment text-based systems with extra audio features derived from audio.

We took a radically different approach. Instead of trying to fix text-based systems, we holistically model all these layers from raw audio at the same time — achieving realistic audio rendering of emotive expressions for the first time. The key to this achievement is GSLM’s ability to capture generic audio events irrespective of whether they are verbal, in particular nonverbal vocalizations, like laughter or yawning, which inform the expression and perception of emotional states or intentions that can meaningfully influence conversations.

Something Went Wrong We're having trouble playing this video. Learn more

An illustration of the proposed system. We use pink to denote models and green to denote representations. The input signal is first encoded as a discrete sequence of units (representing speech content). Next, a sequence-to-sequence translation model (conditioned by the target emotion tokens) is applied over the discrete units to translate between emotions. This is followed by a duration prediction and F0 estimation for each of the units (also conditioned by target emotion). Finally, the speech signal is synthesized based on the translated units and predicted prosodic features, together with the speaker features and the target emotional expressions.

We illustrated this in an emotion conversion task, where a sentence is presented in one of five emotional expressions (neutrality, anger, amusement, sleepiness, or disgust) and converted to another one of these emotional expressions. We treated this problem as a sequence-to-sequence translation problem, enabling the easy insertion, deletion, or substitution of nonverbal vocalizations. In addition, we conditioned the generation of rhythm (duration), intonation (f0), and voice by the target emotion, which made it possible, for the first time, to cover all of the layers of the expression of emotion.

Upon evaluation, our model achieved vastly higher quality compared with previous best emotional voice conversion models. In fact, results are very close in quality to the original audio, illustrated in light green in the chart below:

We compared the proposed approach to state-of-the-art text-based emotional voice conversion model, Seq2seq-EVC. We also evaluated an expressive text-to-speech system based on Tacotron2. We report Mean-Opinion Scores (MOS) as Emotion Mean-Opinion Classification (eMOS) on the above figures. Results suggest that the proposed approach (dark green) is vastly superior to past approaches (dark purple and light purple) in terms of speech generation quality (MOS) and in capturing the target emotion (eMOS), and in fact is very close in quality to the original audio (light green).

Generating chit-chat with pauses, ‘ums,’ and overlapping speech

Unlike written exchanges, spoken dialogues take place in real time. That means even small perturbations in timing caused by transmission delays can disrupt the smoothness and naturality of spontaneous exchanges in a video call.

Something Went Wrong We're having trouble playing this video. Learn more

Research shows that in any given verbal conversation, speakers spontaneously interpret timing between speech units. People generally time their own speech with utmost precision. Gaps and pauses are informative, and overlapping speech or nonverbals can signal agreement, disagreement, or a willingness to take the floor.

Something Went Wrong We're having trouble playing this video. Learn more

Demonstration of annotated data sets with rich expressivity of oral language.

Until now, such richness has been very difficult to address with AI, and required highly complex, carefully annotated data sets like the one pictured above.

Our approach was to model speech content, nonverbal vocalization, and timing in a holistic way. The key idea is simple: We modeled a dialogue as two parallel streams of speech units automatically derived as in GSLM. We used two identical transformers — one per stream, trained to predict its own future sets of units but informed about the states of the other transformer through cross-attention.

Something Went Wrong We're having trouble playing this video. Learn more

Animation of architecture of textless chit-chat bots — it will include demonstration of chit-chat dialogue as well.

A few examples of generations are found here. The model is prompted by 10 seconds of real conversation and continues with its own version of the chit-chat. The model is able to reproduce naturalistic distributions of gaps, turn durations, and overlaps.

What’s next for textless NLP?

In the near future, we will focus on applying textless techniques to build useful downstream applications without requiring either resource-intensive text labels or automatic speech recognition systems (ASR), such as question answering (e.g., “How’s the weather?”). We believe prosody in speech can help better parse a sentence, which in turn facilitates understanding the intent and improves the performance of question answering.

One of these applications is speech-to-speech translation, or dubbing. Due to the resource-intensive nature of traditional AI systems, dubbing is typically done by going through text. For instance, it typically works by converting audio to text, performing translation, and converting text back to audio. This makes the entire system complicated and difficult to train. It also misses expressivity of oral language not just because intonations and nonverbal expressions are lost through text but also because language models are trained on text that are missing these idiomatic expressions specific to oral language. Because self-supervised speech representation approaches are able to learn discrete units from raw audio, it’s now possible to remove the need for text and replace it with the pseudo text extracted from each of the target and source languages.

We believe that textless NLP can outperform traditional composite systems (ASR+NLP) because of the possibility of integrating nonverbal vocalizations and prosodic information, which conveys rich semantic and pragmatic information on top of phonemes — typically not available in text.

Stay tuned for more on our efforts toward the new era of textless NLP.

The work discussed in this blog post reflects the contributions from Meta AI researchers Yossi Adi, Jade Copet, Emmanuel Dupoux, Wei-Ning Hsu, Evgeny Kharitonov, Felix Kreuk, Abdelrahman Mohamed, and Tu Anh Nguyen (listed in alphabetical order).

1) Textless Speech Emotion Conversion

Read the paper

Check out the samples

2) Textless Spoken Dialogue Generation

Read the paper

Check out the samples

3) Textless NLP Github

Read the paper

Github library"
Meta_Blog,https://ai.meta.com/blog/generating-chit-chat-including-laughs-yawns-ums-and-other-nonverbal-cues-from-raw-audio/,,"Generating chit-chat including laughs, yawns, 'ums,' and other nonverbal cues from raw audio","In any given conversation, people exchange chock-full of nonverbal signals, like intonations, emotional expression, pauses, accents, rhythms — all of which are important to human interactions. But today’s AI systems fail to capture these rich, expressive signals because they learn only from written text, which captures what we say but not how we say it.

Last year, we introduced a breakthrough natural language processing (NLP) model that breaks free of the traditional dependence on text, called Generative Spoken Language Model (GSLM). GSLM discovers structured content by addressing raw audio signals, without any labels or text — like a person would do. It enables NLP models to capture the expressivity of oral language, and it can be used as a form of pretraining for downstream applications or as a generative tool, producing possible continuations from a given input audio prompt.

Today, we’re announcing three milestones toward more expressive NLP models:

First, we’ve open-sourced the Textless Python Library, which machine learning practitioners can use to quickly build experiments on top of GSLM components (encoder, language model, and decoder). Check out the library here.

Second, we can now model expressive vocalizations, like laughter, yawning, and cries. These expressions are essential to understanding the context of an interaction the way a person would, making it possible to convey nuances about their communicative intention or the sentiment they are trying to convey — whether that’s irony, irritation, boredom, etc. Check out samples here and below.

Third, we can model spontaneous, real-time chit-chat between two AI agents. The agents factor in behavior, like occasional overlaps or pauses, which will be important for building agents like virtual assistants that can understand nuanced social cues and signals, like interruptions, as well as positive or negative feedback when chatting with someone. Check out samples here and below.

As the world becomes more digital, and as we leverage AI in the metaverse, AI-powered applications will create new experiences that go beyond typing text toward more fluid ways of interaction, like voice and gesture. All these advancements using representation and self-supervised learning have the potential to help researchers break free of traditional text-based models and build more natural, engaging AI systems of the future.

Beyond lacking expressivity, traditional NLP applications, which rely on massive text resources, are available in only a handful of languages in the world. In the long term, we believe the advancement of textless NLP systems will also help make AI more inclusive of more people, particularly people who speak languages and dialects without standardized writing systems, such as dialectal Arabic or Swiss German.

Rendering realistic audio of emotive expression, from amused to sleepy

Neutral to amused

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

Neutral to angry

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

Neutral to sleepy

Something Went Wrong We're having trouble playing this video. Learn more Something Went Wrong We're having trouble playing this video. Learn more

Demonstration of textless NLP model generating emotion from raw audio only.

One of the technical challenges of capturing emotional expressiveness in speech is that such expressions typically affect many aspects of language at once. When people, for instance, shift from expressing happiness to anger, they may use different vocabulary and insert cries, grunts, and other nonverbal vocalizations. They may modify prosody, like intonation and rhythm, and they may change voice quality due to stress. Likewise, each aspect of language contributes to the perception of the apparent emotional state of the speaker, where the nonverbal can completely change the meaning.

Text-only systems yield poor representations of these layers, often making it ambiguous without context, yielding miscomprehensions. To make things worse, all these manifestations of emotional expression are complex and resource-intensive to annotate, making it difficult to augment text-based systems with extra audio features derived from audio.

We took a radically different approach. Instead of trying to fix text-based systems, we holistically model all these layers from raw audio at the same time — achieving realistic audio rendering of emotive expressions for the first time. The key to this achievement is GSLM’s ability to capture generic audio events irrespective of whether they are verbal, in particular nonverbal vocalizations, like laughter or yawning, which inform the expression and perception of emotional states or intentions that can meaningfully influence conversations.

Something Went Wrong We're having trouble playing this video. Learn more

An illustration of the proposed system. We use pink to denote models and green to denote representations. The input signal is first encoded as a discrete sequence of units (representing speech content). Next, a sequence-to-sequence translation model (conditioned by the target emotion tokens) is applied over the discrete units to translate between emotions. This is followed by a duration prediction and F0 estimation for each of the units (also conditioned by target emotion). Finally, the speech signal is synthesized based on the translated units and predicted prosodic features, together with the speaker features and the target emotional expressions.

We illustrated this in an emotion conversion task, where a sentence is presented in one of five emotional expressions (neutrality, anger, amusement, sleepiness, or disgust) and converted to another one of these emotional expressions. We treated this problem as a sequence-to-sequence translation problem, enabling the easy insertion, deletion, or substitution of nonverbal vocalizations. In addition, we conditioned the generation of rhythm (duration), intonation (f0), and voice by the target emotion, which made it possible, for the first time, to cover all of the layers of the expression of emotion.

Upon evaluation, our model achieved vastly higher quality compared with previous best emotional voice conversion models. In fact, results are very close in quality to the original audio, illustrated in light green in the chart below:

We compared the proposed approach to state-of-the-art text-based emotional voice conversion model, Seq2seq-EVC. We also evaluated an expressive text-to-speech system based on Tacotron2. We report Mean-Opinion Scores (MOS) as Emotion Mean-Opinion Classification (eMOS) on the above figures. Results suggest that the proposed approach (dark green) is vastly superior to past approaches (dark purple and light purple) in terms of speech generation quality (MOS) and in capturing the target emotion (eMOS), and in fact is very close in quality to the original audio (light green).

Generating chit-chat with pauses, ‘ums,’ and overlapping speech

Unlike written exchanges, spoken dialogues take place in real time. That means even small perturbations in timing caused by transmission delays can disrupt the smoothness and naturality of spontaneous exchanges in a video call.

Something Went Wrong We're having trouble playing this video. Learn more

Research shows that in any given verbal conversation, speakers spontaneously interpret timing between speech units. People generally time their own speech with utmost precision. Gaps and pauses are informative, and overlapping speech or nonverbals can signal agreement, disagreement, or a willingness to take the floor.

Something Went Wrong We're having trouble playing this video. Learn more

Demonstration of annotated data sets with rich expressivity of oral language.

Until now, such richness has been very difficult to address with AI, and required highly complex, carefully annotated data sets like the one pictured above.

Our approach was to model speech content, nonverbal vocalization, and timing in a holistic way. The key idea is simple: We modeled a dialogue as two parallel streams of speech units automatically derived as in GSLM. We used two identical transformers — one per stream, trained to predict its own future sets of units but informed about the states of the other transformer through cross-attention.

Something Went Wrong We're having trouble playing this video. Learn more

Animation of architecture of textless chit-chat bots — it will include demonstration of chit-chat dialogue as well.

A few examples of generations are found here. The model is prompted by 10 seconds of real conversation and continues with its own version of the chit-chat. The model is able to reproduce naturalistic distributions of gaps, turn durations, and overlaps.

What’s next for textless NLP?

In the near future, we will focus on applying textless techniques to build useful downstream applications without requiring either resource-intensive text labels or automatic speech recognition systems (ASR), such as question answering (e.g., “How’s the weather?”). We believe prosody in speech can help better parse a sentence, which in turn facilitates understanding the intent and improves the performance of question answering.

One of these applications is speech-to-speech translation, or dubbing. Due to the resource-intensive nature of traditional AI systems, dubbing is typically done by going through text. For instance, it typically works by converting audio to text, performing translation, and converting text back to audio. This makes the entire system complicated and difficult to train. It also misses expressivity of oral language not just because intonations and nonverbal expressions are lost through text but also because language models are trained on text that are missing these idiomatic expressions specific to oral language. Because self-supervised speech representation approaches are able to learn discrete units from raw audio, it’s now possible to remove the need for text and replace it with the pseudo text extracted from each of the target and source languages.

We believe that textless NLP can outperform traditional composite systems (ASR+NLP) because of the possibility of integrating nonverbal vocalizations and prosodic information, which conveys rich semantic and pragmatic information on top of phonemes — typically not available in text.

Stay tuned for more on our efforts toward the new era of textless NLP.

The work discussed in this blog post reflects the contributions from Meta AI researchers Yossi Adi, Jade Copet, Emmanuel Dupoux, Wei-Ning Hsu, Evgeny Kharitonov, Felix Kreuk, Abdelrahman Mohamed, and Tu Anh Nguyen (listed in alphabetical order).

1) Textless Speech Emotion Conversion

Read the paper

Check out the samples

2) Textless Spoken Dialogue Generation

Read the paper

Check out the samples

3) Textless NLP Github

Read the paper

Github library"
Meta_Blog,https://ai.meta.com/blog/generating-wikipedia-biographies/,,Using AI to deliver more inclusive biographical content on Wikipedia,"Wikipedia, which is consistently ranked one of the top 10 most visited websites, is often the first stop for many people looking for information about historical figures and changemakers. But not everyone is equally represented on Wikipedia. Only about 20 percent of biographies on the English site are about women, according to the Wikimedia Foundation, and we imagine that percentage is even smaller for women from intersectional groups, such as women in science, women in Africa, and women in Asia.

For my PhD project as a computer science student at the Université de Lorraine, CNRS, in France, I worked with my adviser, Claire Gardent , to develop a new way to address this imbalance using artificial intelligence. Together, we built an AI system that can research and write first drafts of Wikipedia-style biographical entries. There is more work to do, but we hope this new system will one day help Wikipedia editors create many thousands of accurate, compelling biography entries for important people who are currently not on the site.

Something Went Wrong We're having trouble playing this video. Learn more

To me, the problem was personal and based on the lack of representation I saw reflected in libraries when I was in grade school. When I was in third grade, I was assigned to write an essay about a historical figure, and the only requirement was that the library had to have a book about the person. I wanted to write about Eleanor Roosevelt but had to settle for Teddy Roosevelt. And what if I wanted to write about someone who looked like me — would that even have been an option? If we imagine the same assignment today, students would undoubtedly turn to the internet, most likely Wikipedia. While Wikipedia has millions of articles in English — including an excellent one about Eleanor Roosevelt — we know there are still plenty of women whose stories and achievements aren’t reaching future generations.

While women are more likely to write biographies about other women, Wikimedia’s Community Insights 2021 Report, which covers the previous year, found that only 15 percent of Wikipedia editors identified as women. This leaves women overlooked and underrepresented, despite the enormous impact they’ve had throughout history in science, entrepreneurship, politics, and every other part of society. Canadian physicist Donna Strickland won the Nobel Prize in Physics in 2018, however, anyone looking for information about her on Wikipedia wouldn’t have been able to find it until a Wikipedia biography was finally published about her esteemed work — days after she won the biggest prize in her field of study. Various studies, including from the Wikimedia Foundation itself, have also called out the gender imbalance on the platform. Even with a lack of representation, biographies about women were still being disproportionately nominated for deletion. One study found that in 2017, 41 percent of biographies nominated for deletion were about women.

We believe open, reproducible science can provide a starting point to address this issue. Today, we are open-sourcing an end-to-end AI model that automatically creates high-quality biographical articles about important real-world public figures.

Our model searches websites for relevant information and drafts a Wikipedia-style entry about that person, complete with citations. Along with the model, we are releasing a novel data set that was created to evaluate model performance on 1,527 biographies of women from marginalized groups. This data set can be used to train models, evaluate performance, and push the model forward. We believe these AI-generated entries can be used as a starting point for people writing Wikipedia content and fact checkers to publish more biographies of underrepresented groups on the site.

There is still plenty more we can do to help bring a wider representation of notable people from all backgrounds to Wikipedia. Fundamentally, AI systems like the one we built will have to confront broader societal and technical challenges in order to fully address the problem. This starts with the web content used to create Wikipedia entries, which may be flawed or reflect cultural biases. On the technical side, the text generation system may be prone to “hallucinating” nonfactual content. Even today’s best language models struggle to create text that is coherent over many paragraphs. We’re hoping to improve these through advances in the neural architectures that power such models and through breakthroughs in the responsible development of AI. Eventually, we hope this approach will be able to help nonexperts produce accurate articles to add to the collection of information on the web, with only minimal editing needed.

How AI can complement existing efforts to address bias

While our model isn’t a panacea, it’s an important step forward to support and supplement other existing efforts that are working to address gender representation on Wikipedia. Volunteer editors Jessica Wade and Penny Richards have worked independently to write and publish thousands of biographies on Wikipedia about women who deserve the spotlight. Another great, collective effort, the Women in Red Wiki Project, mobilizes editors to create new biographies and expand existing ones about notable women past and present.

We decided to take a complementary approach. Doing research, creating a bibliography, and writing are intensive, yet there is a trove of information available on the web that can be used to tell the stories of women whose achievements, voices, and legacies have otherwise been forgotten about or marginalized.

For example, we used our model to generate a short biography of Libbie Hyman, a pioneer in the study of invertebrate zoology. The green text is pulled from the reference article we started with, the purple text is from the web evidence, and the orange text indicates hallucination — meaning the model makes up information that can’t be verified.

hyman is best known for her work on the classification of invertebrates. she was the author of a six-volume set of reference books titled the invertebrate treatise, which was published by mcgraw-hill in the united states and in germany. she also wrote a series of laboratory manuals for the teaching of zoology classes nationwide. hyman’s work has had a lasting influence on scientific thinking about a number of animal groups, and the only works that can be compared with hers are of composite authorship.

The model retrieved relevant biographical information about Hyman, including her focus on invertebrates, significant publications, and the impact of her work, which can then be used as a starting point for editors to fact check (an area where the model still has shortcomings) and expand on her life and accomplishments.

Using pretraining and a retrieval model to generate Wikipedia biographies

We start the process of generating a biography by using a retrieval-augmented generation architecture based on large-scale pretraining, which teaches the model to identify only relevant information, such as birthplace or where the person attended school, as it builds the biography.

The model first retrieves relevant information from the internet to introduce the subject. Next, the generation module creates the text, while the third step, the citation module, builds the bibliography linking back to the sources that were used. The process then repeats, with each section predicting the next, covering all the elements that make up a robust Wikipedia biography, including the subject’s early life, education, and career.

We generate section by section, using a caching mechanism similar to Transformer-XL to reference previously written sections and achieve greater document-level context. Caching is important because it allows the model to better track what it previously generated.

Automatic and human evaluations show that the model is capable of finding relevant information and using it to generate biographies, but there is still work to do. Those evaluations found that 68 percent of the generated text in the biographies we created wasn’t found in the reference text. This could mean several things. It could suggest that the model does a good job of finding and synthesizing relevant information while not acting as a plagiarism bot. However, it’s also unclear, since it is difficult to know which information is accurate and which is not. We asked evaluators to determine whether full sentences were factual, and found many cases in which sentences were only partially verifiable. These challenges are similar to those faced by the field of text generation broadly, though they are exacerbated in the case of marginalized groups, as there is very little data about them. We hope that releasing this data set will allow other researchers to study this problem.

There were several other obstacles we encountered during our research. First, the lack of training data, or biographical articles that already exist about women, was very difficult to overcome. Existing articles about women, especially those from marginalized groups, are substantially shorter than the average article about men, are less detailed, and use different language — for example, “female scientist” instead of simply “scientist.” This bias in training data caused models to internalize such bias. Beyond this, Wikipedia articles must be written based on factual evidence, often sourced from the internet. However, the bias on Wikipedia extends to bias on the internet: There are very few web-based locations that could be used as evidence.

While deeply rooted problems can’t be solved quickly, this is exactly the type of problem where technology can be used to help engineer positive change.

What’s next? Spotlighting more underrepresented people on Wikipedia

We are excited to share this work with the community to help foster discussions, experimentation, and drive progress to help create a more equitable availability of content on Wikipedia.

Our model addresses just one piece of a multifaceted problem, so there are additional areas where new techniques should be explored. When a Wikipedia editor or our AI model writes a biography, information is pulled from around the internet and cited. However, for all of the enriching knowledge the internet has provided, some sources have a bias that must be considered. For example, when women are represented, their biographies are more likely to include extra details about their personal lives. A 2015 study found the word “divorced” appears four times as often in women’s biographies as it does in biographies of men. This could be for many reasons, including the tabloid fodder that tends to follow the lives of notable women more closely than those of men. As a result, personal details end up being more likely to be mentioned in articles about women, distracting from accomplishments that should be in the spotlight and celebrated.

Technology has already shown promise in helping address various imbalances, which is proof that there is even more the community can do to help make a difference. For example, the site’s former chief executive explained how an algorithm discovered an important mistake on the site: While Wikipedia health articles are vetted by medical editors, for years some articles on critical women’s health issues, such as breastfeeding, were labeled “low importance.”

There is even more work to be done for other marginalized and intersectional groups around the world and across languages. Our evaluation and data set focuses on women, which excludes many other groups, including nonbinary people. Articles about transgender and nonbinary people tend to be longer, but much of the additional space is devoted to their personal life instead of expanding on the person’s accomplishments, according to a 2021 study that looked at social biases in Wikipedia articles. It is important to recognize that bias exists in varying forms, especially in default online sources of information.

We are passionate about sharing this as an important research area with the broader generation community. We hope that our techniques can eventually be used as a starting point for human Wikipedia writers — and ultimately lead to a more equitable availability of information online that can be accessed by students writing biographies — and beyond.

This post was updated to clarify the name of the university. It's Université de Lorraine, CNRS, in France.

Read the paper

Get the code

Learn more about this research"
Meta_Blog,https://ai.meta.com/blog/generating-wikipedia-biographies/,,Using AI to deliver more inclusive biographical content on Wikipedia,"Wikipedia, which is consistently ranked one of the top 10 most visited websites, is often the first stop for many people looking for information about historical figures and changemakers. But not everyone is equally represented on Wikipedia. Only about 20 percent of biographies on the English site are about women, according to the Wikimedia Foundation, and we imagine that percentage is even smaller for women from intersectional groups, such as women in science, women in Africa, and women in Asia.

For my PhD project as a computer science student at the Université de Lorraine, CNRS, in France, I worked with my adviser, Claire Gardent , to develop a new way to address this imbalance using artificial intelligence. Together, we built an AI system that can research and write first drafts of Wikipedia-style biographical entries. There is more work to do, but we hope this new system will one day help Wikipedia editors create many thousands of accurate, compelling biography entries for important people who are currently not on the site.

Something Went Wrong We're having trouble playing this video. Learn more

To me, the problem was personal and based on the lack of representation I saw reflected in libraries when I was in grade school. When I was in third grade, I was assigned to write an essay about a historical figure, and the only requirement was that the library had to have a book about the person. I wanted to write about Eleanor Roosevelt but had to settle for Teddy Roosevelt. And what if I wanted to write about someone who looked like me — would that even have been an option? If we imagine the same assignment today, students would undoubtedly turn to the internet, most likely Wikipedia. While Wikipedia has millions of articles in English — including an excellent one about Eleanor Roosevelt — we know there are still plenty of women whose stories and achievements aren’t reaching future generations.

While women are more likely to write biographies about other women, Wikimedia’s Community Insights 2021 Report, which covers the previous year, found that only 15 percent of Wikipedia editors identified as women. This leaves women overlooked and underrepresented, despite the enormous impact they’ve had throughout history in science, entrepreneurship, politics, and every other part of society. Canadian physicist Donna Strickland won the Nobel Prize in Physics in 2018, however, anyone looking for information about her on Wikipedia wouldn’t have been able to find it until a Wikipedia biography was finally published about her esteemed work — days after she won the biggest prize in her field of study. Various studies, including from the Wikimedia Foundation itself, have also called out the gender imbalance on the platform. Even with a lack of representation, biographies about women were still being disproportionately nominated for deletion. One study found that in 2017, 41 percent of biographies nominated for deletion were about women.

We believe open, reproducible science can provide a starting point to address this issue. Today, we are open-sourcing an end-to-end AI model that automatically creates high-quality biographical articles about important real-world public figures.

Our model searches websites for relevant information and drafts a Wikipedia-style entry about that person, complete with citations. Along with the model, we are releasing a novel data set that was created to evaluate model performance on 1,527 biographies of women from marginalized groups. This data set can be used to train models, evaluate performance, and push the model forward. We believe these AI-generated entries can be used as a starting point for people writing Wikipedia content and fact checkers to publish more biographies of underrepresented groups on the site.

There is still plenty more we can do to help bring a wider representation of notable people from all backgrounds to Wikipedia. Fundamentally, AI systems like the one we built will have to confront broader societal and technical challenges in order to fully address the problem. This starts with the web content used to create Wikipedia entries, which may be flawed or reflect cultural biases. On the technical side, the text generation system may be prone to “hallucinating” nonfactual content. Even today’s best language models struggle to create text that is coherent over many paragraphs. We’re hoping to improve these through advances in the neural architectures that power such models and through breakthroughs in the responsible development of AI. Eventually, we hope this approach will be able to help nonexperts produce accurate articles to add to the collection of information on the web, with only minimal editing needed.

How AI can complement existing efforts to address bias

While our model isn’t a panacea, it’s an important step forward to support and supplement other existing efforts that are working to address gender representation on Wikipedia. Volunteer editors Jessica Wade and Penny Richards have worked independently to write and publish thousands of biographies on Wikipedia about women who deserve the spotlight. Another great, collective effort, the Women in Red Wiki Project, mobilizes editors to create new biographies and expand existing ones about notable women past and present.

We decided to take a complementary approach. Doing research, creating a bibliography, and writing are intensive, yet there is a trove of information available on the web that can be used to tell the stories of women whose achievements, voices, and legacies have otherwise been forgotten about or marginalized.

For example, we used our model to generate a short biography of Libbie Hyman, a pioneer in the study of invertebrate zoology. The green text is pulled from the reference article we started with, the purple text is from the web evidence, and the orange text indicates hallucination — meaning the model makes up information that can’t be verified.

hyman is best known for her work on the classification of invertebrates. she was the author of a six-volume set of reference books titled the invertebrate treatise, which was published by mcgraw-hill in the united states and in germany. she also wrote a series of laboratory manuals for the teaching of zoology classes nationwide. hyman’s work has had a lasting influence on scientific thinking about a number of animal groups, and the only works that can be compared with hers are of composite authorship.

The model retrieved relevant biographical information about Hyman, including her focus on invertebrates, significant publications, and the impact of her work, which can then be used as a starting point for editors to fact check (an area where the model still has shortcomings) and expand on her life and accomplishments.

Using pretraining and a retrieval model to generate Wikipedia biographies

We start the process of generating a biography by using a retrieval-augmented generation architecture based on large-scale pretraining, which teaches the model to identify only relevant information, such as birthplace or where the person attended school, as it builds the biography.

The model first retrieves relevant information from the internet to introduce the subject. Next, the generation module creates the text, while the third step, the citation module, builds the bibliography linking back to the sources that were used. The process then repeats, with each section predicting the next, covering all the elements that make up a robust Wikipedia biography, including the subject’s early life, education, and career.

We generate section by section, using a caching mechanism similar to Transformer-XL to reference previously written sections and achieve greater document-level context. Caching is important because it allows the model to better track what it previously generated.

Automatic and human evaluations show that the model is capable of finding relevant information and using it to generate biographies, but there is still work to do. Those evaluations found that 68 percent of the generated text in the biographies we created wasn’t found in the reference text. This could mean several things. It could suggest that the model does a good job of finding and synthesizing relevant information while not acting as a plagiarism bot. However, it’s also unclear, since it is difficult to know which information is accurate and which is not. We asked evaluators to determine whether full sentences were factual, and found many cases in which sentences were only partially verifiable. These challenges are similar to those faced by the field of text generation broadly, though they are exacerbated in the case of marginalized groups, as there is very little data about them. We hope that releasing this data set will allow other researchers to study this problem.

There were several other obstacles we encountered during our research. First, the lack of training data, or biographical articles that already exist about women, was very difficult to overcome. Existing articles about women, especially those from marginalized groups, are substantially shorter than the average article about men, are less detailed, and use different language — for example, “female scientist” instead of simply “scientist.” This bias in training data caused models to internalize such bias. Beyond this, Wikipedia articles must be written based on factual evidence, often sourced from the internet. However, the bias on Wikipedia extends to bias on the internet: There are very few web-based locations that could be used as evidence.

While deeply rooted problems can’t be solved quickly, this is exactly the type of problem where technology can be used to help engineer positive change.

What’s next? Spotlighting more underrepresented people on Wikipedia

We are excited to share this work with the community to help foster discussions, experimentation, and drive progress to help create a more equitable availability of content on Wikipedia.

Our model addresses just one piece of a multifaceted problem, so there are additional areas where new techniques should be explored. When a Wikipedia editor or our AI model writes a biography, information is pulled from around the internet and cited. However, for all of the enriching knowledge the internet has provided, some sources have a bias that must be considered. For example, when women are represented, their biographies are more likely to include extra details about their personal lives. A 2015 study found the word “divorced” appears four times as often in women’s biographies as it does in biographies of men. This could be for many reasons, including the tabloid fodder that tends to follow the lives of notable women more closely than those of men. As a result, personal details end up being more likely to be mentioned in articles about women, distracting from accomplishments that should be in the spotlight and celebrated.

Technology has already shown promise in helping address various imbalances, which is proof that there is even more the community can do to help make a difference. For example, the site’s former chief executive explained how an algorithm discovered an important mistake on the site: While Wikipedia health articles are vetted by medical editors, for years some articles on critical women’s health issues, such as breastfeeding, were labeled “low importance.”

There is even more work to be done for other marginalized and intersectional groups around the world and across languages. Our evaluation and data set focuses on women, which excludes many other groups, including nonbinary people. Articles about transgender and nonbinary people tend to be longer, but much of the additional space is devoted to their personal life instead of expanding on the person’s accomplishments, according to a 2021 study that looked at social biases in Wikipedia articles. It is important to recognize that bias exists in varying forms, especially in default online sources of information.

We are passionate about sharing this as an important research area with the broader generation community. We hope that our techniques can eventually be used as a starting point for human Wikipedia writers — and ultimately lead to a more equitable availability of information online that can be accessed by students writing biographies — and beyond.

This post was updated to clarify the name of the university. It's Université de Lorraine, CNRS, in France.

Read the paper

Get the code

Learn more about this research"
Meta_Blog,https://ai.meta.com/blog/introducing-mephisto-a-new-platform-for-more-open-collaborative-data-collection/,,"Introducing Mephisto: A new platform for more open, collaborative data collection","What the research is:

To develop new AI systems, researchers must be able to experiment with different data to train their models. But many widely used public data sets (and even test sets) have labeling errors, which can make it difficult to train robust models that work as intended, especially on novel tasks. Researchers typically overcome these challenges by applying various techniques for controlling data quality; however, there is no centralized collection of examples for using these methods. As a result, researchers often risk running into the same pitfalls of data collection that prior work has already resolved.

As part of our commitment to open and reproducible science, we’re introducing Mephisto, a new open, collaborative way to collect, share, and iterate on best practices for collecting data to train AI models. Mephisto allows researchers to share novel collection methodologies in an immediately reusable and iterable form. Researchers can swap out components and easily find the exact annotations they need, lowering the barrier for creating custom tasks.

In Mephisto, we identify a number of common workflows for driving a complex annotation task from the idea stage through gathering the data. This allows for iterating on task design and quality control in a meaningful way before any data collection task. It also allows us to publish our methodologies for the wider AI community to use or improve upon.

How it works:

With Mephisto, researchers and engineers collecting data across different research domains, crowdsourcing, and server configurations can all use the same code to run their tasks. Mephisto handles this with a number of plug-and-play abstractions that do the heavy lifting to get a data collection job started. It also provides workflow guides of the process from ideation to full-fledged creation.

For example, researchers may first find an existing task that looks relevant to what they want to collect. This blueprint will serve as a starting point, and the researcher can make changes directly to the code to tweak the data displayed, the types of annotations to return, and more. At this stage, researchers could be using anything from simple HTML forms to powerful tasks invoking models in the loop, or annotations requiring live collaboration between workers. Mephisto makes it easy to test and iterate locally before piloting.

Once the task appears ready, Mephisto offers a clean workflow for launching small pilot batches and viewing the results across many workers, making it easy to identify possible issues with the task or identify workers intentionally submitting invalid data. Researchers can then use a number of existing quality control methods to improve the task quality, or they can construct their own heuristics specific to the data being collected.

Once the pilots display high-quality results, they can launch the complete task and monitor progress while it’s in flight. From here, researchers can package up their data set and publish the complete code by which others can collect something similar.

Why it matters:

Open research is a core value for Meta AI, and gathering training data is a crucial component of AI research. By publishing the code for data collection, we’re also making it reproducible, which allows others to re-create similar data sets or extend existing ones.

Our goal is to set an industry-wide standard for including collection methodologies as part of data set releases. In the longer term, we believe this work will enable everyone to share, adopt, and standardize on more responsible techniques. For example, Mephisito currently includes important privacy protection protocols, like hiding worker identification. In the future, we plan to add additional features that report worker statistics on contributions to a data set, include warnings about fair pay and protections that support responsible treatment of workers, and highlight projects that explicitly try to debias data sets.

Improving data quality is an ongoing process, and we hope Mephisto improves not only the quality of data sets in AI research and models trained on them, but also the experience of both the researchers and the annotators that construct the data set.

Learn more about Mephisto:

More information can be found on our documentation site and on GitHub."
Meta_Blog,https://ai.meta.com/blog/introducing-mephisto-a-new-platform-for-more-open-collaborative-data-collection/,,"Introducing Mephisto: A new platform for more open, collaborative data collection","What the research is:

To develop new AI systems, researchers must be able to experiment with different data to train their models. But many widely used public data sets (and even test sets) have labeling errors, which can make it difficult to train robust models that work as intended, especially on novel tasks. Researchers typically overcome these challenges by applying various techniques for controlling data quality; however, there is no centralized collection of examples for using these methods. As a result, researchers often risk running into the same pitfalls of data collection that prior work has already resolved.

As part of our commitment to open and reproducible science, we’re introducing Mephisto, a new open, collaborative way to collect, share, and iterate on best practices for collecting data to train AI models. Mephisto allows researchers to share novel collection methodologies in an immediately reusable and iterable form. Researchers can swap out components and easily find the exact annotations they need, lowering the barrier for creating custom tasks.

In Mephisto, we identify a number of common workflows for driving a complex annotation task from the idea stage through gathering the data. This allows for iterating on task design and quality control in a meaningful way before any data collection task. It also allows us to publish our methodologies for the wider AI community to use or improve upon.

How it works:

With Mephisto, researchers and engineers collecting data across different research domains, crowdsourcing, and server configurations can all use the same code to run their tasks. Mephisto handles this with a number of plug-and-play abstractions that do the heavy lifting to get a data collection job started. It also provides workflow guides of the process from ideation to full-fledged creation.

For example, researchers may first find an existing task that looks relevant to what they want to collect. This blueprint will serve as a starting point, and the researcher can make changes directly to the code to tweak the data displayed, the types of annotations to return, and more. At this stage, researchers could be using anything from simple HTML forms to powerful tasks invoking models in the loop, or annotations requiring live collaboration between workers. Mephisto makes it easy to test and iterate locally before piloting.

Once the task appears ready, Mephisto offers a clean workflow for launching small pilot batches and viewing the results across many workers, making it easy to identify possible issues with the task or identify workers intentionally submitting invalid data. Researchers can then use a number of existing quality control methods to improve the task quality, or they can construct their own heuristics specific to the data being collected.

Once the pilots display high-quality results, they can launch the complete task and monitor progress while it’s in flight. From here, researchers can package up their data set and publish the complete code by which others can collect something similar.

Why it matters:

Open research is a core value for Meta AI, and gathering training data is a crucial component of AI research. By publishing the code for data collection, we’re also making it reproducible, which allows others to re-create similar data sets or extend existing ones.

Our goal is to set an industry-wide standard for including collection methodologies as part of data set releases. In the longer term, we believe this work will enable everyone to share, adopt, and standardize on more responsible techniques. For example, Mephisito currently includes important privacy protection protocols, like hiding worker identification. In the future, we plan to add additional features that report worker statistics on contributions to a data set, include warnings about fair pay and protections that support responsible treatment of workers, and highlight projects that explicitly try to debias data sets.

Improving data quality is an ongoing process, and we hope Mephisto improves not only the quality of data sets in AI research and models trained on them, but also the experience of both the researchers and the annotators that construct the data set.

Learn more about Mephisto:

More information can be found on our documentation site and on GitHub."
Meta_Blog,https://ai.meta.com/blog/qa-with-mohamed-gamal-and-fernando-hernandez-from-raise-meta-ais-rotational-program/,,"Q&A with Mohamed Gamal and Fernando Hernandez from RAISE, Meta AI’s rotational program","This is the second of a three-part series that spotlights software engineers participating in Meta AI’s Rotational AI Science and Engineering (RAISE) program, which aims to give participants from diverse backgrounds an opportunity to start their AI journey full-time at Meta. See the first part of the series here.

Over his 10-year career, which by 2021 had spanned three continents and some of the biggest companies in the tech industry, software engineer Mohamed Gamal had grown more and more interested in AI. It powered every product he had helped build, and he began to realize AI must hold the key to better solutions for even bigger problems.

Mohamed was working as part of Meta’s creator wellbeing team when he heard about our Rotational AI Science and Engineering (RAISE) program, designed for software engineers who want to kickstart a career in the field but lack professional AI experience. Over 18 months, participants — called RAISErs — rotate through three world-class research and engineering teams at Meta AI and gain hands-on experience with a variety of cutting-edge technologies. Joining the program meant a significant career pivot, but Mohamed knew the opportunity was too good to turn down. “Don’t fear the change,” he said. “Trust your gut and take a leap of faith.”

In round two of our three-part RAISE Q&A series, we check in with Mohamed Gamal and Fernando Hernandez, RAISErs from the same cohort. Fernando, based in Seattle, had significant experience with large-scale database systems before joining the program. Mohamed and Fernando share how they learned about RAISE, how they found themselves at Meta, what they’re currently working on, and where they see themselves after the program.

Check the first post of the series here.

Want to learn more about RAISE? RSVP here for the upcoming virtual panel and info session on March 31, 2022.

Applications for the 2022 RAISE program will close on April 15, 2022. For more information and to apply, visit our program page.

Tell us a bit about your career path.

Mohamed Gamal: After I got my B.S. in computer science from Cairo University in Egypt, I relocated to the United States for a yearlong internship at Meta. From there I went to Google Shopping Ads in Zurich, where I learned the fundamentals of engineering best practices. After three years working on infrastructure and backend, I wanted to build features and products that I could see people use. I transferred to Google’s Mountain View office and stayed until mid 2020. After almost six years at Google, I moved to Meta, where I worked on the Facebook App Creators wellbeing team. It was an eye-opener to see the complexity involved in solving integrity problems, given the diversity of our users and the differences in policies from country to country.

I I joined RAISE in July 2021. The transition to AI was a really new experience.

Fernando Hernandez: I grew up and attended college in Mexico. My major was computer engineering, but late in the program I realized I’d find more opportunities writing code than designing chips. I went to Germany for my master’s in computer science, where I learned about traditional machine learning — deep learning wasn’t taught widely yet. My courses were highly theoretical, mostly math and experimentation. Even though machine learning was starting to take off, I didn’t enjoy the math-heavy aspect. I consider myself a builder, and I found it more rewarding to build systems that ship to production. AI seemed like more of a research activity.

After grad school, I moved to the United States to pursue traditional software engineering at a database startup. Then I worked on large-scale database systems at AWS — my first big company job. I learned a ton and was exposed to industry-leading software development practices.

When I was offered a position at Meta, I was also given the chance to join the RAISE program, where I could learn how the company builds large-scale AI systems. The opportunity came at the perfect time, just when I was looking for a challenge in a new field.

How did you realize you wanted to pursue a career in AI?

MG: AI has been a core component of each product I’ve worked on. I realized that experience in the field would inspire new ways of thinking, so I could solve more problems and build better products.

FH: When I was deciding whether to join the RAISE program, I talked to my prospective manager about what their team actually builds, and I finally understood that there’s more to AI than research. The idea of developing large-scale AI systems won me over.

What drew you to the RAISE program?

MG: AI is a large field with different domains. The chance to rotate through Meta’s diverse teams — natural language processing, computer vision, ads, ranking, integrity, infra — is giving me a taste of the different aspects of AI, so I can decide which domain to concentrate on.

FH: Meta is a clear leader in this field. AI is part of the foundation of the products we ship so I was excited by the prospect of working on those projects, both during and after the program.

Now that you’ve completed your first rotation, how would you describe your experience?

MG: I was new to both the team and the domain. I told myself beforehand that the program is a step in my career, so I had more tolerance for the learning curve and didn’t feel bad about it. Because the RAISErs are all new to AI, the first month of the program allowed for formal training and co-learning. That time at the beginning just for ramp-up, without the stress of managing anything else, allowed us some welcome breathing room.

“ Like any company of this size and pace, there’s always more work to be done than time to do it in, but the encouragement and advice from my manager, peers, and collaborators has exceeded my expectations.

Fernando Hernandez, Software Engineer, RAISE

I’ve had great, dedicated mentorship. My manager put me on a project that stretched me just enough. My peers really want to help, and they’ve saved me a lot of time by summarizing their experiences.

FH: It’s been amazing! I’ve learned so much. Like any company of this size and pace, there’s always more work to be done than time to do it in, but the encouragement and advice from my manager, peers, and collaborators has exceeded my expectations. That strong support network helps me align my goals with my team’s expectations, so I can achieve more with less effort.

What are you working on?

MG: I’m learning how to solve product problems with AI core and abstract tech. In the first rotation, I worked on natural language processing. I got hands-on experience with every stage in the AI development life cycle: data exploration, data collection, modeling, and production. Each step has been an education, from balancing supervised and synthetic data labels to evaluating the trained model. I’ve worked with different technologies and codebases over the course of my career, so I can ramp up fast on any code and quickly understand the logic flow of each part of the system. That has helped a lot.

In my second rotation, we’re using AI to place and style text captions on display ads, so I’m learning the fundamentals of computer vision as well.

“ I got hands-on experience with every stage in the AI development life cycle: data exploration, data collection, modeling, and production.

Mohamed Gamal, Software Engineer, RAISE

FH: My focus has been learning about the AI platform and infrastructure stack at Meta. I’ve worked on teams supporting different stages of model development: one in inference and the other in training. As it turns out, there’s a large gap between developing models for research and actually deploying them at scale. Some of the skills I’ve learned from productionizing other kinds of software are in huge demand in this kind of work, which has made my ramp-up somewhat easier.

What are your plans after RAISE?

MG: Either staying within applied AI or something in product machine learning. Applied AI aligns very well with my interest in building products.

FH: I hope to help solve hard AI problems. I’m at my best building infrastructure, but designing with other users in mind would certainly strengthen my skill set.

What advice would you give someone thinking of applying to RAISE?

MG: It’s the sweetest spot when the team’s goals match yours, so pick the team that’s doing what you want to learn. But even if a particular rotation turns out not to be the right field for you, you get a new experience.

Also, don’t fall in the trap of thinking you won’t be able to contribute to the team. Offer them your expertise — in coding practices, leadership and ownership, people skills, you name it.

FH: Ask a lot of questions about how the program relates to your experience and goals. It’s not all research — but it can be if that’s your thing. As I discovered, there are many paths for people from different backgrounds to break into AI."
Meta_Blog,https://ai.meta.com/blog/qa-with-mohamed-gamal-and-fernando-hernandez-from-raise-meta-ais-rotational-program/,,"Q&A with Mohamed Gamal and Fernando Hernandez from RAISE, Meta AI’s rotational program","This is the second of a three-part series that spotlights software engineers participating in Meta AI’s Rotational AI Science and Engineering (RAISE) program, which aims to give participants from diverse backgrounds an opportunity to start their AI journey full-time at Meta. See the first part of the series here.

Over his 10-year career, which by 2021 had spanned three continents and some of the biggest companies in the tech industry, software engineer Mohamed Gamal had grown more and more interested in AI. It powered every product he had helped build, and he began to realize AI must hold the key to better solutions for even bigger problems.

Mohamed was working as part of Meta’s creator wellbeing team when he heard about our Rotational AI Science and Engineering (RAISE) program, designed for software engineers who want to kickstart a career in the field but lack professional AI experience. Over 18 months, participants — called RAISErs — rotate through three world-class research and engineering teams at Meta AI and gain hands-on experience with a variety of cutting-edge technologies. Joining the program meant a significant career pivot, but Mohamed knew the opportunity was too good to turn down. “Don’t fear the change,” he said. “Trust your gut and take a leap of faith.”

In round two of our three-part RAISE Q&A series, we check in with Mohamed Gamal and Fernando Hernandez, RAISErs from the same cohort. Fernando, based in Seattle, had significant experience with large-scale database systems before joining the program. Mohamed and Fernando share how they learned about RAISE, how they found themselves at Meta, what they’re currently working on, and where they see themselves after the program.

Check the first post of the series here.

Want to learn more about RAISE? RSVP here for the upcoming virtual panel and info session on March 31, 2022.

Applications for the 2022 RAISE program will close on April 15, 2022. For more information and to apply, visit our program page.

Tell us a bit about your career path.

Mohamed Gamal: After I got my B.S. in computer science from Cairo University in Egypt, I relocated to the United States for a yearlong internship at Meta. From there I went to Google Shopping Ads in Zurich, where I learned the fundamentals of engineering best practices. After three years working on infrastructure and backend, I wanted to build features and products that I could see people use. I transferred to Google’s Mountain View office and stayed until mid 2020. After almost six years at Google, I moved to Meta, where I worked on the Facebook App Creators wellbeing team. It was an eye-opener to see the complexity involved in solving integrity problems, given the diversity of our users and the differences in policies from country to country.

I I joined RAISE in July 2021. The transition to AI was a really new experience.

Fernando Hernandez: I grew up and attended college in Mexico. My major was computer engineering, but late in the program I realized I’d find more opportunities writing code than designing chips. I went to Germany for my master’s in computer science, where I learned about traditional machine learning — deep learning wasn’t taught widely yet. My courses were highly theoretical, mostly math and experimentation. Even though machine learning was starting to take off, I didn’t enjoy the math-heavy aspect. I consider myself a builder, and I found it more rewarding to build systems that ship to production. AI seemed like more of a research activity.

After grad school, I moved to the United States to pursue traditional software engineering at a database startup. Then I worked on large-scale database systems at AWS — my first big company job. I learned a ton and was exposed to industry-leading software development practices.

When I was offered a position at Meta, I was also given the chance to join the RAISE program, where I could learn how the company builds large-scale AI systems. The opportunity came at the perfect time, just when I was looking for a challenge in a new field.

How did you realize you wanted to pursue a career in AI?

MG: AI has been a core component of each product I’ve worked on. I realized that experience in the field would inspire new ways of thinking, so I could solve more problems and build better products.

FH: When I was deciding whether to join the RAISE program, I talked to my prospective manager about what their team actually builds, and I finally understood that there’s more to AI than research. The idea of developing large-scale AI systems won me over.

What drew you to the RAISE program?

MG: AI is a large field with different domains. The chance to rotate through Meta’s diverse teams — natural language processing, computer vision, ads, ranking, integrity, infra — is giving me a taste of the different aspects of AI, so I can decide which domain to concentrate on.

FH: Meta is a clear leader in this field. AI is part of the foundation of the products we ship so I was excited by the prospect of working on those projects, both during and after the program.

Now that you’ve completed your first rotation, how would you describe your experience?

MG: I was new to both the team and the domain. I told myself beforehand that the program is a step in my career, so I had more tolerance for the learning curve and didn’t feel bad about it. Because the RAISErs are all new to AI, the first month of the program allowed for formal training and co-learning. That time at the beginning just for ramp-up, without the stress of managing anything else, allowed us some welcome breathing room.

“ Like any company of this size and pace, there’s always more work to be done than time to do it in, but the encouragement and advice from my manager, peers, and collaborators has exceeded my expectations.

Fernando Hernandez, Software Engineer, RAISE

I’ve had great, dedicated mentorship. My manager put me on a project that stretched me just enough. My peers really want to help, and they’ve saved me a lot of time by summarizing their experiences.

FH: It’s been amazing! I’ve learned so much. Like any company of this size and pace, there’s always more work to be done than time to do it in, but the encouragement and advice from my manager, peers, and collaborators has exceeded my expectations. That strong support network helps me align my goals with my team’s expectations, so I can achieve more with less effort.

What are you working on?

MG: I’m learning how to solve product problems with AI core and abstract tech. In the first rotation, I worked on natural language processing. I got hands-on experience with every stage in the AI development life cycle: data exploration, data collection, modeling, and production. Each step has been an education, from balancing supervised and synthetic data labels to evaluating the trained model. I’ve worked with different technologies and codebases over the course of my career, so I can ramp up fast on any code and quickly understand the logic flow of each part of the system. That has helped a lot.

In my second rotation, we’re using AI to place and style text captions on display ads, so I’m learning the fundamentals of computer vision as well.

“ I got hands-on experience with every stage in the AI development life cycle: data exploration, data collection, modeling, and production.

Mohamed Gamal, Software Engineer, RAISE

FH: My focus has been learning about the AI platform and infrastructure stack at Meta. I’ve worked on teams supporting different stages of model development: one in inference and the other in training. As it turns out, there’s a large gap between developing models for research and actually deploying them at scale. Some of the skills I’ve learned from productionizing other kinds of software are in huge demand in this kind of work, which has made my ramp-up somewhat easier.

What are your plans after RAISE?

MG: Either staying within applied AI or something in product machine learning. Applied AI aligns very well with my interest in building products.

FH: I hope to help solve hard AI problems. I’m at my best building infrastructure, but designing with other users in mind would certainly strengthen my skill set.

What advice would you give someone thinking of applying to RAISE?

MG: It’s the sweetest spot when the team’s goals match yours, so pick the team that’s doing what you want to learn. But even if a particular rotation turns out not to be the right field for you, you get a new experience.

Also, don’t fall in the trap of thinking you won’t be able to contribute to the team. Offer them your expertise — in coding practices, leadership and ownership, people skills, you name it.

FH: Ask a lot of questions about how the program relates to your experience and goals. It’s not all research — but it can be if that’s your thing. As I discovered, there are many paths for people from different backgrounds to break into AI."
Meta_Blog,https://ai.meta.com/blog/qa-with-alejandro-perez-munoz-and-kun-huang-from-raise-a-new-way-to-enter-the-field-of-ai/,,"Q&A with Alejandro Perez Munoz and Kun Huang from RAISE, a new way to enter the field of AI","This is the first of a three-part series that spotlights software engineers participating in Meta AI’s Rotational AI Science and Engineering (RAISE) program, which aims to give participants from diverse backgrounds an opportunity to start their AI journey full-time at Meta.

“I’ve always been a sucker for learning new things, and the first rotation was like drinking water from a hose. This is the most rewarding job I’ve ever had.” That’s how Alejandro Perez Munoz, a 20-year tech-industry veteran, described his first six months in Meta’s Rotational AI Science and Engineering (RAISE) program. Launched in 2021, RAISE is an 18-month journey through the AI landscape for software engineers who want to kick-start a career in the field but lack professional AI experience. The program offers a panorama of AI research at Meta while also zooming in on key avenues of exploration.

Participants — called RAISErs — rotate through three of our world-class research and engineering teams, where they get hands-on experience collaborating with some of the field’s foremost innovators on real-world projects. Depending on their preferences, RAISErs might, for example, build and test approaches to emerging challenges as part of our Responsible AI team, develop cutting-edge tools with our platform specialists, or apply advanced research to bring state-of-the-art models through to deployment in augmented reality products. RAISErs work closely with a domain expert in each area and receive personal guidance and encouragement from an assigned mentor.

RAISErs are hired as full-time software engineers. After completing the program, they can leverage their experience to identify a permanent Meta AI team to join. With hiring now underway for 2022, we sat down with Alejandro Perez Munoz and Kun Huang, RAISErs from the inaugural cohort, to learn about their experience in the program so far. Perez Munoz joined RAISE after building expertise in testing and developer experience at several major tech companies. Huang previously worked on Google Shopping’s image search monetization.

In the first of three blog posts in this year’s RAISE Q&A series, Perez Munoz and Huang explain how they learned about RAISE, how they found themselves at Meta, what they’re currently working on, and where they see themselves after the program.

Want to learn more about RAISE? RSVP here for the upcoming virtual panel and info session on March 31, 2022.

Applications for the 2022 RAISE program will close on April 15, 2022. For more information and to apply, visit our program page.

Tell us a bit about your career path.

Alejandro Perez Munoz I was born and raised in Monterrey, Mexico. I studied at Monterrey Tech for my bachelor’s in electronics engineering and my master’s in software engineering. Then Microsoft offered me an internship — that was a big break! After Microsoft, I worked full-time for Hotmail (I’ve dated myself now!), then Intuit, and then Google.

I’ve always worked on testing and developer tools, which Google is famous for. But after six years there, I was ready for new challenges. I admired Meta’s open source libraries, so I applied here. During the interview process, someone told me about the RAISE program, and I got really excited about the opportunity to grow in an entirely new direction.

Kun Huang: I majored in optical engineering at Zhejiang University, in China. I came to the United States for graduate study in electrical engineering at Caltech. I decided to pursue a career in computer science when I was taking a course there called “The Ideas Behind Our Networked World.” The professor brought his profound industry experience to the class, and I got the chance to implement what we learned with projects that had actual industry impact. That was pivotal.

My first job was at Oracle Cloud, where I worked on mobile applications for corporate clients. Then I joined Google Ads (called Adwords at the time). Google was my dream company. I worked with world-class engineers on a large-scope project, and I learned high-quality design and coding. After two years, I moved over to Google Shopping and found my passion for improving the user shopping experience. In 2021, I decided to focus on using AI to improve the commerce experience, and that brought me to RAISE.

Why did you decide to pursue a career in AI?

APM: For the past 20 years, we’ve continuously heard promises of new languages or tooling that would make coding so easy that anyone could do it. All of them failed. But the way ML is used now in speech recognition, vision, and particularly personalization, we really are all programming — without even trying. ML allows us to teach computers our preferences and needs, and that is a form of programming. AI has delivered technologies that seemed impossible a few years ago: take great speech recognition, for example. It will solve even bigger problems as more and more smart people get involved in the field. That got me really excited.

KH: I worked closely with the team that builds the AI behind Google Shopping. I am always amazed by the seemingly magical power of AI models to improve the shopping experience. I wanted to be a wizard myself, so I strove to learn and contribute.

What drew you to the RAISE program?

APM: Most companies say they hire people based on their potential, but as you grow in your career, they mostly hire you for what you already know. With RAISE, Meta really is banking on my potential to grow in an entirely new field. They’ve bet on me, so I’ll bet on them.

“ With RAISE, Meta really is banking on my potential to grow in an entirely new field. They’ve bet on me, so I’ll bet on them.

Alejandro Perez Munoz

KH: When I interviewed with Meta, I was offered two options: general software development or RAISE. What attracted me to RAISE was the opportunity to immerse myself in a variety of areas. Then I can make a wiser decision about which domain to focus on.

Now that you’ve completed your first rotation, how would you describe your experience?

APM: There was a lot to learn — about Meta, the internal tools, my first team’s product, my first project — and I had to do it fast. Ramping up in this field, which is changing so quickly, requires time and a lot of prioritization. But I wasn’t in it alone. My manager, my project mentor, and my RAISE cohort were there to help.

KH: Meta is famous for moving fast, and I’ve learned so much in a short time. I jumped into a new company culture and a new field at once, which doubled my excitement — and my confusion and uncertainty. It’s been a challenge, but the learning and growth are totally worth it.

My mentor and team members have given me the greatest support. They really set aside time and helped me stand on my feet. My manager, Tamara Berg, is the director of AI Commerce. Working with Tamara has been a gift. She’s the perfect role model, with her calm, can-do attitude. Whenever we discussed a challenge, she reminded me that even if there isn’t a solution, there is always a path forward.

What are you working on?

APM: Because I had previous experience in testing and release engineering, my first rotation was on the AI Infra team, trying to improve the performance of tests and to diagnose breakages faster. My second rotation is higher-level. I’m working with the AI platform on a tool to re-create previous runs. One of the most important things I’ve learned is the “code as data and data as code” approach to AI models.

KH: I joined the AI Commerce team for my first rotation. I got firsthand experience training a model, building the infrastructure, and working as a tech lead to support client applications in my first half. I’ve benefited a lot from my previous experience in shopping and commerce — it feels great to be able to offer the team a new perspective. I’ll keep working on an extension of my first project with the AI Commerce team in my second rotation. We’re aiming to launch the results on Instagram and Facebook.

What are your plans after RAISE?

APM: I tend to take opportunities when they show up — that’s how I got involved with RAISE. I don’t know what opportunities will come next, but there are a few areas in AI that I find really exciting, including responsible AI, speech, and vision.

KH: It’s probably rare to find the perfect team in your first rotation, but I did. I hope to join AI Commerce and continue to contribute to their projects.

What advice would you give someone thinking of applying to RAISE?

APM: The network of people you will interact with is big and constantly growing. You can take advantage of that, but you must be very organized to keep track of everything. Also, identify your project’s stakeholders quickly, and keep in constant contact with them to make sure you are spending your time on the right tasks. The RAISE program is a huge commitment, but opportunities for substantial change in your career are few and far between. If you want to delve into a new, thrilling area of software development, this is it.

KH: RAISE lets you explore different branches of AI in an industry environment, so it will help you decide where to steer your career. If you want to discover your potential and dig deeper into AI, RAISE is the way to go."
Meta_Blog,https://ai.meta.com/blog/qa-with-alejandro-perez-munoz-and-kun-huang-from-raise-a-new-way-to-enter-the-field-of-ai/,,"Q&A with Alejandro Perez Munoz and Kun Huang from RAISE, a new way to enter the field of AI","This is the first of a three-part series that spotlights software engineers participating in Meta AI’s Rotational AI Science and Engineering (RAISE) program, which aims to give participants from diverse backgrounds an opportunity to start their AI journey full-time at Meta.

“I’ve always been a sucker for learning new things, and the first rotation was like drinking water from a hose. This is the most rewarding job I’ve ever had.” That’s how Alejandro Perez Munoz, a 20-year tech-industry veteran, described his first six months in Meta’s Rotational AI Science and Engineering (RAISE) program. Launched in 2021, RAISE is an 18-month journey through the AI landscape for software engineers who want to kick-start a career in the field but lack professional AI experience. The program offers a panorama of AI research at Meta while also zooming in on key avenues of exploration.

Participants — called RAISErs — rotate through three of our world-class research and engineering teams, where they get hands-on experience collaborating with some of the field’s foremost innovators on real-world projects. Depending on their preferences, RAISErs might, for example, build and test approaches to emerging challenges as part of our Responsible AI team, develop cutting-edge tools with our platform specialists, or apply advanced research to bring state-of-the-art models through to deployment in augmented reality products. RAISErs work closely with a domain expert in each area and receive personal guidance and encouragement from an assigned mentor.

RAISErs are hired as full-time software engineers. After completing the program, they can leverage their experience to identify a permanent Meta AI team to join. With hiring now underway for 2022, we sat down with Alejandro Perez Munoz and Kun Huang, RAISErs from the inaugural cohort, to learn about their experience in the program so far. Perez Munoz joined RAISE after building expertise in testing and developer experience at several major tech companies. Huang previously worked on Google Shopping’s image search monetization.

In the first of three blog posts in this year’s RAISE Q&A series, Perez Munoz and Huang explain how they learned about RAISE, how they found themselves at Meta, what they’re currently working on, and where they see themselves after the program.

Want to learn more about RAISE? RSVP here for the upcoming virtual panel and info session on March 31, 2022.

Applications for the 2022 RAISE program will close on April 15, 2022. For more information and to apply, visit our program page.

Tell us a bit about your career path.

Alejandro Perez Munoz I was born and raised in Monterrey, Mexico. I studied at Monterrey Tech for my bachelor’s in electronics engineering and my master’s in software engineering. Then Microsoft offered me an internship — that was a big break! After Microsoft, I worked full-time for Hotmail (I’ve dated myself now!), then Intuit, and then Google.

I’ve always worked on testing and developer tools, which Google is famous for. But after six years there, I was ready for new challenges. I admired Meta’s open source libraries, so I applied here. During the interview process, someone told me about the RAISE program, and I got really excited about the opportunity to grow in an entirely new direction.

Kun Huang: I majored in optical engineering at Zhejiang University, in China. I came to the United States for graduate study in electrical engineering at Caltech. I decided to pursue a career in computer science when I was taking a course there called “The Ideas Behind Our Networked World.” The professor brought his profound industry experience to the class, and I got the chance to implement what we learned with projects that had actual industry impact. That was pivotal.

My first job was at Oracle Cloud, where I worked on mobile applications for corporate clients. Then I joined Google Ads (called Adwords at the time). Google was my dream company. I worked with world-class engineers on a large-scope project, and I learned high-quality design and coding. After two years, I moved over to Google Shopping and found my passion for improving the user shopping experience. In 2021, I decided to focus on using AI to improve the commerce experience, and that brought me to RAISE.

Why did you decide to pursue a career in AI?

APM: For the past 20 years, we’ve continuously heard promises of new languages or tooling that would make coding so easy that anyone could do it. All of them failed. But the way ML is used now in speech recognition, vision, and particularly personalization, we really are all programming — without even trying. ML allows us to teach computers our preferences and needs, and that is a form of programming. AI has delivered technologies that seemed impossible a few years ago: take great speech recognition, for example. It will solve even bigger problems as more and more smart people get involved in the field. That got me really excited.

KH: I worked closely with the team that builds the AI behind Google Shopping. I am always amazed by the seemingly magical power of AI models to improve the shopping experience. I wanted to be a wizard myself, so I strove to learn and contribute.

What drew you to the RAISE program?

APM: Most companies say they hire people based on their potential, but as you grow in your career, they mostly hire you for what you already know. With RAISE, Meta really is banking on my potential to grow in an entirely new field. They’ve bet on me, so I’ll bet on them.

“ With RAISE, Meta really is banking on my potential to grow in an entirely new field. They’ve bet on me, so I’ll bet on them.

Alejandro Perez Munoz

KH: When I interviewed with Meta, I was offered two options: general software development or RAISE. What attracted me to RAISE was the opportunity to immerse myself in a variety of areas. Then I can make a wiser decision about which domain to focus on.

Now that you’ve completed your first rotation, how would you describe your experience?

APM: There was a lot to learn — about Meta, the internal tools, my first team’s product, my first project — and I had to do it fast. Ramping up in this field, which is changing so quickly, requires time and a lot of prioritization. But I wasn’t in it alone. My manager, my project mentor, and my RAISE cohort were there to help.

KH: Meta is famous for moving fast, and I’ve learned so much in a short time. I jumped into a new company culture and a new field at once, which doubled my excitement — and my confusion and uncertainty. It’s been a challenge, but the learning and growth are totally worth it.

My mentor and team members have given me the greatest support. They really set aside time and helped me stand on my feet. My manager, Tamara Berg, is the director of AI Commerce. Working with Tamara has been a gift. She’s the perfect role model, with her calm, can-do attitude. Whenever we discussed a challenge, she reminded me that even if there isn’t a solution, there is always a path forward.

What are you working on?

APM: Because I had previous experience in testing and release engineering, my first rotation was on the AI Infra team, trying to improve the performance of tests and to diagnose breakages faster. My second rotation is higher-level. I’m working with the AI platform on a tool to re-create previous runs. One of the most important things I’ve learned is the “code as data and data as code” approach to AI models.

KH: I joined the AI Commerce team for my first rotation. I got firsthand experience training a model, building the infrastructure, and working as a tech lead to support client applications in my first half. I’ve benefited a lot from my previous experience in shopping and commerce — it feels great to be able to offer the team a new perspective. I’ll keep working on an extension of my first project with the AI Commerce team in my second rotation. We’re aiming to launch the results on Instagram and Facebook.

What are your plans after RAISE?

APM: I tend to take opportunities when they show up — that’s how I got involved with RAISE. I don’t know what opportunities will come next, but there are a few areas in AI that I find really exciting, including responsible AI, speech, and vision.

KH: It’s probably rare to find the perfect team in your first rotation, but I did. I hope to join AI Commerce and continue to contribute to their projects.

What advice would you give someone thinking of applying to RAISE?

APM: The network of people you will interact with is big and constantly growing. You can take advantage of that, but you must be very organized to keep track of everything. Also, identify your project’s stakeholders quickly, and keep in constant contact with them to make sure you are spending your time on the right tasks. The RAISE program is a huge commitment, but opportunities for substantial change in your career are few and far between. If you want to delve into a new, thrilling area of software development, this is it.

KH: RAISE lets you explore different branches of AI in an industry environment, so it will help you decide where to steer your career. If you want to discover your potential and dig deeper into AI, RAISE is the way to go."
Meta_Blog,https://ai.meta.com/blog/advances-in-multimodal-understanding-research-at-meta-ai/,,Advances in multimodal understanding research at Meta AI,"In order for AI to become a more useful tool, it has to learn how to accurately interpret content more holistically. This means working in multiple modalities (such as text, speech, and images) at once. For example, recognizing whether a meme is hateful requires considering both the image and the text content of the meme. Similarly, building the metaverse will require integrating multimodal models with augmented and virtual reality devices, so they can recognize the sound of a siren, for example, and display an alert showing which direction the sound is coming from.

Historically, analyzing such different formats of data together — text, images, speech waveforms, and video, each with a distinct architecture — has been extremely challenging for ​​machines.

Over the last couple of years, Meta AI has produced a slew of research projects, each addressing an important challenge of multimodal perception — from solving a shortage of publicly available data for training (Hateful Memes) , to a creating single algorithm for vision, speech, and text (Data2vec) , to building foundational models that work across many tasks (FLAVA) , to finding the right model parameters (Omnivore) , and many others. Taken together, they represent a clear trend: Multimodal understanding will be crucial to smarter AI systems in the near future.

Today, we’re sharing a roundup of Meta AI’s recent cutting-edge multimodal research, which we believe will collectively lead to more interactive, immersive, and smarter AI systems.

Omnivore: A single model for images, videos, and 3D data

Our new Omnivore model can operate on image, video, and 3D data using the same parameters — without degrading performance on modality-specific tasks. For example, it can recognize 3D models of pumpkins or videos of yachts even though at training time it observed images only of pumpkins and yachts, respectively. This enables radically new capabilities, such as AI systems that can search and detect content in both images and videos. Omnivore has achieved state-of-the-art results on popular recognition tasks from all three modalities, with particularly strong performance on video recognition. Read the paper here.

FLAVA: A foundational model spanning dozens of multimodal tasks

FLAVA represents a new class of “foundational model” that’s jointly trained to do over 35 tasks across domains, including image recognition, text recognition, and joint text-image tasks. For instance, the FLAVA model can single-handedly describe the content of an image, reason about its text entailment, and answer questions about the image. FLAVA also leads to impressive zero-shot text and image understanding abilities over a range of tasks, such as image classification, image retrieval, and text retrieval.

FLAVA not only improves over prior work that is typically only good at one task, but, unlike prior work, it also uses a shared trunk that was pretrained on openly available public pairs — which we hope will help further advance research. Read the paper here.

CM3: Generalizing to new multimodal tasks

CM3 is one of the most general open source multimodal models available today. By training on a large corpus of structured multimodal documents, it can generate completely new images and captions for those images. It can also be used in our setting to infill complete images or larger structured text sections, conditioned on the rest of the document. Using prompts generated in an HTML-like syntax, the exact same CM3 model can generate new images or text, caption images, and disambiguate entities in text.

Traditional approaches to pretraining have focused on mixing the architectural choices (e.g., encoder-decoder) with objective choices (e.g., masking). Our novel approach of “causally masked objective” gets the best of both worlds by introducing a hybrid of causal and masked language models. Read the paper here.

Data2vec: The first self-supervised model that achieves SOTA for speech, vision, and text

Research in self-supervised learning today is almost always focused on one particular modality. In our recent breakthrough data2vec research, we show that the exact same model architecture and self-supervised training procedure can be used to develop state-of-the-art models for recognition of images, speech, and text. The illustration below shows how data2vec is used with images, but the same procedure can also be used to train models for speech or natural languages. Data2vec demonstrates that the same self-supervised algorithm can work well in different modalities — and it often outperforms the best existing algorithms. Read more about Data2vec here.

What’s next for multimodal understanding?

Our data2vec models are currently trained separately for each of the various modalities. But our results from Omnivore, FLAVA, and CM3 suggest that, over the horizon, we may be able to train a single AI model that solves challenging tasks across all the modalities. Such a multimodal model would unlock many new opportunities. For example, it would further enhance our ability to comprehensively understand the content of social media posts in order to recognize hate speech or other harmful content. It could also help us build AR glasses that have a more comprehensive understanding of the world around them, unlocking exciting new applications in the metaverse.

As interest in multimodality has grown, we want researchers to have great tools for quickly building and experimenting with multimodal, multitask models at scale. We are open-sourcing TorchMultimodal — a library of multimodal primitives (models, fusion layers, loss functions, data sets, and utilities) and a repository of examples that bring together components and common infrastructure from across the PyTorch ecosystem. As a first open source example, researchers will be able to train and extend FLAVA using this new library. Keep a look out for more details on this soon.

As part of our continued commitment to open science, we are excited to share our most recent research results and are looking forward to building the multimodal AI future together with the wider AI community."
Meta_Blog,https://ai.meta.com/blog/advances-in-multimodal-understanding-research-at-meta-ai/,,Advances in multimodal understanding research at Meta AI,"In order for AI to become a more useful tool, it has to learn how to accurately interpret content more holistically. This means working in multiple modalities (such as text, speech, and images) at once. For example, recognizing whether a meme is hateful requires considering both the image and the text content of the meme. Similarly, building the metaverse will require integrating multimodal models with augmented and virtual reality devices, so they can recognize the sound of a siren, for example, and display an alert showing which direction the sound is coming from.

Historically, analyzing such different formats of data together — text, images, speech waveforms, and video, each with a distinct architecture — has been extremely challenging for ​​machines.

Over the last couple of years, Meta AI has produced a slew of research projects, each addressing an important challenge of multimodal perception — from solving a shortage of publicly available data for training (Hateful Memes) , to a creating single algorithm for vision, speech, and text (Data2vec) , to building foundational models that work across many tasks (FLAVA) , to finding the right model parameters (Omnivore) , and many others. Taken together, they represent a clear trend: Multimodal understanding will be crucial to smarter AI systems in the near future.

Today, we’re sharing a roundup of Meta AI’s recent cutting-edge multimodal research, which we believe will collectively lead to more interactive, immersive, and smarter AI systems.

Omnivore: A single model for images, videos, and 3D data

Our new Omnivore model can operate on image, video, and 3D data using the same parameters — without degrading performance on modality-specific tasks. For example, it can recognize 3D models of pumpkins or videos of yachts even though at training time it observed images only of pumpkins and yachts, respectively. This enables radically new capabilities, such as AI systems that can search and detect content in both images and videos. Omnivore has achieved state-of-the-art results on popular recognition tasks from all three modalities, with particularly strong performance on video recognition. Read the paper here.

FLAVA: A foundational model spanning dozens of multimodal tasks

FLAVA represents a new class of “foundational model” that’s jointly trained to do over 35 tasks across domains, including image recognition, text recognition, and joint text-image tasks. For instance, the FLAVA model can single-handedly describe the content of an image, reason about its text entailment, and answer questions about the image. FLAVA also leads to impressive zero-shot text and image understanding abilities over a range of tasks, such as image classification, image retrieval, and text retrieval.

FLAVA not only improves over prior work that is typically only good at one task, but, unlike prior work, it also uses a shared trunk that was pretrained on openly available public pairs — which we hope will help further advance research. Read the paper here.

CM3: Generalizing to new multimodal tasks

CM3 is one of the most general open source multimodal models available today. By training on a large corpus of structured multimodal documents, it can generate completely new images and captions for those images. It can also be used in our setting to infill complete images or larger structured text sections, conditioned on the rest of the document. Using prompts generated in an HTML-like syntax, the exact same CM3 model can generate new images or text, caption images, and disambiguate entities in text.

Traditional approaches to pretraining have focused on mixing the architectural choices (e.g., encoder-decoder) with objective choices (e.g., masking). Our novel approach of “causally masked objective” gets the best of both worlds by introducing a hybrid of causal and masked language models. Read the paper here.

Data2vec: The first self-supervised model that achieves SOTA for speech, vision, and text

Research in self-supervised learning today is almost always focused on one particular modality. In our recent breakthrough data2vec research, we show that the exact same model architecture and self-supervised training procedure can be used to develop state-of-the-art models for recognition of images, speech, and text. The illustration below shows how data2vec is used with images, but the same procedure can also be used to train models for speech or natural languages. Data2vec demonstrates that the same self-supervised algorithm can work well in different modalities — and it often outperforms the best existing algorithms. Read more about Data2vec here.

What’s next for multimodal understanding?

Our data2vec models are currently trained separately for each of the various modalities. But our results from Omnivore, FLAVA, and CM3 suggest that, over the horizon, we may be able to train a single AI model that solves challenging tasks across all the modalities. Such a multimodal model would unlock many new opportunities. For example, it would further enhance our ability to comprehensively understand the content of social media posts in order to recognize hate speech or other harmful content. It could also help us build AR glasses that have a more comprehensive understanding of the world around them, unlocking exciting new applications in the metaverse.

As interest in multimodality has grown, we want researchers to have great tools for quickly building and experimenting with multimodal, multitask models at scale. We are open-sourcing TorchMultimodal — a library of multimodal primitives (models, fusion layers, loss functions, data sets, and utilities) and a repository of examples that bring together components and common infrastructure from across the PyTorch ecosystem. As a first open source example, researchers will be able to train and extend FLAVA using this new library. Keep a look out for more details on this soon.

As part of our continued commitment to open science, we are excited to share our most recent research results and are looking forward to building the multimodal AI future together with the wider AI community."
Meta_Blog,https://ai.meta.com/blog/-advances-toward-ubiquitous-neural-information-retrieval/,,Advances toward ubiquitous neural information retrieval,"Information retrieval (IR), the task of searching for and accessing relevant knowledge, is arguably among the most defining challenges of the information age. People use it every day to find books in a digital library, shoes from an online retailer, songs in a streaming music service, and much more.

To excel at this task, however, an IR system must be able to parse the intricacies and subtleties of human language. If you typed in “blue shoes,” would teal suffice? If you asked about George Clinton’s influence on hip-hop, should the system give you articles about the funk musician or the 19th-century U.S. vice president?

Neural models would be the natural solution because of their ability to understand language deeply, but they are not widespread in IR due to computational constraints and scale. Just as people in knowledge-intensive jobs are commonly required to access knowledge on the web, neural networks must search larger-scale knowledge sources in an efficient way. Recently, researchers have made great strides in improving the accuracy and efficiency of pretrained language models.

Today, we’re sharing cutting-edge dense retrieval models that generalize well to tasks outside the training set while maintaining similar efficiency and scalability as traditional text matching systems. These new techniques will help pave the way for ubiquitous neural information retrieval, which will improve search as we currently use it and enable new AI agents that can responsibly harness the full knowledge on the internet, or more intelligent fact-checking systems to combat misinformation.

Challenges of neural information retrieval

Traditional IR techniques, such as TF-IDF and BM25, calculate how many words and phrases in candidate documents match those in the query. These functions assign more importance to uncommon terms, which likely have a greater influence on the meaning of the sentence. (Common words, such as “the” or “a,” are usually less informative.)

These heuristics can scale extremely well with distributed inverted indices, which store lists of the documents each word appears in, and have been remarkably effective. However, these techniques don’t comprehend the text’s actual meaning, and so they are oblivious to the significance of synonyms, paraphrases, inversions, and other nuances. They are also rigidly defined and cannot adapt to different definitions of relevance in other contexts.

Neural retrieval addresses these shortcomings. Instead of indexing words or phrases directly, a neural encoder translates the query and each document into separate vector representations, assigning pieces of text numerical positions within hundreds of dimensions. The dimensions represent the continuums along which a word, phrase, or passage might be similar to or different from another, including part of speech, case, tense, gender, sentiment, formality, perspective, and a host of other shades of meaning.

In this multidimensional vector space, similar items will cluster together, and unrelated terms will spread apart. With pretrained text encoders, a wealth of semantic and world knowledge can be incorporated into these representations. The retriever then pinpoints relevant documents by determining which passages are closest to the query in the vector space. The relevance function can be learned on a supervised data set of query-document pairs using a bi-encoder architecture (pictured below). This neural retriever architecture is referred to as dense passage retrieval (DPR), after the dense passage retrievalit is based on. With its simple, effective design and our efficient and popular open source implementation, DPR was the first work to demonstrate the effectiveness of neural retrieval in the fully supervised setting, and has been widely adopted in research and industry.

But this basic dense retriever has its own shortcomings. For instance, dense vector representations might fail to capture rare terms that don’t appear in the supervised training data, and the learned model might not generalize well to new domains. The example below shows typical failure cases of a basic dense retrieval system compared with a traditional IR system.

Improving generalization in neural retrieval

One way to create a more generalizable dense retrieval model is to train it on multiple tasks simultaneously: We created a multitask dense retrieval model. We trained a neural retriever on eight knowledge-intensive NLP tasks, ranging from fact checking to entity linking to question answering, in the first application of multitasking to neural retrieval. The resulting model shows strong zero-shot generalization: It has been pretrained to learn the similarities and differences between data points; the model can now adapt to a new task with little to no fine tuning.

Another approach is to augment the training data with synthetic examples: An AI model generates queries based on text in the knowledge source, and we train the network to retrieve the original documents when fed these queries. We have experimented with several methods of creating artificial data. For example, we assembled a pipeline of models to generate 65 million questions from Wikipedia paragraphs, creating a corpus two orders of magnitude larger than a typical fully supervised retrieval data set. We then used the questions to pretrain robust, high-performing dense retriever models

To teach a dense retriever to recognize rare terms, we generated artificial examples by querying a traditional IR system. Since a traditional retriever can be queried with arbitrary text, we randomly selected sentences from each Wikipedia passage in the corpus. Using those examples, we built a salient phrase-aware dense retriever, which combines the advantages of traditional and neural systems in a single architecture.

We can create an unlimited amount of artificial training data by cropping passages from the knowledge source and deleting, masking, or replacing random words. We used this sort of data to train models that, with no supervision, match the zero-shot generalization ability of traditional IR systems.

Improving scalability in neural retrieval

Given the default sizes of the vector representation created by standard pretrained encoders, an index of several million documents — a modest number for a modern knowledge source — can exceed the memory limits of a single server. To address this, we introduced distributed-faiss., which apportions our popular and efficient FAISS vector search library across multiple machines. We have used it with a collection of one billion documents, demonstrating the feasibility of neural retrieval at a large scale.

Ultimately, two factors determine whether a dense retrieval model can be deployed in real-world applications: the size of the index and the retrieval-time latency. Both factors are directly correlated with vector representation size.

In DrBoost, we tackle this problem by training an ensemble of dense retrievers in stages. We incrementally develop weak retrievers of very compact representation size. Component models are taught in sequence; they learn to specialize by focusing only on mistakes made by the current ensemble. The final representation is the concatenation of the output vectors from all the component models. Ensembles trained in this way can match the accuracy of standard dense retrieval models but with vectors a quarter or one-fifth the size.

DrBoost also performs superbly under fast approximate search, the setting which is most relevant for real-world, large-scale applications. DrBoost can retain accuracy while reducing bandwidth and latency requirements by four- to 64-fold. In principle, this allows for the approximate index to be served on-disk rather than in expensive and limited RAM, making it feasible to deploy dense retrieval systems more cost effectively and at a much larger scale.

Read the papers:"
Meta_Blog,https://ai.meta.com/blog/-advances-toward-ubiquitous-neural-information-retrieval/,,Advances toward ubiquitous neural information retrieval,"Information retrieval (IR), the task of searching for and accessing relevant knowledge, is arguably among the most defining challenges of the information age. People use it every day to find books in a digital library, shoes from an online retailer, songs in a streaming music service, and much more.

To excel at this task, however, an IR system must be able to parse the intricacies and subtleties of human language. If you typed in “blue shoes,” would teal suffice? If you asked about George Clinton’s influence on hip-hop, should the system give you articles about the funk musician or the 19th-century U.S. vice president?

Neural models would be the natural solution because of their ability to understand language deeply, but they are not widespread in IR due to computational constraints and scale. Just as people in knowledge-intensive jobs are commonly required to access knowledge on the web, neural networks must search larger-scale knowledge sources in an efficient way. Recently, researchers have made great strides in improving the accuracy and efficiency of pretrained language models.

Today, we’re sharing cutting-edge dense retrieval models that generalize well to tasks outside the training set while maintaining similar efficiency and scalability as traditional text matching systems. These new techniques will help pave the way for ubiquitous neural information retrieval, which will improve search as we currently use it and enable new AI agents that can responsibly harness the full knowledge on the internet, or more intelligent fact-checking systems to combat misinformation.

Challenges of neural information retrieval

Traditional IR techniques, such as TF-IDF and BM25, calculate how many words and phrases in candidate documents match those in the query. These functions assign more importance to uncommon terms, which likely have a greater influence on the meaning of the sentence. (Common words, such as “the” or “a,” are usually less informative.)

These heuristics can scale extremely well with distributed inverted indices, which store lists of the documents each word appears in, and have been remarkably effective. However, these techniques don’t comprehend the text’s actual meaning, and so they are oblivious to the significance of synonyms, paraphrases, inversions, and other nuances. They are also rigidly defined and cannot adapt to different definitions of relevance in other contexts.

Neural retrieval addresses these shortcomings. Instead of indexing words or phrases directly, a neural encoder translates the query and each document into separate vector representations, assigning pieces of text numerical positions within hundreds of dimensions. The dimensions represent the continuums along which a word, phrase, or passage might be similar to or different from another, including part of speech, case, tense, gender, sentiment, formality, perspective, and a host of other shades of meaning.

In this multidimensional vector space, similar items will cluster together, and unrelated terms will spread apart. With pretrained text encoders, a wealth of semantic and world knowledge can be incorporated into these representations. The retriever then pinpoints relevant documents by determining which passages are closest to the query in the vector space. The relevance function can be learned on a supervised data set of query-document pairs using a bi-encoder architecture (pictured below). This neural retriever architecture is referred to as dense passage retrieval (DPR), after the dense passage retrievalit is based on. With its simple, effective design and our efficient and popular open source implementation, DPR was the first work to demonstrate the effectiveness of neural retrieval in the fully supervised setting, and has been widely adopted in research and industry.

But this basic dense retriever has its own shortcomings. For instance, dense vector representations might fail to capture rare terms that don’t appear in the supervised training data, and the learned model might not generalize well to new domains. The example below shows typical failure cases of a basic dense retrieval system compared with a traditional IR system.

Improving generalization in neural retrieval

One way to create a more generalizable dense retrieval model is to train it on multiple tasks simultaneously: We created a multitask dense retrieval model. We trained a neural retriever on eight knowledge-intensive NLP tasks, ranging from fact checking to entity linking to question answering, in the first application of multitasking to neural retrieval. The resulting model shows strong zero-shot generalization: It has been pretrained to learn the similarities and differences between data points; the model can now adapt to a new task with little to no fine tuning.

Another approach is to augment the training data with synthetic examples: An AI model generates queries based on text in the knowledge source, and we train the network to retrieve the original documents when fed these queries. We have experimented with several methods of creating artificial data. For example, we assembled a pipeline of models to generate 65 million questions from Wikipedia paragraphs, creating a corpus two orders of magnitude larger than a typical fully supervised retrieval data set. We then used the questions to pretrain robust, high-performing dense retriever models

To teach a dense retriever to recognize rare terms, we generated artificial examples by querying a traditional IR system. Since a traditional retriever can be queried with arbitrary text, we randomly selected sentences from each Wikipedia passage in the corpus. Using those examples, we built a salient phrase-aware dense retriever, which combines the advantages of traditional and neural systems in a single architecture.

We can create an unlimited amount of artificial training data by cropping passages from the knowledge source and deleting, masking, or replacing random words. We used this sort of data to train models that, with no supervision, match the zero-shot generalization ability of traditional IR systems.

Improving scalability in neural retrieval

Given the default sizes of the vector representation created by standard pretrained encoders, an index of several million documents — a modest number for a modern knowledge source — can exceed the memory limits of a single server. To address this, we introduced distributed-faiss., which apportions our popular and efficient FAISS vector search library across multiple machines. We have used it with a collection of one billion documents, demonstrating the feasibility of neural retrieval at a large scale.

Ultimately, two factors determine whether a dense retrieval model can be deployed in real-world applications: the size of the index and the retrieval-time latency. Both factors are directly correlated with vector representation size.

In DrBoost, we tackle this problem by training an ensemble of dense retrievers in stages. We incrementally develop weak retrievers of very compact representation size. Component models are taught in sequence; they learn to specialize by focusing only on mistakes made by the current ensemble. The final representation is the concatenation of the output vectors from all the component models. Ensembles trained in this way can match the accuracy of standard dense retrieval models but with vectors a quarter or one-fifth the size.

DrBoost also performs superbly under fast approximate search, the setting which is most relevant for real-world, large-scale applications. DrBoost can retain accuracy while reducing bandwidth and latency requirements by four- to 64-fold. In principle, this allows for the approximate index to be served on-disk rather than in expensive and limited RAM, making it feasible to deploy dense retrieval systems more cost effectively and at a much larger scale.

Read the papers:"
Meta_Blog,https://ai.meta.com/blog/advancing-first-person-perception-with-2022-ego4d-challenge/,,Advancing first-person perception with 2022 Ego4D challenge,"Last year, Meta AI, together with a consortium of 13 universities and labs across nine countries, launched an ambitious long-term research project called Egocentric Live 4D Perception (Ego4D), which aims to train AI to understand and interact with the world like we do, from a first-person (or egocentric) perspective. As wearable devices like augmented and virtual reality headsets continue to improve and begin to power new experiences in the metaverse, AI will need to learn from entirely different data than what’s shown in typical videos filmed from handheld cameras. It must understand the world through human eyes in the context of continuous visual, motion, audio, and behavioral cues.

Ego4D features the world’s largest data set of more than 3,000 hours of egocentric videos, combined with five new research benchmarks, and is designed to push the frontier of first-person perception.

Today, together with the consortium, we’re announcing the public release of the data set and annotations, and the launch of a new large-scale public competition, which will run from March through October this year. We’ll declare the first round of winners at the Joint International 1st Ego4D and 10th EPIC Workshop at CVPR 2022. This set of large-scale challenges entails 16 egocentric tasks, including querying episodic memories, analyzing hand-object manipulations, audio-visual conversation, social interactions, and forecasting the camera wearer’s future activity. The Ego4D challenge begins today with a subset of six competition tracks – the remaining 10 tracks will be launched in the weeks ahead.

As a complement to these efforts, Meta AI is also releasing EgoObjects, the largest object-centric data set containing more than 110 hours of egocentric videos and focusing on object detection tasks. It includes 40,000 videos, and 1.2M object annotations from up to 600 object categories. This is part of the CLVision workshop at CVPR 2022, which supports the research development of continual learning of object detection at both category- and instance level.

We hope these resources and competitions will inspire AI researchers to improve our baselines, push the state of the art, and join our efforts in advancing egocentric perception. By collaborating and openly sharing our work, we hope to accelerate progress in this important research area.

Ego4D: Pushing the frontier of first-person perception

Ego4D data set

Ego4D data set Ego4D challenge participants will use Ego4D’s unique annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Ego4D footage is unscripted and “in the wild.” To build the data set, each university team was responsible for complying with its own institutional research policy. The process involved developing a study protocol compliant with standards from institutional research ethics committees and/or review boards, including a process to obtain informed consent and/or video release from participants.

It showcases daily activities encompassing life around the world. Portions of this data are accompanied by audio, 3D environment meshes, eye gaze, stereo, and synchronized video from multiple head-mounted cameras. Ego4D is richly annotated, offering temporal, spatial, and semantic labels, natural language queries, and speech transcription. It contains annotations built on textual summaries of the entire data set, with annotators describing actions appearing in the data set roughly every five seconds, resulting in more than 3.85M unique sentences.

Ego4D Challenges

Something Went Wrong We're having trouble playing this video. Learn more

There will be 16 independent competition tracks, all leveraging the Ego4D data sets and benchmark suite. For each track, challenge entrants will use Ego4D to train models to perform our benchmark tasks and compete to achieve the highest accuracy scores. After a period of validation, the highest scoring teams will receive cash prizes.

Episodic memory challenge: What did I do? Developing superhuman memory, indexing egocentric videos, and answering queries about past objects, activities, or locations.

Challenge participants will build systems responsive to three query types:

Visual queries with 2D and 3D localization: Given an egocentric video clip and an image crop depicting the query object, the goal is to return the last time the object was seen in the input video, in terms of the tracked bounding box (2D + temporal localization) and the 3D displacement vector from the camera to the object in the environment.

Natural language queries: Given a video clip and a query expressed in natural language, the goal is to localize the temporal window within all the video history where the answer to the question is evident. So, you could ask an AI system: “What did I pick up before leaving the party?”

Moments queries Audio-visual diarization: Who said what when? (e.g., “What was the main topic during class?”). Given an egocentric video and an activity name (e.g., a “moment”), the goal is to localize all instances of that activity in the past video, like “When are all the times I drank coffee today?”

Hands and objects challenge: What am I doing now? Understanding the present by detecting and classifying moments when camera wearers change the state of an object they are manipulating.

Challenge participants will address two tasks:

Temporal localization and classification: Given an egocentric video clip, the goal is to localize temporally the key frames that indicate an object state change (e.g., chopping a tomato or assembling pieces of wood), and identify what kind of state change it is.

State change object detection: Given an egocentric video clip, the goal is to identify the objects whose states are changing and outline them with bounding boxes.

Audio-visual diarization and social challenges: Which person said what, when? Which person is talking to me? Which person is looking at me? Understanding spoken language and social interactions from the egocentric perspective using the simultaneous capture of video and audio.The audio-visual diarization challenge enables the understanding of the discourse of conversations, while the audio-visual social challenge enables embodied approaches to understand social behaviors and settings. We begin this process with efforts to identify communicative acts and their content, as well as attention directed toward the camera wearer.

Challenge participants will address these problems through the following tasks:

Audio-visual localization: Given an egocentric video clip, the goal is to identify which person spoke and when they spoke.

Speech transcription: Given an egocentric video clip, the goal is to automatically transcribe the speech of each person.

Talking to me: Given an egocentric video clip, the goal is to identify whether someone in the scene is talking to the camera wearer.

Looking at me: Given an egocentric video clip, the goal is to identify whether someone in the scene is looking at the camera wearer.

As organizers of the audio-visual diarization and social challenges, we’re aware of the privacy implications of their long-term research agendas. We feel that Ego4D’s effort to fundamentally advance audio and social understanding are necessary precursors to developing future safeguards.

To support these efforts, we are integrating a privacy-oriented challenge into our 2022 audio-visual diarization and social challenges. We’re calling on the global research community to review audio-visual recordings of conversations and interactions along with their annotations and propose novel methods to use these resources for privacy-preserving research. Challenge participants can submit a research prospectus, and those considered most practical or impactful will receive prize funding to support their work. Areas can include but are not limited to masking personal identifiable information (PII) in audio-visual data or technologies for understanding your audience (i.e., people that you are conversing with versus bystanders). We hope crowdsourcing research concepts and ideas will generate critical momentum as Ego4D launches its own efforts in these areas. The privacy-oriented challenge will formally launch later this month.

Forecasting challenge: What will I do next? Anticipating the camera wearers’ next movements and interactions. This challenge will be important for building more useful applications for AI-powered assistants, like teaching people how to play the drums or warning that someone has already added salt to a recipe as you reach for the salt shaker. Our challenge focuses on:

Locomotion forecasting: Given a video frame and the past trajectory, the goal is to predict the future ego positions of the camera wearer (in the form of a 3D trajectory).

Hand forecasting: Given a video clip, the goal is to predict the next active objects, the next action, and the time to contact.

Long-term activity prediction: Given a video clip, the goal is to predict what sequence of activities will happen in the future? For example, after kneading dough, what will the baker do next?

Please review the submission guidelines before entering and note that participants must submit their submissions to EvalAI. The winning team from each track will be invited to nominate a team member to share their work at a CVPR 2022 event, where we will also share the challenge leaderboards.

Partners in the 2022 Ego4D Challenge:

Carnegie Mellon University (Pittsburgh, Kigali)

Georgia Institute of Technology

Indiana University Bloomington

Massachusetts Institute of Technology

University of Minnesota

University of Pennsylvania

University of Catania

University of Bristol

University of Tokyo

International Institute of Information Technology, Hyderabad

King Abdullah University of Science and Technology

National University of Singapore

University of Los Andes

University of California, Berkeley

Meta AI Research

EvalAI

Learn more about the Ego4D data set and challenges

EgoObjects: Large-scale egocentric dataset of objects

To further push the limits of egocentric perception, we’ve also created the first large-scale data set focused on object detectors for egocentric video — featuring diverse viewpoints as well as different scale, background, and lighting conditions. While most existing comparable data sets are either not object-centric or not large-scale, our initial release will cover over 40,000 videos (110+ hours) across 200 main object categories in over 25 countries. Besides the main objects, the videos also capture various surrounding objects in the background. The total number of object categories can go up to 600.

Data collection is conducted with a wide range of egocentric recording devices (Rayban Stories, Snap Spectacles, Aria glasses, and Mobile) in realistic household scenarios. EgoObjects also features an array of rich data annotations, like bounding boxes, category labels, instance IDs, as well as rich meta information, like background description, lighting condition and location.

The EgoObjects dataset will be used for continual learning challenge at the CLVision workshop at CVPR 2022 with three tracks including

Continual instance-level object classification. This task is to handle a stream of training experiences containing images of common household or workplace objects. The solution will be able to access the ground-truth label of each training image in order to incrementally train its internal knowledge model (fully supervised). Images will depict a single object and the expected prediction is a classification label. The solution must return predictions at the instance level. That is, solutions will need to disentangle between objects belonging to common categories.

Continual category-level object detection. In this task, incremental experiences will carry short videos of common household or workplace objects. Objects will be depicted in common household and workplace environments, with each image depicting more than one object. The goal is to predict the bounding box and label of the depicted objects. This can be a very good starting point when first approaching continual detection tasks.

Continual instance-level object detection. In this task, incremental experiences will carry short videos of common household or workplace objects. Differently from its category-level counterpart, the goal is to predict the object labels at the instance level. Each video will feature a single “reference” object (possibly surrounded by other unrelated objects). The goal is to predict the position and instance label of that reference object. This task is harder than the category-level counterpart.

These tracks are designed to advance object understanding in the egocentric perspective, a fundamental building block for AR applications. Some examples include but are not limited to object search for both visible or hidden items, object-anchored contextual reminder, automatic grocery list building and object-anchored immersive virtual content.

Besides the continual learning of detecting 2D objects, EgoObjects also supports research on 3D object detection and pose estimation given that the data set also has 3D annotations. In this task, the solution has access to 2D RGB images and oriented 3D bounding boxes of objects as the ground truth. The goal is to predict the 3D bounding boxes with correct pose from a single 2D image."
Meta_Blog,https://ai.meta.com/blog/advancing-first-person-perception-with-2022-ego4d-challenge/,,Advancing first-person perception with 2022 Ego4D challenge,"Last year, Meta AI, together with a consortium of 13 universities and labs across nine countries, launched an ambitious long-term research project called Egocentric Live 4D Perception (Ego4D), which aims to train AI to understand and interact with the world like we do, from a first-person (or egocentric) perspective. As wearable devices like augmented and virtual reality headsets continue to improve and begin to power new experiences in the metaverse, AI will need to learn from entirely different data than what’s shown in typical videos filmed from handheld cameras. It must understand the world through human eyes in the context of continuous visual, motion, audio, and behavioral cues.

Ego4D features the world’s largest data set of more than 3,000 hours of egocentric videos, combined with five new research benchmarks, and is designed to push the frontier of first-person perception.

Today, together with the consortium, we’re announcing the public release of the data set and annotations, and the launch of a new large-scale public competition, which will run from March through October this year. We’ll declare the first round of winners at the Joint International 1st Ego4D and 10th EPIC Workshop at CVPR 2022. This set of large-scale challenges entails 16 egocentric tasks, including querying episodic memories, analyzing hand-object manipulations, audio-visual conversation, social interactions, and forecasting the camera wearer’s future activity. The Ego4D challenge begins today with a subset of six competition tracks – the remaining 10 tracks will be launched in the weeks ahead.

As a complement to these efforts, Meta AI is also releasing EgoObjects, the largest object-centric data set containing more than 110 hours of egocentric videos and focusing on object detection tasks. It includes 40,000 videos, and 1.2M object annotations from up to 600 object categories. This is part of the CLVision workshop at CVPR 2022, which supports the research development of continual learning of object detection at both category- and instance level.

We hope these resources and competitions will inspire AI researchers to improve our baselines, push the state of the art, and join our efforts in advancing egocentric perception. By collaborating and openly sharing our work, we hope to accelerate progress in this important research area.

Ego4D: Pushing the frontier of first-person perception

Ego4D data set

Ego4D data set Ego4D challenge participants will use Ego4D’s unique annotated data set of more than 3,670 hours of video data, capturing the daily-life scenarios of more than 900 unique individuals from nine different countries around the world. Ego4D footage is unscripted and “in the wild.” To build the data set, each university team was responsible for complying with its own institutional research policy. The process involved developing a study protocol compliant with standards from institutional research ethics committees and/or review boards, including a process to obtain informed consent and/or video release from participants.

It showcases daily activities encompassing life around the world. Portions of this data are accompanied by audio, 3D environment meshes, eye gaze, stereo, and synchronized video from multiple head-mounted cameras. Ego4D is richly annotated, offering temporal, spatial, and semantic labels, natural language queries, and speech transcription. It contains annotations built on textual summaries of the entire data set, with annotators describing actions appearing in the data set roughly every five seconds, resulting in more than 3.85M unique sentences.

Ego4D Challenges

Something Went Wrong We're having trouble playing this video. Learn more

There will be 16 independent competition tracks, all leveraging the Ego4D data sets and benchmark suite. For each track, challenge entrants will use Ego4D to train models to perform our benchmark tasks and compete to achieve the highest accuracy scores. After a period of validation, the highest scoring teams will receive cash prizes.

Episodic memory challenge: What did I do? Developing superhuman memory, indexing egocentric videos, and answering queries about past objects, activities, or locations.

Challenge participants will build systems responsive to three query types:

Visual queries with 2D and 3D localization: Given an egocentric video clip and an image crop depicting the query object, the goal is to return the last time the object was seen in the input video, in terms of the tracked bounding box (2D + temporal localization) and the 3D displacement vector from the camera to the object in the environment.

Natural language queries: Given a video clip and a query expressed in natural language, the goal is to localize the temporal window within all the video history where the answer to the question is evident. So, you could ask an AI system: “What did I pick up before leaving the party?”

Moments queries Audio-visual diarization: Who said what when? (e.g., “What was the main topic during class?”). Given an egocentric video and an activity name (e.g., a “moment”), the goal is to localize all instances of that activity in the past video, like “When are all the times I drank coffee today?”

Hands and objects challenge: What am I doing now? Understanding the present by detecting and classifying moments when camera wearers change the state of an object they are manipulating.

Challenge participants will address two tasks:

Temporal localization and classification: Given an egocentric video clip, the goal is to localize temporally the key frames that indicate an object state change (e.g., chopping a tomato or assembling pieces of wood), and identify what kind of state change it is.

State change object detection: Given an egocentric video clip, the goal is to identify the objects whose states are changing and outline them with bounding boxes.

Audio-visual diarization and social challenges: Which person said what, when? Which person is talking to me? Which person is looking at me? Understanding spoken language and social interactions from the egocentric perspective using the simultaneous capture of video and audio.The audio-visual diarization challenge enables the understanding of the discourse of conversations, while the audio-visual social challenge enables embodied approaches to understand social behaviors and settings. We begin this process with efforts to identify communicative acts and their content, as well as attention directed toward the camera wearer.

Challenge participants will address these problems through the following tasks:

Audio-visual localization: Given an egocentric video clip, the goal is to identify which person spoke and when they spoke.

Speech transcription: Given an egocentric video clip, the goal is to automatically transcribe the speech of each person.

Talking to me: Given an egocentric video clip, the goal is to identify whether someone in the scene is talking to the camera wearer.

Looking at me: Given an egocentric video clip, the goal is to identify whether someone in the scene is looking at the camera wearer.

As organizers of the audio-visual diarization and social challenges, we’re aware of the privacy implications of their long-term research agendas. We feel that Ego4D’s effort to fundamentally advance audio and social understanding are necessary precursors to developing future safeguards.

To support these efforts, we are integrating a privacy-oriented challenge into our 2022 audio-visual diarization and social challenges. We’re calling on the global research community to review audio-visual recordings of conversations and interactions along with their annotations and propose novel methods to use these resources for privacy-preserving research. Challenge participants can submit a research prospectus, and those considered most practical or impactful will receive prize funding to support their work. Areas can include but are not limited to masking personal identifiable information (PII) in audio-visual data or technologies for understanding your audience (i.e., people that you are conversing with versus bystanders). We hope crowdsourcing research concepts and ideas will generate critical momentum as Ego4D launches its own efforts in these areas. The privacy-oriented challenge will formally launch later this month.

Forecasting challenge: What will I do next? Anticipating the camera wearers’ next movements and interactions. This challenge will be important for building more useful applications for AI-powered assistants, like teaching people how to play the drums or warning that someone has already added salt to a recipe as you reach for the salt shaker. Our challenge focuses on:

Locomotion forecasting: Given a video frame and the past trajectory, the goal is to predict the future ego positions of the camera wearer (in the form of a 3D trajectory).

Hand forecasting: Given a video clip, the goal is to predict the next active objects, the next action, and the time to contact.

Long-term activity prediction: Given a video clip, the goal is to predict what sequence of activities will happen in the future? For example, after kneading dough, what will the baker do next?

Please review the submission guidelines before entering and note that participants must submit their submissions to EvalAI. The winning team from each track will be invited to nominate a team member to share their work at a CVPR 2022 event, where we will also share the challenge leaderboards.

Partners in the 2022 Ego4D Challenge:

Carnegie Mellon University (Pittsburgh, Kigali)

Georgia Institute of Technology

Indiana University Bloomington

Massachusetts Institute of Technology

University of Minnesota

University of Pennsylvania

University of Catania

University of Bristol

University of Tokyo

International Institute of Information Technology, Hyderabad

King Abdullah University of Science and Technology

National University of Singapore

University of Los Andes

University of California, Berkeley

Meta AI Research

EvalAI

Learn more about the Ego4D data set and challenges

EgoObjects: Large-scale egocentric dataset of objects

To further push the limits of egocentric perception, we’ve also created the first large-scale data set focused on object detectors for egocentric video — featuring diverse viewpoints as well as different scale, background, and lighting conditions. While most existing comparable data sets are either not object-centric or not large-scale, our initial release will cover over 40,000 videos (110+ hours) across 200 main object categories in over 25 countries. Besides the main objects, the videos also capture various surrounding objects in the background. The total number of object categories can go up to 600.

Data collection is conducted with a wide range of egocentric recording devices (Rayban Stories, Snap Spectacles, Aria glasses, and Mobile) in realistic household scenarios. EgoObjects also features an array of rich data annotations, like bounding boxes, category labels, instance IDs, as well as rich meta information, like background description, lighting condition and location.

The EgoObjects dataset will be used for continual learning challenge at the CLVision workshop at CVPR 2022 with three tracks including

Continual instance-level object classification. This task is to handle a stream of training experiences containing images of common household or workplace objects. The solution will be able to access the ground-truth label of each training image in order to incrementally train its internal knowledge model (fully supervised). Images will depict a single object and the expected prediction is a classification label. The solution must return predictions at the instance level. That is, solutions will need to disentangle between objects belonging to common categories.

Continual category-level object detection. In this task, incremental experiences will carry short videos of common household or workplace objects. Objects will be depicted in common household and workplace environments, with each image depicting more than one object. The goal is to predict the bounding box and label of the depicted objects. This can be a very good starting point when first approaching continual detection tasks.

Continual instance-level object detection. In this task, incremental experiences will carry short videos of common household or workplace objects. Differently from its category-level counterpart, the goal is to predict the object labels at the instance level. Each video will feature a single “reference” object (possibly surrounded by other unrelated objects). The goal is to predict the position and instance label of that reference object. This task is harder than the category-level counterpart.

These tracks are designed to advance object understanding in the egocentric perspective, a fundamental building block for AR applications. Some examples include but are not limited to object search for both visible or hidden items, object-anchored contextual reminder, automatic grocery list building and object-anchored immersive virtual content.

Besides the continual learning of detecting 2D objects, EgoObjects also supports research on 3D object detection and pose estimation given that the data set also has 3D annotations. In this task, the solution has access to 2D RGB images and oriented 3D bounding boxes of objects as the ground truth. The goal is to predict the 3D bounding boxes with correct pose from a single 2D image."
Meta_Blog,https://ai.meta.com/blog/building-systems-to-reason-securely-over-private-data/,,Building systems to securely reason over private data,"What the research is:

People today rely on AI systems such as assistants and chatbots to help with countless tasks, from answering questions about the weather to scheduling a meeting at work. For systems to execute these tasks, users must provide them with relevant information — such as one’s location or work calendar. In some cases, however, people would prefer to keep information private, which means not uploading it to cloud-based AI systems or sharing it with others. Today’s reasoning systems are not optimized to do this, however. In particular, today’s retrieval-based systems — systems that reason by retrieving information from knowledge bases — aren’t designed to take into consideration the privacy of their underlying data.

To address this limitation and spur research in this and related areas, Meta AI is releasing ConcurrentQA, the first public data set for studying information retrieval and question answering (QA) with data from multiple privacy scopes. Alongside the data set and problem exploration, we have developed a new methodology as a starting point for thinking about privacy in retrieval-based settings called Public-Private Autoregressive Information Retrieval (PAIR).

ConcurrentQA contains questions that might require both public and private information to answer, such as “With my GPA and SAT score, which universities should I apply to in the United States?” PAIR provides a guide for designing systems that can answer these questions without needing to tell a QA system one’s grades or SAT scores — it provides a way to reason about building systems that retrieve information from public and private sources without compromising the integrity of the private information.

How it works:

QA and reasoning over data with multiple privacy scopes remains an underexplored area in natural language processing. This is in large part due to a lack of public domain data sets and benchmarks for studying and comparing approaches. ConcurrentQA contains about 18,000 question-answer pairs. Each pair has a corresponding pair of passages originating either from Wikipedia or from the publicly available and widely used Enron Email Dataset. Each passage contains information that must be successively understood in a logical step to answer its corresponding question — these steps are called reasoning hops.

For example, to answer the question above about which universities to apply to, an AI system must make one hop to retrieve the asker’s GPA and SAT score and another hop to retrieve information about admissions practices at universities in the United States. This type of multi-hop reasoning has been well studied when the information needing to be retrieved can be found in a single privacy scope, such as a public domain knowledge base (e.g., Wikipedia), but not when some of that information is private, as is the case for a person’s grades and test scores.

As a starting point for studying privacy-aware reasoning with ConcurrentQA, we use the PAIR framework to define the design space for possible multi-hop QA models. AI systems based on PAIR should follow the following properties:

Data in a theoretical system is stored in at least two separate enclaves. Public and private information are stored separately in that system, so that private actors or retrievers can access public data, but public actors cannot access private data.

Systems should be designed so that public data retrievals precede private data retrievals. Systems designed with PAIR should make separate retrievals in sequence, so data retrieved from private knowledge bases doesn’t leak into public systems retrieving from public knowledge bases, which may be communal.

This example shows PAIR’s method for multi-hop information retrieval.

PAIR itself provides a starting point that researchers can use to design their own systems that can reason over public and private data with good privacy controls. We hope that researchers will use ConcurrentQA to create new reasoning systems that both extend PAIR and explore other approaches.

Why it matters:

Technology is tightly woven into the daily lives of people around the world — a trend that will only accelerate as we build for the metaverse. It is important that AI systems be able to perform useful tasks for people while also keeping personal information private. Privacy and security are a core part of Meta AI’s approach to responsible AI. As part of this effort, we’ve built and shared tools such as CrypTen, which allows AI researchers to more easily experiment with secure computing techniques, and Opacus, a library for training models with differential privacy.

The AI research community is only beginning to develop effective ways to perform question-answering tasks in a privacy-preserving way. The release of ConcurrentQA and PAIR as a starting point aims to accelerate this research; there is much more work to do for us and for our colleagues outside of Meta. We hope that new data sets like ConcurrentQA will help AI researchers and practitioners build systems that can help better protect people’s privacy. Absent further research in this area, it will be more difficult for the industry to develop AI systems that can retrieve information while also preserving people’s privacy.

We are aware of the biases and limitations of the Enron Email Dataset. ConcurrentQA and PAIR can be improved by using other, more representative data sets and finding other ways to mitigate bias. We further note measures taken by experts in government and academia to redact personal information from the Enron Email Dataset and to address privacy concerns. This was done prior to the data set’s release in its current form for use by AI researchers.

There are many important challenges ahead, but improving performance on the ConcurrentQA benchmark with PAIR or other new privacy frameworks will be critical. We hope the release of our data set will spur interest and innovation in how we model and reason about private information, and we hope that the PAIR framework and our straightforward baselines set a high bar for future work.

The work discussed in this blog post was completed by Simran Arora, Patrick Lewis, Angela Fan, Jacob Kahn, and Chris Re.

Read the paper

Get it on GitHub"
Meta_Blog,https://ai.meta.com/blog/building-systems-to-reason-securely-over-private-data/,,Building systems to securely reason over private data,"What the research is:

People today rely on AI systems such as assistants and chatbots to help with countless tasks, from answering questions about the weather to scheduling a meeting at work. For systems to execute these tasks, users must provide them with relevant information — such as one’s location or work calendar. In some cases, however, people would prefer to keep information private, which means not uploading it to cloud-based AI systems or sharing it with others. Today’s reasoning systems are not optimized to do this, however. In particular, today’s retrieval-based systems — systems that reason by retrieving information from knowledge bases — aren’t designed to take into consideration the privacy of their underlying data.

To address this limitation and spur research in this and related areas, Meta AI is releasing ConcurrentQA, the first public data set for studying information retrieval and question answering (QA) with data from multiple privacy scopes. Alongside the data set and problem exploration, we have developed a new methodology as a starting point for thinking about privacy in retrieval-based settings called Public-Private Autoregressive Information Retrieval (PAIR).

ConcurrentQA contains questions that might require both public and private information to answer, such as “With my GPA and SAT score, which universities should I apply to in the United States?” PAIR provides a guide for designing systems that can answer these questions without needing to tell a QA system one’s grades or SAT scores — it provides a way to reason about building systems that retrieve information from public and private sources without compromising the integrity of the private information.

How it works:

QA and reasoning over data with multiple privacy scopes remains an underexplored area in natural language processing. This is in large part due to a lack of public domain data sets and benchmarks for studying and comparing approaches. ConcurrentQA contains about 18,000 question-answer pairs. Each pair has a corresponding pair of passages originating either from Wikipedia or from the publicly available and widely used Enron Email Dataset. Each passage contains information that must be successively understood in a logical step to answer its corresponding question — these steps are called reasoning hops.

For example, to answer the question above about which universities to apply to, an AI system must make one hop to retrieve the asker’s GPA and SAT score and another hop to retrieve information about admissions practices at universities in the United States. This type of multi-hop reasoning has been well studied when the information needing to be retrieved can be found in a single privacy scope, such as a public domain knowledge base (e.g., Wikipedia), but not when some of that information is private, as is the case for a person’s grades and test scores.

As a starting point for studying privacy-aware reasoning with ConcurrentQA, we use the PAIR framework to define the design space for possible multi-hop QA models. AI systems based on PAIR should follow the following properties:

Data in a theoretical system is stored in at least two separate enclaves. Public and private information are stored separately in that system, so that private actors or retrievers can access public data, but public actors cannot access private data.

Systems should be designed so that public data retrievals precede private data retrievals. Systems designed with PAIR should make separate retrievals in sequence, so data retrieved from private knowledge bases doesn’t leak into public systems retrieving from public knowledge bases, which may be communal.

This example shows PAIR’s method for multi-hop information retrieval.

PAIR itself provides a starting point that researchers can use to design their own systems that can reason over public and private data with good privacy controls. We hope that researchers will use ConcurrentQA to create new reasoning systems that both extend PAIR and explore other approaches.

Why it matters:

Technology is tightly woven into the daily lives of people around the world — a trend that will only accelerate as we build for the metaverse. It is important that AI systems be able to perform useful tasks for people while also keeping personal information private. Privacy and security are a core part of Meta AI’s approach to responsible AI. As part of this effort, we’ve built and shared tools such as CrypTen, which allows AI researchers to more easily experiment with secure computing techniques, and Opacus, a library for training models with differential privacy.

The AI research community is only beginning to develop effective ways to perform question-answering tasks in a privacy-preserving way. The release of ConcurrentQA and PAIR as a starting point aims to accelerate this research; there is much more work to do for us and for our colleagues outside of Meta. We hope that new data sets like ConcurrentQA will help AI researchers and practitioners build systems that can help better protect people’s privacy. Absent further research in this area, it will be more difficult for the industry to develop AI systems that can retrieve information while also preserving people’s privacy.

We are aware of the biases and limitations of the Enron Email Dataset. ConcurrentQA and PAIR can be improved by using other, more representative data sets and finding other ways to mitigate bias. We further note measures taken by experts in government and academia to redact personal information from the Enron Email Dataset and to address privacy concerns. This was done prior to the data set’s release in its current form for use by AI researchers.

There are many important challenges ahead, but improving performance on the ConcurrentQA benchmark with PAIR or other new privacy frameworks will be critical. We hope the release of our data set will spur interest and innovation in how we model and reason about private information, and we hope that the PAIR framework and our straightforward baselines set a high bar for future work.

The work discussed in this blog post was completed by Simran Arora, Patrick Lewis, Angela Fan, Jacob Kahn, and Chris Re.

Read the paper

Get it on GitHub"
Meta_Blog,https://ai.meta.com/blog/pseudo-labeling-speech-recognition-using-multilingual-unlabeled-data/,,Pseudo labeling: Speech recognition using multilingual unlabeled data,"What the research is:

There’s a massive amount of publicly shared audio data out in the world, from audiobooks to archived radio programs. Using Meta AI’s continuously developing automatic speech recognition (ASR) systems, we’re able to build ASR models to make the most of that information – including raw, unlabeled audio. Traditionally, to build a speech recognition system, you need both an audio sample and the corresponding transcript, which are then inputted to train the models. Transcribing large quantities of audio is incredibly labor intensive, however, and is not scalable to hundreds or thousands of languages and dialects. To help address this gap, Meta AI is developing a new high-performance open-source multilingual ASR model that uses pseudo labeling, a popular machine learning technique that leverages unlabeled data. Our latest work in pseudo labeling makes it possible to build an effective ASR model using unlabeled data across 60 languages.

Pseudo labeling complements Meta AI’s recent advances with wav2vec 2.0, HuBERT, textless NLP and data2vec in learning self-supervised speech representations. Pseudo labeling aims to solve the same overall challenge: how to make the best use of massive amounts of audio data that has not been transcribed by humans. Most importantly, this work extends pseudo labeling to a multilingual setting, where we use a small number of labeled examples to predict labels for an entire data set spanning multiple languages. This enables the development of high-performance open-source multilingual ASR models for many more languages and dialects than previously possible.

For the last few years, pseudo labeling has helped create better speech recognition in monolingual systems but not in multilingual ones. Our most recent work in pseudo labeling builds on advances we have made over the years in iterative pseudo-labeling, a semi-supervised algorithm that efficiently performs pseudo-labeling on unlabeled data and massively multilingual ASR, which trains a single acoustic model across more than 50 languages.

We are also releasing a large-scale multilingual open-source ASR model available to the community, trained only with open-source data.

How it works:

Pseudo labeling works by using a model trained on labeled data to predict the labels for unlabeled data, and then using those “pseudo labels” to train the model in a supervised way on the unlabeled data. It enables accurate ASR models to be built using far less transcript data. This means that when working with many thousands of hours of unlabeled audio data, we would need to get a transcript for a small subset of the data and then use pseudo labeling to predict the remaining unlabeled audio hours, allowing us to use all the hours of audio data to train the recognition learning system.

The greatest challenge here comes in doing pseudo labeling effectively across many languages, because each one will have its own character set that could interfere with others. Making sure multilingual ASR systems perform well across many languages was the hard part. Multilingual systems also require a lot of engineering innovation to efficiently train these large models. This is where the experience within our group of Meta AI researchers working on pseudo labeling was helpful, as it enabled us to scale to a multilingual model effectively.

Leveraging pseudo labeling to train multilingual ASR systems in particular has many advantages. Building speech recognition systems for 60 languages would normally mean training 60 different ASR models. But multilingual pseudo labeling takes into account commonalities between languages (in a similar manner as our M2M-100 translation model). Rather than developing 60 different models, you train a compact model that can perform better on all 60 of those languages by using cross-lingual learning. Having a multilingual system helps share the knowledge between languages, which facilitates cross-learning for similar languages like Spanish and Italian. We transcribe one to two percent of the data, and the remainder is translated with pseudo labeling, which allows us to leverage a large amount of data far more efficiently.

We use a simple pseudo labeling recipe that works well even with low-resource languages. To make this possible, we input a combination of character sets, including audio from all 60 languages, and for the output, we predict both the characters and the language they belong to. We train a supervised multilingual model, fine-tune it with semisupervised learning on a target language, generate pseudo labels for that language, and train a final model using pseudo labels for all languages, either from scratch or by fine-tuning.

Meta AI is committed to an open-science approach to research, and one of our main goals with this work is to open-source these pseudo labeling models in order to enable others to use them. For this reason, it was important for us to use public data sets to help promote open research. We chose the Common Voice and VoxPopuli data sets, given that they are the most popular multilingual data sets available. Training on the 19 languages of VoxPopuli with pseudo labels improves performance not only on the Common Voice test sets for those 19 languages but also on many other languages, enables training a larger model without overfitting, and helps the model generalize better to a new domain such as LibriSpeech audiobooks.

Why it matters:

Large quantities of labeled data are difficult to produce (or simply not available) for many languages. But these advances in pseudo labeling will go a long way in training multilingual ASR systems because this technique is much more efficient than traditional labeling methods.

A multilingual system would be simpler to maintain than a collection of monolingual models, it would enable users to comfortably speak any language without needing to tell the system which language to expect in advance, and it would share knowledge between all languages for improved performance.

This blog post was made possible by the work of Ronan Collobert, Tatiana Likhomanenko, Loren Lugosch, Vineel Pratap, Gabriel Synnaeve, Qiantong Xu (in alphabetical order).

Get it on GitHub

Read the paper"
Meta_Blog,https://ai.meta.com/blog/pseudo-labeling-speech-recognition-using-multilingual-unlabeled-data/,,Pseudo labeling: Speech recognition using multilingual unlabeled data,"What the research is:

There’s a massive amount of publicly shared audio data out in the world, from audiobooks to archived radio programs. Using Meta AI’s continuously developing automatic speech recognition (ASR) systems, we’re able to build ASR models to make the most of that information – including raw, unlabeled audio. Traditionally, to build a speech recognition system, you need both an audio sample and the corresponding transcript, which are then inputted to train the models. Transcribing large quantities of audio is incredibly labor intensive, however, and is not scalable to hundreds or thousands of languages and dialects. To help address this gap, Meta AI is developing a new high-performance open-source multilingual ASR model that uses pseudo labeling, a popular machine learning technique that leverages unlabeled data. Our latest work in pseudo labeling makes it possible to build an effective ASR model using unlabeled data across 60 languages.

Pseudo labeling complements Meta AI’s recent advances with wav2vec 2.0, HuBERT, textless NLP and data2vec in learning self-supervised speech representations. Pseudo labeling aims to solve the same overall challenge: how to make the best use of massive amounts of audio data that has not been transcribed by humans. Most importantly, this work extends pseudo labeling to a multilingual setting, where we use a small number of labeled examples to predict labels for an entire data set spanning multiple languages. This enables the development of high-performance open-source multilingual ASR models for many more languages and dialects than previously possible.

For the last few years, pseudo labeling has helped create better speech recognition in monolingual systems but not in multilingual ones. Our most recent work in pseudo labeling builds on advances we have made over the years in iterative pseudo-labeling, a semi-supervised algorithm that efficiently performs pseudo-labeling on unlabeled data and massively multilingual ASR, which trains a single acoustic model across more than 50 languages.

We are also releasing a large-scale multilingual open-source ASR model available to the community, trained only with open-source data.

How it works:

Pseudo labeling works by using a model trained on labeled data to predict the labels for unlabeled data, and then using those “pseudo labels” to train the model in a supervised way on the unlabeled data. It enables accurate ASR models to be built using far less transcript data. This means that when working with many thousands of hours of unlabeled audio data, we would need to get a transcript for a small subset of the data and then use pseudo labeling to predict the remaining unlabeled audio hours, allowing us to use all the hours of audio data to train the recognition learning system.

The greatest challenge here comes in doing pseudo labeling effectively across many languages, because each one will have its own character set that could interfere with others. Making sure multilingual ASR systems perform well across many languages was the hard part. Multilingual systems also require a lot of engineering innovation to efficiently train these large models. This is where the experience within our group of Meta AI researchers working on pseudo labeling was helpful, as it enabled us to scale to a multilingual model effectively.

Leveraging pseudo labeling to train multilingual ASR systems in particular has many advantages. Building speech recognition systems for 60 languages would normally mean training 60 different ASR models. But multilingual pseudo labeling takes into account commonalities between languages (in a similar manner as our M2M-100 translation model). Rather than developing 60 different models, you train a compact model that can perform better on all 60 of those languages by using cross-lingual learning. Having a multilingual system helps share the knowledge between languages, which facilitates cross-learning for similar languages like Spanish and Italian. We transcribe one to two percent of the data, and the remainder is translated with pseudo labeling, which allows us to leverage a large amount of data far more efficiently.

We use a simple pseudo labeling recipe that works well even with low-resource languages. To make this possible, we input a combination of character sets, including audio from all 60 languages, and for the output, we predict both the characters and the language they belong to. We train a supervised multilingual model, fine-tune it with semisupervised learning on a target language, generate pseudo labels for that language, and train a final model using pseudo labels for all languages, either from scratch or by fine-tuning.

Meta AI is committed to an open-science approach to research, and one of our main goals with this work is to open-source these pseudo labeling models in order to enable others to use them. For this reason, it was important for us to use public data sets to help promote open research. We chose the Common Voice and VoxPopuli data sets, given that they are the most popular multilingual data sets available. Training on the 19 languages of VoxPopuli with pseudo labels improves performance not only on the Common Voice test sets for those 19 languages but also on many other languages, enables training a larger model without overfitting, and helps the model generalize better to a new domain such as LibriSpeech audiobooks.

Why it matters:

Large quantities of labeled data are difficult to produce (or simply not available) for many languages. But these advances in pseudo labeling will go a long way in training multilingual ASR systems because this technique is much more efficient than traditional labeling methods.

A multilingual system would be simpler to maintain than a collection of monolingual models, it would enable users to comfortably speak any language without needing to tell the system which language to expect in advance, and it would share knowledge between all languages for improved performance.

This blog post was made possible by the work of Ronan Collobert, Tatiana Likhomanenko, Loren Lugosch, Vineel Pratap, Gabriel Synnaeve, Qiantong Xu (in alphabetical order).

Get it on GitHub

Read the paper"
Meta_Blog,https://ai.meta.com/blog/meta-ai-research-explores-new-public-fairness-benchmarks-for-computer-vision-models/,,Meta AI Research explores new public fairness benchmarks for computer vision models,"An overview of three fairness indicators explored by Meta AI Research for diagnosing fairness issues in computer vision systems.

In order to build socially responsible AI systems that work well for everyone, we need effective ways to diagnose potential fairness issues in both existing systems and the new ones that will power future technology. To move the AI research community towards more standardized fairness audits for computer vision systems, Meta AI Research is exploring new public fairness indicators to quantitatively assess three well-documented types of potential harms and biases in computer vision models. These fairness indicators complement existing approaches to responsible AI development, such as data / model documentation, and are specifically designed to adapt and evolve as research advances and new approaches are proposed.

The fairness indicators we are exploring are applicable to a broad range of computer vision systems irrespective of whether these systems are designed to predict labels or only generate the embeddings on a given image. Using these indicators, researchers across the AI community can measure and monitor key elements of fairness across their systems (and in particular, how they impact marginalized populations).

Swift progress in AI often requires tools that are available to everyone working in the field. So we are publishing details of this approach, along with experimental protocols, guidance, and code for automated fairness assessments using these explored indicators. By making our work public, we hope to foster collaboration across the entire AI community to develop a more comprehensive, ever-growing set of fairness assessments. As we and others work to advance the field, we believe this flexible approach, where data sets and metrics can be added as they are collected and developed, will move the industry closer to developing computer vision systems that work well for everyone. Researching new approaches to measuring fairness in AI will help inform our own work here at Meta so we can expand and improve the systems we have in place today.

A systematic and flexible way of quantitatively measuring fairness in CV

Computer vision technologies offer a wide range of benefits, ranging from helping people with low vision to improving medical imaging. But important societal issues have come to surface as new computer vision systems are deployed at large scale. Motivated by these societal concerns, we developed three fairness indicators based on publicly available datasets for fairness evaluation. These indicators allow measuring the frequency of concerning mistakes in a controlled setting and for various populations. These indicators can be applied to any feature extractor, so other practitioners can not only test their systems but compare performance with others’ work.

As detailed in this paper, our indicators allow auditing for three types of fairness issues:

Harmful label associations, where images of people are mistakenly assigned a label that is offensive, derogatory, or reinforces stereotypes.

Differences in performance on images from different places and populations across the world.

Differences in a pretrained model's learned representations of specific features -- in particular ones can be used to predict social and demographic labels.(For example, misgendering darker-skinned women.)

To illustrate how our indicators help in measuring progress towards fairer computer vision models, we performed an audit of some publicly available visual feature extractors. Our experiments study the effect of model size, data scale and training paradigm (self-supervised vs weakly supervised vs supervised learning) on the fairness indicators. Our findings suggest that large self-supervised models trained on large amounts of diverse, real, and unfiltered internet data show the most promise towards building fairer models compared to systems trained on highly curated object centric datasets such as ImageNet.

Nevertheless, significant discrepancies remain for even the best-performing models today. We hope our results will help the industry move to systematically incorporate fairness considerations as a core component of computer vision development.

Collaborating to build AI systems responsibly

There is no universal definition of fairness, and whether a system is fair cannot be summarized by just a few numbers. Nevertheless, we believe that reproducible and widely applicable assessments are critical to motivate and measure progress towards building fairer models.

Our goal is to catalyze research to define clear guidelines – within standardized protocols – that can continue to be applied as new data sets and models are collected. With time, the set of available fairness assessments will become more comprehensive and standardized frameworks will facilitate systematic fairness assessment of new models.

We believe that collaboration between the entire community will enable faster progress -- between researchers who develop new fairness protocols, as well as computer vision researchers who would systematically assess fairness as a foundational standard similar to the measurement of computational complexity or accuracy.

We invite the community to apply these benchmarks to their models and also to contribute new fairness tests or data sets. By working together we can continue to iterate on benchmarks to better spot and address fairness gaps in AI.

Read the paper: Fairness indicators for systematic assessments of visual feature extractors and get the code"
Meta_Blog,https://ai.meta.com/blog/meta-ai-research-explores-new-public-fairness-benchmarks-for-computer-vision-models/,,Meta AI Research explores new public fairness benchmarks for computer vision models,"An overview of three fairness indicators explored by Meta AI Research for diagnosing fairness issues in computer vision systems.

In order to build socially responsible AI systems that work well for everyone, we need effective ways to diagnose potential fairness issues in both existing systems and the new ones that will power future technology. To move the AI research community towards more standardized fairness audits for computer vision systems, Meta AI Research is exploring new public fairness indicators to quantitatively assess three well-documented types of potential harms and biases in computer vision models. These fairness indicators complement existing approaches to responsible AI development, such as data / model documentation, and are specifically designed to adapt and evolve as research advances and new approaches are proposed.

The fairness indicators we are exploring are applicable to a broad range of computer vision systems irrespective of whether these systems are designed to predict labels or only generate the embeddings on a given image. Using these indicators, researchers across the AI community can measure and monitor key elements of fairness across their systems (and in particular, how they impact marginalized populations).

Swift progress in AI often requires tools that are available to everyone working in the field. So we are publishing details of this approach, along with experimental protocols, guidance, and code for automated fairness assessments using these explored indicators. By making our work public, we hope to foster collaboration across the entire AI community to develop a more comprehensive, ever-growing set of fairness assessments. As we and others work to advance the field, we believe this flexible approach, where data sets and metrics can be added as they are collected and developed, will move the industry closer to developing computer vision systems that work well for everyone. Researching new approaches to measuring fairness in AI will help inform our own work here at Meta so we can expand and improve the systems we have in place today.

A systematic and flexible way of quantitatively measuring fairness in CV

Computer vision technologies offer a wide range of benefits, ranging from helping people with low vision to improving medical imaging. But important societal issues have come to surface as new computer vision systems are deployed at large scale. Motivated by these societal concerns, we developed three fairness indicators based on publicly available datasets for fairness evaluation. These indicators allow measuring the frequency of concerning mistakes in a controlled setting and for various populations. These indicators can be applied to any feature extractor, so other practitioners can not only test their systems but compare performance with others’ work.

As detailed in this paper, our indicators allow auditing for three types of fairness issues:

Harmful label associations, where images of people are mistakenly assigned a label that is offensive, derogatory, or reinforces stereotypes.

Differences in performance on images from different places and populations across the world.

Differences in a pretrained model's learned representations of specific features -- in particular ones can be used to predict social and demographic labels.(For example, misgendering darker-skinned women.)

To illustrate how our indicators help in measuring progress towards fairer computer vision models, we performed an audit of some publicly available visual feature extractors. Our experiments study the effect of model size, data scale and training paradigm (self-supervised vs weakly supervised vs supervised learning) on the fairness indicators. Our findings suggest that large self-supervised models trained on large amounts of diverse, real, and unfiltered internet data show the most promise towards building fairer models compared to systems trained on highly curated object centric datasets such as ImageNet.

Nevertheless, significant discrepancies remain for even the best-performing models today. We hope our results will help the industry move to systematically incorporate fairness considerations as a core component of computer vision development.

Collaborating to build AI systems responsibly

There is no universal definition of fairness, and whether a system is fair cannot be summarized by just a few numbers. Nevertheless, we believe that reproducible and widely applicable assessments are critical to motivate and measure progress towards building fairer models.

Our goal is to catalyze research to define clear guidelines – within standardized protocols – that can continue to be applied as new data sets and models are collected. With time, the set of available fairness assessments will become more comprehensive and standardized frameworks will facilitate systematic fairness assessment of new models.

We believe that collaboration between the entire community will enable faster progress -- between researchers who develop new fairness protocols, as well as computer vision researchers who would systematically assess fairness as a foundational standard similar to the measurement of computational complexity or accuracy.

We invite the community to apply these benchmarks to their models and also to contribute new fairness tests or data sets. By working together we can continue to iterate on benchmarks to better spot and address fairness gaps in AI.

Read the paper: Fairness indicators for systematic assessments of visual feature extractors and get the code"
Meta_Blog,https://ai.meta.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/,,"SEER 10B: Better, fairer computer vision through self-supervised learning on diverse datasets","We’re excited to announce new advances in SEER (SElf-SupERvised), Meta AI Research’s groundbreaking self-supervised computer vision model that can learn directly from any random collection of images on the internet — without the need for careful data curation and labeling that goes into conventional computer vision training — and then output an image embedding. SEER is now not only much more powerful, it also produces fairer, more robust computer vision models and can discover salient information in images, similar to how humans learn about the world by considering the relationships between the different objects they observe. SEER can help build breakthrough computer vision systems and advance towards building AI that works well for everyone. We’re also publicly releasing the model and sharing new technical details about how it works. While SEER is purely a research model for now, it will help Meta AI build better computer vision systems for products used by billions of people around the world.

When we first announced SEER last spring, it outperformed state-of-the-art systems, demonstrating that self-supervised learning can excel at computer vision tasks in real world settings. We’ve now scaled SEER from 1 billion to 10 billion dense parameters, making it to our knowledge the largest dense computer vision model of its kind.

Because of its increased size, SEER can extract better quality visual features and salient information present in real world large-scale data sets with trillions of random, uncurated images world-wide. This helps SEER perform better on tasks where smaller unsupervised models have struggled. With 10B parameters, SEER outperforms other models on important fairness benchmarks recently proposed by Meta AI Research. Traditional computer vision systems are trained primarily on examples from the U.S. and wealthy countries in Europe, so they often don’t work well for images from other places with different socioeconomic characteristics. But SEER delivers strong results for images from all around the globe – including non-U.S. and non-Europe regions with a wide range of income levels. Further, the 10B SEER model drastically improved performance on fairness benchmarks across different gender, apparent skin tone, and age groups. Apart from its improved performance on fairness benchmarks, this model understands images from across the world well enough to geolocalize them with unprecedented precision.

Something Went Wrong We're having trouble playing this video. Learn more

On top of achieving strong performance on standard computer vision benchmarks (for example, 85.8 percent top-1 accuracy on ImageNet), the model also excels at challenging tasks and increases robustness to out-of-domain generalization. For example, it can correctly identify animals in sketches and artistic renditions and also handles challenges in images such as camouflage, blur, occlusion, motion, and unusual perspectives.

We remain committed to Meta AI Research’s principles of open science, so to facilitate further research and progress we are making SEER accessible by publicly releasing the model weights, implementation details, and sharing additional technical documentation explaining how it works and how it was trained.

We’ve long focused on self-supervised learning , because it allows us to move past the constraints of labeled data sets, and on building AI models that work at billion-scale . We’ve also prioritized an open science approach , where we publish our research and share our code and models so that collectively, the AI research community can learn from each others’ work, engage in peer review, and accelerate progress. SEER itself is an example of how collaboration can fuel progress in this field. The SEER model is based on SwAV, an algorithm developed jointly by Meta AI Research and Inria. And finally we are committed to developing AI responsibly and building a robust system to address issues of privacy, fairness, accountability, and transparency . As part of thateffort, we also conducted extensive state-of-the-art adversarial attacks on the SEER model to confirm the privacy of training data is protected.

Better performance and fairer predictions

We studied and validated SEER’s performance on more than 50 benchmarks – including fairness, robustness, fine-grained recognition, and a variety of image classification data sets from domains like medical imaging, satellite images, and optical character recognition (OCR). The 10 billion-parameter SEER model consistently outperformed its 1 billion-parameter predecessor, generating better visual features. Despite training on random collections of images on the internet with no data curation, the 10B model outperformed state-of-the-art supervised and self-supervised models trained on ImageNet on 70 percent of the benchmarks while achieving equal performance on the rest.

To test the model’s robustness to adversarial attacks, we evaluated it on the task of image copy detection, where the model must identify images that have been distorted through blurring, insertions, cropping, and other editing techniques. SEER outperformed the previous best results by achieving 90.6 percent mean average precision on the CopyDays benchmark, a 5.1 percent improvement. Further, SEER outperformed the state-of-the-art self-supervised models trained on ImageNet on out-of-domain robustness benchmarks and the model robustness consistently improved as we increased the size of the model.

The large SEER model captures salient information present in a large set of random and unfiltered internet images even across diverse geographies and linguistic concepts. For example, even though the model is trained only on the images themselves with no location information or other metadata, it is able to group together the same concepts in multiple languages all over the world. For example, the concept wedding from all over the world is embedded together in the model's feature space.

Something Went Wrong We're having trouble playing this video. Learn more

To assess the models’ ability to work well for different groups (and motivated by prior research in the field of fairness and computer vision demonstrating widespread limitations in this area), we looked at whether the model could equally recognize social membership attributes, such as the gender of people with different skin tones. In this analysis we used Meta AI Research’s recently open-sourced Casual Conversations data set along with our recently announced research proposing new fairness benchmarks for computer vision models. We found the SEER 10B model more accurately recognized these social membership attributes compared to the smaller SEER models as well as ImageNet-trained supervised and self-supervised models. The larger SEER model works well for people across different genders, skin tones, and ages.

This graph shows accuracy of gender retrieval, using the Casual Conversations data set.

Also using the Casual Conversations data set, we evaluated the model labels for inaccuracies, such as predicting labels such as “non-human” or “crime” when given an image of a particular person. Here, too, SEER 10B did not produce significant numbers of these associations, whereas supervised models trained on ImageNet did.

This graph shows the rate of meaningfully inaccurate predictions for different groups of people.

Last year, we tested our 1 billion-parameter SEER model on images of everyday items from around the world and found it outperformed conventional computer vision systems in recognizing objects that, while representative of life for billions of people, are less represented in conventional image data sets used to train AI systems. The 10 billion-parameter SEER model improves upon the performance of the smaller self-supervised model and significantly outperforms supervised methods. Using Gapminder’s Dollar Street data set, which collects images of objects in households around the world along with information on their household income, we found performance improved the most for low-medium income households across the world as well as for households in non-Western regions.

These graphs look at different income and geographic groups across the world and show how much the 10 billion-parameter SEER outperformed a supervised model trained on ImageNet.

SEER also outperformed supervised ImageNet-trained models by 2 points when detecting multimodal (image + text) hate speech using the Hateful Memes dataset, which Meta AI created and shared in 2020.

Performing adversarial attacks to open-source SEER responsibly

Recent research has shown how some AI models can be vulnerable to data extraction attacks, where an adversary tries to discern whether a particular example was part of the data set used to train it. In some cases, it has been shown that adversaries can even query a model and then reconstruct specific samples from that training dataset. Because Meta AI Research focuses on open research and aims to drive innovation in AI, we prioritized identifying a privacy- and security-protective process to open source SEER, as well as to build a blueprint for responsibly open-sourcing similar models.

We set out to test the SEER model with the goal of assessing whether it is possible to infer that a particular image was part of the training data, by comparing the loss on an image to the loss given to the same image by other models. Using the Privacy Linter, an open-source tool developed at Meta, we found the accuracy of the attack was only slightly better than a completely random guess (50-50). Specifically, the maximum accuracy is at 50.02 percent, whereas the accuracy of a random attack would have 50 percent accuracy for equally sized train and held-out sets. Moreover, we computed the precision at various recall levels to make sure that no training images were exposed at low recall levels – this could happen when all the samples with the highest scores all belong in the train set: here again, precision stays below 50.15 percent for all levels of recall (including the lowest ones).

Overall, currently published state-of-the-art attacks are unable to extract membership information from the SEER 10B model trained on 1B images. In addition to publishing the weights of the pretrained SEER 10B model, we are providing model documentation detailing how SEER was created and its intended uses. We believe that the details of the model will help AI practitioners understand the model’s performance when they use it in downstream tasks.

Using self-supervision to advance AI research

There is so much richness and variety in the world, but only a tiny fraction is contained in labeled data sets. The original SEER model showed that self-supervised learning can leverage random, unannotated images to deliver state of the art performance. And now by scaling to 10 billion parameters, SEER is more robust, more private, and more fair.

Because SEER does not rely on labeled data sets, we were able to train the model on a set of examples that is much more geographically diverse than ImageNet.

We hope to advance SEER further, improving its overall performance. Meta uses self-supervised learning extensively in our production systems today and we are exploring ways to use our work with SEER to improve our existing products and services and to create entirely new ones.

In particular, advancing computer vision is an important part of building the Metaverse. For example, to build AR glasses that can guide you to your misplaced keys or show you how to make a favorite recipe, we will need machines that understand the visual world as people do. They will need to work well in kitchens not just in Kansas and Kyoto but also in Kuala Lumpur, Kinshasa, and myriad other places around the world. This means recognizing all the different variations of everyday objects like house keys or stoves or spices. SEER breaks new ground in achieving this robust performance.

We are excited to forge ahead with SEER and self-supervised learning in other domains. Ultimately, we hope to create AI systems that understand the world holistically, across modalities such as images, text, speech, and even touch. These intelligent machines will unlock the metaverse and help people perform tasks in work and in everyday life. And by sharing our work with SEER here, we hope other researchers and engineers will accelerate progress in this important field.

Read the paper: Vision models are more robust and fair when pretrained on uncurated images without supervision

Implementation details and code

Model weights and model license

Model technical documentation"
Meta_Blog,https://ai.meta.com/blog/seer-10b-better-fairer-computer-vision-through-self-supervised-learning-training-on-diverse-datasets/,,"SEER 10B: Better, fairer computer vision through self-supervised learning on diverse datasets","We’re excited to announce new advances in SEER (SElf-SupERvised), Meta AI Research’s groundbreaking self-supervised computer vision model that can learn directly from any random collection of images on the internet — without the need for careful data curation and labeling that goes into conventional computer vision training — and then output an image embedding. SEER is now not only much more powerful, it also produces fairer, more robust computer vision models and can discover salient information in images, similar to how humans learn about the world by considering the relationships between the different objects they observe. SEER can help build breakthrough computer vision systems and advance towards building AI that works well for everyone. We’re also publicly releasing the model and sharing new technical details about how it works. While SEER is purely a research model for now, it will help Meta AI build better computer vision systems for products used by billions of people around the world.

When we first announced SEER last spring, it outperformed state-of-the-art systems, demonstrating that self-supervised learning can excel at computer vision tasks in real world settings. We’ve now scaled SEER from 1 billion to 10 billion dense parameters, making it to our knowledge the largest dense computer vision model of its kind.

Because of its increased size, SEER can extract better quality visual features and salient information present in real world large-scale data sets with trillions of random, uncurated images world-wide. This helps SEER perform better on tasks where smaller unsupervised models have struggled. With 10B parameters, SEER outperforms other models on important fairness benchmarks recently proposed by Meta AI Research. Traditional computer vision systems are trained primarily on examples from the U.S. and wealthy countries in Europe, so they often don’t work well for images from other places with different socioeconomic characteristics. But SEER delivers strong results for images from all around the globe – including non-U.S. and non-Europe regions with a wide range of income levels. Further, the 10B SEER model drastically improved performance on fairness benchmarks across different gender, apparent skin tone, and age groups. Apart from its improved performance on fairness benchmarks, this model understands images from across the world well enough to geolocalize them with unprecedented precision.

Something Went Wrong We're having trouble playing this video. Learn more

On top of achieving strong performance on standard computer vision benchmarks (for example, 85.8 percent top-1 accuracy on ImageNet), the model also excels at challenging tasks and increases robustness to out-of-domain generalization. For example, it can correctly identify animals in sketches and artistic renditions and also handles challenges in images such as camouflage, blur, occlusion, motion, and unusual perspectives.

We remain committed to Meta AI Research’s principles of open science, so to facilitate further research and progress we are making SEER accessible by publicly releasing the model weights, implementation details, and sharing additional technical documentation explaining how it works and how it was trained.

We’ve long focused on self-supervised learning , because it allows us to move past the constraints of labeled data sets, and on building AI models that work at billion-scale . We’ve also prioritized an open science approach , where we publish our research and share our code and models so that collectively, the AI research community can learn from each others’ work, engage in peer review, and accelerate progress. SEER itself is an example of how collaboration can fuel progress in this field. The SEER model is based on SwAV, an algorithm developed jointly by Meta AI Research and Inria. And finally we are committed to developing AI responsibly and building a robust system to address issues of privacy, fairness, accountability, and transparency . As part of thateffort, we also conducted extensive state-of-the-art adversarial attacks on the SEER model to confirm the privacy of training data is protected.

Better performance and fairer predictions

We studied and validated SEER’s performance on more than 50 benchmarks – including fairness, robustness, fine-grained recognition, and a variety of image classification data sets from domains like medical imaging, satellite images, and optical character recognition (OCR). The 10 billion-parameter SEER model consistently outperformed its 1 billion-parameter predecessor, generating better visual features. Despite training on random collections of images on the internet with no data curation, the 10B model outperformed state-of-the-art supervised and self-supervised models trained on ImageNet on 70 percent of the benchmarks while achieving equal performance on the rest.

To test the model’s robustness to adversarial attacks, we evaluated it on the task of image copy detection, where the model must identify images that have been distorted through blurring, insertions, cropping, and other editing techniques. SEER outperformed the previous best results by achieving 90.6 percent mean average precision on the CopyDays benchmark, a 5.1 percent improvement. Further, SEER outperformed the state-of-the-art self-supervised models trained on ImageNet on out-of-domain robustness benchmarks and the model robustness consistently improved as we increased the size of the model.

The large SEER model captures salient information present in a large set of random and unfiltered internet images even across diverse geographies and linguistic concepts. For example, even though the model is trained only on the images themselves with no location information or other metadata, it is able to group together the same concepts in multiple languages all over the world. For example, the concept wedding from all over the world is embedded together in the model's feature space.

Something Went Wrong We're having trouble playing this video. Learn more

To assess the models’ ability to work well for different groups (and motivated by prior research in the field of fairness and computer vision demonstrating widespread limitations in this area), we looked at whether the model could equally recognize social membership attributes, such as the gender of people with different skin tones. In this analysis we used Meta AI Research’s recently open-sourced Casual Conversations data set along with our recently announced research proposing new fairness benchmarks for computer vision models. We found the SEER 10B model more accurately recognized these social membership attributes compared to the smaller SEER models as well as ImageNet-trained supervised and self-supervised models. The larger SEER model works well for people across different genders, skin tones, and ages.

This graph shows accuracy of gender retrieval, using the Casual Conversations data set.

Also using the Casual Conversations data set, we evaluated the model labels for inaccuracies, such as predicting labels such as “non-human” or “crime” when given an image of a particular person. Here, too, SEER 10B did not produce significant numbers of these associations, whereas supervised models trained on ImageNet did.

This graph shows the rate of meaningfully inaccurate predictions for different groups of people.

Last year, we tested our 1 billion-parameter SEER model on images of everyday items from around the world and found it outperformed conventional computer vision systems in recognizing objects that, while representative of life for billions of people, are less represented in conventional image data sets used to train AI systems. The 10 billion-parameter SEER model improves upon the performance of the smaller self-supervised model and significantly outperforms supervised methods. Using Gapminder’s Dollar Street data set, which collects images of objects in households around the world along with information on their household income, we found performance improved the most for low-medium income households across the world as well as for households in non-Western regions.

These graphs look at different income and geographic groups across the world and show how much the 10 billion-parameter SEER outperformed a supervised model trained on ImageNet.

SEER also outperformed supervised ImageNet-trained models by 2 points when detecting multimodal (image + text) hate speech using the Hateful Memes dataset, which Meta AI created and shared in 2020.

Performing adversarial attacks to open-source SEER responsibly

Recent research has shown how some AI models can be vulnerable to data extraction attacks, where an adversary tries to discern whether a particular example was part of the data set used to train it. In some cases, it has been shown that adversaries can even query a model and then reconstruct specific samples from that training dataset. Because Meta AI Research focuses on open research and aims to drive innovation in AI, we prioritized identifying a privacy- and security-protective process to open source SEER, as well as to build a blueprint for responsibly open-sourcing similar models.

We set out to test the SEER model with the goal of assessing whether it is possible to infer that a particular image was part of the training data, by comparing the loss on an image to the loss given to the same image by other models. Using the Privacy Linter, an open-source tool developed at Meta, we found the accuracy of the attack was only slightly better than a completely random guess (50-50). Specifically, the maximum accuracy is at 50.02 percent, whereas the accuracy of a random attack would have 50 percent accuracy for equally sized train and held-out sets. Moreover, we computed the precision at various recall levels to make sure that no training images were exposed at low recall levels – this could happen when all the samples with the highest scores all belong in the train set: here again, precision stays below 50.15 percent for all levels of recall (including the lowest ones).

Overall, currently published state-of-the-art attacks are unable to extract membership information from the SEER 10B model trained on 1B images. In addition to publishing the weights of the pretrained SEER 10B model, we are providing model documentation detailing how SEER was created and its intended uses. We believe that the details of the model will help AI practitioners understand the model’s performance when they use it in downstream tasks.

Using self-supervision to advance AI research

There is so much richness and variety in the world, but only a tiny fraction is contained in labeled data sets. The original SEER model showed that self-supervised learning can leverage random, unannotated images to deliver state of the art performance. And now by scaling to 10 billion parameters, SEER is more robust, more private, and more fair.

Because SEER does not rely on labeled data sets, we were able to train the model on a set of examples that is much more geographically diverse than ImageNet.

We hope to advance SEER further, improving its overall performance. Meta uses self-supervised learning extensively in our production systems today and we are exploring ways to use our work with SEER to improve our existing products and services and to create entirely new ones.

In particular, advancing computer vision is an important part of building the Metaverse. For example, to build AR glasses that can guide you to your misplaced keys or show you how to make a favorite recipe, we will need machines that understand the visual world as people do. They will need to work well in kitchens not just in Kansas and Kyoto but also in Kuala Lumpur, Kinshasa, and myriad other places around the world. This means recognizing all the different variations of everyday objects like house keys or stoves or spices. SEER breaks new ground in achieving this robust performance.

We are excited to forge ahead with SEER and self-supervised learning in other domains. Ultimately, we hope to create AI systems that understand the world holistically, across modalities such as images, text, speech, and even touch. These intelligent machines will unlock the metaverse and help people perform tasks in work and in everyday life. And by sharing our work with SEER here, we hope other researchers and engineers will accelerate progress in this important field.

Read the paper: Vision models are more robust and fair when pretrained on uncurated images without supervision

Implementation details and code

Model weights and model license

Model technical documentation"
Meta_Blog,https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/,,Yann LeCun on a vision to make AI systems learn and reason like animals and humans,"Watch the Meta AI Inside the Lab event here.

For all the remarkable recent progress in AI research, we are still very far from creating machines that think and learn as well as people do. As Meta AI’s Chief AI Scientist Yann LeCun notes, a teenager who has never sat behind a steering wheel can learn to drive in about 20 hours, while the best autonomous driving systems today need millions or billions of pieces of labeled training data and millions of reinforcement learning trials in virtual environments. And even then, they fall short of human’s ability to drive a car reliably.

What will it take to build AI that approaches human-level capabilities? Is it simply a matter of more data and bigger AI models?

As part of Meta AI’s Inside the Lab event on February 23, 2022, LeCun is sketching an alternate vision for building human-level AI. LeCun proposes that the ability to learn “world models” — internal models of how the world works — may be the key.

Meta AI is sharing some of LeCun’s ideas in brief here, including his proposal for a modular, configurable architecture for autonomous intelligence, as well as key challenges the AI research community must address to build such a system. We typically share the results of our research — by publishing papers, code, and data sets, as well as blog posts — when they are completed. But in keeping with Meta AI’s open-science approach, we are taking this opportunity to preview our research vision and ideas in the hope that it spurs discussion and collaboration among AI researchers. The simple fact is that we will need to work together to solve these extraordinarily challenging, exciting problems.

We plan to share more details on LeCun’s vision in an upcoming position paper.

AI that can model how the world works

“Human and nonhuman animals seem able to learn enormous amounts of background knowledge about how the world works through observation and through an incomprehensibly small amount of interactions in a task-independent, unsupervised way,” LeCun says. “It can be hypothesized that this accumulated knowledge may constitute the basis for what is often called common sense.”

And common sense can be seen as a collection of models of the world that can guide on what is likely, what is plausible, and what is impossible.

This allows humans to plan effectively in unfamiliar situations. That teen driver may not have driven over snow before, for example, but he (hopefully) knows that snow can be slippery and send his car into a skid he drives too aggressively.

Common sense knowledge allows animals not just to predict future outcomes but also to fill in missing information, whether temporally or spatially. When a driver hears the sound of metal smashing together nearby, he knows immediately that there’s been an accident — even without seeing the vehicles involved.

The idea that humans, animals, and intelligent systems use world models goes back many decades in psychology and in fields of engineering such as control and robotics. LeCun proposes that one of the most important challenges in AI today is devising learning paradigms and architectures that would allow machines to learn world models in a self-supervised fashion and then use those models to predict, reason, and plan. His outline regroups ideas that have been proposed in various disciplines, such as cognitive science, systems neuroscience, optimal control, reinforcement learning, and “traditional” AI, and combines them with new concepts in machine learning, such as self-supervised learning and joint-embedding architectures.

Proposing an architecture for autonomous intelligence

LeCun proposes an architecture composed of six separate modules. Each is assumed to be differentiable, in that it can easily compute gradient estimates of some objective function with respect to its own input and propagate the gradient information to upstream modules.

A system architecture for autonomous intelligence. The configurator gets inputs from other modules, but we have omitted those arrows in order to simplify the diagram.

The configurator module performs executive control: Given a task to be executed, it preconfigures the perception module, the world model, the cost, and the actor for the task at hand, possibly by modulating the parameters of those modules.

The perception module receives signals from sensors and estimates the current state of the world. For a given task, only a small subset of the perceived state of the world is relevant and useful. The configurator module primes the perception system to extract the relevant information from the percept for the task at hand.

The world model module constitutes the most complex piece of the architecture. Its role is twofold: (1) to estimate missing information about the state of the world not provided by perception, and (2) to predict plausible future states of the world. The world model may predict natural evolutions of the world or predict future world states resulting from a sequence of actions proposed by the actor module. The world model is a kind of simulator of the part of the world relevant to the task at hand. Since the world is full of uncertainty, the model must be able to represent multiple possible predictions. A driver approaching an intersection may slow down in case another car approaching the intersection doesn’t stop at the stop sign.

The cost module computes a single scalar output that predicts the level of discomfort of the agent. It is composed of two submodules: the intrinsic cost, which is hard-wired and immutable (not trainable), and computes the immediate discomfort (such as damage to the agent, violation of hard-coded behavioral constraints, etc.), and the critic, which is a trainable module that predicts future values of the intrinsic cost. The ultimate goal of the agent is to minimize the intrinsic cost over the long run. “This is where basic behavioral drives and intrinsic motivations reside,” LeCun says. So it will factor in intrinsic costs, such as not wasting energy, as well as costs specific to the task at hand. “Because the cost module is differentiable, the gradient of the cost can be back-propagated through the other modules for planning, reasoning, or learning.”

The actor module computes proposals for action sequences. “The actor can find an optimal action sequence that minimizes the estimated future cost, and output the first action in the optimal sequence, in a fashion similar to classical optimal control,” LeCun says.

The short-term memory module keeps track of the current and predicted world state, as well as associated costs.

World model architecture and self-supervised training

The centerpiece of the architecture is the predictive world model. A critical challenge with constructing it is how to enable it to represent multiple plausible predictions. The real world is not entirely predictable: There are many possible ways a particular situation can evolve, and there are many details of a situation that are irrelevant to the task at hand. I may need to anticipate what cars around me are going to do while I drive, but I don’t need to predict the detailed position of individual leaves in the trees that are near the road. How can a world model learn abstract representations of the world so that important details are preserved, irrelevant details are ignored, and predictions can be performed in the space of abstract representations?

One key element of a solution is the Joint Embedding Predictive Architecture (JEPA). The JEPA captures the dependencies between two inputs, x and y. For example, x could be a segment of video, and y the next segment of the video. Inputs x and y are fed to trainable encoders that extract abstract representations of them, s x and s y . A predictor module is trained to predict s y from s x . The predictor may use a latent variable, z, to represent information present in s y that is not present in s x . The JEPA handles uncertainty in predictions in two ways: (1) The encoder may choose to drop information about y that is difficult to predict, (2) the latent variable z, when varied over a set, will cause the prediction to vary over a set of plausible predictions.

How do we train a JEPA? Until recently, the only approach would have been to use contrastive methods, which consist of showing examples of compatible x and y, together with numerous examples of x and incompatible y’s. But this is rather impractical when the representations are high-dimensional. An alternative training strategy has emerged in the last two years: regularized methods. When applied to JEPA, the method uses four criteria:

Make the representation of x maximally informative about x Make the representation of y maximally informative about y Make the representation of y maximally predictable from the representation of x Make the predictor use as little information as possible from the latent variable to represent uncertainty in the prediction.

These criteria can be translated into differentiable cost functions in various ways. One way is the VICReg method, which stands for Variance, Invariance, Covariance Regularization. In VICReg, the information content of the representations of x and y are maximized by maintaining the variances of their components over a threshold and by making these components as independent of each other as possible. Simultaneously, the model tries to make the representation of y predictable from that of x. Additionally, the information content of the latent variable is minimized by making it discrete, low-dimensional, sparse, or noisy.

The beauty of the JEPA is that it naturally produces informative abstract representations of the input that eliminate irrelevant details and with which predictions can be performed. This enables JEPAs to be stacked on top of one another so as to learn representations with higher levels of abstraction that can perform longer-term prediction. For example, a scenario can be described at a high level as “a cook is making crêpes.” One can predict that the cook will fetch flour, milk, and eggs; mix the ingredients; ladle batter into a pan; let the batter fry; flip the crêpe; and repeat. At a lower level, pouring a ladle involves scooping some batter and spreading it around the pan. This continues all the way down to the precise trajectories of the chef’s hands millisecond by millisecond. At the low level of hand trajectories, our world model can only make accurate predictions in the short term. But at a higher level of abstraction, it can make long-term predictions.

The Hierarchical JEPA can be used to perform predictions at several levels of abstraction and several time scales. How can it be trained? Largely by passive observation, and less often by interaction.

A baby learns how the world works largely by observation in the first few months of life. She learns that the world is three-dimensional, that some objects are in front of others, that when an object is occluded it still exists. Eventually, around nine months of age, babies learn intuitive physics — for example, that unsupported objects fall through gravity.

The hope is that a hierarchical JEPA could learn how the world works by watching videos and interacting with its environment. By training itself to predict what will happen in the video, it will produce hierarchical representations of the world. By taking actions in the world and observing the result, the world model will learn to predict the consequences of its actions, which will allow it to reason and plan.

A perception-action episode

With a Hierarchical JEPA properly trained as a world model, an agent could perform hierarchical planning of complex actions, decomposing a complex task into a sequence of less complex and less abstract subtasks, all the way down to low-level actions on effectors.

A typical perception-action episode would go as follows. The diagram illustrates the situation for a two-level hierarchy. The perception module extracts a hierarchical representation of the state of the world (s1[0]=Enc1(x) and s2[0]=Enc2(s[0]) in the diagram). Then, the second-level predictor is applied multiple times to predict future states, given a sequence of abstract actions proposed by the second-level actor. The actor optimizes the second-level action sequence so as to minimize the overall cost (C(s2[4]) in the diagram). This process is akin to Model-Predictive Control in optimal control. The process is repeated for multiple drawings of the second-level latent variables, which may produce different high-level scenarios. The resulting high-level actions do not constitute real actions but merely define constraints that the lower-level state sequence must satisfy (e.g., are the ingredients properly mixed?). They really constitute subgoals. The entire process is repeated at the lower level: running the lower-level predictor, optimizing the low-level action sequence to minimize the intermediate costs coming from the upper layer, and repeating the process for multiple drawings of the low-level latent variables. Once the process is complete, the agent outputs the first low-level action to the effectors, and the whole episode can be repeated.

If we are successful at building such a model, all the modules would be differentiable, so that this whole action optimization process could be performed using gradient-based methods.

Moving closer to human-level intelligence in AI

LeCun’s vision requires much deeper exploration than is possible in a brief blog post, and many difficult challenges lie ahead. One of the most interesting and difficult of these is instantiating the details of the architectures and training procedures for the world model. In fact, it could be argued that training world models constitutes the main challenge toward real progress in AI over the next decades.

But many other aspects of the architecture are still to be defined, including how precisely to train the critic, how to construct and train the configurator, and how to use the short-term memory to keep track of the world state and to store a history of world states, actions, and associated intrinsic cost to tune the critic.

LeCun and other Meta AI researchers look forward to exploring these in the months and years ahead, as well as exchanging ideas and learning from others in the field. Creating machines that can learn and understand as effectively as humans is a long-term scientific endeavor — and one with no guarantees of success. But we are confident that fundamental research will continue to produce a deeper understanding of both minds and machines, and will lead to advances that benefit everyone who uses AI.

Watch the Meta AI Inside the Lab event here."
Meta_Blog,https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/,,Yann LeCun on a vision to make AI systems learn and reason like animals and humans,"Watch the Meta AI Inside the Lab event here.

For all the remarkable recent progress in AI research, we are still very far from creating machines that think and learn as well as people do. As Meta AI’s Chief AI Scientist Yann LeCun notes, a teenager who has never sat behind a steering wheel can learn to drive in about 20 hours, while the best autonomous driving systems today need millions or billions of pieces of labeled training data and millions of reinforcement learning trials in virtual environments. And even then, they fall short of human’s ability to drive a car reliably.

What will it take to build AI that approaches human-level capabilities? Is it simply a matter of more data and bigger AI models?

As part of Meta AI’s Inside the Lab event on February 23, 2022, LeCun is sketching an alternate vision for building human-level AI. LeCun proposes that the ability to learn “world models” — internal models of how the world works — may be the key.

Meta AI is sharing some of LeCun’s ideas in brief here, including his proposal for a modular, configurable architecture for autonomous intelligence, as well as key challenges the AI research community must address to build such a system. We typically share the results of our research — by publishing papers, code, and data sets, as well as blog posts — when they are completed. But in keeping with Meta AI’s open-science approach, we are taking this opportunity to preview our research vision and ideas in the hope that it spurs discussion and collaboration among AI researchers. The simple fact is that we will need to work together to solve these extraordinarily challenging, exciting problems.

We plan to share more details on LeCun’s vision in an upcoming position paper.

AI that can model how the world works

“Human and nonhuman animals seem able to learn enormous amounts of background knowledge about how the world works through observation and through an incomprehensibly small amount of interactions in a task-independent, unsupervised way,” LeCun says. “It can be hypothesized that this accumulated knowledge may constitute the basis for what is often called common sense.”

And common sense can be seen as a collection of models of the world that can guide on what is likely, what is plausible, and what is impossible.

This allows humans to plan effectively in unfamiliar situations. That teen driver may not have driven over snow before, for example, but he (hopefully) knows that snow can be slippery and send his car into a skid he drives too aggressively.

Common sense knowledge allows animals not just to predict future outcomes but also to fill in missing information, whether temporally or spatially. When a driver hears the sound of metal smashing together nearby, he knows immediately that there’s been an accident — even without seeing the vehicles involved.

The idea that humans, animals, and intelligent systems use world models goes back many decades in psychology and in fields of engineering such as control and robotics. LeCun proposes that one of the most important challenges in AI today is devising learning paradigms and architectures that would allow machines to learn world models in a self-supervised fashion and then use those models to predict, reason, and plan. His outline regroups ideas that have been proposed in various disciplines, such as cognitive science, systems neuroscience, optimal control, reinforcement learning, and “traditional” AI, and combines them with new concepts in machine learning, such as self-supervised learning and joint-embedding architectures.

Proposing an architecture for autonomous intelligence

LeCun proposes an architecture composed of six separate modules. Each is assumed to be differentiable, in that it can easily compute gradient estimates of some objective function with respect to its own input and propagate the gradient information to upstream modules.

A system architecture for autonomous intelligence. The configurator gets inputs from other modules, but we have omitted those arrows in order to simplify the diagram.

The configurator module performs executive control: Given a task to be executed, it preconfigures the perception module, the world model, the cost, and the actor for the task at hand, possibly by modulating the parameters of those modules.

The perception module receives signals from sensors and estimates the current state of the world. For a given task, only a small subset of the perceived state of the world is relevant and useful. The configurator module primes the perception system to extract the relevant information from the percept for the task at hand.

The world model module constitutes the most complex piece of the architecture. Its role is twofold: (1) to estimate missing information about the state of the world not provided by perception, and (2) to predict plausible future states of the world. The world model may predict natural evolutions of the world or predict future world states resulting from a sequence of actions proposed by the actor module. The world model is a kind of simulator of the part of the world relevant to the task at hand. Since the world is full of uncertainty, the model must be able to represent multiple possible predictions. A driver approaching an intersection may slow down in case another car approaching the intersection doesn’t stop at the stop sign.

The cost module computes a single scalar output that predicts the level of discomfort of the agent. It is composed of two submodules: the intrinsic cost, which is hard-wired and immutable (not trainable), and computes the immediate discomfort (such as damage to the agent, violation of hard-coded behavioral constraints, etc.), and the critic, which is a trainable module that predicts future values of the intrinsic cost. The ultimate goal of the agent is to minimize the intrinsic cost over the long run. “This is where basic behavioral drives and intrinsic motivations reside,” LeCun says. So it will factor in intrinsic costs, such as not wasting energy, as well as costs specific to the task at hand. “Because the cost module is differentiable, the gradient of the cost can be back-propagated through the other modules for planning, reasoning, or learning.”

The actor module computes proposals for action sequences. “The actor can find an optimal action sequence that minimizes the estimated future cost, and output the first action in the optimal sequence, in a fashion similar to classical optimal control,” LeCun says.

The short-term memory module keeps track of the current and predicted world state, as well as associated costs.

World model architecture and self-supervised training

The centerpiece of the architecture is the predictive world model. A critical challenge with constructing it is how to enable it to represent multiple plausible predictions. The real world is not entirely predictable: There are many possible ways a particular situation can evolve, and there are many details of a situation that are irrelevant to the task at hand. I may need to anticipate what cars around me are going to do while I drive, but I don’t need to predict the detailed position of individual leaves in the trees that are near the road. How can a world model learn abstract representations of the world so that important details are preserved, irrelevant details are ignored, and predictions can be performed in the space of abstract representations?

One key element of a solution is the Joint Embedding Predictive Architecture (JEPA). The JEPA captures the dependencies between two inputs, x and y. For example, x could be a segment of video, and y the next segment of the video. Inputs x and y are fed to trainable encoders that extract abstract representations of them, s x and s y . A predictor module is trained to predict s y from s x . The predictor may use a latent variable, z, to represent information present in s y that is not present in s x . The JEPA handles uncertainty in predictions in two ways: (1) The encoder may choose to drop information about y that is difficult to predict, (2) the latent variable z, when varied over a set, will cause the prediction to vary over a set of plausible predictions.

How do we train a JEPA? Until recently, the only approach would have been to use contrastive methods, which consist of showing examples of compatible x and y, together with numerous examples of x and incompatible y’s. But this is rather impractical when the representations are high-dimensional. An alternative training strategy has emerged in the last two years: regularized methods. When applied to JEPA, the method uses four criteria:

Make the representation of x maximally informative about x Make the representation of y maximally informative about y Make the representation of y maximally predictable from the representation of x Make the predictor use as little information as possible from the latent variable to represent uncertainty in the prediction.

These criteria can be translated into differentiable cost functions in various ways. One way is the VICReg method, which stands for Variance, Invariance, Covariance Regularization. In VICReg, the information content of the representations of x and y are maximized by maintaining the variances of their components over a threshold and by making these components as independent of each other as possible. Simultaneously, the model tries to make the representation of y predictable from that of x. Additionally, the information content of the latent variable is minimized by making it discrete, low-dimensional, sparse, or noisy.

The beauty of the JEPA is that it naturally produces informative abstract representations of the input that eliminate irrelevant details and with which predictions can be performed. This enables JEPAs to be stacked on top of one another so as to learn representations with higher levels of abstraction that can perform longer-term prediction. For example, a scenario can be described at a high level as “a cook is making crêpes.” One can predict that the cook will fetch flour, milk, and eggs; mix the ingredients; ladle batter into a pan; let the batter fry; flip the crêpe; and repeat. At a lower level, pouring a ladle involves scooping some batter and spreading it around the pan. This continues all the way down to the precise trajectories of the chef’s hands millisecond by millisecond. At the low level of hand trajectories, our world model can only make accurate predictions in the short term. But at a higher level of abstraction, it can make long-term predictions.

The Hierarchical JEPA can be used to perform predictions at several levels of abstraction and several time scales. How can it be trained? Largely by passive observation, and less often by interaction.

A baby learns how the world works largely by observation in the first few months of life. She learns that the world is three-dimensional, that some objects are in front of others, that when an object is occluded it still exists. Eventually, around nine months of age, babies learn intuitive physics — for example, that unsupported objects fall through gravity.

The hope is that a hierarchical JEPA could learn how the world works by watching videos and interacting with its environment. By training itself to predict what will happen in the video, it will produce hierarchical representations of the world. By taking actions in the world and observing the result, the world model will learn to predict the consequences of its actions, which will allow it to reason and plan.

A perception-action episode

With a Hierarchical JEPA properly trained as a world model, an agent could perform hierarchical planning of complex actions, decomposing a complex task into a sequence of less complex and less abstract subtasks, all the way down to low-level actions on effectors.

A typical perception-action episode would go as follows. The diagram illustrates the situation for a two-level hierarchy. The perception module extracts a hierarchical representation of the state of the world (s1[0]=Enc1(x) and s2[0]=Enc2(s[0]) in the diagram). Then, the second-level predictor is applied multiple times to predict future states, given a sequence of abstract actions proposed by the second-level actor. The actor optimizes the second-level action sequence so as to minimize the overall cost (C(s2[4]) in the diagram). This process is akin to Model-Predictive Control in optimal control. The process is repeated for multiple drawings of the second-level latent variables, which may produce different high-level scenarios. The resulting high-level actions do not constitute real actions but merely define constraints that the lower-level state sequence must satisfy (e.g., are the ingredients properly mixed?). They really constitute subgoals. The entire process is repeated at the lower level: running the lower-level predictor, optimizing the low-level action sequence to minimize the intermediate costs coming from the upper layer, and repeating the process for multiple drawings of the low-level latent variables. Once the process is complete, the agent outputs the first low-level action to the effectors, and the whole episode can be repeated.

If we are successful at building such a model, all the modules would be differentiable, so that this whole action optimization process could be performed using gradient-based methods.

Moving closer to human-level intelligence in AI

LeCun’s vision requires much deeper exploration than is possible in a brief blog post, and many difficult challenges lie ahead. One of the most interesting and difficult of these is instantiating the details of the architectures and training procedures for the world model. In fact, it could be argued that training world models constitutes the main challenge toward real progress in AI over the next decades.

But many other aspects of the architecture are still to be defined, including how precisely to train the critic, how to construct and train the configurator, and how to use the short-term memory to keep track of the world state and to store a history of world states, actions, and associated intrinsic cost to tune the critic.

LeCun and other Meta AI researchers look forward to exploring these in the months and years ahead, as well as exchanging ideas and learning from others in the field. Creating machines that can learn and understand as effectively as humans is a long-term scientific endeavor — and one with no guarantees of success. But we are confident that fundamental research will continue to produce a deeper understanding of both minds and machines, and will lead to advances that benefit everyone who uses AI.

Watch the Meta AI Inside the Lab event here."
Meta_Blog,https://ai.meta.com/blog/meta-ai-inside-the-lab/,,Inside the Lab: AI breakthroughs that will power the metaverse,"Building for the metaverse is the most ambitious long-term project Meta has ever attempted, and the experiences we’re envisioning are impossible to deliver with the software and hardware that exists today. Getting there will require major advances in almost every technology we work with.

A common link between many of these advances, from ultra-realistic immersive visuals to miniaturized devices capable of high-performance computing, is that they will be enabled by breakthroughs in artificial intelligence (AI).

Meta’s AI labs are already making these breakthroughs, as part of a long-term effort to create foundational technologies to enable the next era of computing. Today we are showcasing some of this work, such as our Builder Bot demo, which enables people to generate or import things into a virtual world just by using voice commands. And we’re announcing new research advances and sharing more about where we see progress heading in 2022 and beyond. Here’s some of what we are sharing in today’s Meta AI: Inside the Lab event, which you can watch here:

As we build for the metaverse, we’ll need AI to do much of the heavy lifting that makes next-generation computing experiences possible. This means continuing to break ground in areas like self-supervised learning, so we aren’t dependent on limited labeled data sets, and truly multimodal AI, so we can accurately interpret and predict the kind of interactions that will take place in persistent, virtual 3D spaces with lots of participants. It also includes our efforts in building the world’s most powerful AI supercomputer, to ensure that we have the compute resources needed to drive the future of AI research breakthroughs. We are at the beginning of the journey, and today’s advances will provide a snapshot of what’s possible through the power of AI and open science.

Watch the Meta AI Inside the Lab event here."
Meta_Blog,https://ai.meta.com/blog/meta-ai-inside-the-lab/,,Inside the Lab: AI breakthroughs that will power the metaverse,"Building for the metaverse is the most ambitious long-term project Meta has ever attempted, and the experiences we’re envisioning are impossible to deliver with the software and hardware that exists today. Getting there will require major advances in almost every technology we work with.

A common link between many of these advances, from ultra-realistic immersive visuals to miniaturized devices capable of high-performance computing, is that they will be enabled by breakthroughs in artificial intelligence (AI).

Meta’s AI labs are already making these breakthroughs, as part of a long-term effort to create foundational technologies to enable the next era of computing. Today we are showcasing some of this work, such as our Builder Bot demo, which enables people to generate or import things into a virtual world just by using voice commands. And we’re announcing new research advances and sharing more about where we see progress heading in 2022 and beyond. Here’s some of what we are sharing in today’s Meta AI: Inside the Lab event, which you can watch here:

As we build for the metaverse, we’ll need AI to do much of the heavy lifting that makes next-generation computing experiences possible. This means continuing to break ground in areas like self-supervised learning, so we aren’t dependent on limited labeled data sets, and truly multimodal AI, so we can accurately interpret and predict the kind of interactions that will take place in persistent, virtual 3D spaces with lots of participants. It also includes our efforts in building the world’s most powerful AI supercomputer, to ensure that we have the compute resources needed to drive the future of AI research breakthroughs. We are at the beginning of the journey, and today’s advances will provide a snapshot of what’s possible through the power of AI and open science.

Watch the Meta AI Inside the Lab event here."
Meta_Blog,https://ai.meta.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/,,"System Cards, a new resource for understanding how AI systems work","AI powers back-end services like personalization, recommendation, and ranking that help enable a seamless, customizable experience for people who use our products and services. But understanding how and why AI operates can be difficult for everyday users and others. We’re aiming to change that.

At Meta, we believe it’s important to empower people with tools and resources that help them to understand how AI shapes their product experiences, which is why we’ve committed to Transparency and Control as one of our five pillars of Responsible AI. One of the ways we are exploring increased explainability is through model and system documentation. Today, we are sharing the next step in this journey by publishing a prototype AI System Card tool that is designed to provide insight into an AI system’s underlying architecture and help better explain how the AI operates.

This inaugural AI System Card outlines the AI models that comprise an AI system and can help enable a better understanding of how these systems operate based on an individual’s history, preferences, settings, and more. The pilot System Card we’ve developed, and continue to test, is for Instagram feed ranking, which is the process of taking as-yet-unseen posts from accounts that a person follows and then ranking them based on how likely that person is to be interested in them.

Developing new tools to make AI more explainable

Making AI more explainable is a cross-industry, cross-disciplinary dialogue. Companies, regulators, and academics are all testing ways of better communicating how AI works through various forms of guidance and frameworks that can empower everyday people with more AI knowledge.

Because AI systems are complex, it is both important and challenging to develop documentation that consistently addresses people’s need for transparency and their desire for simplicity in such explanations. As such, data sheets, model cards, System Cards, and fact sheets have different audiences in mind. We hope that a System Card can be understood by experts and nonexperts and can provide a unique in-depth view into the very complex world of AI system to human interface in a repeatable and scalable way for Meta. Providing a framework that is technically accurate, able to capture the nuance of how AI systems operate at Meta’s scale, and is easily digestible for everyday people using our technologies is a delicate balance, especially as we continue to push the state of the art in the field.

With this AI System Card release and continued exploration, we hope to lay a foundation for, and continually iterate, what elements of the AI system should be discussed, at which intervention points, and with which audiences via simple, easily digestible external tools.

System Cards as a first step

Many machine learning (ML) models are typically part of a larger AI system, a group of ML models, AI and non-AI technologies that work together to achieve specific tasks. Because ML models don’t always work in isolation to produce outcomes, and models may interact differently depending on what systems they’re a part of, model cards — a broadly accepted standard for model documentation — don’t paint a comprehensive picture of what an AI system does. For example, while our image classification models are all designed to predict what’s in a given image, they may be used differently in an integrity system that flags harmful content versus a recommender system used to show people posts they might be interested in.

After consulting with external experts both in the United States and abroad, Meta’s Responsible AI (RAI) team chose to explore System Cards as our initial approach to looking holistically across an AI system, versus one-off models. Feedback from parties the team consulted helped solidify this approach, as it emphasized the importance of understanding how the outputs of a model are used in a wider product, or downstream in other models, as well as what policy actions result from their use and what impact they have on groups of people who use a given AI-powered product or service.

Instagram feed ranking pilot

The pilot project System Card, launched by Instagram’s Equity team builds on their prior introduction of Model Cards. As the initial driver for that workstream, Instagram’s models provided a well-documented case for us to exemplify how our systems actually work, bringing us here with an AI System Card to show, as well as tell, how the AI feed ranking system dynamically works to deliver a personalized experience.

System Cards’ limitations and continued iteration

While System Cards help explain how an AI system functions in a digestible format, they also have some limitations. Here we outline a few limitations we’re considering as we progress this work:

System Cards aren’t definitive because AI systems constantly learn and evolve

AI systems are designed to learn and change constantly, requiring continuous updates. And at Meta, many of our systems do not consist of AI exclusively; humans are in the loop as well. Even then, a single System Card may not be relevant in the same way to each person that sees it because we continue to test new experiences for our users. Time stamps letting people know when a System Card was last updated are one possible mitigation we’ll explore.

Technical information can be difficult to simplify, and the landscape changes in real time

Our models do not exist in a vacuum, and while we continue to provide more transparency into how they work, they are always evolving. Our goal here is to make it easier to understand the principles behind what our systems recognize and recommend, rather than offer a playbook on what’s to succeed, or outline the rules of the platform. Translating highly technical information into general terms that can be widely understood, while still being accurate, is a challenge. The addition or deletion of a single word or phrase has the potential to compromise or invalidate the technical explanation. Because of Meta’s global scale, we also have to take into consideration language barriers and translation, as well as the difference in meaning that a technical term or code has once we’ve given it a simplified explanation.

Need to consider unintended consequences

The technology behind our systems are still powered by people — and we must diligently continue ensuring that everything is being done to make the systems as fair as possible. In Instagram’s introduction of Model Cards, the primary goal was to provide a specific set of checks along the way, to make sure teams were thinking about the unintended consequences of what they were launching before it impacted the community.

Transparency and security can sometimes be in tension

Revealing the exact workings of certain AI systems could compromise the systems’ security or open up a model to adversarial attacks, thus potentially harming the people who use our products. Too much information in some of our System Cards could give malicious actors enough knowledge about a system or model to reverse-engineer it, but we believe it’s important to educate our users about how our AI systems work, and we seek to strike the right balance of transparency.

What’s next? The future of AI transparency at Meta

System Cards can serve as an important step in the journey toward helping people understand what AI transparency looks like at Meta’s scale. As the industry evolves and discussions about model documentation and transparency continue, we will continue to identify other pilots to undertake and iterate on our approach over time, so we can reflect product changes, evolving industry standards, and expectations around AI transparency.

To learn more, check out this technical paper that explains the research that led to developing System Cards."
Meta_Blog,https://ai.meta.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/,,"System Cards, a new resource for understanding how AI systems work","AI powers back-end services like personalization, recommendation, and ranking that help enable a seamless, customizable experience for people who use our products and services. But understanding how and why AI operates can be difficult for everyday users and others. We’re aiming to change that.

At Meta, we believe it’s important to empower people with tools and resources that help them to understand how AI shapes their product experiences, which is why we’ve committed to Transparency and Control as one of our five pillars of Responsible AI. One of the ways we are exploring increased explainability is through model and system documentation. Today, we are sharing the next step in this journey by publishing a prototype AI System Card tool that is designed to provide insight into an AI system’s underlying architecture and help better explain how the AI operates.

This inaugural AI System Card outlines the AI models that comprise an AI system and can help enable a better understanding of how these systems operate based on an individual’s history, preferences, settings, and more. The pilot System Card we’ve developed, and continue to test, is for Instagram feed ranking, which is the process of taking as-yet-unseen posts from accounts that a person follows and then ranking them based on how likely that person is to be interested in them.

Developing new tools to make AI more explainable

Making AI more explainable is a cross-industry, cross-disciplinary dialogue. Companies, regulators, and academics are all testing ways of better communicating how AI works through various forms of guidance and frameworks that can empower everyday people with more AI knowledge.

Because AI systems are complex, it is both important and challenging to develop documentation that consistently addresses people’s need for transparency and their desire for simplicity in such explanations. As such, data sheets, model cards, System Cards, and fact sheets have different audiences in mind. We hope that a System Card can be understood by experts and nonexperts and can provide a unique in-depth view into the very complex world of AI system to human interface in a repeatable and scalable way for Meta. Providing a framework that is technically accurate, able to capture the nuance of how AI systems operate at Meta’s scale, and is easily digestible for everyday people using our technologies is a delicate balance, especially as we continue to push the state of the art in the field.

With this AI System Card release and continued exploration, we hope to lay a foundation for, and continually iterate, what elements of the AI system should be discussed, at which intervention points, and with which audiences via simple, easily digestible external tools.

System Cards as a first step

Many machine learning (ML) models are typically part of a larger AI system, a group of ML models, AI and non-AI technologies that work together to achieve specific tasks. Because ML models don’t always work in isolation to produce outcomes, and models may interact differently depending on what systems they’re a part of, model cards — a broadly accepted standard for model documentation — don’t paint a comprehensive picture of what an AI system does. For example, while our image classification models are all designed to predict what’s in a given image, they may be used differently in an integrity system that flags harmful content versus a recommender system used to show people posts they might be interested in.

After consulting with external experts both in the United States and abroad, Meta’s Responsible AI (RAI) team chose to explore System Cards as our initial approach to looking holistically across an AI system, versus one-off models. Feedback from parties the team consulted helped solidify this approach, as it emphasized the importance of understanding how the outputs of a model are used in a wider product, or downstream in other models, as well as what policy actions result from their use and what impact they have on groups of people who use a given AI-powered product or service.

Instagram feed ranking pilot

The pilot project System Card, launched by Instagram’s Equity team builds on their prior introduction of Model Cards. As the initial driver for that workstream, Instagram’s models provided a well-documented case for us to exemplify how our systems actually work, bringing us here with an AI System Card to show, as well as tell, how the AI feed ranking system dynamically works to deliver a personalized experience.

System Cards’ limitations and continued iteration

While System Cards help explain how an AI system functions in a digestible format, they also have some limitations. Here we outline a few limitations we’re considering as we progress this work:

System Cards aren’t definitive because AI systems constantly learn and evolve

AI systems are designed to learn and change constantly, requiring continuous updates. And at Meta, many of our systems do not consist of AI exclusively; humans are in the loop as well. Even then, a single System Card may not be relevant in the same way to each person that sees it because we continue to test new experiences for our users. Time stamps letting people know when a System Card was last updated are one possible mitigation we’ll explore.

Technical information can be difficult to simplify, and the landscape changes in real time

Our models do not exist in a vacuum, and while we continue to provide more transparency into how they work, they are always evolving. Our goal here is to make it easier to understand the principles behind what our systems recognize and recommend, rather than offer a playbook on what’s to succeed, or outline the rules of the platform. Translating highly technical information into general terms that can be widely understood, while still being accurate, is a challenge. The addition or deletion of a single word or phrase has the potential to compromise or invalidate the technical explanation. Because of Meta’s global scale, we also have to take into consideration language barriers and translation, as well as the difference in meaning that a technical term or code has once we’ve given it a simplified explanation.

Need to consider unintended consequences

The technology behind our systems are still powered by people — and we must diligently continue ensuring that everything is being done to make the systems as fair as possible. In Instagram’s introduction of Model Cards, the primary goal was to provide a specific set of checks along the way, to make sure teams were thinking about the unintended consequences of what they were launching before it impacted the community.

Transparency and security can sometimes be in tension

Revealing the exact workings of certain AI systems could compromise the systems’ security or open up a model to adversarial attacks, thus potentially harming the people who use our products. Too much information in some of our System Cards could give malicious actors enough knowledge about a system or model to reverse-engineer it, but we believe it’s important to educate our users about how our AI systems work, and we seek to strike the right balance of transparency.

What’s next? The future of AI transparency at Meta

System Cards can serve as an important step in the journey toward helping people understand what AI transparency looks like at Meta’s scale. As the industry evolves and discussions about model documentation and transparency continue, we will continue to identify other pilots to undertake and iterate on our approach over time, so we can reflect product changes, evolving industry standards, and expectations around AI transparency.

To learn more, check out this technical paper that explains the research that led to developing System Cards."
Meta_Blog,https://ai.meta.com/blog/project-cairaoke/,,Project CAIRaoke: Building the assistants of the future with breakthroughs in conversational AI,"If we could interact with an AI assistant in natural, conversational language, the same way we interact with people, it could make our lives easier in countless ways. But assistants today are underwhelming, whether we’re interacting with them via voice or text. They are easily stumped by routine requests like “Silence all notifications for the rest of the day, unless it’s my mom calling,” let alone questions like “Can I rent the local community center for a private party?” or tasks like “Plan an affordable family beach vacation for the Fourth of July weekend.”

It’s time for better conversational AI.

To get us there, we’re excited to announce Project CAIRaoke. We developed an end-to-end neural model that can power much more personal and contextual conversations than the systems people are familiar with today. We’re already using the model that resulted from Project CAIRaoke in one of our products, Portal, and we aim to integrate it with augmented and virtual reality devices to enable immersive, multimodal interactions with assistants in the future.

Perhaps the largest obstacle to better conversational AI has been the architecture that powers even today’s most advanced assistants. Even though these systems provide one single service, they actually rely on four separate components: natural language understanding (NLU), dialog state tracking (DST), dialog policy (DP) management, and natural language generation (NLG). These distinct AI systems must then be linked together, which makes them difficult to optimize, poor at adapting to new or unfamiliar tasks, and highly dependent on labor-intensive annotated data sets.

This is one reason why the digital assistants that power most devices today keep the user in a box with limited options, forget the context of conversation, and follow mostly prescribed dialog flows. You might be able to ask an assistant for the local weather forecast, for example, but it will be flummoxed if you follow up with something simple but unexpected, like “Is it hotter than it was last week?”

With models created with Project CAIRaoke, people will be able to talk naturally with their conversational assistants, so they can refer back to something from earlier in a conversation, change topics altogether, or mention things that rely on understanding complex, nuanced context. They will also be able to interact with them in new ways, such as by using gestures.

Something Went Wrong We're having trouble playing this video. Learn more

We’ve begun using the model on Portal, Meta’s video calling device, to make it easier to create and manage reminders. For example, you can quickly clarify a request like the following without having to repeat it:

👩‍: Set a reminder for 6:30.

✅ : Is that in the morning or evening?

👩‍: In the evening and let’s call it buy eggs.

✅ : OK, your reminder to buy eggs is set for 6:30 PM tomorrow.

Even in this early test, we believe the model outperforms standard approaches. On Portal, we observed a significant improvement compared with our existing approach in the evaluation of the reminders domain as measured by the success rate of completing a set of reminders goals, while maintaining on-par number of turns.

But this is just a first step toward leveraging this new technology. We believe that the progress made with Project CAIRaoke will enable us to deliver richer communication between people and AI that will be an essential tool as we build for the metaverse. A Project CAIRaoke-powered assistant built into AR glasses could one day follow along in many new and useful ways. For example, if you asked, “What goes with these pants?” it could respond, “Here’s a shirt in your favorite color, red,” and show an image of an item it found for you. And if you said, “I like it, but the stripes are too broad,” it would show you a pinstriped version instead.

In the future, we hope to leverage models that result from this project in everyday applications like this for millions of people around the world.

Building truly interactive conversational AI

One necessary step in advancing conversational AI is understanding the full scope of the problem. Many people see the numerous recent advances in NLU, such as BART and GPT-3, and think the challenge of understanding and generating human-like text has been solved. To discern why we’re not there yet, we have to tease apart AI for understanding and AI for interaction. The former is well researched and developed across the industry. It’s used to extract meaning from various input modalities, such as automatic speech recognition, image classification, and NLU. The latter is how we use our understanding of the world to interact with other people using technology. This can be sending a text, a voice command, haptic feedback, showing an image, a video, an avatar face, or a combination of all these.

Researchers and engineers across the industry agree that good conversational systems need a solid understanding layer powered by AI models. But many feel interaction is an engineering problem, rather than an AI problem. Hence an engineer who knows the state of the world can create an elaborate logic to handle the required interaction. The engineering approach makes it easy to understand how the system works and to quickly debug the logic when necessary. Yet this common belief leads to less robust conversational AI — which is a major reason why you can’t easily plan your vacation through such assistants.

A new, unified approach

These sample dialogs show key skills we want assistants to have: Not just providing accurate, up-to-date real-world knowledge but also working multimodally (in this case, across vision and speech), working across domains (sending a message and also estimating your time of arrival), and letting you drive the conversation rather than needing to conform to a rigid conversational template.

The canonical approach for AI-powered assistants requires four sets of inputs and outputs — one for each layer of the pipeline (NLU, DST, DP, and NLG). And it also requires defined standards for inputs and outputs for each layer. For example, for NLU, a traditional conversational AI system requires defined ontologies (e.g., various intents and entities).

Our model, however, uses a neural network and doesn’t prescribe conversational flow at all. With this model, we need just one set of training data.

Something Went Wrong We're having trouble playing this video. Learn more

Project CAIRaoke reduces the work required to add a new domain. In the canonical approach, expanding to a new domain requires sequentially building and fixing each module before the next one can be trained reliably. In other words, training DP cannot be done effectively if NLU and DST change daily. Changes in one component could break the others, triggering a retraining of all subsequent modules. This interdependency slows down progress in subsequent modules. But with our end-to-end technique, we remove this dependency on upstream modules, boosting the development and training speed, and enabling us to fine-tune other models with less effort and less data.

With our new approach, dialogs are much more robust because they’re able to make decisions by looking at the full range of information in a single place. Previously, even a small error in one component could propagate to other components in unexpected, difficult-to-address ways. For example, current rule-based assistants are explicitly programmed to look for specific words or phrases — “p.m.” after a number to indicate afternoon — whereas Project CAIRaoke leverages advanced pretrained language models that better understand context and can recognize different ways to say the same thing.

Finally, Project CAIRaoke fuses the technology supporting Meta AI’s latest conversational bot, BlenderBot 2.0, into task-oriented dialogs. This means that assistants built using our model could exhibit empathetic language, relay knowledge found by searching the internet in real time, and exhibit a consistent personality.

When systems generate natural language, it’s essential to address the potential safety and privacy challenges. Most NLG components today are scripted so that content moderators ensure assistants don’t provide objectionable responses to users. But by connecting the assistant directly to the user, there’s a danger of mistakes or offensive interactions, as has been widely and infamously seen.

Importantly, we’ve incorporated safeguards built into BlenderBot that will help reduce instances of offensive responses. We are also building assistant technology with privacy in mind. For example, with both Ray-Ban Stories and Portal, the use of voice commands is optional, you can view and delete your transcripts of your voice commands, and you always have the option to turn off voice storage.

To mitigate the risk of generating objectionable responses to users, the first milestone of Project CAIRaoke was to generate both dialog action and natural language. In the short term, we generate dialog actions and rely on a tested and tightly constrained NLG system to provide the user response. In the long term, we’ll expose the generated sentences after ensuring the end-to-end integrity of our model.

Another issue, which is shared by other kinds of NLP systems, is hallucination, which is when a model confidently states information that isn’t correct. This is a big challenge for end-to-end techniques, as models may be prone to introduce or alter entities in the dialog based on training data. For example, if you ask an assistant to “set a reminder to call Ankita,” it may set up a reminder to call Ankit, since Ankita is a less common name. We used various data augmentation techniques and attention networks to add robustness to Project CAIRaoke and leveraged our work with BlenderBot 2.0 to reduce hallucination.

Using voice for myriad everyday tasks

While our short-term implementation of the Project CAIRaoke model is for reminders on Portal, we hope to soon be utilizing it on much larger domains which will help personalize people’s shopping experiences, enable assistants to maintain context over numerous chats, and let people drive the flow of conversation.

We also think this advancement is particularly useful for building AI-driven dialog capabilities for augmented reality. In the not-too-distant future, people will regularly use voice assistants on their AR glasses as they do today with smart speakers, watches, and other devices. With that in mind, we are working to reduce the size of end-to-end models like this one to fit them on-device, since on-device models also offer additional security, privacy, and performance benefits. We are also working to make the model easier to debug — a complicated challenge because in this new framework, information is represented in the embedding space, whereas in the canonical model, it’s explicit. To fully realize our vision for Project CAIRaoke, we will also need to scale it to many languages and find ways to use the model efficiently at billion-scale.

We can imagine that in a few years, the technology from Project CAIRaoke will underlie next-generation interaction between people and devices. On devices like VR headsets and AR glasses, we expect this type of communication to eventually be the ubiquitous, seamless method for navigation and interaction, much as how touchscreens replaced keypads on smartphones. Our current model is an important step forward, but we have more work to do to fully realize this vision. We are excited by both the progress we’ve made so far and the challenges ahead.

Watch the Meta AI Inside the Lab event here.

This work is being undertaken by a multidisciplinary team that includes Chinnadhurai Sankar, Maryam Daneshi, Nicholas Flores, Ahmad Beirami, David Whitney, Kaushik Ram Sadagopan, Ankita De, Stephen Roller, Paul Crook, Xiaoyu Zhai, Jeet Shah, Moya Chen, Eric Smith, Melanie Kambadur, Mary Williamson, Tushar Agarwal, Zhe Zhang, Shahin Shayandeh, Christopher Lin, Zhi Liu, Pooyan Amini, Jeremiah Chung, Satwik Kottur, Alexander Zotov, Paul Lee, Kyle Archie, Gabrielle Moskey, Soumya Banerjee, Piyush Khemka, Zhiqi Wang, John Bourassa, Yang Liu, Gerald Demeunynck, and Becka Silvert."
Meta_Blog,https://ai.meta.com/blog/project-cairaoke/,,Project CAIRaoke: Building the assistants of the future with breakthroughs in conversational AI,"If we could interact with an AI assistant in natural, conversational language, the same way we interact with people, it could make our lives easier in countless ways. But assistants today are underwhelming, whether we’re interacting with them via voice or text. They are easily stumped by routine requests like “Silence all notifications for the rest of the day, unless it’s my mom calling,” let alone questions like “Can I rent the local community center for a private party?” or tasks like “Plan an affordable family beach vacation for the Fourth of July weekend.”

It’s time for better conversational AI.

To get us there, we’re excited to announce Project CAIRaoke. We developed an end-to-end neural model that can power much more personal and contextual conversations than the systems people are familiar with today. We’re already using the model that resulted from Project CAIRaoke in one of our products, Portal, and we aim to integrate it with augmented and virtual reality devices to enable immersive, multimodal interactions with assistants in the future.

Perhaps the largest obstacle to better conversational AI has been the architecture that powers even today’s most advanced assistants. Even though these systems provide one single service, they actually rely on four separate components: natural language understanding (NLU), dialog state tracking (DST), dialog policy (DP) management, and natural language generation (NLG). These distinct AI systems must then be linked together, which makes them difficult to optimize, poor at adapting to new or unfamiliar tasks, and highly dependent on labor-intensive annotated data sets.

This is one reason why the digital assistants that power most devices today keep the user in a box with limited options, forget the context of conversation, and follow mostly prescribed dialog flows. You might be able to ask an assistant for the local weather forecast, for example, but it will be flummoxed if you follow up with something simple but unexpected, like “Is it hotter than it was last week?”

With models created with Project CAIRaoke, people will be able to talk naturally with their conversational assistants, so they can refer back to something from earlier in a conversation, change topics altogether, or mention things that rely on understanding complex, nuanced context. They will also be able to interact with them in new ways, such as by using gestures.

Something Went Wrong We're having trouble playing this video. Learn more

We’ve begun using the model on Portal, Meta’s video calling device, to make it easier to create and manage reminders. For example, you can quickly clarify a request like the following without having to repeat it:

👩‍: Set a reminder for 6:30.

✅ : Is that in the morning or evening?

👩‍: In the evening and let’s call it buy eggs.

✅ : OK, your reminder to buy eggs is set for 6:30 PM tomorrow.

Even in this early test, we believe the model outperforms standard approaches. On Portal, we observed a significant improvement compared with our existing approach in the evaluation of the reminders domain as measured by the success rate of completing a set of reminders goals, while maintaining on-par number of turns.

But this is just a first step toward leveraging this new technology. We believe that the progress made with Project CAIRaoke will enable us to deliver richer communication between people and AI that will be an essential tool as we build for the metaverse. A Project CAIRaoke-powered assistant built into AR glasses could one day follow along in many new and useful ways. For example, if you asked, “What goes with these pants?” it could respond, “Here’s a shirt in your favorite color, red,” and show an image of an item it found for you. And if you said, “I like it, but the stripes are too broad,” it would show you a pinstriped version instead.

In the future, we hope to leverage models that result from this project in everyday applications like this for millions of people around the world.

Building truly interactive conversational AI

One necessary step in advancing conversational AI is understanding the full scope of the problem. Many people see the numerous recent advances in NLU, such as BART and GPT-3, and think the challenge of understanding and generating human-like text has been solved. To discern why we’re not there yet, we have to tease apart AI for understanding and AI for interaction. The former is well researched and developed across the industry. It’s used to extract meaning from various input modalities, such as automatic speech recognition, image classification, and NLU. The latter is how we use our understanding of the world to interact with other people using technology. This can be sending a text, a voice command, haptic feedback, showing an image, a video, an avatar face, or a combination of all these.

Researchers and engineers across the industry agree that good conversational systems need a solid understanding layer powered by AI models. But many feel interaction is an engineering problem, rather than an AI problem. Hence an engineer who knows the state of the world can create an elaborate logic to handle the required interaction. The engineering approach makes it easy to understand how the system works and to quickly debug the logic when necessary. Yet this common belief leads to less robust conversational AI — which is a major reason why you can’t easily plan your vacation through such assistants.

A new, unified approach

These sample dialogs show key skills we want assistants to have: Not just providing accurate, up-to-date real-world knowledge but also working multimodally (in this case, across vision and speech), working across domains (sending a message and also estimating your time of arrival), and letting you drive the conversation rather than needing to conform to a rigid conversational template.

The canonical approach for AI-powered assistants requires four sets of inputs and outputs — one for each layer of the pipeline (NLU, DST, DP, and NLG). And it also requires defined standards for inputs and outputs for each layer. For example, for NLU, a traditional conversational AI system requires defined ontologies (e.g., various intents and entities).

Our model, however, uses a neural network and doesn’t prescribe conversational flow at all. With this model, we need just one set of training data.

Something Went Wrong We're having trouble playing this video. Learn more

Project CAIRaoke reduces the work required to add a new domain. In the canonical approach, expanding to a new domain requires sequentially building and fixing each module before the next one can be trained reliably. In other words, training DP cannot be done effectively if NLU and DST change daily. Changes in one component could break the others, triggering a retraining of all subsequent modules. This interdependency slows down progress in subsequent modules. But with our end-to-end technique, we remove this dependency on upstream modules, boosting the development and training speed, and enabling us to fine-tune other models with less effort and less data.

With our new approach, dialogs are much more robust because they’re able to make decisions by looking at the full range of information in a single place. Previously, even a small error in one component could propagate to other components in unexpected, difficult-to-address ways. For example, current rule-based assistants are explicitly programmed to look for specific words or phrases — “p.m.” after a number to indicate afternoon — whereas Project CAIRaoke leverages advanced pretrained language models that better understand context and can recognize different ways to say the same thing.

Finally, Project CAIRaoke fuses the technology supporting Meta AI’s latest conversational bot, BlenderBot 2.0, into task-oriented dialogs. This means that assistants built using our model could exhibit empathetic language, relay knowledge found by searching the internet in real time, and exhibit a consistent personality.

When systems generate natural language, it’s essential to address the potential safety and privacy challenges. Most NLG components today are scripted so that content moderators ensure assistants don’t provide objectionable responses to users. But by connecting the assistant directly to the user, there’s a danger of mistakes or offensive interactions, as has been widely and infamously seen.

Importantly, we’ve incorporated safeguards built into BlenderBot that will help reduce instances of offensive responses. We are also building assistant technology with privacy in mind. For example, with both Ray-Ban Stories and Portal, the use of voice commands is optional, you can view and delete your transcripts of your voice commands, and you always have the option to turn off voice storage.

To mitigate the risk of generating objectionable responses to users, the first milestone of Project CAIRaoke was to generate both dialog action and natural language. In the short term, we generate dialog actions and rely on a tested and tightly constrained NLG system to provide the user response. In the long term, we’ll expose the generated sentences after ensuring the end-to-end integrity of our model.

Another issue, which is shared by other kinds of NLP systems, is hallucination, which is when a model confidently states information that isn’t correct. This is a big challenge for end-to-end techniques, as models may be prone to introduce or alter entities in the dialog based on training data. For example, if you ask an assistant to “set a reminder to call Ankita,” it may set up a reminder to call Ankit, since Ankita is a less common name. We used various data augmentation techniques and attention networks to add robustness to Project CAIRaoke and leveraged our work with BlenderBot 2.0 to reduce hallucination.

Using voice for myriad everyday tasks

While our short-term implementation of the Project CAIRaoke model is for reminders on Portal, we hope to soon be utilizing it on much larger domains which will help personalize people’s shopping experiences, enable assistants to maintain context over numerous chats, and let people drive the flow of conversation.

We also think this advancement is particularly useful for building AI-driven dialog capabilities for augmented reality. In the not-too-distant future, people will regularly use voice assistants on their AR glasses as they do today with smart speakers, watches, and other devices. With that in mind, we are working to reduce the size of end-to-end models like this one to fit them on-device, since on-device models also offer additional security, privacy, and performance benefits. We are also working to make the model easier to debug — a complicated challenge because in this new framework, information is represented in the embedding space, whereas in the canonical model, it’s explicit. To fully realize our vision for Project CAIRaoke, we will also need to scale it to many languages and find ways to use the model efficiently at billion-scale.

We can imagine that in a few years, the technology from Project CAIRaoke will underlie next-generation interaction between people and devices. On devices like VR headsets and AR glasses, we expect this type of communication to eventually be the ubiquitous, seamless method for navigation and interaction, much as how touchscreens replaced keypads on smartphones. Our current model is an important step forward, but we have more work to do to fully realize this vision. We are excited by both the progress we’ve made so far and the challenges ahead.

Watch the Meta AI Inside the Lab event here.

This work is being undertaken by a multidisciplinary team that includes Chinnadhurai Sankar, Maryam Daneshi, Nicholas Flores, Ahmad Beirami, David Whitney, Kaushik Ram Sadagopan, Ankita De, Stephen Roller, Paul Crook, Xiaoyu Zhai, Jeet Shah, Moya Chen, Eric Smith, Melanie Kambadur, Mary Williamson, Tushar Agarwal, Zhe Zhang, Shahin Shayandeh, Christopher Lin, Zhi Liu, Pooyan Amini, Jeremiah Chung, Satwik Kottur, Alexander Zotov, Paul Lee, Kyle Archie, Gabrielle Moskey, Soumya Banerjee, Piyush Khemka, Zhiqi Wang, John Bourassa, Yang Liu, Gerald Demeunynck, and Becka Silvert."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time/,,Teaching AI to translate 100s of spoken and written languages in real time,"Something Went Wrong We're having trouble playing this video. Learn more

For people who understand languages like English, Mandarin, or Spanish, it may seem like today’s apps and web tools already provide the translation technology we need. But billions of people are being left out — unable to easily access most of the information on the internet or connect with most of the online world in their native language. Today’s machine translation (MT) systems are improving rapidly, but they still rely heavily on learning from large amounts of textual data, so they do not generally work well for low-resource languages, i.e., languages that lack training data, and for languages that don’t have a standardized writing system.

Eliminating language barriers would be profound, making it possible for billions of people to access information online in their native or preferred languages. Advances in MT won’t just help those people who don’t speak one of the languages that dominates the internet today; they’ll also fundamentally change the way people in the world connect and share ideas.

Something Went Wrong We're having trouble playing this video. Learn more

Imagine, for example, people in a marketplace who speak different languages being able to communicate with one another in real time using a phone, watch, or glasses. Or multimedia content on the web that’s accessible to anyone in the world in their preferred language. In the not too distant future, when emerging technologies like virtual and augmented reality bring the digital and physical worlds together in the metaverse, translation tools will enable people to do everyday activities — hosting a book club or collaborating on a work project — with anyone, anywhere, just as they would with someone next door.

Meta AI is announcing a long-term effort to build language and MT tools that will include most of the world’s languages. This includes two new projects. The first is No Language Left Behind, where we are building a new advanced AI model that can learn from languages with fewer examples to train from, and we will use it to enable expert-quality translations in hundreds of languages, ranging from Asturian to Luganda to Urdu. The second is Universal Speech Translator, where we are designing novel approaches to translating from speech in one language to another in real time so we can support languages without a standard writing system as well as those that are both written and spoken.

It will take much more work to provide everyone around the world with truly universal translation tools. But we believe the efforts described here are an important step forward. Sharing details and open-sourcing our code and models in the future means that others can build on our work and bring us closer to achieving this important goal.

The challenges of translating every language

The AI translation systems of today are not designed to serve the thousands of languages used around the world, or to provide real-time speech-to-speech translation. To truly serve everyone, the MT research community will need to overcome three important challenges. We will need to overcome data scarcity by acquiring more training data in more languages as well as finding new ways to leverage the data already available today. We’ll also need to overcome the modeling challenges that arise as models grow to serve many more languages. And we will need to find new ways to evaluate and improve on their results.

Data scarcity remains one of the biggest hurdles to expanding translation tools across more languages. MT systems for text translations typically rely on learning from millions of sentences of annotated data. Because of this, MT systems capable of high-quality translations have been developed for only the handful of languages that dominate the web. Expanding to other languages means finding ways to acquire and use training examples from languages with sparse web presences.

For direct speech-to-speech translation, the challenge of acquiring data is even more severe. Most speech MT systems use text as an intermediary step, meaning speech in one language is first converted to text, then translated to text in the target language, and then finally input into a text-to-speech system to generate audio. This makes speech-to-speech translations dependent on text in ways that limit their efficiency and make them difficult to scale to languages that are primarily oral.

Direct speech-to-speech translation models can enable translations for languages that don’t have standardized writing systems. This speech-based approach could also lead to much faster, more efficient translation systems, since they won’t require the additional steps of converting speech to text, translating it, and then generating speech in the target language.

In addition to their needing suitable training data in thousands of languages, MT systems today are simply not designed to scale to meet the needs of everyone around the globe. Many MT systems are bilingual, meaning there is a separate model for each language pair, such as English-Russian or Japanese-Spanish. This approach is extraordinarily difficult to scale to dozens of language pairs, let alone to all the languages in use around the world. Imagine needing to create and maintain many thousands of different models for every combination from Thai-Lao to Nepali-Assamese. Many experts have suggested that multilingual systems might be helpful here. But it has been tremendously difficult to incorporate many languages into a single efficient, high-performance multilingual model that has the capacity to represent all languages.

Real-time speech-to-speech MT models face many of the same challenges as text-based models but also need to overcome latency — the lag that occurs when one language is being translated to another — before they can be effectively used to enable real-time translations. The main challenge comes from the fact that a sentence can be spoken in different word orders in different languages. Even professional simultaneous interpreters lag behind the original speech by around three seconds. Consider a sentence in German, “Ich möchte alle Sprachen übersetzen,” and its equivalent in Spanish, “Quisiera traducir todos los idiomas.” Both mean “I would like to translate all languages.” But translating from German to English in real time would be more challenging because the verb “translate” appears at the end of the sentence, while the word order in Spanish and English is similar.

Finally, as we scale to more and more languages, we also need to develop new ways of evaluating the work produced by MT models. There are already resources to evaluate the quality of translations from, say, English to Russian, but what about from Amharic to Kazakh? As we expand the number of languages our MT models can translate, we’ll also have to develop new approaches to training data and measurement to cover more languages. Besides evaluating the performance of MT systems for accuracy, it’s also important to make sure that translations are being done responsibly. We’ll need to find ways to make sure that MT systems preserve cultural sensitivities and do not create or intensify biases. As we describe in the sections below, Meta AI is tackling each of these three challenges.

Training low-resource and direct speech-to-speech translation systems

To enable translations for low-resource languages and to create the building blocks for future translations of more languages no matter how widely written or spoken, we’re expanding our automatic data set creation techniques. One such technique is LASER, an open source toolkit that now encompasses more than 125 languages written in 28 different scripts.

LASER converts sentences of various languages into a single multilingual representation. Then we use large-scale multilingual similarity search to identify sentences that have a similar representation, i.e., are likely to have the same meaning in different languages. We used LASER to build systems like ccMatrix and ccAligned, which are capable of finding parallel texts on the internet. Because low-resource languages have little data available, we created a new teacher-student training method that enables LASER to focus on specific language subgroups — such as Bantu languages — and learn from much smaller data sets. This allows LASER to operate effectively at scale across languages. Each of these advances will allow us to cover more languages as we work toward scaling, improving, and expanding them to support mining for hundreds of languages, and eventually to every language with a writing system. We recently extended LASER to also work with speech: By building representations for speech and text in the same multilingual space, we are able to extract translations between speech in one language and text in another — or even direct speech-to-speech translations. With this method, we have already identified nearly 1,400 hours of aligned speech in French, German, Spanish, and English.

Text data is important but not sufficient for building translation tools to serve everyone’s needs. Speech translation benchmark data was previously available for a handful of languages, so we created CoVoST 2, which covers 22 languages and 36 language directions with different resource conditions. Moreover, it is difficult to find large amounts of audio in different languages. VoxPopuli, which contains 400,000 hours of speech in 23 languages, enables large-scale semisupervised and self-supervised learning for speech applications such as speech recognition and speech translation. VoxPopuli was subsequently used to build the largest open and universal pretrained model for 128 languages and speech tasks, including speech translation. This model improved the previous state of the art for speech-to-text translation from 21 languages into English by 7.4 BLEU on the CoVoST 2 data set.

Building models that work across many languages and different modalities

Something Went Wrong We're having trouble playing this video. Learn more

Besides producing more data for training MT systems and making them available to other researchers, we’re working to improve model capacity in order to handle translations between a much wider range of languages. MT systems today often work within a single modality and across a limited set of languages. If the model is too small to represent many languages, its performance might suffer, introducing inaccuracies with both text and speech translations. Innovations in modeling will help us create a future where translations move quickly and seamlessly across modalities, going from speech to text, text to speech, text to text, or speech to speech in a multitude of languages.

To achieve improved performance for our MT models, we invested heavily in creating models that train efficiently despite large capacity, focusing on sparsely gated mixture-of-expert models. By increasing model size and learning an automatic routing function so different tokens use different expert capacity, we were able to balance high-resource and low-resource translation performance.

To scale text-based MT to 101 languages, we created the first multilingual text translation system that is not English-centric. Bilingual systems usually work by first translating from the source language into English and then from English into the target language. To make these systems more efficient and higher quality, we eliminated English as a medium so that languages could be translated directly into other languages without going through English. While eliminating English increased the model’s capacity, multilingual models were previously unable to reach the same level of quality as customized bilingual systems. Recently, though, our multilingual translation system won the Workshop on Machine Translation competition, outperforming even the best bilingual models.

We aim for our technology to be inclusive: It should support both written languages and languages without a standard writing system. With that in mind, we are developing a speech-to-speech translation system that does not rely on generating an intermediate textual representation during inference. This approach has been demonstrated to be faster than a traditional cascaded system that combines separate speech recognition, machine translation, and speech synthesis models. With improved efficiency and a simpler architecture, direct speech-to-speech could unlock near human-quality real-time translation for future devices, like AR glasses. Finally, in order to create spoken translations that preserve the expressiveness and character in everyone’s speech, we are working to include some aspects of the input audio, such as intonation, in the generated audio translations.

Measuring success across hundreds of languages

Developing large-scale models that can translate between many more languages brings up an important question: How can we determine whether we have developed better data or better models? Evaluating a large-scale, multilingual model’s performance is tricky, especially because it requires us to have on-ground expertise in all the languages that the model covers — a challenge that is time-consuming, resource intensive, and often impractical.

We have created FLORES-101, the first multilingual translation evaluation data sets covering 101 languages, enabling researchers to rapidly test and improve upon multilingual translation models. Unlike existing data sets, FLORES-101 allows researchers to quantify the performance of systems through any language direction — not just translating into and out of English. For the millions of people worldwide who live in places with dozens of official languages, this enables the creation of translation systems that serve important real-world needs.

Using FLORES-101, we have collaborated with other leaders in the AI research community to advance multilingual low-resource translation. At the 2021 Workshop on Machine Translation, we hosted a shared task to collectively make progress in this domain. Researchers from all over the world participated, many focusing on languages that were personally relevant to them. We look forward to continuing to expand FLORES to cover hundreds of languages.

As we make tangible progress toward universal translation, we’re also focused on doing this work responsibly. We’re working with linguists to help us understand the challenges of producing accurate data set collections, and networks of evaluators to help us make sure that translations are accurate. We’re also conducting case studies with speakers of more than 20 languages to understand what translation features are important to people from different backgrounds and how they will be using the translations our AI models produce. There are many more aspects to developing universal translation responsibly, including mitigating bias and toxicity and preserving cultural sensitivities as information passes from one language to another. Achieving our long-term translation goals will require not just expertise in AI but also the sustained input of numerous experts, researchers, and individuals from around the world.

What’s next?

If No Language Left Behind and Universal Speech Translator, combined with the efforts of the MT research community, succeed in creating translation technologies that include everyone in the world, it will open up the digital and physical worlds in ways previously not possible. We’re already making advances in enabling translations for low-resource languages, a significant barrier to universal translation for most of the world’s population. By advancing and open-sourcing our work in corpus creation, multilingual modeling, and evaluation, we’re hoping that other researchers can build on this work and bring the real world uses of translation systems closer to reality.

Our ability to communicate is one of the most fundamental aspects of being human. Technologies — from the printing press to video chat — have often transformed our ways of communicating and sharing ideas. The power of these and other technologies will be extended when they can work in the same way for billions of people around the world — giving them similar access to information and letting them communicate with a much wider audience, regardless of the languages they speak or write. As we strive for a more inclusive and connected world, it’ll be even more important to break down existing barriers to information and opportunity by empowering people in their chosen languages.

This work is being undertaken by a multidisciplinary team that includes Yossi Adi, Bapi Akula, Pierre Andrews, Shruti Bhosale, Brian Bui, Onur Celebi, Peng-Jen Chen, James Cross, Ning Dong, Maha Elbayad, Gustavo Gandia Rivera, Cynthia Gao, Hongyu Gong, Vedanuj Goswami, Jiatao Gu, Kenneth Heafield, Kevin Heffernan, Wei-Ning Hsu, Semarley Jarrett, Kyle Johnson, Justine Kao, Elahe Kalbassi, Philipp Koehn, Janice Lam, Ann Lee, Daniel Licht, Xutai Ma, Jean Maillard, Brian O’Horo, Adam Polyak, Sravya Popuri, Christophe Ropers, Dirk Rowe, Safiyyah Saleem, Anna Sun, Chau Tran, Holger Schwenk, Shannon Spruit, Yun Tang, Changhan Wang, Jeff Wang, Guillaume Wenzek, and Al Youngblood."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time/,,Teaching AI to translate 100s of spoken and written languages in real time,"Something Went Wrong We're having trouble playing this video. Learn more

For people who understand languages like English, Mandarin, or Spanish, it may seem like today’s apps and web tools already provide the translation technology we need. But billions of people are being left out — unable to easily access most of the information on the internet or connect with most of the online world in their native language. Today’s machine translation (MT) systems are improving rapidly, but they still rely heavily on learning from large amounts of textual data, so they do not generally work well for low-resource languages, i.e., languages that lack training data, and for languages that don’t have a standardized writing system.

Eliminating language barriers would be profound, making it possible for billions of people to access information online in their native or preferred languages. Advances in MT won’t just help those people who don’t speak one of the languages that dominates the internet today; they’ll also fundamentally change the way people in the world connect and share ideas.

Something Went Wrong We're having trouble playing this video. Learn more

Imagine, for example, people in a marketplace who speak different languages being able to communicate with one another in real time using a phone, watch, or glasses. Or multimedia content on the web that’s accessible to anyone in the world in their preferred language. In the not too distant future, when emerging technologies like virtual and augmented reality bring the digital and physical worlds together in the metaverse, translation tools will enable people to do everyday activities — hosting a book club or collaborating on a work project — with anyone, anywhere, just as they would with someone next door.

Meta AI is announcing a long-term effort to build language and MT tools that will include most of the world’s languages. This includes two new projects. The first is No Language Left Behind, where we are building a new advanced AI model that can learn from languages with fewer examples to train from, and we will use it to enable expert-quality translations in hundreds of languages, ranging from Asturian to Luganda to Urdu. The second is Universal Speech Translator, where we are designing novel approaches to translating from speech in one language to another in real time so we can support languages without a standard writing system as well as those that are both written and spoken.

It will take much more work to provide everyone around the world with truly universal translation tools. But we believe the efforts described here are an important step forward. Sharing details and open-sourcing our code and models in the future means that others can build on our work and bring us closer to achieving this important goal.

The challenges of translating every language

The AI translation systems of today are not designed to serve the thousands of languages used around the world, or to provide real-time speech-to-speech translation. To truly serve everyone, the MT research community will need to overcome three important challenges. We will need to overcome data scarcity by acquiring more training data in more languages as well as finding new ways to leverage the data already available today. We’ll also need to overcome the modeling challenges that arise as models grow to serve many more languages. And we will need to find new ways to evaluate and improve on their results.

Data scarcity remains one of the biggest hurdles to expanding translation tools across more languages. MT systems for text translations typically rely on learning from millions of sentences of annotated data. Because of this, MT systems capable of high-quality translations have been developed for only the handful of languages that dominate the web. Expanding to other languages means finding ways to acquire and use training examples from languages with sparse web presences.

For direct speech-to-speech translation, the challenge of acquiring data is even more severe. Most speech MT systems use text as an intermediary step, meaning speech in one language is first converted to text, then translated to text in the target language, and then finally input into a text-to-speech system to generate audio. This makes speech-to-speech translations dependent on text in ways that limit their efficiency and make them difficult to scale to languages that are primarily oral.

Direct speech-to-speech translation models can enable translations for languages that don’t have standardized writing systems. This speech-based approach could also lead to much faster, more efficient translation systems, since they won’t require the additional steps of converting speech to text, translating it, and then generating speech in the target language.

In addition to their needing suitable training data in thousands of languages, MT systems today are simply not designed to scale to meet the needs of everyone around the globe. Many MT systems are bilingual, meaning there is a separate model for each language pair, such as English-Russian or Japanese-Spanish. This approach is extraordinarily difficult to scale to dozens of language pairs, let alone to all the languages in use around the world. Imagine needing to create and maintain many thousands of different models for every combination from Thai-Lao to Nepali-Assamese. Many experts have suggested that multilingual systems might be helpful here. But it has been tremendously difficult to incorporate many languages into a single efficient, high-performance multilingual model that has the capacity to represent all languages.

Real-time speech-to-speech MT models face many of the same challenges as text-based models but also need to overcome latency — the lag that occurs when one language is being translated to another — before they can be effectively used to enable real-time translations. The main challenge comes from the fact that a sentence can be spoken in different word orders in different languages. Even professional simultaneous interpreters lag behind the original speech by around three seconds. Consider a sentence in German, “Ich möchte alle Sprachen übersetzen,” and its equivalent in Spanish, “Quisiera traducir todos los idiomas.” Both mean “I would like to translate all languages.” But translating from German to English in real time would be more challenging because the verb “translate” appears at the end of the sentence, while the word order in Spanish and English is similar.

Finally, as we scale to more and more languages, we also need to develop new ways of evaluating the work produced by MT models. There are already resources to evaluate the quality of translations from, say, English to Russian, but what about from Amharic to Kazakh? As we expand the number of languages our MT models can translate, we’ll also have to develop new approaches to training data and measurement to cover more languages. Besides evaluating the performance of MT systems for accuracy, it’s also important to make sure that translations are being done responsibly. We’ll need to find ways to make sure that MT systems preserve cultural sensitivities and do not create or intensify biases. As we describe in the sections below, Meta AI is tackling each of these three challenges.

Training low-resource and direct speech-to-speech translation systems

To enable translations for low-resource languages and to create the building blocks for future translations of more languages no matter how widely written or spoken, we’re expanding our automatic data set creation techniques. One such technique is LASER, an open source toolkit that now encompasses more than 125 languages written in 28 different scripts.

LASER converts sentences of various languages into a single multilingual representation. Then we use large-scale multilingual similarity search to identify sentences that have a similar representation, i.e., are likely to have the same meaning in different languages. We used LASER to build systems like ccMatrix and ccAligned, which are capable of finding parallel texts on the internet. Because low-resource languages have little data available, we created a new teacher-student training method that enables LASER to focus on specific language subgroups — such as Bantu languages — and learn from much smaller data sets. This allows LASER to operate effectively at scale across languages. Each of these advances will allow us to cover more languages as we work toward scaling, improving, and expanding them to support mining for hundreds of languages, and eventually to every language with a writing system. We recently extended LASER to also work with speech: By building representations for speech and text in the same multilingual space, we are able to extract translations between speech in one language and text in another — or even direct speech-to-speech translations. With this method, we have already identified nearly 1,400 hours of aligned speech in French, German, Spanish, and English.

Text data is important but not sufficient for building translation tools to serve everyone’s needs. Speech translation benchmark data was previously available for a handful of languages, so we created CoVoST 2, which covers 22 languages and 36 language directions with different resource conditions. Moreover, it is difficult to find large amounts of audio in different languages. VoxPopuli, which contains 400,000 hours of speech in 23 languages, enables large-scale semisupervised and self-supervised learning for speech applications such as speech recognition and speech translation. VoxPopuli was subsequently used to build the largest open and universal pretrained model for 128 languages and speech tasks, including speech translation. This model improved the previous state of the art for speech-to-text translation from 21 languages into English by 7.4 BLEU on the CoVoST 2 data set.

Building models that work across many languages and different modalities

Something Went Wrong We're having trouble playing this video. Learn more

Besides producing more data for training MT systems and making them available to other researchers, we’re working to improve model capacity in order to handle translations between a much wider range of languages. MT systems today often work within a single modality and across a limited set of languages. If the model is too small to represent many languages, its performance might suffer, introducing inaccuracies with both text and speech translations. Innovations in modeling will help us create a future where translations move quickly and seamlessly across modalities, going from speech to text, text to speech, text to text, or speech to speech in a multitude of languages.

To achieve improved performance for our MT models, we invested heavily in creating models that train efficiently despite large capacity, focusing on sparsely gated mixture-of-expert models. By increasing model size and learning an automatic routing function so different tokens use different expert capacity, we were able to balance high-resource and low-resource translation performance.

To scale text-based MT to 101 languages, we created the first multilingual text translation system that is not English-centric. Bilingual systems usually work by first translating from the source language into English and then from English into the target language. To make these systems more efficient and higher quality, we eliminated English as a medium so that languages could be translated directly into other languages without going through English. While eliminating English increased the model’s capacity, multilingual models were previously unable to reach the same level of quality as customized bilingual systems. Recently, though, our multilingual translation system won the Workshop on Machine Translation competition, outperforming even the best bilingual models.

We aim for our technology to be inclusive: It should support both written languages and languages without a standard writing system. With that in mind, we are developing a speech-to-speech translation system that does not rely on generating an intermediate textual representation during inference. This approach has been demonstrated to be faster than a traditional cascaded system that combines separate speech recognition, machine translation, and speech synthesis models. With improved efficiency and a simpler architecture, direct speech-to-speech could unlock near human-quality real-time translation for future devices, like AR glasses. Finally, in order to create spoken translations that preserve the expressiveness and character in everyone’s speech, we are working to include some aspects of the input audio, such as intonation, in the generated audio translations.

Measuring success across hundreds of languages

Developing large-scale models that can translate between many more languages brings up an important question: How can we determine whether we have developed better data or better models? Evaluating a large-scale, multilingual model’s performance is tricky, especially because it requires us to have on-ground expertise in all the languages that the model covers — a challenge that is time-consuming, resource intensive, and often impractical.

We have created FLORES-101, the first multilingual translation evaluation data sets covering 101 languages, enabling researchers to rapidly test and improve upon multilingual translation models. Unlike existing data sets, FLORES-101 allows researchers to quantify the performance of systems through any language direction — not just translating into and out of English. For the millions of people worldwide who live in places with dozens of official languages, this enables the creation of translation systems that serve important real-world needs.

Using FLORES-101, we have collaborated with other leaders in the AI research community to advance multilingual low-resource translation. At the 2021 Workshop on Machine Translation, we hosted a shared task to collectively make progress in this domain. Researchers from all over the world participated, many focusing on languages that were personally relevant to them. We look forward to continuing to expand FLORES to cover hundreds of languages.

As we make tangible progress toward universal translation, we’re also focused on doing this work responsibly. We’re working with linguists to help us understand the challenges of producing accurate data set collections, and networks of evaluators to help us make sure that translations are accurate. We’re also conducting case studies with speakers of more than 20 languages to understand what translation features are important to people from different backgrounds and how they will be using the translations our AI models produce. There are many more aspects to developing universal translation responsibly, including mitigating bias and toxicity and preserving cultural sensitivities as information passes from one language to another. Achieving our long-term translation goals will require not just expertise in AI but also the sustained input of numerous experts, researchers, and individuals from around the world.

What’s next?

If No Language Left Behind and Universal Speech Translator, combined with the efforts of the MT research community, succeed in creating translation technologies that include everyone in the world, it will open up the digital and physical worlds in ways previously not possible. We’re already making advances in enabling translations for low-resource languages, a significant barrier to universal translation for most of the world’s population. By advancing and open-sourcing our work in corpus creation, multilingual modeling, and evaluation, we’re hoping that other researchers can build on this work and bring the real world uses of translation systems closer to reality.

Our ability to communicate is one of the most fundamental aspects of being human. Technologies — from the printing press to video chat — have often transformed our ways of communicating and sharing ideas. The power of these and other technologies will be extended when they can work in the same way for billions of people around the world — giving them similar access to information and letting them communicate with a much wider audience, regardless of the languages they speak or write. As we strive for a more inclusive and connected world, it’ll be even more important to break down existing barriers to information and opportunity by empowering people in their chosen languages.

This work is being undertaken by a multidisciplinary team that includes Yossi Adi, Bapi Akula, Pierre Andrews, Shruti Bhosale, Brian Bui, Onur Celebi, Peng-Jen Chen, James Cross, Ning Dong, Maha Elbayad, Gustavo Gandia Rivera, Cynthia Gao, Hongyu Gong, Vedanuj Goswami, Jiatao Gu, Kenneth Heafield, Kevin Heffernan, Wei-Ning Hsu, Semarley Jarrett, Kyle Johnson, Justine Kao, Elahe Kalbassi, Philipp Koehn, Janice Lam, Ann Lee, Daniel Licht, Xutai Ma, Jean Maillard, Brian O’Horo, Adam Polyak, Sravya Popuri, Christophe Ropers, Dirk Rowe, Safiyyah Saleem, Anna Sun, Chau Tran, Holger Schwenk, Shannon Spruit, Yun Tang, Changhan Wang, Jeff Wang, Guillaume Wenzek, and Al Youngblood."
Meta_Blog,https://ai.meta.com/blog/ai-learning-alliance/,,Meta’s AI Learning Alliance aims to expand pipeline of underrepresented students in AI,"Meta is announcing the Artificial Intelligence Learning Alliance (AILA), an initiative to strengthen diversity and increase equity in the field of artificial intelligence.

Together with Georgia Tech, we built a deep learning course curriculum that has been taken online by 2,400 students since its inception in the Fall of 2020. Now we are making the course content available free to all and are working with professors at historically Black colleges and universities (HBCUs), Hispanic-serving institutions (HSIs), and Asian American and Native American Pacific Islander–serving institutions (AANAPISIs) in our newly established consortium to further develop and teach the curriculum. These universities will also be coming together to contribute additional cutting-edge AI education lectures and resources for all to use.

The curriculum available now has been created in partnership with expert educators and is designed to be taken in tandem with enrollment in official degree programs. The shared goal of AILA’s creators is to have more people from underrepresented groups trained in the technology of artificial intelligence. That is why the course is now available to everyone for free via the newly created AILA Education Hub, where students can access AI course content, educators can collaborate and receive training on curriculum, and employers can connect students with job opportunities.

Click here to access the existing curriculum.

Artificial intelligence (AI) offers important benefits for people in areas such as transportation, commerce, and beyond. While AI is constantly improving at understanding the dynamics of the world we live in, there is much more work to do. We’re firm believers that it will progress faster with early involvement from experts with diverse perspectives. If the researchers, developers, and engineers advancing AI come from a limited variety of backgrounds, the technology’s point of view will be similarly limited.

Addressing this starts in classrooms. That’s why Meta, together with Georgia Tech, created the AI Learning Alliance (AILA), an expansion of the co-teaching program that we piloted in the Fall of 2020. Meta AI researchers both developed the curriculum for and guest-lectured in the deep learning course that Georgia Tech assistant professor Zsolt Kira has taught for the past two years as part of the university’s Online Master’s of Science in Computer Science program (OMSCS). The goal was to help students experience real-world examples and learn valuable techniques used in deploying and scaling algorithms.

The statistics above were taken from the 50th annual Taulbee Survey. (https://cra.org/wp-content/uploads/2021/05/2020-CRA-Taulbee-Survey.pdf)

The original plan was to teach the course on campus in Atlanta, but due to the pandemic, all course instruction was moved to a virtual model after only three in-person classes. In the end, more than 1,600 students took the course in its inaugural year.

Now, through our online learning platform Meta Blueprint, we’re poised to teach thousands more by opening the AILA Education Hub up to everyone — educators, students, researchers, and hobbyists alike. We are also developing two new courses to complement AILA’s deep learning program: Introduction to AI and Natural Language Processing. Both will be added to the hub in 2022 and are geared toward those who have a computer science background. Students will also gain important and relevant real-world experience by working with members of Meta AI teams on active projects from the open-source community like PyTorch, the Hateful Memes Challenge, fastMRI, and AI Habitat.These new courses will be added to the hub in 2022. By eliminating barriers to learning and increasing access to opportunity we hope to help increase the number of diverse students studying AI and open new pathways to careers in the AI field for people from underrepresented groups.

“Many people who want to work in AI cannot access the resources they need to get there,” said Charles Isbell, the Dean and John P. Imlay, Jr., Chair of Computing at Georgia Tech. “By moving AI instruction online, we can reach more people from a wider range of backgrounds than ever before. This is not only a great opportunity for learners, but also for the field as a whole, which needs a diverse set of voices if it is to responsibly serve a diverse set of communities.”

We believe deeply in this model, so we took the additional step of creating a consortium of professors who see value in the curriculum we’re developing and are open to teaching some or all of the material at their universities in various capacities including a mix of both online and in-person offerings. These professors teach at Historically Black Colleges and Universities (HBCUs), Hispanic-serving Institutions (HSIs), and Asian American/Native American Pacific Islander-serving Institutions (AANAPISIs):

Georgia Tech University

Florida Agricultural and Mechanical University (through their foundation)

Morgan State University (through their foundation)

Alabama A&M University (through their foundation)

University of Central Florida (through their research foundation)

Georgia State University (through their research foundation)

Florida International University (through their foundation)

University of California Irvine (through their foundation)

North Carolina A&T State University

To further support these universities, Meta partnered with Dr. Kira at Georgia Tech to create a series of webinars which will be hosted on the AILA Education Hub and are intended to help guide professors on how to teach the course content. We’ll host, record, and share Q&A sessions and work through modules and exercises related to the curriculum.

Professors at universities around the world know first hand how challenging it is to keep graduate-level CS students up to date on the latest deep learning techniques being applied today by scientists and researchers.

“We have several courses in which we plan to explore system integration, including our classes on Introductory ML and Advanced ML (Deep Learning), as well as Natural Language Processing,” says Mark A. Finlayson, an Eminent Scholar Chaired Associate Professor of Computer Science in the Knight Foundation School of Computing and Information Sciences at Florida International University. “These fields move very quickly and it makes sense to partner with other academics and organizations to reduce the effort required to keep our curriculum up to date with high quality materials.”

Teaming up with Georgia Tech to create AILA was a deeply thought out partnership that took a lot of effort. We were looking to collaborate with a university known for producing large numbers of graduates from diverse backgrounds, and OMSCS is one of the country’s leading programs in doing just that.

The school’s online CS Master’s program is the largest of its kind in the United States and is one of the country’s most racial and ethnic diverse universities. Its laudable success at enrolling members of underrepresented groups into graduate-level CS programs makes it the perfect institution for us to team up with in pursuit of our longstanding goal of increasing diversity in the field of AI.

This is crucial given that the number of people from underrepresented groups earning advanced degrees in computer science is consistently low. In 2020, according to the Computing Research Association’s Taulbee Survey, just 3.8% of CS doctoral graduates were Black, Hispanic, or Native American.

The statistics above were taken from the 50th annual Taulbee Survey. (https://cra.org/wp-content/uploads/2021/05/2020-CRA-Taulbee-Survey.pdf)

If we don’t grow that number, it’s unlikely that the algorithms used to detect diseases, improve transportation, or even help connect people, will adequately reflect the diverse audience they are intended to serve. As the World Economic Forum puts it, by “addressing diversity gaps today, tech leaders can mitigate bias in the systems built for tomorrow.” Working alongside schools that enroll and support large numbers of students from underrepresented groups, we’re aiming to do just that. Collectively, we’re putting the curriculum in the hands of educators and truly anyone who wants to learn, creating a gateway to this content that requires no upfront investment. All they need is the time for taking the course, an appetite for learning, and if they choose, to avail themselves of an opportunity to pursue a career in AI. This is just one piece of our ongoing commitment to manifesting this reality.

“Morgan State promotes unbiased and trustworthy AI and AILA provides cutting edge curricula that enables students to learn the knowledge and skills from top industry experts and closes the gap between academia and industry,” says Dr. Paul Wang, Director of the Trustworthy AI Lab and Professor and Chair of Computer Science at Morgan State University. “The collaboration between Morgan Computer Science and Meta transforms learning to our mostly underrepresented students and prepares them to enter the workforce with competence and confidence.”

A curriculum built for the real world

Our semester-long deep learning course teaches the fundamentals of neural networks and applications like computer vision and language understanding that underpin some of today’s most-important industrial uses of AI. The course bridges AI theory and application and covers deploying and scaling algorithms in the real world. This approach has earned high marks from those who took the course.

Please click here to register for the course or to learn more about it."
Meta_Blog,https://ai.meta.com/blog/ai-learning-alliance/,,Meta’s AI Learning Alliance aims to expand pipeline of underrepresented students in AI,"Meta is announcing the Artificial Intelligence Learning Alliance (AILA), an initiative to strengthen diversity and increase equity in the field of artificial intelligence.

Together with Georgia Tech, we built a deep learning course curriculum that has been taken online by 2,400 students since its inception in the Fall of 2020. Now we are making the course content available free to all and are working with professors at historically Black colleges and universities (HBCUs), Hispanic-serving institutions (HSIs), and Asian American and Native American Pacific Islander–serving institutions (AANAPISIs) in our newly established consortium to further develop and teach the curriculum. These universities will also be coming together to contribute additional cutting-edge AI education lectures and resources for all to use.

The curriculum available now has been created in partnership with expert educators and is designed to be taken in tandem with enrollment in official degree programs. The shared goal of AILA’s creators is to have more people from underrepresented groups trained in the technology of artificial intelligence. That is why the course is now available to everyone for free via the newly created AILA Education Hub, where students can access AI course content, educators can collaborate and receive training on curriculum, and employers can connect students with job opportunities.

Click here to access the existing curriculum.

Artificial intelligence (AI) offers important benefits for people in areas such as transportation, commerce, and beyond. While AI is constantly improving at understanding the dynamics of the world we live in, there is much more work to do. We’re firm believers that it will progress faster with early involvement from experts with diverse perspectives. If the researchers, developers, and engineers advancing AI come from a limited variety of backgrounds, the technology’s point of view will be similarly limited.

Addressing this starts in classrooms. That’s why Meta, together with Georgia Tech, created the AI Learning Alliance (AILA), an expansion of the co-teaching program that we piloted in the Fall of 2020. Meta AI researchers both developed the curriculum for and guest-lectured in the deep learning course that Georgia Tech assistant professor Zsolt Kira has taught for the past two years as part of the university’s Online Master’s of Science in Computer Science program (OMSCS). The goal was to help students experience real-world examples and learn valuable techniques used in deploying and scaling algorithms.

The statistics above were taken from the 50th annual Taulbee Survey. (https://cra.org/wp-content/uploads/2021/05/2020-CRA-Taulbee-Survey.pdf)

The original plan was to teach the course on campus in Atlanta, but due to the pandemic, all course instruction was moved to a virtual model after only three in-person classes. In the end, more than 1,600 students took the course in its inaugural year.

Now, through our online learning platform Meta Blueprint, we’re poised to teach thousands more by opening the AILA Education Hub up to everyone — educators, students, researchers, and hobbyists alike. We are also developing two new courses to complement AILA’s deep learning program: Introduction to AI and Natural Language Processing. Both will be added to the hub in 2022 and are geared toward those who have a computer science background. Students will also gain important and relevant real-world experience by working with members of Meta AI teams on active projects from the open-source community like PyTorch, the Hateful Memes Challenge, fastMRI, and AI Habitat.These new courses will be added to the hub in 2022. By eliminating barriers to learning and increasing access to opportunity we hope to help increase the number of diverse students studying AI and open new pathways to careers in the AI field for people from underrepresented groups.

“Many people who want to work in AI cannot access the resources they need to get there,” said Charles Isbell, the Dean and John P. Imlay, Jr., Chair of Computing at Georgia Tech. “By moving AI instruction online, we can reach more people from a wider range of backgrounds than ever before. This is not only a great opportunity for learners, but also for the field as a whole, which needs a diverse set of voices if it is to responsibly serve a diverse set of communities.”

We believe deeply in this model, so we took the additional step of creating a consortium of professors who see value in the curriculum we’re developing and are open to teaching some or all of the material at their universities in various capacities including a mix of both online and in-person offerings. These professors teach at Historically Black Colleges and Universities (HBCUs), Hispanic-serving Institutions (HSIs), and Asian American/Native American Pacific Islander-serving Institutions (AANAPISIs):

Georgia Tech University

Florida Agricultural and Mechanical University (through their foundation)

Morgan State University (through their foundation)

Alabama A&M University (through their foundation)

University of Central Florida (through their research foundation)

Georgia State University (through their research foundation)

Florida International University (through their foundation)

University of California Irvine (through their foundation)

North Carolina A&T State University

To further support these universities, Meta partnered with Dr. Kira at Georgia Tech to create a series of webinars which will be hosted on the AILA Education Hub and are intended to help guide professors on how to teach the course content. We’ll host, record, and share Q&A sessions and work through modules and exercises related to the curriculum.

Professors at universities around the world know first hand how challenging it is to keep graduate-level CS students up to date on the latest deep learning techniques being applied today by scientists and researchers.

“We have several courses in which we plan to explore system integration, including our classes on Introductory ML and Advanced ML (Deep Learning), as well as Natural Language Processing,” says Mark A. Finlayson, an Eminent Scholar Chaired Associate Professor of Computer Science in the Knight Foundation School of Computing and Information Sciences at Florida International University. “These fields move very quickly and it makes sense to partner with other academics and organizations to reduce the effort required to keep our curriculum up to date with high quality materials.”

Teaming up with Georgia Tech to create AILA was a deeply thought out partnership that took a lot of effort. We were looking to collaborate with a university known for producing large numbers of graduates from diverse backgrounds, and OMSCS is one of the country’s leading programs in doing just that.

The school’s online CS Master’s program is the largest of its kind in the United States and is one of the country’s most racial and ethnic diverse universities. Its laudable success at enrolling members of underrepresented groups into graduate-level CS programs makes it the perfect institution for us to team up with in pursuit of our longstanding goal of increasing diversity in the field of AI.

This is crucial given that the number of people from underrepresented groups earning advanced degrees in computer science is consistently low. In 2020, according to the Computing Research Association’s Taulbee Survey, just 3.8% of CS doctoral graduates were Black, Hispanic, or Native American.

The statistics above were taken from the 50th annual Taulbee Survey. (https://cra.org/wp-content/uploads/2021/05/2020-CRA-Taulbee-Survey.pdf)

If we don’t grow that number, it’s unlikely that the algorithms used to detect diseases, improve transportation, or even help connect people, will adequately reflect the diverse audience they are intended to serve. As the World Economic Forum puts it, by “addressing diversity gaps today, tech leaders can mitigate bias in the systems built for tomorrow.” Working alongside schools that enroll and support large numbers of students from underrepresented groups, we’re aiming to do just that. Collectively, we’re putting the curriculum in the hands of educators and truly anyone who wants to learn, creating a gateway to this content that requires no upfront investment. All they need is the time for taking the course, an appetite for learning, and if they choose, to avail themselves of an opportunity to pursue a career in AI. This is just one piece of our ongoing commitment to manifesting this reality.

“Morgan State promotes unbiased and trustworthy AI and AILA provides cutting edge curricula that enables students to learn the knowledge and skills from top industry experts and closes the gap between academia and industry,” says Dr. Paul Wang, Director of the Trustworthy AI Lab and Professor and Chair of Computer Science at Morgan State University. “The collaboration between Morgan Computer Science and Meta transforms learning to our mostly underrepresented students and prepares them to enter the workforce with competence and confidence.”

A curriculum built for the real world

Our semester-long deep learning course teaches the fundamentals of neural networks and applications like computer vision and language understanding that underpin some of today’s most-important industrial uses of AI. The course bridges AI theory and application and covers deploying and scaling algorithms in the real world. This approach has earned high marks from those who took the course.

Please click here to register for the course or to learn more about it."
Meta_Blog,https://ai.meta.com/blog/a-new-resource-to-help-measure-fairness-in-ai-speech-recognition-systems/,,A new resource to help measure fairness in AI speech recognition systems,"Just as in other applications of AI, fairness is an important concern for speech recognition systems. If a tool performs well only for certain accents or speaking styles or vocal ranges, it fails to serve everyone as intended. But how can we assess whether a speech recognition system is working well for different groups of people — young and old; men, women, and nonbinary people; different ethnicities and cultural groups; and others?

AI systems need data and benchmarks in order to measure performance, and the research community simply hasn’t had adequate ways to assess fairness concerns for speech recognition systems. There have been only a handful of studies of how well speech recognition models perform for different groups of people and these analyses have utilized existing speech recognition data sets, which each reflect only some attributes of the speakers. For example, one data set may have gender data but not age data, while another has the reverse. That is why we’ve added thousands of expert human speech transcriptions to Meta AI’s open source Casual Conversations data set, so that researchers can utilize a shared data set for detecting speech recognition systems’ performance gaps for different groups of people. Researchers can also use the audio and transcription data, in compliance with the data set terms and conditions, to develop new speech recognition systems that exhibit less bias.

The Casual Conversations data set contains 45,186 videos of people having unscripted conversations. Meta AI shared it last spring to help researchers evaluate their computer vision models across a diverse set of age groups, genders, apparent skin tones, and ambient lighting conditions. To our knowledge, it’s the first publicly available data set featuring paid individuals who provided their age and gender themselves — as opposed to information labeled by third parties or estimated using AI models.

We believe this human-centered approach has produced a valuable resource for researchers with a more accurate representation of the attributes released. We hope this can help the research community in working collaboratively to make AI fairer and more inclusive. Now, with the addition of transcriptions to the data set, we're excited to expand Casual Conversation's utility to a new domain – automatic speech recognition.

Assessing performance discrepancies along gender, age, and apparent skin tone

In order to explore how model architectures and training regimens would affect performance for different groups of people, we trained four new speech recognition systems from scratch using state-of-the-art architectures and evaluated them using the Casual Conversations data set. All four used recurrent neural network transducer models, but two were trained with supervised data only, while two included semi-supervised training. We also varied the training data, with one model using audiobook data and the others using audio from publicly shared social media videos. Details are available in our accompanying research paper. We found several noteworthy results:

All four models demonstrated a performance gap (as measured by Word Error Rate, or WER) between gender groups, with fewer speech recognition errors for females than for males. For the “other” category, the amount of available data is insufficient to draw any conclusions at the moment.

We did not observe statistically significant performance gaps for different age groups.

The speech recognition systems performed worse for speakers with relatively darker skin tones than for those with lighter skin tones. We made this measurement as we had the relevant labels and it is important to understand performance on this axis. While apparent skin tone is not the most directly relevant characteristic to test auditory models, it is possible that it may correlate to certain auditory characteristics such as accent as we see some speech recognition performance differences between skin tone groups. However, more investigation is needed to surface the cause beyond speculation.

We found some models exhibited a larger variation in performance across the different subgroups. For example, the models trained with data from audiobooks showed a larger performance gap between sub-groups as compared to the ones trained using public social media videos. The video data set contains a diverse array of speakers, accents, topics, and acoustic conditions, whereas the audiobooks data set consists solely of books read aloud by a small group of speakers in controlled environments. This may account for some of the disparity between these models’ performance.

While the models built from scratch and tested in this work are not currently in production at Meta, these findings can inform research at Meta and in the community at large to ensure that ASR models of all kinds are built with fairness in mind.

Another step toward fairer AI

These early findings are only limited to the specific data set and models explored above, but they show that more work is needed to assess model bias in speech recognition systems. They also demonstrate how data disaggregated by demographic attributes can be useful in evaluating the extent of these challenges. Our findings do not show how best to modify AI systems to improve fairness, however. For example, our initial analysis found that using Casual Conversations data to fine-tune pre-trained models did not significantly reduce the error rate gaps between groups. The data set can be used to develop novel techniques for training models that perform similarly across all groups without knowing the specific protected attributes (e.g., gender or age) of the data points. Other possible mitigation strategies include using training data that features a diverse set of speakers, recording situations, and speaking styles. More research is needed to develop either of these options.

There is another important limitation of the Casual Conversations data set: It was collected only in the United States and does not cover all English-speaking populations. To measure performance differences across groups more thoroughly, we will need data sets and benchmarks that work across different languages and locales. A further limitation to note is that the data set does not have information on the speakers’ accents or first language. We recommend that further research consider responsibly collecting accent or first language characteristics so that researchers can evaluate performance differences across these relevant subpopulations.

What's next

Benchmark data sets are crucial to advancing AI research. Considering the impact of the availability of relevant data sets on the advancement of solutions to various problems (e.g., Imagenet on object recognition), Casual Conversations is an initial effort aimed towards driving research in the under-investigated area of fairness in speech recognition. Although there are certain speech data sets investigating certain underrepresented groups (e.g., CORAAL), the Casual Conversations data set provides more diversity and larger amounts of data for this type of research.

Building on this starting point, Meta AI is planning to perform research on speech recognition models that exhibit minimal performance differences across subpopulations. Our findings will inform the work of our cross-disciplinary Responsible AI (RAI) team, which has built and deployed tools like Fairness Flow to help our AI engineers detect certain forms of potential statistical bias in certain types of AI models and labels commonly used at Meta.

We hope the research community extends annotations of Casual Conversations for their own computer vision and audio applications, in line with our data use agreement. Collaborative research — through open source tools, papers, and discussions — is vital to address fairness issues in AI. By leveraging a diverse range of perspectives from the research community, we can identify new fairness concerns and questions and look for new ways of addressing them to ensure this technology is being developed responsibly and serving everyone.

Read the paper

Download the data set"
Meta_Blog,https://ai.meta.com/blog/a-new-resource-to-help-measure-fairness-in-ai-speech-recognition-systems/,,A new resource to help measure fairness in AI speech recognition systems,"Just as in other applications of AI, fairness is an important concern for speech recognition systems. If a tool performs well only for certain accents or speaking styles or vocal ranges, it fails to serve everyone as intended. But how can we assess whether a speech recognition system is working well for different groups of people — young and old; men, women, and nonbinary people; different ethnicities and cultural groups; and others?

AI systems need data and benchmarks in order to measure performance, and the research community simply hasn’t had adequate ways to assess fairness concerns for speech recognition systems. There have been only a handful of studies of how well speech recognition models perform for different groups of people and these analyses have utilized existing speech recognition data sets, which each reflect only some attributes of the speakers. For example, one data set may have gender data but not age data, while another has the reverse. That is why we’ve added thousands of expert human speech transcriptions to Meta AI’s open source Casual Conversations data set, so that researchers can utilize a shared data set for detecting speech recognition systems’ performance gaps for different groups of people. Researchers can also use the audio and transcription data, in compliance with the data set terms and conditions, to develop new speech recognition systems that exhibit less bias.

The Casual Conversations data set contains 45,186 videos of people having unscripted conversations. Meta AI shared it last spring to help researchers evaluate their computer vision models across a diverse set of age groups, genders, apparent skin tones, and ambient lighting conditions. To our knowledge, it’s the first publicly available data set featuring paid individuals who provided their age and gender themselves — as opposed to information labeled by third parties or estimated using AI models.

We believe this human-centered approach has produced a valuable resource for researchers with a more accurate representation of the attributes released. We hope this can help the research community in working collaboratively to make AI fairer and more inclusive. Now, with the addition of transcriptions to the data set, we're excited to expand Casual Conversation's utility to a new domain – automatic speech recognition.

Assessing performance discrepancies along gender, age, and apparent skin tone

In order to explore how model architectures and training regimens would affect performance for different groups of people, we trained four new speech recognition systems from scratch using state-of-the-art architectures and evaluated them using the Casual Conversations data set. All four used recurrent neural network transducer models, but two were trained with supervised data only, while two included semi-supervised training. We also varied the training data, with one model using audiobook data and the others using audio from publicly shared social media videos. Details are available in our accompanying research paper. We found several noteworthy results:

All four models demonstrated a performance gap (as measured by Word Error Rate, or WER) between gender groups, with fewer speech recognition errors for females than for males. For the “other” category, the amount of available data is insufficient to draw any conclusions at the moment.

We did not observe statistically significant performance gaps for different age groups.

The speech recognition systems performed worse for speakers with relatively darker skin tones than for those with lighter skin tones. We made this measurement as we had the relevant labels and it is important to understand performance on this axis. While apparent skin tone is not the most directly relevant characteristic to test auditory models, it is possible that it may correlate to certain auditory characteristics such as accent as we see some speech recognition performance differences between skin tone groups. However, more investigation is needed to surface the cause beyond speculation.

We found some models exhibited a larger variation in performance across the different subgroups. For example, the models trained with data from audiobooks showed a larger performance gap between sub-groups as compared to the ones trained using public social media videos. The video data set contains a diverse array of speakers, accents, topics, and acoustic conditions, whereas the audiobooks data set consists solely of books read aloud by a small group of speakers in controlled environments. This may account for some of the disparity between these models’ performance.

While the models built from scratch and tested in this work are not currently in production at Meta, these findings can inform research at Meta and in the community at large to ensure that ASR models of all kinds are built with fairness in mind.

Another step toward fairer AI

These early findings are only limited to the specific data set and models explored above, but they show that more work is needed to assess model bias in speech recognition systems. They also demonstrate how data disaggregated by demographic attributes can be useful in evaluating the extent of these challenges. Our findings do not show how best to modify AI systems to improve fairness, however. For example, our initial analysis found that using Casual Conversations data to fine-tune pre-trained models did not significantly reduce the error rate gaps between groups. The data set can be used to develop novel techniques for training models that perform similarly across all groups without knowing the specific protected attributes (e.g., gender or age) of the data points. Other possible mitigation strategies include using training data that features a diverse set of speakers, recording situations, and speaking styles. More research is needed to develop either of these options.

There is another important limitation of the Casual Conversations data set: It was collected only in the United States and does not cover all English-speaking populations. To measure performance differences across groups more thoroughly, we will need data sets and benchmarks that work across different languages and locales. A further limitation to note is that the data set does not have information on the speakers’ accents or first language. We recommend that further research consider responsibly collecting accent or first language characteristics so that researchers can evaluate performance differences across these relevant subpopulations.

What's next

Benchmark data sets are crucial to advancing AI research. Considering the impact of the availability of relevant data sets on the advancement of solutions to various problems (e.g., Imagenet on object recognition), Casual Conversations is an initial effort aimed towards driving research in the under-investigated area of fairness in speech recognition. Although there are certain speech data sets investigating certain underrepresented groups (e.g., CORAAL), the Casual Conversations data set provides more diversity and larger amounts of data for this type of research.

Building on this starting point, Meta AI is planning to perform research on speech recognition models that exhibit minimal performance differences across subpopulations. Our findings will inform the work of our cross-disciplinary Responsible AI (RAI) team, which has built and deployed tools like Fairness Flow to help our AI engineers detect certain forms of potential statistical bias in certain types of AI models and labels commonly used at Meta.

We hope the research community extends annotations of Casual Conversations for their own computer vision and audio applications, in line with our data use agreement. Collaborative research — through open source tools, papers, and discussions — is vital to address fairness issues in AI. By leveraging a diverse range of perspectives from the research community, we can identify new fairness concerns and questions and look for new ways of addressing them to ensure this technology is being developed responsibly and serving everyone.

Read the paper

Download the data set"
Meta_Blog,https://ai.meta.com/blog/creating-better-virtual-backdrops-for-video-calling-remote-presence-and-ar/,,"Creating better virtual backdrops for video calling, remote presence, and AR","Something Went Wrong We're having trouble playing this video. Learn more

This video shows how Meta’s person-segmentation model works with both close-up views of people and full-body views.

Since the start of the COVID-19 pandemic, many of us have become accustomed to using or viewing virtual backgrounds and background filters when video chatting with friends, coworkers, or family. Altering our backgrounds during video calls gives us greater control over our environments, helping us eliminate distractions, protect the privacy of the people and spaces around us, and even liven up our virtual presentations and get-togethers. But background filters don’t always work as intended, and they don’t always perform optimally for everyone. Most of us are familiar with background filters mistakenly covering someone’s face as they move, for example, or a filter failing to distinguish between a hand and a table.

To improve background blurring, virtual backgrounds, and many other augmented reality (AR) effects in Meta’s products and services, we’ve recently deployed enhanced AI models for image segmentation — the computer vision task of separating out the different parts of a photo or video. A cross-functional team of researchers and engineers from Meta AI, Reality Labs, and other parts of the company has built new segmentation models that are now in production for real-time video calling in Spark AR on multiple surfaces across Portal, Messenger, and Instagram. We’ve also improved the two-person segmentation models that are running today on Instagram and Messenger.

These systems are now more efficient, more stable, and more versatile, which will help enhance the quality and consistency of background filter effects in our products. For example, our improved segmentation models can now be used for multiple people and for people’s full bodies, as well as for people occluded by an object, such as a sofa, desk, or table. And beyond video calling, improved segmentation can also bring new dimensions to augmented and virtual reality (AR/VR) by merging virtual environments with people and objects in the real world. This will be especially important as we build new immersive experiences for the metaverse.

As we worked to advance image segmentation, we focused broadly on three important challenges:

Teaching our AI models to work well in a wide variety of circumstances, such as with dark lighting conditions, variations in skin tones and situations where skin tones are similar to background colors, less common body poses (e.g., someone bending forward to tie a shoe or to stretch), occlusions, and movements.

Improving boundary smoothness, stability, and general consistency. These qualities are less discussed in existing research literature, but user studies have shown they greatly affect people’s experience when using background effects.

Ensuring that our models are efficient and flexible enough to work well on billions of smartphones currently in use around the world, not just the small fraction that are current-gen devices with cutting-edge processors. The models also had to support very different aspect ratios, in order to work well on laptop computers, Meta’s Portal video calling device, and portrait and landscape modes on people’s phones.

The challenges of person segmentation in the real world

While the concept of image segmentation is easy to grasp, achieving highly accurate person segmentation presents significant challenges. To create a good experience, the model must be extremely consistent and lag-free. Artifacts caused by incorrect segmentation output can easily distract people using virtual background applications during a video call. Even more important, segmentation errors may lead to unwanted exposure of people’s physical environments when they are using background effects.

For these reasons, it’s important to achieve high accuracy — greater than 90 percent intersection over union (IoU), a commonly used metric for measuring the overlap between the image segmentation prediction and the ground truth — in order to deploy person segmentation models into production. Because of the huge variety of possible use cases, the last 10 percent gap in IoU is exponentially more difficult to overcome than the first 90 percent. We found when IoU reaches 90 percent, the metric becomes saturated and cannot capture further improvements in temporal consistency and boundary stability. We thus developed a video based measurement system together with several metrics to capture these additional dimensions.

Developing training and measurement strategies for the real world

AI models learn from the data they’re given, so it’s not enough to simply use examples of video callers sitting still in well-lit rooms. In order to create highly accurate segmentation models for a very wide variety of circumstances, we needed many other kinds of examples as well.

We used Meta AI’s ClusterFit model to retrieve from our data set a broad range of examples across gender, skin tone, age, body pose, motion, background complexity, number of people, and so forth.

Metrics on static images don’t accurately reflect a model’s quality in real time, because real-time models usually have a tracking mode that relies on temporal information. To measure our models’ qualities in real time, we designed a quantitative video evaluation framework that computes metrics at each frame when the model inference reaches.

Unlike standard academic segmentation problems, the quality of our person segmentation model is best judged by its performance in everyday situations. If the effect is jarring, distracting, or otherwise lacking, its performance against any specific benchmark is inconsequential. So we surveyed people who use our products, asking about the quality of our segmentation applications. We found that non-smooth and ambiguous boundaries affect user experiences the most. To capture this signal, we augmented our framework with an additional metric, Boundary IoU, a new segmentation evaluation created by Meta AI researchers to measure boundary quality. Boundary IoU is of higher interest when general IoU is close to saturated, i.e., above 90 percent. Additionally, the jitters (temporal inconsistency) at the boundary also detract from the experience. We measure temporal consistency using two methods. First, we assume adjacent video frames are identical to each other, and any prediction discrepancy indicates the model temporal inconsistency. Second, we consider the foreground movement between adjacent video frames. Optical flow can help us transform the prediction of frame N to N+1. We then can compare this transformed prediction with the raw prediction at frame N+1 and use IoU to represent the discrepancy. We use IoU to measure the discrepancies in both cases.

We also analyzed our models for differences in performance across specific groups of people. We labeled evaluation videos with metadata from more than 100 classes (more than 30 categories) including three skin tones (informed by clustering Fitzpatrick scale skin types) and two apparent gender categories. The model showed similar accuracy across both apparent skin tone and apparent binary gender categories. Despite some very minor differences between categories, which we will prioritize addressing in our ongoing work, the model demonstrates good performance across all subcategories.

Per class IoU from fairness analysis. We collected 1,100 videos dedicated to this project with diverse attributes, e.g., skin tone, apparent gender, pose, lighting condition, etc. (greater than 30 categories with greater than 100 classes). Points in the plots represent the per class IoU. Error bars represent 95 percent confidence intervals. We see some differences between classes, although the difference is very small.

Optimizing the model

Architecture

To optimize our models, we use FBNet V3 as their backbone. The architecture is in encoder-decoder structure with a fusion of layers with the same spatial resolution. We design a heavy weight encoder with a light decoder that achieves better quality than the symmetric design of architecture. The resulting architecture is supported by Neural Architecture Search and highly optimized for speed on-device.

Architecture of the semantic segmentation model. The green rectangles represent convolution layers, and the black circles represent concatenation.

Data-efficient learning

We used an offline high-capacity PointRend model to generate pseudo ground truth labels for the unannotated data to increase the volume of training data. Similarly, we used the student-teacher semi-supervised model to remove biases in pseudo labeling.

Aspect ratio dependent resampling

A traditional deep learning model resamples an image to a small squared one as its input to the network. Because of this resampling, distortions occur. And because images have different aspect ratios, the distortions are also different. The presence of, and variations in, distortions causes the network to learn low-level features not robust to different aspect ratios. Limitations caused by such distortions are amplified in segmentation applications. When the majority of the training images have portrait ratios, for example, the model performs much worse on landscape images and videos. To address this challenge, we adopted Detectron 2’s aspect ratio dependent resampling method, which groups images with similar aspect ratios and resamples them all to the same size.

Illustration of the importance of aspect-ratio-dependent resampling. The left image is the model resampling an image to a squared size (the output mask is very unstable). The right image is the model trained with aspect-ratio-dependent resampling.

Customized padding

Aspect ratio dependent resampling requires padding group images with similar aspect ratios, but the commonly used zero-padding method produces artifacts. Even worse, the artifacts propagate to other areas when the network gets deeper. We use replicate padding to remove these artifacts. In a recent study, we found reflection padding in convolution layers can further improve model quality by minimizing the artifacts’ propagation, but the latency cost increases accordingly. An example of the artifact and the result of removing it is shown below.

Left: segmentation output using zero padding, right: segmentation output using customized padding.

Tracking

Temporal inconsistency represents frame-to-frame prediction discrepancies, known as flickers, and hurts the user experience. To improve the temporal consistency, we designed a detect-with-mask process. It takes three channels from the current frame (YUV), and there is a fourth channel. For the first frame, the fourth channel is an empty matrix, while in following frames, the fourth channel is from the prediction of the last frame. We found this tracking strategy improves temporal consistency significantly. We also adopted some ideas from state-of-the-art tracking models, such as CRVOS and transform invariant CNN modeling strategies, to obtain a temporally stable segmentation model.

Illustration of the detect-with-mask model.

Boundary cross entropy

Creating smooth and clear boundaries is critical for AR applications of segmentation. Besides the standard cross entropy loss for segmentation, we need to consider boundary weighted loss as well. The authors of U-Net and most later variations recommend trimap weighted loss to improve a model’s quality given the observation that interiors of objects are easier to be segmented. However, one limitation of the trimap loss is that it calculates the boundary area only based on the ground truth, thus it’s an asymmetric loss insensitive to false positives. Inspired by Boundary IoU, we adopt their method of retrieving boundary areas for both ground truth and prediction, and build a cross entropy loss in these areas. The model trained on boundary cross entropy outperforms the baseline significantly. Besides making the boundary area clearer in the final mask output, false positives from the new models occur less frequently, which can be expected according to its theory.

Something Went Wrong We're having trouble playing this video. Learn more

This video illustrates the improvement based on boundary cross entropy loss. The right side uses the traditional trimap weighted loss. The left side shows the output the model trained with this new loss. The left side exhibits much heavier boundary flickers as well as false positives.

Performance

All our models are trained offline using PyTorch and then deployed into production using the Spark AR platform. We use PyTorch Lite to optimize on-device deep learning model inference. Because use cases and hardware are different for apps and devices, we design different models to satisfy these requirements. After 1.5 years of development, our team has successfully improved the person segmentation models on multiple Facebook apps and devices.

Building better segmentation models for video chat and much more

We’ve made substantial improvements in our segmentation models, but there is more work to do. We hope to enable segmentation-powered effects that seamlessly adjust to even the most complex, challenging, and uncommon use cases. In particular, we continue to work on new ways to improve boundary stability and temporal consistency, which are vital for AR/VR human-centric segmentation applications. We are working to create advanced tracking methods that will provide more consistent prediction, especially at edges of objects.

We will also continue to improve our tools for assessment of model performance across the various dimensions of diversity of people around the world.

We hope that by sharing details here on our work, we will help other researchers and engineers create better segmentation-powered applications that work well for everyone.

This project is a collaboration of around 30 engineers and scientists. We’d like to thank everyone who contributed to making Meta’s on-device person segmentation solution better."
Meta_Blog,https://ai.meta.com/blog/creating-better-virtual-backdrops-for-video-calling-remote-presence-and-ar/,,"Creating better virtual backdrops for video calling, remote presence, and AR","Something Went Wrong We're having trouble playing this video. Learn more

This video shows how Meta’s person-segmentation model works with both close-up views of people and full-body views.

Since the start of the COVID-19 pandemic, many of us have become accustomed to using or viewing virtual backgrounds and background filters when video chatting with friends, coworkers, or family. Altering our backgrounds during video calls gives us greater control over our environments, helping us eliminate distractions, protect the privacy of the people and spaces around us, and even liven up our virtual presentations and get-togethers. But background filters don’t always work as intended, and they don’t always perform optimally for everyone. Most of us are familiar with background filters mistakenly covering someone’s face as they move, for example, or a filter failing to distinguish between a hand and a table.

To improve background blurring, virtual backgrounds, and many other augmented reality (AR) effects in Meta’s products and services, we’ve recently deployed enhanced AI models for image segmentation — the computer vision task of separating out the different parts of a photo or video. A cross-functional team of researchers and engineers from Meta AI, Reality Labs, and other parts of the company has built new segmentation models that are now in production for real-time video calling in Spark AR on multiple surfaces across Portal, Messenger, and Instagram. We’ve also improved the two-person segmentation models that are running today on Instagram and Messenger.

These systems are now more efficient, more stable, and more versatile, which will help enhance the quality and consistency of background filter effects in our products. For example, our improved segmentation models can now be used for multiple people and for people’s full bodies, as well as for people occluded by an object, such as a sofa, desk, or table. And beyond video calling, improved segmentation can also bring new dimensions to augmented and virtual reality (AR/VR) by merging virtual environments with people and objects in the real world. This will be especially important as we build new immersive experiences for the metaverse.

As we worked to advance image segmentation, we focused broadly on three important challenges:

Teaching our AI models to work well in a wide variety of circumstances, such as with dark lighting conditions, variations in skin tones and situations where skin tones are similar to background colors, less common body poses (e.g., someone bending forward to tie a shoe or to stretch), occlusions, and movements.

Improving boundary smoothness, stability, and general consistency. These qualities are less discussed in existing research literature, but user studies have shown they greatly affect people’s experience when using background effects.

Ensuring that our models are efficient and flexible enough to work well on billions of smartphones currently in use around the world, not just the small fraction that are current-gen devices with cutting-edge processors. The models also had to support very different aspect ratios, in order to work well on laptop computers, Meta’s Portal video calling device, and portrait and landscape modes on people’s phones.

The challenges of person segmentation in the real world

While the concept of image segmentation is easy to grasp, achieving highly accurate person segmentation presents significant challenges. To create a good experience, the model must be extremely consistent and lag-free. Artifacts caused by incorrect segmentation output can easily distract people using virtual background applications during a video call. Even more important, segmentation errors may lead to unwanted exposure of people’s physical environments when they are using background effects.

For these reasons, it’s important to achieve high accuracy — greater than 90 percent intersection over union (IoU), a commonly used metric for measuring the overlap between the image segmentation prediction and the ground truth — in order to deploy person segmentation models into production. Because of the huge variety of possible use cases, the last 10 percent gap in IoU is exponentially more difficult to overcome than the first 90 percent. We found when IoU reaches 90 percent, the metric becomes saturated and cannot capture further improvements in temporal consistency and boundary stability. We thus developed a video based measurement system together with several metrics to capture these additional dimensions.

Developing training and measurement strategies for the real world

AI models learn from the data they’re given, so it’s not enough to simply use examples of video callers sitting still in well-lit rooms. In order to create highly accurate segmentation models for a very wide variety of circumstances, we needed many other kinds of examples as well.

We used Meta AI’s ClusterFit model to retrieve from our data set a broad range of examples across gender, skin tone, age, body pose, motion, background complexity, number of people, and so forth.

Metrics on static images don’t accurately reflect a model’s quality in real time, because real-time models usually have a tracking mode that relies on temporal information. To measure our models’ qualities in real time, we designed a quantitative video evaluation framework that computes metrics at each frame when the model inference reaches.

Unlike standard academic segmentation problems, the quality of our person segmentation model is best judged by its performance in everyday situations. If the effect is jarring, distracting, or otherwise lacking, its performance against any specific benchmark is inconsequential. So we surveyed people who use our products, asking about the quality of our segmentation applications. We found that non-smooth and ambiguous boundaries affect user experiences the most. To capture this signal, we augmented our framework with an additional metric, Boundary IoU, a new segmentation evaluation created by Meta AI researchers to measure boundary quality. Boundary IoU is of higher interest when general IoU is close to saturated, i.e., above 90 percent. Additionally, the jitters (temporal inconsistency) at the boundary also detract from the experience. We measure temporal consistency using two methods. First, we assume adjacent video frames are identical to each other, and any prediction discrepancy indicates the model temporal inconsistency. Second, we consider the foreground movement between adjacent video frames. Optical flow can help us transform the prediction of frame N to N+1. We then can compare this transformed prediction with the raw prediction at frame N+1 and use IoU to represent the discrepancy. We use IoU to measure the discrepancies in both cases.

We also analyzed our models for differences in performance across specific groups of people. We labeled evaluation videos with metadata from more than 100 classes (more than 30 categories) including three skin tones (informed by clustering Fitzpatrick scale skin types) and two apparent gender categories. The model showed similar accuracy across both apparent skin tone and apparent binary gender categories. Despite some very minor differences between categories, which we will prioritize addressing in our ongoing work, the model demonstrates good performance across all subcategories.

Per class IoU from fairness analysis. We collected 1,100 videos dedicated to this project with diverse attributes, e.g., skin tone, apparent gender, pose, lighting condition, etc. (greater than 30 categories with greater than 100 classes). Points in the plots represent the per class IoU. Error bars represent 95 percent confidence intervals. We see some differences between classes, although the difference is very small.

Optimizing the model

Architecture

To optimize our models, we use FBNet V3 as their backbone. The architecture is in encoder-decoder structure with a fusion of layers with the same spatial resolution. We design a heavy weight encoder with a light decoder that achieves better quality than the symmetric design of architecture. The resulting architecture is supported by Neural Architecture Search and highly optimized for speed on-device.

Architecture of the semantic segmentation model. The green rectangles represent convolution layers, and the black circles represent concatenation.

Data-efficient learning

We used an offline high-capacity PointRend model to generate pseudo ground truth labels for the unannotated data to increase the volume of training data. Similarly, we used the student-teacher semi-supervised model to remove biases in pseudo labeling.

Aspect ratio dependent resampling

A traditional deep learning model resamples an image to a small squared one as its input to the network. Because of this resampling, distortions occur. And because images have different aspect ratios, the distortions are also different. The presence of, and variations in, distortions causes the network to learn low-level features not robust to different aspect ratios. Limitations caused by such distortions are amplified in segmentation applications. When the majority of the training images have portrait ratios, for example, the model performs much worse on landscape images and videos. To address this challenge, we adopted Detectron 2’s aspect ratio dependent resampling method, which groups images with similar aspect ratios and resamples them all to the same size.

Illustration of the importance of aspect-ratio-dependent resampling. The left image is the model resampling an image to a squared size (the output mask is very unstable). The right image is the model trained with aspect-ratio-dependent resampling.

Customized padding

Aspect ratio dependent resampling requires padding group images with similar aspect ratios, but the commonly used zero-padding method produces artifacts. Even worse, the artifacts propagate to other areas when the network gets deeper. We use replicate padding to remove these artifacts. In a recent study, we found reflection padding in convolution layers can further improve model quality by minimizing the artifacts’ propagation, but the latency cost increases accordingly. An example of the artifact and the result of removing it is shown below.

Left: segmentation output using zero padding, right: segmentation output using customized padding.

Tracking

Temporal inconsistency represents frame-to-frame prediction discrepancies, known as flickers, and hurts the user experience. To improve the temporal consistency, we designed a detect-with-mask process. It takes three channels from the current frame (YUV), and there is a fourth channel. For the first frame, the fourth channel is an empty matrix, while in following frames, the fourth channel is from the prediction of the last frame. We found this tracking strategy improves temporal consistency significantly. We also adopted some ideas from state-of-the-art tracking models, such as CRVOS and transform invariant CNN modeling strategies, to obtain a temporally stable segmentation model.

Illustration of the detect-with-mask model.

Boundary cross entropy

Creating smooth and clear boundaries is critical for AR applications of segmentation. Besides the standard cross entropy loss for segmentation, we need to consider boundary weighted loss as well. The authors of U-Net and most later variations recommend trimap weighted loss to improve a model’s quality given the observation that interiors of objects are easier to be segmented. However, one limitation of the trimap loss is that it calculates the boundary area only based on the ground truth, thus it’s an asymmetric loss insensitive to false positives. Inspired by Boundary IoU, we adopt their method of retrieving boundary areas for both ground truth and prediction, and build a cross entropy loss in these areas. The model trained on boundary cross entropy outperforms the baseline significantly. Besides making the boundary area clearer in the final mask output, false positives from the new models occur less frequently, which can be expected according to its theory.

Something Went Wrong We're having trouble playing this video. Learn more

This video illustrates the improvement based on boundary cross entropy loss. The right side uses the traditional trimap weighted loss. The left side shows the output the model trained with this new loss. The left side exhibits much heavier boundary flickers as well as false positives.

Performance

All our models are trained offline using PyTorch and then deployed into production using the Spark AR platform. We use PyTorch Lite to optimize on-device deep learning model inference. Because use cases and hardware are different for apps and devices, we design different models to satisfy these requirements. After 1.5 years of development, our team has successfully improved the person segmentation models on multiple Facebook apps and devices.

Building better segmentation models for video chat and much more

We’ve made substantial improvements in our segmentation models, but there is more work to do. We hope to enable segmentation-powered effects that seamlessly adjust to even the most complex, challenging, and uncommon use cases. In particular, we continue to work on new ways to improve boundary stability and temporal consistency, which are vital for AR/VR human-centric segmentation applications. We are working to create advanced tracking methods that will provide more consistent prediction, especially at edges of objects.

We will also continue to improve our tools for assessment of model performance across the various dimensions of diversity of people around the world.

We hope that by sharing details here on our work, we will help other researchers and engineers create better segmentation-powered applications that work well for everyone.

This project is a collaboration of around 30 engineers and scientists. We’d like to thank everyone who contributed to making Meta’s on-device person segmentation solution better."
Meta_Blog,https://ai.meta.com/blog/ai-rsc/,,Introducing the AI Research SuperCluster — Meta’s cutting-edge AI supercomputer for AI research,"Something Went Wrong We're having trouble playing this video. Learn more

Developing the next generation of advanced AI will require powerful new computers capable of quintillions of operations per second. Today, Meta is announcing that we’ve designed and built the AI Research SuperCluster (RSC) — which we believe is among the fastest AI supercomputers running today and will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022. Our researchers have already started using RSC to train large models in natural language processing (NLP) and computer vision for research, with the aim of one day training models with trillions of parameters.

RSC will help Meta’s AI researchers build new and better AI models that can learn from trillions of examples; work across hundreds of different languages; seamlessly analyze text, images, and video together; develop new augmented reality tools; and much more. Our researchers will be able to train the largest models needed to develop advanced AI for computer vision, NLP, speech recognition, and more. We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together. Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.

Why do we need an AI supercomputer at this scale?

Meta has been committed to long-term investment in AI since 2013, when we created the Facebook AI Research lab. In recent years, we’ve made significant strides in AI thanks to our leadership in a number of areas, including self-supervised learning, where algorithms can learn from vast numbers of unlabeled examples, and transformers, which allow AI models to reason more effectively by focusing on certain areas of their input.

To fully realize the benefits of self-supervised learning and transformer-based models, various domains, whether vision, speech, language, or for critical use cases like identifying harmful content, will require training increasingly large, complex, and adaptable models. Computer vision, for example, needs to process larger, longer videos with higher data sampling rates. Speech recognition needs to work well even in challenging scenarios with lots of background noise, such as parties or concerts. NLP needs to understand more languages, dialects, and accents. And advances in other areas, including robotics, embodied AI, and multimodal AI will help people accomplish useful tasks in the real world.

High-performance computing infrastructure is a critical component in training such large models, and Meta’s AI research team has been building these high-powered systems for many years. The first generation of this infrastructure, designed in 2017, has 22,000 NVIDIA V100 Tensor Core GPUs in a single cluster that performs 35,000 training jobs a day. Up until now, this infrastructure has set the bar for Meta’s researchers in terms of its performance, reliability, and productivity.

In early 2020, we decided the best way to accelerate progress was to design a new computing infrastructure from a clean slate to take advantage of new GPU and network fabric technology. We wanted this infrastructure to be able to train models with more than a trillion parameters on data sets as large as an exabyte — which, to provide a sense of scale, is the equivalent of 36,000 years of high-quality video.

While the high-performance computing community has been tackling scale for decades, we also had to make sure we have all the needed security and privacy controls in place to protect any training data we use. Unlike with our previous AI research infrastructure, which leveraged only open source and other publicly available data sets, RSC also helps us ensure that our research translates effectively into practice by allowing us to include real-world examples from Meta’s production systems in model training. By doing this, we can help advance research to perform downstream tasks such as identifying harmful content on our platforms as well as research into embodied AI and multimodal AI to help improve user experiences on our family of apps. We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale.

RSC: Under the hood

AI supercomputers are built by combining multiple GPUs into compute nodes, which are then connected by a high-performance network fabric to allow fast communication between those GPUs. RSC today comprises a total of 760 NVIDIA DGX A100 systems as its compute nodes, for a total of 6,080 GPUs — with each A100 GPU being more powerful than the V100 used in our previous system. Each DGX communicates via an NVIDIA Quantum 1600 Gb/s InfiniBand two-level Clos fabric that has no oversubscription. RSC’s storage tier has 175 petabytes of Pure Storage FlashArray, 46 petabytes of cache storage in Penguin Computing Altus systems, and 10 petabytes of Pure Storage FlashBlade.

Early benchmarks on RSC, compared with Meta’s legacy production and research infrastructure, have shown that it runs computer vision workflows up to 20 times faster, runs the NVIDIA Collective Communication Library (NCCL) more than nine times faster, and trains large-scale NLP models three times faster. That means a model with tens of billions of parameters can finish training in three weeks, compared with nine weeks before.

Building an AI supercomputer…

Designing and building something like RSC isn’t a matter of performance alone but performance at the largest scale possible, with the most advanced technology available today. When RSC is complete, the InfiniBand network fabric will connect 16,000 GPUs as endpoints, making it one of the largest such networks deployed to date. Additionally, we designed a caching and storage system that can serve 16 TB/s of training data, and we plan to scale it up to 1 exabyte.

All this infrastructure must be extremely reliable, as we estimate some experiments could run for weeks and require thousands of GPUs. Lastly, the entire experience of using RSC has to be researcher-friendly so our teams can easily explore a wide range of AI models.

A big part of achieving this was in working with a number of long-time partners, all of whom also helped design the first generation of our AI infrastructure in 2017. Penguin Computing, an SGH company, our architecture and managed services partner, worked with our operations team on hardware integration to deploy the cluster and helped set up major parts of the control plane. Pure Storage provided us with a robust and scalable storage solution. And NVIDIA provided us with its AI computing technologies featuring cutting-edge systems, GPUs, and InfiniBand fabric, and software stack components like NCCL for the cluster.

…and doing it remotely, during a pandemic

But there were other unexpected challenges that arose in RSC’s development — namely the coronavirus pandemic. RSC began as a completely remote project that the team took from a simple shared document to a functioning cluster in about a year and a half. COVID-19 and industry-wide wafer supply constraints also brought supply chain issues that made it difficult to get everything from chips to components like optics and GPUs, and even construction materials — all of which had to be transported in accordance with new safety protocols. To build this cluster efficiently, we had to design it from scratch, creating many entirely new Meta-specific conventions and rethinking previous ones along the way. We had to write new rules around our data center designs — including their cooling, power, rack layout, cabling, and networking (including a completely new control plane), among other important considerations. We had to ensure that all the teams, from construction to hardware to software and AI, were working in lockstep and in coordination with our partners.

Beyond the core system itself, there was also a need for a powerful storage solution, one that can serve terabytes of bandwidth from an exabyte-scale storage system. To serve AI training’s growing bandwidth and capacity needs, we developed a storage service, AI Research Store (AIRStore), from the ground up. To optimize for AI models, AIRStore utilizes a new data preparation phase that preprocesses the data set to be used for training. Once the preparation is performed one time, the prepared data set can be used for multiple training runs until it expires. AIRStore also optimizes data transfers so that cross-region traffic on Meta’s inter-datacenter backbone is minimized.

How we safeguard data in RSC

To build new AI models that benefit the people using our services — whether that’s detecting harmful content or creating new AR experiences — we need to teach models using real-world data from our production systems. RSC has been designed from the ground up with privacy and security in mind, so that Meta’s researchers can safely train models using encrypted user-generated data that is not decrypted until right before training. For example, RSC is isolated from the larger internet, with no direct inbound or outbound connections, and traffic can flow only from Meta’s production data centers.

To meet our privacy and security requirements, the entire data path from our storage systems to the GPUs is end-to-end encrypted and has the necessary tools and processes to verify that these requirements are met at all times. Before data is imported to RSC, it must go through a privacy review process to confirm it has been correctly anonymized or alternative privacy safeguards have been put in place to protect the data. The data is then encrypted before it can be used to train AI models and both the data and the decryption keys are deleted regularly to ensure older data is not still accessible. And since the data is only decrypted at one endpoint, in memory, it is safeguarded even in the unlikely event of a physical breach of the facility. (Updated on January 28, 2022 at 2:30PM PT to note that alternative privacy safeguards must be put in place to protect data in instances where certain types of data (i.e., audio, images, and video) cannot be fully anonymized for research purposes, and to clarify that data is also regularly deleted).

Phase two and beyond

RSC is up and running today, but its development is ongoing. Once we complete phase two of building out RSC, we believe it will be the fastest AI supercomputer in the world, performing at nearly 5 exaflops of mixed precision compute. Through 2022, we’ll work to increase the number of GPUs from 6,080 to 16,000, which will increase AI training performance by more than 2.5x. The InfiniBand fabric will expand to support 16,000 ports in a two-layer topology with no oversubscription. The storage system will have a target delivery bandwidth of 16 TB/s and exabyte-scale capacity to meet increased demand.

We expect such a step function change in compute capability to enable us not only to create more accurate AI models for our existing services, but also to enable completely new user experiences, especially in the metaverse. Our long-term investments in self-supervised learning and in building next-generation AI infrastructure with RSC are helping us create the foundational technologies that will power the metaverse and advance the broader AI community as well."
Meta_Blog,https://ai.meta.com/blog/ai-rsc/,,Introducing the AI Research SuperCluster — Meta’s cutting-edge AI supercomputer for AI research,"Something Went Wrong We're having trouble playing this video. Learn more

Developing the next generation of advanced AI will require powerful new computers capable of quintillions of operations per second. Today, Meta is announcing that we’ve designed and built the AI Research SuperCluster (RSC) — which we believe is among the fastest AI supercomputers running today and will be the fastest AI supercomputer in the world when it’s fully built out in mid-2022. Our researchers have already started using RSC to train large models in natural language processing (NLP) and computer vision for research, with the aim of one day training models with trillions of parameters.

RSC will help Meta’s AI researchers build new and better AI models that can learn from trillions of examples; work across hundreds of different languages; seamlessly analyze text, images, and video together; develop new augmented reality tools; and much more. Our researchers will be able to train the largest models needed to develop advanced AI for computer vision, NLP, speech recognition, and more. We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together. Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.

Why do we need an AI supercomputer at this scale?

Meta has been committed to long-term investment in AI since 2013, when we created the Facebook AI Research lab. In recent years, we’ve made significant strides in AI thanks to our leadership in a number of areas, including self-supervised learning, where algorithms can learn from vast numbers of unlabeled examples, and transformers, which allow AI models to reason more effectively by focusing on certain areas of their input.

To fully realize the benefits of self-supervised learning and transformer-based models, various domains, whether vision, speech, language, or for critical use cases like identifying harmful content, will require training increasingly large, complex, and adaptable models. Computer vision, for example, needs to process larger, longer videos with higher data sampling rates. Speech recognition needs to work well even in challenging scenarios with lots of background noise, such as parties or concerts. NLP needs to understand more languages, dialects, and accents. And advances in other areas, including robotics, embodied AI, and multimodal AI will help people accomplish useful tasks in the real world.

High-performance computing infrastructure is a critical component in training such large models, and Meta’s AI research team has been building these high-powered systems for many years. The first generation of this infrastructure, designed in 2017, has 22,000 NVIDIA V100 Tensor Core GPUs in a single cluster that performs 35,000 training jobs a day. Up until now, this infrastructure has set the bar for Meta’s researchers in terms of its performance, reliability, and productivity.

In early 2020, we decided the best way to accelerate progress was to design a new computing infrastructure from a clean slate to take advantage of new GPU and network fabric technology. We wanted this infrastructure to be able to train models with more than a trillion parameters on data sets as large as an exabyte — which, to provide a sense of scale, is the equivalent of 36,000 years of high-quality video.

While the high-performance computing community has been tackling scale for decades, we also had to make sure we have all the needed security and privacy controls in place to protect any training data we use. Unlike with our previous AI research infrastructure, which leveraged only open source and other publicly available data sets, RSC also helps us ensure that our research translates effectively into practice by allowing us to include real-world examples from Meta’s production systems in model training. By doing this, we can help advance research to perform downstream tasks such as identifying harmful content on our platforms as well as research into embodied AI and multimodal AI to help improve user experiences on our family of apps. We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale.

RSC: Under the hood

AI supercomputers are built by combining multiple GPUs into compute nodes, which are then connected by a high-performance network fabric to allow fast communication between those GPUs. RSC today comprises a total of 760 NVIDIA DGX A100 systems as its compute nodes, for a total of 6,080 GPUs — with each A100 GPU being more powerful than the V100 used in our previous system. Each DGX communicates via an NVIDIA Quantum 1600 Gb/s InfiniBand two-level Clos fabric that has no oversubscription. RSC’s storage tier has 175 petabytes of Pure Storage FlashArray, 46 petabytes of cache storage in Penguin Computing Altus systems, and 10 petabytes of Pure Storage FlashBlade.

Early benchmarks on RSC, compared with Meta’s legacy production and research infrastructure, have shown that it runs computer vision workflows up to 20 times faster, runs the NVIDIA Collective Communication Library (NCCL) more than nine times faster, and trains large-scale NLP models three times faster. That means a model with tens of billions of parameters can finish training in three weeks, compared with nine weeks before.

Building an AI supercomputer…

Designing and building something like RSC isn’t a matter of performance alone but performance at the largest scale possible, with the most advanced technology available today. When RSC is complete, the InfiniBand network fabric will connect 16,000 GPUs as endpoints, making it one of the largest such networks deployed to date. Additionally, we designed a caching and storage system that can serve 16 TB/s of training data, and we plan to scale it up to 1 exabyte.

All this infrastructure must be extremely reliable, as we estimate some experiments could run for weeks and require thousands of GPUs. Lastly, the entire experience of using RSC has to be researcher-friendly so our teams can easily explore a wide range of AI models.

A big part of achieving this was in working with a number of long-time partners, all of whom also helped design the first generation of our AI infrastructure in 2017. Penguin Computing, an SGH company, our architecture and managed services partner, worked with our operations team on hardware integration to deploy the cluster and helped set up major parts of the control plane. Pure Storage provided us with a robust and scalable storage solution. And NVIDIA provided us with its AI computing technologies featuring cutting-edge systems, GPUs, and InfiniBand fabric, and software stack components like NCCL for the cluster.

…and doing it remotely, during a pandemic

But there were other unexpected challenges that arose in RSC’s development — namely the coronavirus pandemic. RSC began as a completely remote project that the team took from a simple shared document to a functioning cluster in about a year and a half. COVID-19 and industry-wide wafer supply constraints also brought supply chain issues that made it difficult to get everything from chips to components like optics and GPUs, and even construction materials — all of which had to be transported in accordance with new safety protocols. To build this cluster efficiently, we had to design it from scratch, creating many entirely new Meta-specific conventions and rethinking previous ones along the way. We had to write new rules around our data center designs — including their cooling, power, rack layout, cabling, and networking (including a completely new control plane), among other important considerations. We had to ensure that all the teams, from construction to hardware to software and AI, were working in lockstep and in coordination with our partners.

Beyond the core system itself, there was also a need for a powerful storage solution, one that can serve terabytes of bandwidth from an exabyte-scale storage system. To serve AI training’s growing bandwidth and capacity needs, we developed a storage service, AI Research Store (AIRStore), from the ground up. To optimize for AI models, AIRStore utilizes a new data preparation phase that preprocesses the data set to be used for training. Once the preparation is performed one time, the prepared data set can be used for multiple training runs until it expires. AIRStore also optimizes data transfers so that cross-region traffic on Meta’s inter-datacenter backbone is minimized.

How we safeguard data in RSC

To build new AI models that benefit the people using our services — whether that’s detecting harmful content or creating new AR experiences — we need to teach models using real-world data from our production systems. RSC has been designed from the ground up with privacy and security in mind, so that Meta’s researchers can safely train models using encrypted user-generated data that is not decrypted until right before training. For example, RSC is isolated from the larger internet, with no direct inbound or outbound connections, and traffic can flow only from Meta’s production data centers.

To meet our privacy and security requirements, the entire data path from our storage systems to the GPUs is end-to-end encrypted and has the necessary tools and processes to verify that these requirements are met at all times. Before data is imported to RSC, it must go through a privacy review process to confirm it has been correctly anonymized or alternative privacy safeguards have been put in place to protect the data. The data is then encrypted before it can be used to train AI models and both the data and the decryption keys are deleted regularly to ensure older data is not still accessible. And since the data is only decrypted at one endpoint, in memory, it is safeguarded even in the unlikely event of a physical breach of the facility. (Updated on January 28, 2022 at 2:30PM PT to note that alternative privacy safeguards must be put in place to protect data in instances where certain types of data (i.e., audio, images, and video) cannot be fully anonymized for research purposes, and to clarify that data is also regularly deleted).

Phase two and beyond

RSC is up and running today, but its development is ongoing. Once we complete phase two of building out RSC, we believe it will be the fastest AI supercomputer in the world, performing at nearly 5 exaflops of mixed precision compute. Through 2022, we’ll work to increase the number of GPUs from 6,080 to 16,000, which will increase AI training performance by more than 2.5x. The InfiniBand fabric will expand to support 16,000 ports in a two-layer topology with no oversubscription. The storage system will have a target delivery bandwidth of 16 TB/s and exabyte-scale capacity to meet increased demand.

We expect such a step function change in compute capability to enable us not only to create more accurate AI models for our existing services, but also to enable completely new user experiences, especially in the metaverse. Our long-term investments in self-supervised learning and in building next-generation AI infrastructure with RSC are helping us create the foundational technologies that will power the metaverse and advance the broader AI community as well."
Meta_Blog,https://ai.meta.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/,,"Data2vec: The first high-performance self-supervised algorithm that works for speech, vision, and text","Self-supervised learning — where machines learn by directly observing the environment rather than being explicitly taught through labeled images, text, audio, and other data sources — has powered many significant recent advances in AI. But while people appear to learn in a similar way regardless of how they get information — whether they use sight or sound, for example — there are currently big differences in the way self-supervised learning algorithms learn from images, speech, text, and other modalities.

This discrepancy has been a significant barrier to applying advances in self-supervised learning more broadly. Because a powerful algorithm designed for, say, understanding images can’t be directly applied to another modality, such as text, it is difficult to push several modalities ahead at the same rate.

This is why Meta AI developed and is excited to announce data2vec, the first high-performance self-supervised algorithm that works for multiple modalities. We apply data2vec separately to speech, images and text and it outperformed the previous best single-purpose algorithms for computer vision and speech and it is competitive on NLP tasks. It also represents a new paradigm of holistic self-supervised learning, where new research improves multiple modalities rather than just one. It also does not rely on contrastive learning or reconstructing the input example. In addition to helping accelerate progress in AI, data2vec brings us closer to building machines that learn seamlessly about different aspects of the world around them. It will enable us to develop more adaptable AI, which we believe will be able to perform tasks beyond what today’s systems can do.

As part of this announcement, we are sharing code and pretrained models on data2vec so that others in the research community can build upon our work.

How data2vec works

Much of AI is still based on supervised learning, which works exclusively with labeled data. But it’s simply not possible to collect labeled data for all the things we would like machines to do. For example, while researchers have done a lot of work in creating large-scale labeled data sets for English speech and text, it is not feasible to do this for the literally thousands of languages spoken on the planet.

Self-supervision enables computers to learn about the world just by observing it and then figuring out the structure of images, speech, or text. Having machines that don’t need to be explicitly taught to classify images or understand spoken language is simply much more scalable.

Research in self-supervised learning today is almost always focused on one particular modality. So, researchers working on one modality often take a very different approach from those working on another. For text, researchers train models to fill in blanks in sentences. Speech models, however, need to learn an inventory of the basic sounds of speech in order to predict missing sounds. In computer vision, models are often trained to assign similar representations to a color image of a cow and the same image flipped upside down, so it associates the two much more closely than it would with an unrelated image, such as that of a duck.

Algorithms also predict different units for each modality: pixels or visual tokens for images, words for text, and learned inventories of sounds for speech. A collection of pixels is very different from an audio waveform or a passage of text, and because of this, algorithm design has been tied to a specific modality. This means that algorithms are still functioning differently in each modality.

data2vec learns in the same way for images, speech, and text.

Data2vec simplifies this by training models to predict their own representations of the input data, regardless of the modality. By focusing on these representations — the layers of a neural network — instead of predicting visual tokens, words, or sounds, a single algorithm can work with completely different types of input. This removes the dependence on modality-specific targets in the learning task. Directly predicting representations is not straightforward, and it required defining a robust normalization of the features for the task that would be reliable in different modalities.

Our method uses a teacher network to first compute target representations from an image, a piece of text, or a speech utterance. Next, we mask part of the input and repeat the process with a student network, which then predicts the latent representations of the teacher. The student model has to predict representations of the full input data even though it has a view of only some of the information. The teacher network is identical to the student model but with weights that are slightly out of date.

We tested the method on the popular ImageNet computer vision benchmark, where it performed better than existing methods for popular model sizes. On speech, we found that it performed better than wav2vec 2.0 or HuBERT, two previous Meta AI self-supervised algorithm for speech. For text, we tested it on the popular GLUE benchmark suite, and it performed as well as RoBERTa, a reimplementation of BERT.

Data2vec for computer vision: performance on the popular ImageNet benchmark for ViT-B models compared with other recent methods.

Data2vec for speech: performance for Base models on the LibriSpeech benchmark with 10h labeled data compared with other recent methods. Lower error rate indicates better performance.

Data2vec for text: performance on the GLUE natural language understanding benchmark for Base models compared with RoBERTa when retrained with the original BERT settings. Higher score indicates better performance.

Toward machines that learn from observing the world around them

While self-supervised learning has made great progress in computer vision, videos, and other individual modalities through different learning objectives, the core idea of this approach is to learn more generally: AI should be able to learn to do many different tasks, including those that are entirely unfamiliar. We want a machine to not only recognize animals shown in its training data but also adapt to recognize new creatures if we tell it what they look like. Data2vec demonstrates that the same self-supervised algorithm can work well in different modalities — and often better than the best existing algorithms. This paves the way for more general self-supervised learning and brings us closer to a world where AI might use videos, articles, and audio recordings to learn about complicated subjects, such as the game of soccer or different ways to bake bread. We also hope data2vec will bring us closer to a world where computers need very little labeled data in order to accomplish tasks. Since it is difficult and sometimes impossible to collect annotated examples — to train speech recognition models for thousands of languages, for example — data2vec is an important step toward more general AI. This project complements research on general model architectures, and we hope that in the future we can remove the need for modality-specific feature extractors by combining these two lines of work.

Access the open source code and release pretrained models here and read the paper here.

This blog post was made possible by the work of Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli."
Meta_Blog,https://ai.meta.com/blog/the-first-high-performance-self-supervised-algorithm-that-works-for-speech-vision-and-text/,,"Data2vec: The first high-performance self-supervised algorithm that works for speech, vision, and text","Self-supervised learning — where machines learn by directly observing the environment rather than being explicitly taught through labeled images, text, audio, and other data sources — has powered many significant recent advances in AI. But while people appear to learn in a similar way regardless of how they get information — whether they use sight or sound, for example — there are currently big differences in the way self-supervised learning algorithms learn from images, speech, text, and other modalities.

This discrepancy has been a significant barrier to applying advances in self-supervised learning more broadly. Because a powerful algorithm designed for, say, understanding images can’t be directly applied to another modality, such as text, it is difficult to push several modalities ahead at the same rate.

This is why Meta AI developed and is excited to announce data2vec, the first high-performance self-supervised algorithm that works for multiple modalities. We apply data2vec separately to speech, images and text and it outperformed the previous best single-purpose algorithms for computer vision and speech and it is competitive on NLP tasks. It also represents a new paradigm of holistic self-supervised learning, where new research improves multiple modalities rather than just one. It also does not rely on contrastive learning or reconstructing the input example. In addition to helping accelerate progress in AI, data2vec brings us closer to building machines that learn seamlessly about different aspects of the world around them. It will enable us to develop more adaptable AI, which we believe will be able to perform tasks beyond what today’s systems can do.

As part of this announcement, we are sharing code and pretrained models on data2vec so that others in the research community can build upon our work.

How data2vec works

Much of AI is still based on supervised learning, which works exclusively with labeled data. But it’s simply not possible to collect labeled data for all the things we would like machines to do. For example, while researchers have done a lot of work in creating large-scale labeled data sets for English speech and text, it is not feasible to do this for the literally thousands of languages spoken on the planet.

Self-supervision enables computers to learn about the world just by observing it and then figuring out the structure of images, speech, or text. Having machines that don’t need to be explicitly taught to classify images or understand spoken language is simply much more scalable.

Research in self-supervised learning today is almost always focused on one particular modality. So, researchers working on one modality often take a very different approach from those working on another. For text, researchers train models to fill in blanks in sentences. Speech models, however, need to learn an inventory of the basic sounds of speech in order to predict missing sounds. In computer vision, models are often trained to assign similar representations to a color image of a cow and the same image flipped upside down, so it associates the two much more closely than it would with an unrelated image, such as that of a duck.

Algorithms also predict different units for each modality: pixels or visual tokens for images, words for text, and learned inventories of sounds for speech. A collection of pixels is very different from an audio waveform or a passage of text, and because of this, algorithm design has been tied to a specific modality. This means that algorithms are still functioning differently in each modality.

data2vec learns in the same way for images, speech, and text.

Data2vec simplifies this by training models to predict their own representations of the input data, regardless of the modality. By focusing on these representations — the layers of a neural network — instead of predicting visual tokens, words, or sounds, a single algorithm can work with completely different types of input. This removes the dependence on modality-specific targets in the learning task. Directly predicting representations is not straightforward, and it required defining a robust normalization of the features for the task that would be reliable in different modalities.

Our method uses a teacher network to first compute target representations from an image, a piece of text, or a speech utterance. Next, we mask part of the input and repeat the process with a student network, which then predicts the latent representations of the teacher. The student model has to predict representations of the full input data even though it has a view of only some of the information. The teacher network is identical to the student model but with weights that are slightly out of date.

We tested the method on the popular ImageNet computer vision benchmark, where it performed better than existing methods for popular model sizes. On speech, we found that it performed better than wav2vec 2.0 or HuBERT, two previous Meta AI self-supervised algorithm for speech. For text, we tested it on the popular GLUE benchmark suite, and it performed as well as RoBERTa, a reimplementation of BERT.

Data2vec for computer vision: performance on the popular ImageNet benchmark for ViT-B models compared with other recent methods.

Data2vec for speech: performance for Base models on the LibriSpeech benchmark with 10h labeled data compared with other recent methods. Lower error rate indicates better performance.

Data2vec for text: performance on the GLUE natural language understanding benchmark for Base models compared with RoBERTa when retrained with the original BERT settings. Higher score indicates better performance.

Toward machines that learn from observing the world around them

While self-supervised learning has made great progress in computer vision, videos, and other individual modalities through different learning objectives, the core idea of this approach is to learn more generally: AI should be able to learn to do many different tasks, including those that are entirely unfamiliar. We want a machine to not only recognize animals shown in its training data but also adapt to recognize new creatures if we tell it what they look like. Data2vec demonstrates that the same self-supervised algorithm can work well in different modalities — and often better than the best existing algorithms. This paves the way for more general self-supervised learning and brings us closer to a world where AI might use videos, articles, and audio recordings to learn about complicated subjects, such as the game of soccer or different ways to bake bread. We also hope data2vec will bring us closer to a world where computers need very little labeled data in order to accomplish tasks. Since it is difficult and sometimes impossible to collect annotated examples — to train speech recognition models for thousands of languages, for example — data2vec is an important step toward more general AI. This project complements research on general model architectures, and we hope that in the future we can remove the need for modality-specific feature extractors by combining these two lines of work.

Access the open source code and release pretrained models here and read the paper here.

This blog post was made possible by the work of Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli."
Meta_Blog,https://ai.meta.com/blog/ai-that-understands-speech-by-looking-as-well-as-hearing/,,AI that understands speech by looking as well as hearing,"People use AI for a wide range of speech recognition and understanding tasks, from enabling smart speakers to developing tools for people who are hard of hearing or who have speech impairments. But oftentimes these speech understanding systems don’t work well in the everyday situations when we need them most: Where multiple people are speaking simultaneously or when there’s lots of background noise. Even sophisticated noise-suppression techniques are often no match for, say, the sound of the ocean during a family beach trip or the background chatter of a bustling street market.

One reason why people can understand speech better than AI in these instances is that we use not just our ears but also our eyes. We might see someone’s mouth moving and intuitively know the voice we’re hearing must be coming from her, for example. That’s why Meta AI is working on new conversational AI systems that can recognize the nuanced correlations between what they see and what they hear in conversation, like we do.

To help us build these more versatile and robust speech recognition tools, we are announcing Audio-Visual Hidden Unit BERT (AV-HuBERT), a state-of-the-art self-supervised framework for understanding speech that learns by both seeing and hearing people speak. It is the first system to jointly model speech and lip movements from unlabeled data — raw video that has not already been transcribed. Using the same amount of transcriptions, AV-HuBERT is 75 percent more accurate than the best audio-visual speech recognition systems (which use both sound and images of the speaker to understand what the person is saying). Notably, our system overcomes an important limitation in training AI to perform useful tasks: AV-HuBERT outperforms the previous best audio-visual speech recognition system using one-tenth labeled data. Since large amounts of labeled data are difficult to obtain for most of the world’s languages, AV-HuBERT’s self-supervised approach will help us build noise-robust automatic speech recognition (ASR) systems in more languages and for more applications.

Something Went Wrong We're having trouble playing this video. Learn more

By incorporating data on both visual lip movement and spoken language, AV-HuBERT will bring assistants closer to human-level speech perception. This technology could one day enable assistants on smartphones and augmented reality (AR) glasses to understand what we’re telling them no matter the circumstances — whether on a noisy factory floor, at a concert, or just speaking while a plane flies overhead.

We are making our code and the pretrained AV-HuBERT models available to other researchers working in this domain so the broader research community can build on our work and accelerate progress in ASR.

A new, multimodal way to build speech recognition

Today’s speech recognition models use just audio as their input, so they have to guess at whether one person or several people are speaking or whether a sound is just background noise. AV-HuBERT, however, learns similarly to how people master new skills — multimodally — by perceiving and learning language through a combination of audio and lip-movement cues. We trained the model using video recordings — the publicly available LRS3 and VoxCeleb data sets.

By combining visual cues, such as the movement of the lips and teeth during speaking, along with auditory information for representation learning, AV-HuBERT can capture nuanced associations between the two input streams efficiently even with much smaller amounts of untranscribed video data for pretraining. Once the pretrained model learns the structure and correlation well, only a small amount of labeled data is needed to train a model for a particular task or a different language.

The animation below demonstrates the AV-HuBERT approach. It encodes masked audio and image sequences into audio-visual features via a hybrid ResNet-Transformer architecture to predict the predetermined sequence of discrete cluster assignments. Motivated by Meta AI’s audio HuBERT approach for learning self-supervised speech representations, AV-HuBERT’s target cluster assignments are initially generated from signal processing-based acoustic features (e.g., Mel-frequency cepstral coefficients, or MFCCs) and then iteratively refined using the features learned by the audio-visual encoder via k-means clustering.

AV-HuBERT simultaneously captures linguistic and phonetic information for unmasked regions from both the lip-movement and audio streams into its latent representations, then encodes their long-range temporal relationships to solve the masked-prediction task, similar to the BERT model. The contextualized representations learned by AV-HuBERT also show excellent transferability to tasks where the model can see but not hear the speaker. AV-HuBERT is 20 percent more accurate in this setting than the current state-of-the-art approach, outperforming the best visual-only speech recognition models using one-thousandth the amount of labeled data.

Something Went Wrong We're having trouble playing this video. Learn more

Better performance with less training data

When speech and background noise are equally loud, the previous state-of-the-art AV-ASR achieves a 25.5 percent error rate when trained on 433 hours of labeled data. Using the same amount of labeled data, AV-HuBERT achieves a 3.2 percent error rate, meaning it makes just one mistake in every 30 words it hears. When the interfering speech is as loud as the target speech, it is impossible for an audio-only speech recognition model to know which speaker to transcribe. In contrast, our audio-visual model learns to transcribe the speech of only the person who it sees is speaking. AV-HuBERT achieves a 2.9 percent WER, while an audio-only model without pretraining achieves only a 37.3 percent WER in this scenario.

In a low-resource setup with 30 hours of labeled data, on a test set with four types of noise (babble, interfering speech, music, other) and a wide range of signal-to-noise ratios (from -10dB to 10dB), we observe an average of 51.4 percent absolute WER reduction (59.2 percent → 7.8 percent) from AV-HuBERT compared with an audio-only ASR model without pretraining, and 35.1 percent absolute WER reduction compared with an audio-visual ASR model without pretraining.

When the system can see the speaker but not hear him or her, the previous state-of-the-art model can reach a 33.6 percent WER on the standard LRS3 benchmark data set after being trained on 31,000 hours of transcribed video data. Our approach beats the supervised state of the art, reaching a 28.6 percent WER with just 30 hours of labeled data and an order of magnitude less unsupervised video data. What’s more, when using 433 hours of labeled data, we achieve a new state of the art at 26.9 percent WER.

What’s next

AV-HuBERT will do more than just allow us to develop conversational AI systems that can be used in challenging scenarios. Since it requires far less supervised data for training, it will also open up possibilities for developing conversational AI models for hundreds of millions of people around the globe who don’t speak languages such as English, Mandarin, and Spanish, which have large-scale labeled data sets.

Since AV-HuBERT learns from both voice and mouth movements, it may be useful for researchers working on building more inclusive speech recognition models for people with speech impairments. By capturing the fine correlations between sounds and mouth movements, self-supervised audio-visual representations could also be used to help detect deepfakes and other content that’s been manipulated in order to mislead people. It could also help generate realistic lip movements in virtual reality avatars, in order to deliver a true sense of presence — that feeling of being there with someone even if they’re on the other side of the world.

We will continue to benchmark and develop approaches that improve audio-visual speech recognition models in everyday scenarios where background noise and speaker overlap are commonplace. We would also like to extend our model to multilingual benchmarks beyond English. Ultimately, we hope that AV-HuBERT will help us and others build new speech recognition tools that work well for everyone, regardless of the language they speak or the circumstances in which they are using it.

Read the paper: Learning audio-visual speech representation by masked multimodal cluster prediction

Read the paper: Robust self-supervised audio-visual speech recognition

Check our models and code"
Meta_Blog,https://ai.meta.com/blog/ai-that-understands-speech-by-looking-as-well-as-hearing/,,AI that understands speech by looking as well as hearing,"People use AI for a wide range of speech recognition and understanding tasks, from enabling smart speakers to developing tools for people who are hard of hearing or who have speech impairments. But oftentimes these speech understanding systems don’t work well in the everyday situations when we need them most: Where multiple people are speaking simultaneously or when there’s lots of background noise. Even sophisticated noise-suppression techniques are often no match for, say, the sound of the ocean during a family beach trip or the background chatter of a bustling street market.

One reason why people can understand speech better than AI in these instances is that we use not just our ears but also our eyes. We might see someone’s mouth moving and intuitively know the voice we’re hearing must be coming from her, for example. That’s why Meta AI is working on new conversational AI systems that can recognize the nuanced correlations between what they see and what they hear in conversation, like we do.

To help us build these more versatile and robust speech recognition tools, we are announcing Audio-Visual Hidden Unit BERT (AV-HuBERT), a state-of-the-art self-supervised framework for understanding speech that learns by both seeing and hearing people speak. It is the first system to jointly model speech and lip movements from unlabeled data — raw video that has not already been transcribed. Using the same amount of transcriptions, AV-HuBERT is 75 percent more accurate than the best audio-visual speech recognition systems (which use both sound and images of the speaker to understand what the person is saying). Notably, our system overcomes an important limitation in training AI to perform useful tasks: AV-HuBERT outperforms the previous best audio-visual speech recognition system using one-tenth labeled data. Since large amounts of labeled data are difficult to obtain for most of the world’s languages, AV-HuBERT’s self-supervised approach will help us build noise-robust automatic speech recognition (ASR) systems in more languages and for more applications.

Something Went Wrong We're having trouble playing this video. Learn more

By incorporating data on both visual lip movement and spoken language, AV-HuBERT will bring assistants closer to human-level speech perception. This technology could one day enable assistants on smartphones and augmented reality (AR) glasses to understand what we’re telling them no matter the circumstances — whether on a noisy factory floor, at a concert, or just speaking while a plane flies overhead.

We are making our code and the pretrained AV-HuBERT models available to other researchers working in this domain so the broader research community can build on our work and accelerate progress in ASR.

A new, multimodal way to build speech recognition

Today’s speech recognition models use just audio as their input, so they have to guess at whether one person or several people are speaking or whether a sound is just background noise. AV-HuBERT, however, learns similarly to how people master new skills — multimodally — by perceiving and learning language through a combination of audio and lip-movement cues. We trained the model using video recordings — the publicly available LRS3 and VoxCeleb data sets.

By combining visual cues, such as the movement of the lips and teeth during speaking, along with auditory information for representation learning, AV-HuBERT can capture nuanced associations between the two input streams efficiently even with much smaller amounts of untranscribed video data for pretraining. Once the pretrained model learns the structure and correlation well, only a small amount of labeled data is needed to train a model for a particular task or a different language.

The animation below demonstrates the AV-HuBERT approach. It encodes masked audio and image sequences into audio-visual features via a hybrid ResNet-Transformer architecture to predict the predetermined sequence of discrete cluster assignments. Motivated by Meta AI’s audio HuBERT approach for learning self-supervised speech representations, AV-HuBERT’s target cluster assignments are initially generated from signal processing-based acoustic features (e.g., Mel-frequency cepstral coefficients, or MFCCs) and then iteratively refined using the features learned by the audio-visual encoder via k-means clustering.

AV-HuBERT simultaneously captures linguistic and phonetic information for unmasked regions from both the lip-movement and audio streams into its latent representations, then encodes their long-range temporal relationships to solve the masked-prediction task, similar to the BERT model. The contextualized representations learned by AV-HuBERT also show excellent transferability to tasks where the model can see but not hear the speaker. AV-HuBERT is 20 percent more accurate in this setting than the current state-of-the-art approach, outperforming the best visual-only speech recognition models using one-thousandth the amount of labeled data.

Something Went Wrong We're having trouble playing this video. Learn more

Better performance with less training data

When speech and background noise are equally loud, the previous state-of-the-art AV-ASR achieves a 25.5 percent error rate when trained on 433 hours of labeled data. Using the same amount of labeled data, AV-HuBERT achieves a 3.2 percent error rate, meaning it makes just one mistake in every 30 words it hears. When the interfering speech is as loud as the target speech, it is impossible for an audio-only speech recognition model to know which speaker to transcribe. In contrast, our audio-visual model learns to transcribe the speech of only the person who it sees is speaking. AV-HuBERT achieves a 2.9 percent WER, while an audio-only model without pretraining achieves only a 37.3 percent WER in this scenario.

In a low-resource setup with 30 hours of labeled data, on a test set with four types of noise (babble, interfering speech, music, other) and a wide range of signal-to-noise ratios (from -10dB to 10dB), we observe an average of 51.4 percent absolute WER reduction (59.2 percent → 7.8 percent) from AV-HuBERT compared with an audio-only ASR model without pretraining, and 35.1 percent absolute WER reduction compared with an audio-visual ASR model without pretraining.

When the system can see the speaker but not hear him or her, the previous state-of-the-art model can reach a 33.6 percent WER on the standard LRS3 benchmark data set after being trained on 31,000 hours of transcribed video data. Our approach beats the supervised state of the art, reaching a 28.6 percent WER with just 30 hours of labeled data and an order of magnitude less unsupervised video data. What’s more, when using 433 hours of labeled data, we achieve a new state of the art at 26.9 percent WER.

What’s next

AV-HuBERT will do more than just allow us to develop conversational AI systems that can be used in challenging scenarios. Since it requires far less supervised data for training, it will also open up possibilities for developing conversational AI models for hundreds of millions of people around the globe who don’t speak languages such as English, Mandarin, and Spanish, which have large-scale labeled data sets.

Since AV-HuBERT learns from both voice and mouth movements, it may be useful for researchers working on building more inclusive speech recognition models for people with speech impairments. By capturing the fine correlations between sounds and mouth movements, self-supervised audio-visual representations could also be used to help detect deepfakes and other content that’s been manipulated in order to mislead people. It could also help generate realistic lip movements in virtual reality avatars, in order to deliver a true sense of presence — that feeling of being there with someone even if they’re on the other side of the world.

We will continue to benchmark and develop approaches that improve audio-visual speech recognition models in everyday scenarios where background noise and speaker overlap are commonplace. We would also like to extend our model to multilingual benchmarks beyond English. Ultimately, we hope that AV-HuBERT will help us and others build new speech recognition tools that work well for everyone, regardless of the language they speak or the circumstances in which they are using it.

Read the paper: Learning audio-visual speech representation by masked multimodal cluster prediction

Read the paper: Robust self-supervised audio-visual speech recognition

Check our models and code"
Meta_Blog,https://ai.meta.com/blog/using-ai-to-bring-childrens-drawings-to-life/,,Using AI to bring children’s drawings to life,"Children draw fascinatingly unique and inventive characters that push our imaginations and require us to think a little differently to recognize the people and things in their pictures. While it can be fairly simple for a parent or teacher to see what a child’s drawing is meant to show, AI struggles with this task. Kids’ drawings are often constructed in abstract, fanciful ways, so if a figure’s feet are placed precariously or if both arms are on the same side of its body, it confuses even state-of-the-art AI systems that excel at spotting objects in photorealistic images and drawings.

Meta AI researchers are working to overcome this challenge so that AI systems will be better able to recognize drawings of human figures in the wildly varied ways that children create them.

We’re excited to announce a first-of-its-kind method for automatically animating children’s hand-drawn figures of people and humanlike characters (i.e., a character with two arms, two legs, a head, etc.) that bring these drawings to life in a matter of minutes using AI. By uploading them to our prototype system, parents and children can experience the excitement of watching their drawings become moving characters that dance, skip, and jump. And they can even download their animated drawings to share with friends and family. If parents choose, they can also submit those drawings to help improve the AI model.

Something Went Wrong We're having trouble playing this video. Learn more

By teaching AI to work effectively with this quintessential human form of creativity, we hope this project will move us closer to building AI that can understand the world from a human point of view. We also hope this work will spur more research on using AI to enhance people’s creativity and inspire imaginative new uses for this technology.

Why automatic AI animation tools don’t work on children’s drawings

Our goal was to build an AI system that can identify and automatically animate the humanlike figures in children’s drawings with a high success rate and without any human guidance. While many AI tools and techniques are designed to handle realistic images of humans, children’s drawings add a level of variety and unpredictability that makes identifying what’s being portrayed much more complex. “Humans” in children’s drawings come in many different forms, colors, sizes, and scales, with little similarity when it comes to body symmetry, morphology, and point of view. We approached this AI challenge through a four-step process, fine-tuning our approach at each stage to adapt to the enormous variety present in children’s drawings.

Something Went Wrong We're having trouble playing this video. Learn more

Identifying humanlike figures through object detection

The first step in animating children’s drawings of people is distinguishing the human figures from the background and from other types of characters in the picture. Object detection using existing techniques works quite well on children’s drawings, but the segmentation masks aren’t accurate enough to be used for animation. To address this, we instead use the bounding boxes obtained from the object detector and apply a series of morphological operations and image processing steps to obtain masks.

When extracting the humanlike characters within a child’s drawing for processing, we use Meta AI’s convolutional neural network–based object detection model, Mask R-CNN, as implemented in Detectron2. Mask R-CNN is pretrained on one of the largest publicly available segmentation data sets, but it’s made up of photos of real-world objects, not drawings. To work on drawings, the model needed to be fine-tuned, which we did with ResNet-50+FPN to predict a single class, “human figure.” We invited our colleagues at Meta to share and animate their kids’ artwork using our system, and we obtained approximately 1,000 drawings that helped us train the AI.

After the fine-tuning process, the network did a good job of detecting human figures within the test data set. The failure cases we observed fell into four different categories: not including the entire figure, not separating the figure from the background, not separating several figures drawn close together, and incorrectly identifying nonhuman figures (such as trees). We believe these types of failures stem from the wide variety in human figures in the training set and the model will continue to improve as it gets more drawings to learn from.

Something Went Wrong We're having trouble playing this video. Learn more

Lifting the humanlike figure from the scene using character masking

After identifying and extracting a human figure from a drawing, the next step in preparing for animation is to separate it from other parts of the scene and the background in a process called masking. The mask must closely mirror the contours of the figure because it will be used to create a mesh, which will then be deformed to produce the animation. When properly done, a mask will include all parts of the character and nothing from the background.

Even though Mask R-CNN can output masks, we found they weren’t suitable for animation. The predicted masks often failed to capture the entire figure whenever the body parts varied greatly in appearance, such as in the figure below, which shows a large yellow triangle for a body and a single pencil stroke for the arm. The predicted masks also often failed by leaving out the middle of “hollow” characters, or characters drawn as outlines and not colored in.

Instead, we developed a classical image processing–based approach that is more robust to these variations. With this method, we crop the image using its predicted bounding box for each detected character. We then apply adaptive thresholding and morphological closing/dialating operations, flood fill from the edges of the box, and assume the mask is the largest polygon not touched by the flood fill. While this method is straightforward and effective for extracting accurate masks suitable for animation, it can fail when the background is cluttered, characters are drawn close together, or the paper has wrinkles, tears, or shadows on the page.

Segmentation masks from Mask R-CNN sometimes failed to closely follow the form of the character (middle, top) or to include all parts of the character, such as stick arms (middle, bottom). In many situations, using an image processing pipeline on Mask R-CNN’s predicted bounding box results in masks more suitable for animation (right).

Prepping for animation via rigging

Children draw figures with a huge variety of body shapes, far beyond the conventional human shape with a head, arms, legs, and a torso. Many children start out depicting humans as what are often called “tadpole people,” with no torso and with arms and legs attached directly to the head. Some children progress to “transitional” figures, which have legs extending from the head and arms extending from the upper legs. We needed a method of rigging that could handle this type of morphological variation.

We use AlphaPose, a model trained for human pose detection, to identify key points on the human figures that can serve as hips, shoulders, elbows, knees, wrists, and ankles. AlphaPose was trained on images of real people, so before we could adapt it to detecting poses in children’s drawings, we had to retrain it to handle the types of variation present in children’s drawings. We did this by internally collecting and annotating a small data set of children’s drawings of human figures. Then, using the pose detector trained on this initial data set, we created an internal tool that allows parents to upload and animate their children’s drawings and allows us to use the uploaded drawings for additional training. As more data came in, we iteratively retrained the model until we reached high levels of accuracy.

Animating 2D figures using 3D motion capture

Once we have mask and joint predictions, we have everything we need to produce the animation. We begin by using the extracted mask to generate a mesh, texturing it with the original drawing. Using the predicted joint locations, we create a skeleton for the character. By rotating the bones and using the new joint locations to deform the mesh, we can move the character into various poses. By moving the character into a series of consecutive poses, we can create an animation. We can select different motions to apply depending upon how confident the joint predictions are: In cases where both arms and legs have been predicted correctly, the animation can happen seamlessly. But if a limb is not present in the drawing, its joint confidence values would be low, and we would have to forgo animations that require that limb, ask the user to correct the prediction, or declare the animation a failure.

To animate the 2D figures using 3D motion capture, we take advantage of the fact that many children draw using what we refer to as a twisted perspective. It’s common for many children to initially draw body parts from their most identifiable point of view, which may be different from the way they would appear on actual humans. For instance, they tend to draw legs and feet from a side view and heads and torsos from a front view.

We take advantage of this perspective in our motion retargeting step. Independently for the lower and upper body, we automatically determine whether the motion is more recognizable from a front view or a side view. Using the selected views, we project the motion onto a single 2D plane and use it to drive the character. We validate the results of such a motion retargeting approach using perceptual user studies run via Mechanical Turk.

Left: Prior to animating, we create a rigged character from the drawing. Right: We repose the character by projecting a frame of motion capture data onto a 2D plane and rotating the character’s limbs to match those of the project. We can project the motion capture data from the front (top row), from the side (middle row), and from a twisted perspective (bottom).

Taking the twisted perspective into account is helpful because many types of motion do not cleanly fall onto a single plane of projection. For example, with jumping rope, the arms and wrists tend to move primarily in the frontal plane, while the bending legs tend to move in the sagittal plane. Because of this, we do not determine a single plane of motion for the motion capture pose but determine projection planes for the upper and lower body separately.

Using AI to power more complex animations

AI has become a powerful tool for creativity, empowering artists; inspiring new forms of self-expression, like AR effects; offering fashion advice; and even generating new dance routines. We hope our animation tool will inspire people to experiment with their drawings and take them in uncharted directions.

By sharing our work, we also hope to encourage more computer vision work in the domain of amateur drawings. Future research for this project could focus on identifying and applying more tailored motions to subcategories of figures, such as superheroes, princesses, monsters, and ninjas. A more fine-grained analysis of the parts of a character would also be useful for identifying antennae, tails, and capes, for example, and applying secondary motion elements in order to increase the animation’s appeal. Someday, perhaps, an AI system could take a complex drawing and then instantly create a detailed animated cartoon using multiple fantastical characters interacting with one another and elements from the background. With AR glasses, those stories could even seem to come to life in the real world, dancing or talking with the child who drew it just moments earlier. The possibilities are as limitless as the human imagination.

We invite you to test out the systems’ animation capabilities by uploading your children’s drawings to our prototype. In the coming year, we also hope to release the data set and share more details on our research."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-bring-childrens-drawings-to-life/,,Using AI to bring children’s drawings to life,"Children draw fascinatingly unique and inventive characters that push our imaginations and require us to think a little differently to recognize the people and things in their pictures. While it can be fairly simple for a parent or teacher to see what a child’s drawing is meant to show, AI struggles with this task. Kids’ drawings are often constructed in abstract, fanciful ways, so if a figure’s feet are placed precariously or if both arms are on the same side of its body, it confuses even state-of-the-art AI systems that excel at spotting objects in photorealistic images and drawings.

Meta AI researchers are working to overcome this challenge so that AI systems will be better able to recognize drawings of human figures in the wildly varied ways that children create them.

We’re excited to announce a first-of-its-kind method for automatically animating children’s hand-drawn figures of people and humanlike characters (i.e., a character with two arms, two legs, a head, etc.) that bring these drawings to life in a matter of minutes using AI. By uploading them to our prototype system, parents and children can experience the excitement of watching their drawings become moving characters that dance, skip, and jump. And they can even download their animated drawings to share with friends and family. If parents choose, they can also submit those drawings to help improve the AI model.

Something Went Wrong We're having trouble playing this video. Learn more

By teaching AI to work effectively with this quintessential human form of creativity, we hope this project will move us closer to building AI that can understand the world from a human point of view. We also hope this work will spur more research on using AI to enhance people’s creativity and inspire imaginative new uses for this technology.

Why automatic AI animation tools don’t work on children’s drawings

Our goal was to build an AI system that can identify and automatically animate the humanlike figures in children’s drawings with a high success rate and without any human guidance. While many AI tools and techniques are designed to handle realistic images of humans, children’s drawings add a level of variety and unpredictability that makes identifying what’s being portrayed much more complex. “Humans” in children’s drawings come in many different forms, colors, sizes, and scales, with little similarity when it comes to body symmetry, morphology, and point of view. We approached this AI challenge through a four-step process, fine-tuning our approach at each stage to adapt to the enormous variety present in children’s drawings.

Something Went Wrong We're having trouble playing this video. Learn more

Identifying humanlike figures through object detection

The first step in animating children’s drawings of people is distinguishing the human figures from the background and from other types of characters in the picture. Object detection using existing techniques works quite well on children’s drawings, but the segmentation masks aren’t accurate enough to be used for animation. To address this, we instead use the bounding boxes obtained from the object detector and apply a series of morphological operations and image processing steps to obtain masks.

When extracting the humanlike characters within a child’s drawing for processing, we use Meta AI’s convolutional neural network–based object detection model, Mask R-CNN, as implemented in Detectron2. Mask R-CNN is pretrained on one of the largest publicly available segmentation data sets, but it’s made up of photos of real-world objects, not drawings. To work on drawings, the model needed to be fine-tuned, which we did with ResNet-50+FPN to predict a single class, “human figure.” We invited our colleagues at Meta to share and animate their kids’ artwork using our system, and we obtained approximately 1,000 drawings that helped us train the AI.

After the fine-tuning process, the network did a good job of detecting human figures within the test data set. The failure cases we observed fell into four different categories: not including the entire figure, not separating the figure from the background, not separating several figures drawn close together, and incorrectly identifying nonhuman figures (such as trees). We believe these types of failures stem from the wide variety in human figures in the training set and the model will continue to improve as it gets more drawings to learn from.

Something Went Wrong We're having trouble playing this video. Learn more

Lifting the humanlike figure from the scene using character masking

After identifying and extracting a human figure from a drawing, the next step in preparing for animation is to separate it from other parts of the scene and the background in a process called masking. The mask must closely mirror the contours of the figure because it will be used to create a mesh, which will then be deformed to produce the animation. When properly done, a mask will include all parts of the character and nothing from the background.

Even though Mask R-CNN can output masks, we found they weren’t suitable for animation. The predicted masks often failed to capture the entire figure whenever the body parts varied greatly in appearance, such as in the figure below, which shows a large yellow triangle for a body and a single pencil stroke for the arm. The predicted masks also often failed by leaving out the middle of “hollow” characters, or characters drawn as outlines and not colored in.

Instead, we developed a classical image processing–based approach that is more robust to these variations. With this method, we crop the image using its predicted bounding box for each detected character. We then apply adaptive thresholding and morphological closing/dialating operations, flood fill from the edges of the box, and assume the mask is the largest polygon not touched by the flood fill. While this method is straightforward and effective for extracting accurate masks suitable for animation, it can fail when the background is cluttered, characters are drawn close together, or the paper has wrinkles, tears, or shadows on the page.

Segmentation masks from Mask R-CNN sometimes failed to closely follow the form of the character (middle, top) or to include all parts of the character, such as stick arms (middle, bottom). In many situations, using an image processing pipeline on Mask R-CNN’s predicted bounding box results in masks more suitable for animation (right).

Prepping for animation via rigging

Children draw figures with a huge variety of body shapes, far beyond the conventional human shape with a head, arms, legs, and a torso. Many children start out depicting humans as what are often called “tadpole people,” with no torso and with arms and legs attached directly to the head. Some children progress to “transitional” figures, which have legs extending from the head and arms extending from the upper legs. We needed a method of rigging that could handle this type of morphological variation.

We use AlphaPose, a model trained for human pose detection, to identify key points on the human figures that can serve as hips, shoulders, elbows, knees, wrists, and ankles. AlphaPose was trained on images of real people, so before we could adapt it to detecting poses in children’s drawings, we had to retrain it to handle the types of variation present in children’s drawings. We did this by internally collecting and annotating a small data set of children’s drawings of human figures. Then, using the pose detector trained on this initial data set, we created an internal tool that allows parents to upload and animate their children’s drawings and allows us to use the uploaded drawings for additional training. As more data came in, we iteratively retrained the model until we reached high levels of accuracy.

Animating 2D figures using 3D motion capture

Once we have mask and joint predictions, we have everything we need to produce the animation. We begin by using the extracted mask to generate a mesh, texturing it with the original drawing. Using the predicted joint locations, we create a skeleton for the character. By rotating the bones and using the new joint locations to deform the mesh, we can move the character into various poses. By moving the character into a series of consecutive poses, we can create an animation. We can select different motions to apply depending upon how confident the joint predictions are: In cases where both arms and legs have been predicted correctly, the animation can happen seamlessly. But if a limb is not present in the drawing, its joint confidence values would be low, and we would have to forgo animations that require that limb, ask the user to correct the prediction, or declare the animation a failure.

To animate the 2D figures using 3D motion capture, we take advantage of the fact that many children draw using what we refer to as a twisted perspective. It’s common for many children to initially draw body parts from their most identifiable point of view, which may be different from the way they would appear on actual humans. For instance, they tend to draw legs and feet from a side view and heads and torsos from a front view.

We take advantage of this perspective in our motion retargeting step. Independently for the lower and upper body, we automatically determine whether the motion is more recognizable from a front view or a side view. Using the selected views, we project the motion onto a single 2D plane and use it to drive the character. We validate the results of such a motion retargeting approach using perceptual user studies run via Mechanical Turk.

Left: Prior to animating, we create a rigged character from the drawing. Right: We repose the character by projecting a frame of motion capture data onto a 2D plane and rotating the character’s limbs to match those of the project. We can project the motion capture data from the front (top row), from the side (middle row), and from a twisted perspective (bottom).

Taking the twisted perspective into account is helpful because many types of motion do not cleanly fall onto a single plane of projection. For example, with jumping rope, the arms and wrists tend to move primarily in the frontal plane, while the bending legs tend to move in the sagittal plane. Because of this, we do not determine a single plane of motion for the motion capture pose but determine projection planes for the upper and lower body separately.

Using AI to power more complex animations

AI has become a powerful tool for creativity, empowering artists; inspiring new forms of self-expression, like AR effects; offering fashion advice; and even generating new dance routines. We hope our animation tool will inspire people to experiment with their drawings and take them in uncharted directions.

By sharing our work, we also hope to encourage more computer vision work in the domain of amateur drawings. Future research for this project could focus on identifying and applying more tailored motions to subcategories of figures, such as superheroes, princesses, monsters, and ninjas. A more fine-grained analysis of the parts of a character would also be useful for identifying antennae, tails, and capes, for example, and applying secondary motion elements in order to increase the animation’s appeal. Someday, perhaps, an AI system could take a complex drawing and then instantly create a detailed animated cartoon using multiple fantastical characters interacting with one another and elements from the background. With AR glasses, those stories could even seem to come to life in the real world, dancing or talking with the child who drew it just moments earlier. The possibilities are as limitless as the human imagination.

We invite you to test out the systems’ animation capabilities by uploading your children’s drawings to our prototype. In the coming year, we also hope to release the data set and share more details on our research."
Meta_Blog,https://ai.meta.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/,,Harmful content can evolve quickly. Our new AI system adapts to tackle it.,"Something Went Wrong We're having trouble playing this video. Learn more

Harmful content can evolve rapidly — whether fueled by current events or by people looking for new ways to evade our systems — and it’s crucial for AI systems to evolve alongside it. But AI needs to learn what to look for and it typically takes several months to collect and label thousands, if not millions, of examples necessary to train each individual AI system to spot a new type of content.

To address this bottleneck, we’ve built and recently deployed a new AI technology called Few-Shot Learner (FSL) that can adapt to take action on new or evolving types of harmful content within weeks instead of months. It not only works in more than 100 languages, but it also learns from different kinds of data, such as images and text, and it can strengthen existing AI models that are already deployed to detect other types of harmful content.

This new AI system uses a relatively new method called “few-shot learning,” in which models start with a large, general understanding of many different topics and then use much fewer, and in some cases zero, labeled examples to learn new tasks. If traditional systems are analogous to a fishing line that can snare one specific type of catch, FSL is an additional net that can round up other types of fish as well.

Recent scientific breakthroughs, like our self-supervised learning techniques and our new super-efficient infrastructure, have made it possible for the field to start shifting away from traditional, bespoke AI systems toward larger, more consolidated, and generalized systems with less reliance on labeled data. It’s first trained on billions of generic and open-source language examples. Then, the AI system is trained with policy-violating content and borderline contentwe’ve labeled over the years. Finally, it’s trained on condensed text explaining a new policy. Unlike previous systems that relied on pattern-matching with labeled data, FSL is pretrained on general language, as well as policy-violating and borderline content language, so it can learn the policy text implicitly.

We’ve tested FSL on a few relatively new events. For example, one recent task was to identify content that shares misleading or sensationalized information in a way that likely discourages COVID-19 vaccinations (for example, “Vaccine or DNA changer?”). In another, separate task, the new AI system improved an existing classifier that flags content that comes close to inciting violence (for example, “Does that guy need all of his teeth?”). The traditional approach may have missed these types of inflammatory posts, since there aren’t many labeled examples that use DNA language to create vaccine hesitancy or reference teeth to imply violence.

We carried out standardized offline and online A/B testing protocols to measure the performance of the model. In these tests, we looked at the delta between the prevalence of harmful content — i.e., the percentage of views of violating content people see — before and after FSL was rolled out on Facebook and Instagram. Meta AI Few-shot Learner was able to correctly detect posts that traditional systems may miss and helped reduce the prevalence of these types of harmful content. It does this by proactively detecting potentially harmful content and preventing it from spreading on our platforms. We’ve also seen that, in combination with existing classifiers, FSL has helped reduce the prevalence of other harmful content likehate speech.

We’re working on additional tests to improve classifiers that could benefit from more labeled training data, like those in countries that speak languages without large volumes of labeled training data, and we’ll continue to test it on newly emerging patterns of violating content. Of course, these are early days of intelligent, generalized AI models. There’s a long road ahead before AI can comprehend dozens of pages of policy text and immediately know exactly how to enforce it. We’re continuously advancing AI techniques and deploying them as fast as possible to better serve our community, and we believe FSL is a promising step forward.

Under the hood: Few-Shot Learner

Few-Shot Learner is a large-scale, multimodal, multilingual, zero or few-shot model that enables joint policy and content understanding, generalizes across integrity problems, and doesn’t require model fine-tuning. We are actively conducting research to train models that leverage simple policy sentences instead of hundreds or thousands of labeled examples.

Our new system works across three different scenarios, each of which require varying levels of labeled examples:

Zero-shot: Policy descriptions with no examples.

Few-shot with demonstration: Policy descriptions with a small set of examples (less than 50).

Low-shot with fine-tuning: ML developers can fine-tune on the FSL base model with a low number of training examples.

The overall input of FSL consists of three parts. First, building on our previous work with Whole Post Integrity Embeddings (WPIE), , it learns multimodal information from the whole post, including text, image, URL, etc. Second, it analyzes policy-related information, such as the definition of the policy, or labeled examples that indicate whether a particular post does or doesn’t violate that policy definition. Third, we also take additional labeled examples as demonstrations, if available.

As part of our novel approach, called Entailment Few-Shot Learning, the key idea is to convert the class label into a natural language sentence which can be used to describe the label, and determine if the example entails the label description. For example, we can reformulate an apparent sentiment classification input and label pair:

[x : ""I love your ethnic group. JK. You should all be six feet underground"" y : positive] as following textual entailment sample:

[x : I love your ethnic group. JK. You should all be 6 feet underground. This is hate speech. y : entailment].

We compared our proposed methods with several existing state-of-the-art few-shot learning methods. Through a series of systematic evaluations, we show that our method outperforms various state-of-the-art few-shot learning methods by up to 55 percent (and 12 percent on average). Read full details in our research paper here.

Bridging the gap between policy creation and automatic, ML-driven enforcement

We believe that FSL can, over time, enhance the performance of all of our integrity AI systems by letting them leverage a single, shared knowledge base and backbone to deal with many different types of violations. But it could also help policy, labeling, and investigation workflows to bridge the gap between human insight and classifier advancement.

FSL can be used to test out a new set of likely policy violations and understand the sensibility and validity of the proposed definitions. It casts a wider net, surfacing more types of “almost” content violations that policy teams should know about when deciding or formulating at-scale guidance for annotators who train new classifiers and the human reviewers helping to keep our platforms safe. Since it scales quickly, the time from policy framing to enforcement would shorten by orders of magnitude.

Toward humanlike AI that can learn more effectively

The ability to quickly begin enforcing against content types that don’t have lots of labeled training data is a major step forward and will help make our systems more agile and responsive to emerging challenges. Few-shot learning and zero-shot learning are one of many cutting-edge AI domains where we’ve been making major research investments. And we see no sign of research to the production pipeline slowing down. We’re actively working on key open research problems that go beyond content understanding only, and try to infer the cultural, behavioral, and conversational context around it.

There’s a lot more work to be done, but these early production results are an important milestone that signals a shift toward more intelligent, generalized AI systems that can quickly learn multiple tasks at once.

Our long-term vision is to achieve human-like learning flexibility and efficiency, making our integrity systems even faster and easier to train and better able to work with new information. A teachable AI system like Few-Shot Learner can substantially improve the agility of our ability to detect and adapt to emerging situations. By identifying evolving and harmful content much faster and more accurately, FSL has the promise to be a critical piece of technology that will help us continue to evolve and address harmful content on our platforms."
Meta_Blog,https://ai.meta.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/,,Harmful content can evolve quickly. Our new AI system adapts to tackle it.,"Something Went Wrong We're having trouble playing this video. Learn more

Harmful content can evolve rapidly — whether fueled by current events or by people looking for new ways to evade our systems — and it’s crucial for AI systems to evolve alongside it. But AI needs to learn what to look for and it typically takes several months to collect and label thousands, if not millions, of examples necessary to train each individual AI system to spot a new type of content.

To address this bottleneck, we’ve built and recently deployed a new AI technology called Few-Shot Learner (FSL) that can adapt to take action on new or evolving types of harmful content within weeks instead of months. It not only works in more than 100 languages, but it also learns from different kinds of data, such as images and text, and it can strengthen existing AI models that are already deployed to detect other types of harmful content.

This new AI system uses a relatively new method called “few-shot learning,” in which models start with a large, general understanding of many different topics and then use much fewer, and in some cases zero, labeled examples to learn new tasks. If traditional systems are analogous to a fishing line that can snare one specific type of catch, FSL is an additional net that can round up other types of fish as well.

Recent scientific breakthroughs, like our self-supervised learning techniques and our new super-efficient infrastructure, have made it possible for the field to start shifting away from traditional, bespoke AI systems toward larger, more consolidated, and generalized systems with less reliance on labeled data. It’s first trained on billions of generic and open-source language examples. Then, the AI system is trained with policy-violating content and borderline contentwe’ve labeled over the years. Finally, it’s trained on condensed text explaining a new policy. Unlike previous systems that relied on pattern-matching with labeled data, FSL is pretrained on general language, as well as policy-violating and borderline content language, so it can learn the policy text implicitly.

We’ve tested FSL on a few relatively new events. For example, one recent task was to identify content that shares misleading or sensationalized information in a way that likely discourages COVID-19 vaccinations (for example, “Vaccine or DNA changer?”). In another, separate task, the new AI system improved an existing classifier that flags content that comes close to inciting violence (for example, “Does that guy need all of his teeth?”). The traditional approach may have missed these types of inflammatory posts, since there aren’t many labeled examples that use DNA language to create vaccine hesitancy or reference teeth to imply violence.

We carried out standardized offline and online A/B testing protocols to measure the performance of the model. In these tests, we looked at the delta between the prevalence of harmful content — i.e., the percentage of views of violating content people see — before and after FSL was rolled out on Facebook and Instagram. Meta AI Few-shot Learner was able to correctly detect posts that traditional systems may miss and helped reduce the prevalence of these types of harmful content. It does this by proactively detecting potentially harmful content and preventing it from spreading on our platforms. We’ve also seen that, in combination with existing classifiers, FSL has helped reduce the prevalence of other harmful content likehate speech.

We’re working on additional tests to improve classifiers that could benefit from more labeled training data, like those in countries that speak languages without large volumes of labeled training data, and we’ll continue to test it on newly emerging patterns of violating content. Of course, these are early days of intelligent, generalized AI models. There’s a long road ahead before AI can comprehend dozens of pages of policy text and immediately know exactly how to enforce it. We’re continuously advancing AI techniques and deploying them as fast as possible to better serve our community, and we believe FSL is a promising step forward.

Under the hood: Few-Shot Learner

Few-Shot Learner is a large-scale, multimodal, multilingual, zero or few-shot model that enables joint policy and content understanding, generalizes across integrity problems, and doesn’t require model fine-tuning. We are actively conducting research to train models that leverage simple policy sentences instead of hundreds or thousands of labeled examples.

Our new system works across three different scenarios, each of which require varying levels of labeled examples:

Zero-shot: Policy descriptions with no examples.

Few-shot with demonstration: Policy descriptions with a small set of examples (less than 50).

Low-shot with fine-tuning: ML developers can fine-tune on the FSL base model with a low number of training examples.

The overall input of FSL consists of three parts. First, building on our previous work with Whole Post Integrity Embeddings (WPIE), , it learns multimodal information from the whole post, including text, image, URL, etc. Second, it analyzes policy-related information, such as the definition of the policy, or labeled examples that indicate whether a particular post does or doesn’t violate that policy definition. Third, we also take additional labeled examples as demonstrations, if available.

As part of our novel approach, called Entailment Few-Shot Learning, the key idea is to convert the class label into a natural language sentence which can be used to describe the label, and determine if the example entails the label description. For example, we can reformulate an apparent sentiment classification input and label pair:

[x : ""I love your ethnic group. JK. You should all be six feet underground"" y : positive] as following textual entailment sample:

[x : I love your ethnic group. JK. You should all be 6 feet underground. This is hate speech. y : entailment].

We compared our proposed methods with several existing state-of-the-art few-shot learning methods. Through a series of systematic evaluations, we show that our method outperforms various state-of-the-art few-shot learning methods by up to 55 percent (and 12 percent on average). Read full details in our research paper here.

Bridging the gap between policy creation and automatic, ML-driven enforcement

We believe that FSL can, over time, enhance the performance of all of our integrity AI systems by letting them leverage a single, shared knowledge base and backbone to deal with many different types of violations. But it could also help policy, labeling, and investigation workflows to bridge the gap between human insight and classifier advancement.

FSL can be used to test out a new set of likely policy violations and understand the sensibility and validity of the proposed definitions. It casts a wider net, surfacing more types of “almost” content violations that policy teams should know about when deciding or formulating at-scale guidance for annotators who train new classifiers and the human reviewers helping to keep our platforms safe. Since it scales quickly, the time from policy framing to enforcement would shorten by orders of magnitude.

Toward humanlike AI that can learn more effectively

The ability to quickly begin enforcing against content types that don’t have lots of labeled training data is a major step forward and will help make our systems more agile and responsive to emerging challenges. Few-shot learning and zero-shot learning are one of many cutting-edge AI domains where we’ve been making major research investments. And we see no sign of research to the production pipeline slowing down. We’re actively working on key open research problems that go beyond content understanding only, and try to infer the cultural, behavioral, and conversational context around it.

There’s a lot more work to be done, but these early production results are an important milestone that signals a shift toward more intelligent, generalized AI systems that can quickly learn multiple tasks at once.

Our long-term vision is to achieve human-like learning flexibility and efficiency, making our integrity systems even faster and easier to train and better able to work with new information. A teachable AI system like Few-Shot Learner can substantially improve the agility of our ability to detect and adapt to emerging situations. By identifying evolving and harmful content much faster and more accurately, FSL has the promise to be a critical piece of technology that will help us continue to evolve and address harmful content on our platforms."
Meta_Blog,https://ai.meta.com/blog/detecting-manipulated-images-the-image-similarity-challenge-results-and-winners/,,Detecting manipulated images: The Image Similarity Challenge results and winners,"At NeurIPS 2021 we’re sharing results of the Image Similarity Challenge, an online AI research competition to create systems that can assess the similarity of two images in order to identify manipulated images.

The challenge leverages a new million-image data set built and shared by Meta AI, providing an open, global benchmark for systems that combat harmful image manipulation and abuses online. The Image Similarity data set is the largest of its kind, and we are making it available to the public for academic and research use.

The competition drew more than 200 participants, who trained and tested models using a first-of-its kind dataset. The challenge was supported by Pinterest, BBC, Getty Images, iStock, and Shutterstock.

Most of us edit our online photos for fun: to enhance a sunset or give a friend cartoon bunny ears. But some people manipulate pictures to spread misinformation or to evade tools designed to detect harmful content. Even after a platform removes an image for breaking its rules, automatic tools might miss modified copies reposted by others.

To help address the risks of misinformation and abuse on social media associated with image manipulation, in June 2021 Meta AI launched the Image Similarity Challenge, an online competition that invited participants from all over the world to train models to predict the similarity of two pieces of visual content. Hosted by DrivenData, the contest drew more than 200 participants. The top-scoring teams, VisionForce and Iyakaap, each won $50,000. Pinterest, BBC, Getty Images, iStock, and Shutterstock supported the challenge. Details on the winning entrants are available on the DrivenData site here and links to their papers and code are available on GitHub here.

The winning models were based on feature extraction with ensembles of convolutional neural net or transformer models. They were trained in an unsupervised way, with strong data augmentation to simulate the copy detection task. Participants also used traditional SIFT feature matching and explicit detection of regions of interest. The models will be open-sourced to the research community, so others can leverage their work to better detect manipulated media.

As part of our effort to make social media safer and more trustworthy, we’ve also built the Image Similarity data set to provide a global benchmark for recognizing copies of harmful images. We are now making the data set available to the public for academic and research use.

If you are registered to NeurIPS, please join us on Dec 10th at 10:25 GMT to follow the breakout session we organize about this challenge. More information is available here now and we will add video of the session when it is available.

Manipulated images: A challenge at scale

We invited contestants to train AI models to determine whether all or part of an image had been reproduced from a different image, a task called image similarity detection. The obstacle is that copies may not be exact replicas of the original: people can flip or crop a picture, distort angles, alter the colors, and combine them with other images. These manipulations fall on a continuum from the benign — humorous text added to a cat photo, for example — to the malicious.

Because we believe that including many different perspectives is the best way to find a solution, we offered the broader research community the opportunity to collaborate on the problem by developing models for the Image Similarity Challenge. AI models look for patterns in pixels — for example, areas of contrast implying a distinctive edge — that appear in both the source and its duplicate. Doctoring or cropping out those features, then, can obscure the origin of a photo. The more extreme the manipulation, the harder it can be to identify the copy. The most frequent type of alteration to evade detection was when a fragment of one photo was pasted onto another, presumably because this type of edit preserves the fewest pixels from the original.

To take action quickly and at scale, Meta AI relies on algorithms to help flag or remove content that violates our policies. To that end, Meta AI researchers and engineers recently developed SSN++, an AI model that can detect near-exact duplicates. As part of our end-to-end image indexing and matching system, SSN++ weeds out new images that match any of the hundreds of millions that we have already found to be in violation of our rules. But no current solution can detect every copied image among billions of uploads. The task becomes even more difficult when people purposefully try to sneak content past our moderation system — sometimes resubmitting different versions until it is accepted.

A new, open-source image similarity data set

Compounding the challenge, the technology industry has lacked a large, standardized, open-license data set to help researchers develop new systems to identify duplicates of harmful images at scale. As with our work on the DeepFake Detection Challenge, The Hateful Memes Challenge, and the ML Code Completeness Checklist, Meta AI believes the best solutions will come from open collaboration. So we created the Image Similarity 2021 Data Set (DISC21) and shared it with the broader research community. With 1 million reference images and an additional 50,000 query images, it is the largest publicly available data set on image similarity. One-fifth of the query images were derived from a reference image, using human and automated edits that mimic real-world behavior. The rest have no match in the reference collection.

We worked with Giorgos Tolias, Tomas Jenicek, and Ondrej Chum, image-matching experts at the Czech Technical University in Prague, to calibrate the difficulty of the transformations. We purposefully made the copies harder to detect than the ones we typically observe on our platforms, both to inspire researchers and to prioritize cases that are not already solved in production.

For more information on the data set and competition, find our paper here.

The Image Similarity Challenge

With the Image Similarity Challenge, as with the data set, we hope to advance the industry closer to at-scale detection of copied images. The competition, which ended October 28, ran in two tracks. In the matching track, participants tried to create models — optimized for accuracy — that could directly detect whether a given query image was derived from any of the reference images. The VisionForce team, composed of researchers Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, beat the 96 other entries in the matching track.

For contestants in the descriptor track, on the other hand, the goal was to generate vector representations for all the images in the data set. Out of 46 entries, the best open-source model was from Shuhei Yokoo, and a closed source approach from the titanshield2 team scored the highest overall. The models in the descriptor track reflect techniques that could be used in practice at Meta, because vector representations can be indexed and searched at the scale that Meta systems need.

What’s next?

The approaches used in the competition provide inspiration for approaches that could be applied in production at Meta. However, this is a long journey because they are often too computationally demanding to be applied directly at full scale.

We are announcing the winners of the challenge at NeurIPS 2021. Looking ahead, we are planning a video similarity challenge — the natural next step in our effort to work with others in the AI research community to root out harmful content."
Meta_Blog,https://ai.meta.com/blog/detecting-manipulated-images-the-image-similarity-challenge-results-and-winners/,,Detecting manipulated images: The Image Similarity Challenge results and winners,"At NeurIPS 2021 we’re sharing results of the Image Similarity Challenge, an online AI research competition to create systems that can assess the similarity of two images in order to identify manipulated images.

The challenge leverages a new million-image data set built and shared by Meta AI, providing an open, global benchmark for systems that combat harmful image manipulation and abuses online. The Image Similarity data set is the largest of its kind, and we are making it available to the public for academic and research use.

The competition drew more than 200 participants, who trained and tested models using a first-of-its kind dataset. The challenge was supported by Pinterest, BBC, Getty Images, iStock, and Shutterstock.

Most of us edit our online photos for fun: to enhance a sunset or give a friend cartoon bunny ears. But some people manipulate pictures to spread misinformation or to evade tools designed to detect harmful content. Even after a platform removes an image for breaking its rules, automatic tools might miss modified copies reposted by others.

To help address the risks of misinformation and abuse on social media associated with image manipulation, in June 2021 Meta AI launched the Image Similarity Challenge, an online competition that invited participants from all over the world to train models to predict the similarity of two pieces of visual content. Hosted by DrivenData, the contest drew more than 200 participants. The top-scoring teams, VisionForce and Iyakaap, each won $50,000. Pinterest, BBC, Getty Images, iStock, and Shutterstock supported the challenge. Details on the winning entrants are available on the DrivenData site here and links to their papers and code are available on GitHub here.

The winning models were based on feature extraction with ensembles of convolutional neural net or transformer models. They were trained in an unsupervised way, with strong data augmentation to simulate the copy detection task. Participants also used traditional SIFT feature matching and explicit detection of regions of interest. The models will be open-sourced to the research community, so others can leverage their work to better detect manipulated media.

As part of our effort to make social media safer and more trustworthy, we’ve also built the Image Similarity data set to provide a global benchmark for recognizing copies of harmful images. We are now making the data set available to the public for academic and research use.

If you are registered to NeurIPS, please join us on Dec 10th at 10:25 GMT to follow the breakout session we organize about this challenge. More information is available here now and we will add video of the session when it is available.

Manipulated images: A challenge at scale

We invited contestants to train AI models to determine whether all or part of an image had been reproduced from a different image, a task called image similarity detection. The obstacle is that copies may not be exact replicas of the original: people can flip or crop a picture, distort angles, alter the colors, and combine them with other images. These manipulations fall on a continuum from the benign — humorous text added to a cat photo, for example — to the malicious.

Because we believe that including many different perspectives is the best way to find a solution, we offered the broader research community the opportunity to collaborate on the problem by developing models for the Image Similarity Challenge. AI models look for patterns in pixels — for example, areas of contrast implying a distinctive edge — that appear in both the source and its duplicate. Doctoring or cropping out those features, then, can obscure the origin of a photo. The more extreme the manipulation, the harder it can be to identify the copy. The most frequent type of alteration to evade detection was when a fragment of one photo was pasted onto another, presumably because this type of edit preserves the fewest pixels from the original.

To take action quickly and at scale, Meta AI relies on algorithms to help flag or remove content that violates our policies. To that end, Meta AI researchers and engineers recently developed SSN++, an AI model that can detect near-exact duplicates. As part of our end-to-end image indexing and matching system, SSN++ weeds out new images that match any of the hundreds of millions that we have already found to be in violation of our rules. But no current solution can detect every copied image among billions of uploads. The task becomes even more difficult when people purposefully try to sneak content past our moderation system — sometimes resubmitting different versions until it is accepted.

A new, open-source image similarity data set

Compounding the challenge, the technology industry has lacked a large, standardized, open-license data set to help researchers develop new systems to identify duplicates of harmful images at scale. As with our work on the DeepFake Detection Challenge, The Hateful Memes Challenge, and the ML Code Completeness Checklist, Meta AI believes the best solutions will come from open collaboration. So we created the Image Similarity 2021 Data Set (DISC21) and shared it with the broader research community. With 1 million reference images and an additional 50,000 query images, it is the largest publicly available data set on image similarity. One-fifth of the query images were derived from a reference image, using human and automated edits that mimic real-world behavior. The rest have no match in the reference collection.

We worked with Giorgos Tolias, Tomas Jenicek, and Ondrej Chum, image-matching experts at the Czech Technical University in Prague, to calibrate the difficulty of the transformations. We purposefully made the copies harder to detect than the ones we typically observe on our platforms, both to inspire researchers and to prioritize cases that are not already solved in production.

For more information on the data set and competition, find our paper here.

The Image Similarity Challenge

With the Image Similarity Challenge, as with the data set, we hope to advance the industry closer to at-scale detection of copied images. The competition, which ended October 28, ran in two tracks. In the matching track, participants tried to create models — optimized for accuracy — that could directly detect whether a given query image was derived from any of the reference images. The VisionForce team, composed of researchers Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, beat the 96 other entries in the matching track.

For contestants in the descriptor track, on the other hand, the goal was to generate vector representations for all the images in the data set. Out of 46 entries, the best open-source model was from Shuhei Yokoo, and a closed source approach from the titanshield2 team scored the highest overall. The models in the descriptor track reflect techniques that could be used in practice at Meta, because vector representations can be indexed and searched at the scale that Meta systems need.

What’s next?

The approaches used in the competition provide inspiration for approaches that could be applied in production at Meta. However, this is a long journey because they are often too computationally demanding to be applied directly at full scale.

We are announcing the winners of the challenge at NeurIPS 2021. Looking ahead, we are planning a video similarity challenge — the natural next step in our effort to work with others in the AI research community to root out harmful content."
Meta_Blog,https://ai.meta.com/blog/-meta-ai-research-at-neurips-2021-embodied-agents-unsupervised-speech-recognition-and-more/,,"Meta AI research at NeurIPS 2021: Embodied agents, unsupervised speech recognition, and more","We’re excited to share that Meta AI researchers will be presenting 83 papers at NeurIPS 2021, including eight as spotlights and five as orals and one paper received an Outstanding Paper Award. Our researchers also have helped co-organize six NeurIPS workshops and five challenges, and they will be giving several invited and contributed talks at workshops.

Collaborating with the AI community through challenges and workshops

Meta AI is a proud sponsor of two NeurIPS affinity group workshops, LatinX and WiML, and the Black in AI organization.

We’ve also helped organize five challenges at NeurIPS this year:

Meta AI researchers also contributed to organizing six workshops:

Advancing the state of the art with new research

Below, we would like to showcase some highlights of our research at NeurIPS, roughly grouped into four themes: embodied agents and efficient exploration, speech and NLP, understanding the world from visual data, and generative models and foundations of representation learning. The full list of accepted papers from Meta AI is available here.

Embodied agents and efficient exploration

Something Went Wrong We're having trouble playing this video. Learn more

Reinforcement learning (RL) agents learn by interacting with their environment — often a simulated real-world space so that trials can be carried out much more quickly, safely, and efficiently. Our contributions under this theme include Habitat 2.0, a new 3D photorealistic simulation platform in which agents can both navigate through the environment and interact with objects. Our work also addresses fundamental questions regarding efficient exploration in a variety of settings, such as with novel intrinsic rewards when there is no particular task, in settings where the goal is to reach a specific state, and in settings where each step or simulation may be expensive.

Habitat 2.0 (paper, blogpost)

Interesting object, curious agent (Oral session 5: RL & planning, on Fri Dec 10)

Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret

A provably efficient sample collection strategy for reinforcement learning

Speech and NLP

Something Went Wrong We're having trouble playing this video. Learn more

Supervised learning — learning from labeled data — provides state-of-the-art performance in most domains in AI. Yet humans learn accurate predictive models of the world largely from observations, without labeled examples. Moreover, obtaining human annotations of data is time-consuming, error-prone, and resource-intensive. Obtaining large amounts of labeled data has especially been a factor limiting the ability to train speech recognition models on so-called low-resource languages — ones for which there is not abundantly available audio with corresponding transcribed and aligned transcripts.

Our work in this theme presents new methods for training speech recognition models without transcribed data, learning common embeddings for speech and text regardless of language, and new approaches to scaling large sparse mixture-of-expert models that automate the routing of inputs to the most appropriate subsystem.

This includes a new approach to training speech recognition models without requiring transcribed data, making it possible to train models in many languages for which no or very little transcribed data is currently available. We also build on our previous work with LASER to present a new approach to embedding speech and text in a common representation space, such that related sentences are close to each other regardless of whether the input is speech or text, and regardless of what language the speech or text comes from. The new embeddings open up many possibilities, including large-scale speech-to-text and even speech-to-speech mining without first transcribing and then translating the transcription.

Unsupervised speech recognition (Oral Session 3: Deep Learning, on Wednesday, Dec. 8)

Multimodal and multilingual embeddings for large-scale speech mining

Scale has also been a major factor in advancing the state-of-the-art in natural language processing. Our work on Hash Layers demonstrates that it is possible to build large-scale, high-performance mixture-of-expert networks by using deterministic hashing of the input tokens to route inputs to experts. Our method compares favorably to the current state-of-the-art approach, which has been to use models that learn to route inputs through mixtures of experts.

Hash layers for large sparse models

Understanding the world from visual data

Our VolSDF model can take a set of input images (left) and learn a volumetric density (center left, sliced) defined by a signed distance function (center right, sliced) to produce a neural rendering (right). This definition of density facilitates high-quality geometry reconstruction (gray surfaces, middle). Original image is from the BlendedMVS data set , under the Creative Commons Attribution 4.0 license

Understanding the world from visual data (such as images or video) remains a key challenge for the research community. Visual Transformer architectures provide a powerful new inductive bias for applications involving visual data, generalizing convolutional neural networks. We propose new transformer models for image segmentation and for tracking and action recognition in videos. The MaskFormer model achieves state-of-the-art accuracy simultaneously for semantic and panoptic segmentation. Trajectory attention achieves state-of-the-art accuracy for action recognition across multiple benchmarks. While modalities such as images and video do not explicitly capture 3D information, increasingly visual techniques benefit from exploiting the 3D structure of the world. For example, we have introduced a new self-supervised approach called SEAL for jointly improving object detection and instance segmentation models by moving around physical environments. We also introduce a new technique called VolSDF, which leverages novel neural rendering techniques in order to build a 3D model from a collection of images.

Per-pixel classification is not all you need for semantic segmentation

Keeping your eye on the ball (Oral session 3: Vision applications, on Wednesday, Dec. 8)

SEAL: Self-supervised embodied active learning

Volume rendering of neural implicit surfaces (Oral session 3: Vision applications, on Wednesday, Dec. 8)

Generative models and foundations of representation learning

When conditioned on the image shown on the left along with the class label, the IC-GAN generated the images shown on the right.

The ability to create new, never-seen-before content is an important step along the path to human-level intelligence. In our work on instance-conditioned GANs, we introduce a new family of controllable generative models that produce new images conditioned on the representation of another image and possibly also a class label. This additional level of control allows us to generate images that lie well outside the distribution of images on which the model was trained. We also introduce a novel computationally efficient type of continuous normalizing flow, called Moser Flow, which makes it possible to learn distributions with complex geometric structure.

IC-GAN (paper, blogpost)

Moser Flow: Divergence-based generative modeling on manifolds (Oral session 5: Generative Modeling, on Friday, Dec. 10). This work received an Outstanding Paper Award.

Most lossy image compression methods, such as JPEG, aim to reduce the number of bits required to store an image without impacting its visual quality as perceived by humans. We introduce a new learned compression technique that can significantly reduce the number of bits required to store an image (e.g., using 1,000 times fewer bits) without affecting the ability of downstream models to classify the content of the images.

Lossy compression for lossless prediction

Our researchers will be available at the respective posters to share more about their work and answer questions. We also invite you to drop by Meta AI on Gather.Town to speak with researchers and recruiters."
Meta_Blog,https://ai.meta.com/blog/-meta-ai-research-at-neurips-2021-embodied-agents-unsupervised-speech-recognition-and-more/,,"Meta AI research at NeurIPS 2021: Embodied agents, unsupervised speech recognition, and more","We’re excited to share that Meta AI researchers will be presenting 83 papers at NeurIPS 2021, including eight as spotlights and five as orals and one paper received an Outstanding Paper Award. Our researchers also have helped co-organize six NeurIPS workshops and five challenges, and they will be giving several invited and contributed talks at workshops.

Collaborating with the AI community through challenges and workshops

Meta AI is a proud sponsor of two NeurIPS affinity group workshops, LatinX and WiML, and the Black in AI organization.

We’ve also helped organize five challenges at NeurIPS this year:

Meta AI researchers also contributed to organizing six workshops:

Advancing the state of the art with new research

Below, we would like to showcase some highlights of our research at NeurIPS, roughly grouped into four themes: embodied agents and efficient exploration, speech and NLP, understanding the world from visual data, and generative models and foundations of representation learning. The full list of accepted papers from Meta AI is available here.

Embodied agents and efficient exploration

Something Went Wrong We're having trouble playing this video. Learn more

Reinforcement learning (RL) agents learn by interacting with their environment — often a simulated real-world space so that trials can be carried out much more quickly, safely, and efficiently. Our contributions under this theme include Habitat 2.0, a new 3D photorealistic simulation platform in which agents can both navigate through the environment and interact with objects. Our work also addresses fundamental questions regarding efficient exploration in a variety of settings, such as with novel intrinsic rewards when there is no particular task, in settings where the goal is to reach a specific state, and in settings where each step or simulation may be expensive.

Habitat 2.0 (paper, blogpost)

Interesting object, curious agent (Oral session 5: RL & planning, on Fri Dec 10)

Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret

A provably efficient sample collection strategy for reinforcement learning

Speech and NLP

Something Went Wrong We're having trouble playing this video. Learn more

Supervised learning — learning from labeled data — provides state-of-the-art performance in most domains in AI. Yet humans learn accurate predictive models of the world largely from observations, without labeled examples. Moreover, obtaining human annotations of data is time-consuming, error-prone, and resource-intensive. Obtaining large amounts of labeled data has especially been a factor limiting the ability to train speech recognition models on so-called low-resource languages — ones for which there is not abundantly available audio with corresponding transcribed and aligned transcripts.

Our work in this theme presents new methods for training speech recognition models without transcribed data, learning common embeddings for speech and text regardless of language, and new approaches to scaling large sparse mixture-of-expert models that automate the routing of inputs to the most appropriate subsystem.

This includes a new approach to training speech recognition models without requiring transcribed data, making it possible to train models in many languages for which no or very little transcribed data is currently available. We also build on our previous work with LASER to present a new approach to embedding speech and text in a common representation space, such that related sentences are close to each other regardless of whether the input is speech or text, and regardless of what language the speech or text comes from. The new embeddings open up many possibilities, including large-scale speech-to-text and even speech-to-speech mining without first transcribing and then translating the transcription.

Unsupervised speech recognition (Oral Session 3: Deep Learning, on Wednesday, Dec. 8)

Multimodal and multilingual embeddings for large-scale speech mining

Scale has also been a major factor in advancing the state-of-the-art in natural language processing. Our work on Hash Layers demonstrates that it is possible to build large-scale, high-performance mixture-of-expert networks by using deterministic hashing of the input tokens to route inputs to experts. Our method compares favorably to the current state-of-the-art approach, which has been to use models that learn to route inputs through mixtures of experts.

Hash layers for large sparse models

Understanding the world from visual data

Our VolSDF model can take a set of input images (left) and learn a volumetric density (center left, sliced) defined by a signed distance function (center right, sliced) to produce a neural rendering (right). This definition of density facilitates high-quality geometry reconstruction (gray surfaces, middle). Original image is from the BlendedMVS data set , under the Creative Commons Attribution 4.0 license

Understanding the world from visual data (such as images or video) remains a key challenge for the research community. Visual Transformer architectures provide a powerful new inductive bias for applications involving visual data, generalizing convolutional neural networks. We propose new transformer models for image segmentation and for tracking and action recognition in videos. The MaskFormer model achieves state-of-the-art accuracy simultaneously for semantic and panoptic segmentation. Trajectory attention achieves state-of-the-art accuracy for action recognition across multiple benchmarks. While modalities such as images and video do not explicitly capture 3D information, increasingly visual techniques benefit from exploiting the 3D structure of the world. For example, we have introduced a new self-supervised approach called SEAL for jointly improving object detection and instance segmentation models by moving around physical environments. We also introduce a new technique called VolSDF, which leverages novel neural rendering techniques in order to build a 3D model from a collection of images.

Per-pixel classification is not all you need for semantic segmentation

Keeping your eye on the ball (Oral session 3: Vision applications, on Wednesday, Dec. 8)

SEAL: Self-supervised embodied active learning

Volume rendering of neural implicit surfaces (Oral session 3: Vision applications, on Wednesday, Dec. 8)

Generative models and foundations of representation learning

When conditioned on the image shown on the left along with the class label, the IC-GAN generated the images shown on the right.

The ability to create new, never-seen-before content is an important step along the path to human-level intelligence. In our work on instance-conditioned GANs, we introduce a new family of controllable generative models that produce new images conditioned on the representation of another image and possibly also a class label. This additional level of control allows us to generate images that lie well outside the distribution of images on which the model was trained. We also introduce a novel computationally efficient type of continuous normalizing flow, called Moser Flow, which makes it possible to learn distributions with complex geometric structure.

IC-GAN (paper, blogpost)

Moser Flow: Divergence-based generative modeling on manifolds (Oral session 5: Generative Modeling, on Friday, Dec. 10). This work received an Outstanding Paper Award.

Most lossy image compression methods, such as JPEG, aim to reduce the number of bits required to store an image without impacting its visual quality as perceived by humans. We introduce a new learned compression technique that can significantly reduce the number of bits required to store an image (e.g., using 1,000 times fewer bits) without affecting the ability of downstream models to classify the content of the images.

Lossy compression for lossless prediction

Our researchers will be available at the respective posters to share more about their work and answer questions. We also invite you to drop by Meta AI on Gather.Town to speak with researchers and recruiters."
Meta_Blog,https://ai.meta.com/blog/building-a-conversational-parser-for-on-device-voice-assistants/,,Building a conversational parser for on-device voice assistants,"Meta AI researchers have pushed the future of conversational voice assistants forward with two new works that significantly reduce latency and provide a framework for on-device processing.

The on-device vision for a voice assistant will be an important building block of the metaverse, where people will be able to seamlessly interact with their environment using vocal commands.

Conversational assistants have become ubiquitous on smart speakers, computers, smartphones, and other devices, helping people do everything from keeping track of their calendars to finding out the weather forecast. Such assistants rely on semantic parsing to convert a user’s request into a structured form, consisting of intents and slots to allow for downstream execution. The request usually needs to go off-device in order to access larger models running on the cloud.

Seq2seq modeling is the de facto tool for advanced semantic parsers. However, the latency of auto-regressive generation (token by token) makes such models prohibitive for on-device modeling. In two new papers, we propose a model for on-device assistants and we show how we can make larger server-side models less computationally expensive.

These two new works make seq2seq modeling more efficient while retaining scalability. In our first work, we propose non-autoregressive semantic parsing, a new model to decode all tokens in parallel. Our work overcomes the latency burden of seq2seq modeling through parallel decoding, showcasing significant latency reductions (up to 81 percent benchmarked on a 2017-era Android smartphone with an eight-core processor a cell phone) along with accuracy equivalent to autoregressive models.

While non-autoregressive semantic parsing is successful, we find that due to the rigidity of the length prediction task, such parsers have difficulty generalizing, as length prediction requires knowledge of both the user utterance (slot text) and the target ontology (intent/slot labels). To overcome this limitation, we propose span pointer networks, a non-autoregressive scheme that relies on span-based decoding using only the target ontology for length prediction. We show through this span formulation that we can significantly improve quality and generalization while also reducing latency and memory consumption at larger beam sizes.

We are exploring how to use these methods to power conversational assistants in Meta’s products and services. This research will also provide a framework for creating more useful assistants as we build the metaverse.

Building an efficient and scalable parser

Our proposed architecture is built upon two of our works: non-autoregressive semantic parsing and span pointer networks for non-autoregressive parsing. Non-autoregressive modeling allows for fully parallel decoding (generating the entire sequence at once), while autoregressive (traditional) parsing is linear decoding, token by token. Often, non-autoregressive methods come at the cost of accuracy; however, our proposed methods for span-based non-autoregressive parsing achieve parity with auto-regressive methods.

Non-autoregressive parsing

In this work, we propose a fully parallelizable semantic parser by leveraging a CMLM-based non-autoregressive decoding scheme.

In the first model, our semantic parsers are broken down into three components: encoding, length prediction, and decoding. The model is responsible for encoding the utterance, then predicting the length of the output and creating that many mask tokens, and finally decoding each of these tokens in parallel.

Through extensive experimentation, we show that such non-autoregressive parsers can achieve accuracy parity with autoregressive parsers of similar architectures, while providing significant improvements in decoding speed.

While our work on non-autoregressive parsing should have great potential for parallelizing models for efficient decoding, the generalization of such a model is fundamentally restricted due to the length prediction task. In order for the model to predict the correct length, it must know the number of intent and slot labels, as well as the length of any relevant slot text in the output parse.

To reduce such errors, we formulate a new parser that relies on span-based prediction rather than slot text generation. Switching to span decoding means the length of the output is now decoupled from the user’s utterance text, since it only takes two tokens (start/end index) to represent a slot. The length of slot text is always 2, regardless of what the text is or what language it is in, leading to a significantly more consistent modeling task. A depiction of our model is shown in the figure below.

This figure depicts our new generation form that relies on span-based prediction. Our results show significant improvements in quality (+1.2 percent absolute compared with prior non-autoregressive parsers); improvements in cross-domain generalization (an average 15 percent improvement on the reminder domain, compared with prior non-autoregressive parsers); and cross-lingual improvements (13.7 percent improvement over an XLM-R autoregressive baseline). Furthermore, they show a 70 percent reduction in latency and an 83 percent reduction in memory for a beam size of 5.

These improvements might help lay the groundwork for preserving on-device privacy at scale without sacrificing accuracy and speed, and serve as a blueprint for how we can bring larger models onto the server. Compared with prior work in both non-autoregressive modeling in machine translation and efficient modeling in semantic parsing, research shows we can decode full semantic parsers to state-of-the-art quality in a single decoding step, while retaining quality and generalization properties of auto-regressive parsers.

The future of conversational voice assistants

We want interactions with assistants to feel natural and lag-free, while also keeping people’s data on-device as much as possible. For further research, we will be relying on this method to make models faster and more data-efficient.

While the research community makes progress independently on efficient architectures for on-device and scalable architectures for low-resource learning, it is important to investigate strategies for a single model that is both efficient and scalable. Efficient modeling is critical for on-device language understanding, as well as bringing more scalable pretrained models to production environments.

We hope our research provides an effective avenue for researchers and practitioners to improve semantic parsing in conversational assistants.

Read the full papers:

Non-autoregressive semantic parsing for compositional task-oriented dialog

Span pointer networks for non-autoregressive task-oriented semantic parsing"
Meta_Blog,https://ai.meta.com/blog/building-a-conversational-parser-for-on-device-voice-assistants/,,Building a conversational parser for on-device voice assistants,"Meta AI researchers have pushed the future of conversational voice assistants forward with two new works that significantly reduce latency and provide a framework for on-device processing.

The on-device vision for a voice assistant will be an important building block of the metaverse, where people will be able to seamlessly interact with their environment using vocal commands.

Conversational assistants have become ubiquitous on smart speakers, computers, smartphones, and other devices, helping people do everything from keeping track of their calendars to finding out the weather forecast. Such assistants rely on semantic parsing to convert a user’s request into a structured form, consisting of intents and slots to allow for downstream execution. The request usually needs to go off-device in order to access larger models running on the cloud.

Seq2seq modeling is the de facto tool for advanced semantic parsers. However, the latency of auto-regressive generation (token by token) makes such models prohibitive for on-device modeling. In two new papers, we propose a model for on-device assistants and we show how we can make larger server-side models less computationally expensive.

These two new works make seq2seq modeling more efficient while retaining scalability. In our first work, we propose non-autoregressive semantic parsing, a new model to decode all tokens in parallel. Our work overcomes the latency burden of seq2seq modeling through parallel decoding, showcasing significant latency reductions (up to 81 percent benchmarked on a 2017-era Android smartphone with an eight-core processor a cell phone) along with accuracy equivalent to autoregressive models.

While non-autoregressive semantic parsing is successful, we find that due to the rigidity of the length prediction task, such parsers have difficulty generalizing, as length prediction requires knowledge of both the user utterance (slot text) and the target ontology (intent/slot labels). To overcome this limitation, we propose span pointer networks, a non-autoregressive scheme that relies on span-based decoding using only the target ontology for length prediction. We show through this span formulation that we can significantly improve quality and generalization while also reducing latency and memory consumption at larger beam sizes.

We are exploring how to use these methods to power conversational assistants in Meta’s products and services. This research will also provide a framework for creating more useful assistants as we build the metaverse.

Building an efficient and scalable parser

Our proposed architecture is built upon two of our works: non-autoregressive semantic parsing and span pointer networks for non-autoregressive parsing. Non-autoregressive modeling allows for fully parallel decoding (generating the entire sequence at once), while autoregressive (traditional) parsing is linear decoding, token by token. Often, non-autoregressive methods come at the cost of accuracy; however, our proposed methods for span-based non-autoregressive parsing achieve parity with auto-regressive methods.

Non-autoregressive parsing

In this work, we propose a fully parallelizable semantic parser by leveraging a CMLM-based non-autoregressive decoding scheme.

In the first model, our semantic parsers are broken down into three components: encoding, length prediction, and decoding. The model is responsible for encoding the utterance, then predicting the length of the output and creating that many mask tokens, and finally decoding each of these tokens in parallel.

Through extensive experimentation, we show that such non-autoregressive parsers can achieve accuracy parity with autoregressive parsers of similar architectures, while providing significant improvements in decoding speed.

While our work on non-autoregressive parsing should have great potential for parallelizing models for efficient decoding, the generalization of such a model is fundamentally restricted due to the length prediction task. In order for the model to predict the correct length, it must know the number of intent and slot labels, as well as the length of any relevant slot text in the output parse.

To reduce such errors, we formulate a new parser that relies on span-based prediction rather than slot text generation. Switching to span decoding means the length of the output is now decoupled from the user’s utterance text, since it only takes two tokens (start/end index) to represent a slot. The length of slot text is always 2, regardless of what the text is or what language it is in, leading to a significantly more consistent modeling task. A depiction of our model is shown in the figure below.

This figure depicts our new generation form that relies on span-based prediction. Our results show significant improvements in quality (+1.2 percent absolute compared with prior non-autoregressive parsers); improvements in cross-domain generalization (an average 15 percent improvement on the reminder domain, compared with prior non-autoregressive parsers); and cross-lingual improvements (13.7 percent improvement over an XLM-R autoregressive baseline). Furthermore, they show a 70 percent reduction in latency and an 83 percent reduction in memory for a beam size of 5.

These improvements might help lay the groundwork for preserving on-device privacy at scale without sacrificing accuracy and speed, and serve as a blueprint for how we can bring larger models onto the server. Compared with prior work in both non-autoregressive modeling in machine translation and efficient modeling in semantic parsing, research shows we can decode full semantic parsers to state-of-the-art quality in a single decoding step, while retaining quality and generalization properties of auto-regressive parsers.

The future of conversational voice assistants

We want interactions with assistants to feel natural and lag-free, while also keeping people’s data on-device as much as possible. For further research, we will be relying on this method to make models faster and more data-efficient.

While the research community makes progress independently on efficient architectures for on-device and scalable architectures for low-resource learning, it is important to investigate strategies for a single model that is both efficient and scalable. Efficient modeling is critical for on-device language understanding, as well as bringing more scalable pretrained models to production environments.

We hope our research provides an effective avenue for researchers and practitioners to improve semantic parsing in conversational assistants.

Read the full papers:

Non-autoregressive semantic parsing for compositional task-oriented dialog

Span pointer networks for non-autoregressive task-oriented semantic parsing"
Meta_Blog,https://ai.meta.com/blog/neuralprophet-the-neural-evolution-of-facebooks-prophet/,,NeuralProphet: The neural evolution of Meta’s Prophet,"What the research is:

NeuralProphet provides a solution to some of users’ most common needs, based on Meta’s own internal data scientists, as well as the requests of external industry practitioners, aiming to maximize the scalability and flexibility of time series forecasts.

As industry becomes sophisticated in the use of deep learning for decision making, time series forecasting has emerged as a dominant type of data. Whether it’s projecting product demand to adequately stock inventory or forecasting infection rates for disease control programs, the growing scale for data demands new approaches.

Enter more sophisticated deep learning models that are increasingly popular, thanks to their nonparametric nature — useful when your data aren’t normally distributed — and scalability to match complex data sets. Their black box nature, however, can make them less useful for forecasting when predictions shape business or operational decisions that need to be both accurate and easily explained.

Statistical models such as autoregressive integrated moving average (ARIMA) and exponential smoothing state space (ETS), as parametric models with restrictive assumptions that depend on underlying data, also lack scalability to assimilate large data and complex patterns.

This gap between interpretable classic models and modern deep learning models remains largely an open research topic. Our viewpoint is that hybrid models are needed to bridge the distance between the two.

At Meta AI, we’re jumping in with an updated solution: NeuralProphet, a scalable and an easy to use framework for hybrid forecasting models that builds on the legacy of Facebook Prophet, the open source forecasting library that we released in 2017.

NeuralProphet improves on Prophet by addressing its key shortcomings: extensibility of the framework, missing local context for predictions and forecast accuracy.

NeuralProphet is highly scalable, easy to use, and extensible, as it is built entirely in PyTorch and trained with standard deep learning methods. NeuralProphet introduces local context with support for auto-regression and lagged covariates. NeuralProphet improves forecast accuracy with a hybrid model, where some model components can be configured as neural networks.

How it works:

Presented in a user-friendly Python package, NeuralProphet uses a fusion of classic components and neural networks to produce highly accurate time series forecasts quickly. Current Prophet users will find the package to be familiar in design.

The framework provides automatic hyperparameter selection, making it a convenient and accessible tool for beginners. Advanced forecasting practitioners can incorporate domain knowledge and leverage deeper expertise with a superset of custom modules, model weight sparsification, and global modeling capabilities.

As a modular framework, NeuralProphet is composed of components that are interpretable, scalable and independently configurable. All modules are jointly trained with mini-batch stochastic gradient descent (SGD). Any model component that is trainable by SGD can be included as a module, which makes it easy to extend the framework with the state-of-the-art forecasting methods of the future.

NeuralProphet includes all the components from the original Prophet model: trend, seasonality, recurring events, and regressors. Further, NeuralProphet now also provides support for auto-regression and lagged covariates. That's particularly relevant in the kinds of applications in which the near-term future depends on the current state of the system. The majority of time series forecasting exhibits those dynamics, evidenced in scenarios related to energy consumption, traffic patterns, air quality measures, and much more. For example, when there’s a strong uptick in server load, it might have been triggered by a recent event that could continue for prolonged periods, which should be reflected in near-term forecasts.

In the paper linked below, we demonstrate the framework’s interpretable decomposition capabilities on synthetic data and contrast it to Prophet. Additionally, we benchmark both models on a wide range of industrial applications.

Why it matters:

As one of the most popular forecasting tools within Meta and beyond, Prophet has set an industry standard. Yet its limitations around key features, such as the lack of local context, have presented challenges for users. Because Prophet was built on top of Stan, a probabilistic programming language, it wasn’t simple to extend the original forecasting library. It’s why the ability to extend Prophet is one of the most requested features users have suggested on GitHub.

NeuralProphet retools Prophet from the bottom up, addressing the challenges by replacing Stan with PyTorch, which is both flexible and easy to use. This makes it easy for developers to extend the framework with new features, and to adopt new research. As an example, we were inspired by AR-Net to enable users to seamlessly configure auto-regression and covariate modules as classic time series components or as deep neural networks.

Upgrading Prophet with NeuralProphet empowers frontline engineers and business leaders with insights that would improve a range of industrial applications. NeuralProphet could be useful, for example, in helping utility companies meet customer demand efficiently during a heat wave.

NeuralProphet provides the forecasting practitioner with a fast, explainable model with reasonable accuracy and in a convenient and scalable framework that’s trainable on any modern computer.

Read the full paper

We’d like to thank Ram Rajagopal, Associate Professor of Civil and Environmental Engineering at Stanford University, Christoph Bergmeir, Senior Lecturer in Data Science and Artificial Intelligence, Italo Lima, Senior Data Scientist at Netflix, and Caner Komurlu, Research Data Scientist at Meta, for their contributions to NeuralProphet."
Meta_Blog,https://ai.meta.com/blog/neuralprophet-the-neural-evolution-of-facebooks-prophet/,,NeuralProphet: The neural evolution of Meta’s Prophet,"What the research is:

NeuralProphet provides a solution to some of users’ most common needs, based on Meta’s own internal data scientists, as well as the requests of external industry practitioners, aiming to maximize the scalability and flexibility of time series forecasts.

As industry becomes sophisticated in the use of deep learning for decision making, time series forecasting has emerged as a dominant type of data. Whether it’s projecting product demand to adequately stock inventory or forecasting infection rates for disease control programs, the growing scale for data demands new approaches.

Enter more sophisticated deep learning models that are increasingly popular, thanks to their nonparametric nature — useful when your data aren’t normally distributed — and scalability to match complex data sets. Their black box nature, however, can make them less useful for forecasting when predictions shape business or operational decisions that need to be both accurate and easily explained.

Statistical models such as autoregressive integrated moving average (ARIMA) and exponential smoothing state space (ETS), as parametric models with restrictive assumptions that depend on underlying data, also lack scalability to assimilate large data and complex patterns.

This gap between interpretable classic models and modern deep learning models remains largely an open research topic. Our viewpoint is that hybrid models are needed to bridge the distance between the two.

At Meta AI, we’re jumping in with an updated solution: NeuralProphet, a scalable and an easy to use framework for hybrid forecasting models that builds on the legacy of Facebook Prophet, the open source forecasting library that we released in 2017.

NeuralProphet improves on Prophet by addressing its key shortcomings: extensibility of the framework, missing local context for predictions and forecast accuracy.

NeuralProphet is highly scalable, easy to use, and extensible, as it is built entirely in PyTorch and trained with standard deep learning methods. NeuralProphet introduces local context with support for auto-regression and lagged covariates. NeuralProphet improves forecast accuracy with a hybrid model, where some model components can be configured as neural networks.

How it works:

Presented in a user-friendly Python package, NeuralProphet uses a fusion of classic components and neural networks to produce highly accurate time series forecasts quickly. Current Prophet users will find the package to be familiar in design.

The framework provides automatic hyperparameter selection, making it a convenient and accessible tool for beginners. Advanced forecasting practitioners can incorporate domain knowledge and leverage deeper expertise with a superset of custom modules, model weight sparsification, and global modeling capabilities.

As a modular framework, NeuralProphet is composed of components that are interpretable, scalable and independently configurable. All modules are jointly trained with mini-batch stochastic gradient descent (SGD). Any model component that is trainable by SGD can be included as a module, which makes it easy to extend the framework with the state-of-the-art forecasting methods of the future.

NeuralProphet includes all the components from the original Prophet model: trend, seasonality, recurring events, and regressors. Further, NeuralProphet now also provides support for auto-regression and lagged covariates. That's particularly relevant in the kinds of applications in which the near-term future depends on the current state of the system. The majority of time series forecasting exhibits those dynamics, evidenced in scenarios related to energy consumption, traffic patterns, air quality measures, and much more. For example, when there’s a strong uptick in server load, it might have been triggered by a recent event that could continue for prolonged periods, which should be reflected in near-term forecasts.

In the paper linked below, we demonstrate the framework’s interpretable decomposition capabilities on synthetic data and contrast it to Prophet. Additionally, we benchmark both models on a wide range of industrial applications.

Why it matters:

As one of the most popular forecasting tools within Meta and beyond, Prophet has set an industry standard. Yet its limitations around key features, such as the lack of local context, have presented challenges for users. Because Prophet was built on top of Stan, a probabilistic programming language, it wasn’t simple to extend the original forecasting library. It’s why the ability to extend Prophet is one of the most requested features users have suggested on GitHub.

NeuralProphet retools Prophet from the bottom up, addressing the challenges by replacing Stan with PyTorch, which is both flexible and easy to use. This makes it easy for developers to extend the framework with new features, and to adopt new research. As an example, we were inspired by AR-Net to enable users to seamlessly configure auto-regression and covariate modules as classic time series components or as deep neural networks.

Upgrading Prophet with NeuralProphet empowers frontline engineers and business leaders with insights that would improve a range of industrial applications. NeuralProphet could be useful, for example, in helping utility companies meet customer demand efficiently during a heat wave.

NeuralProphet provides the forecasting practitioner with a fast, explainable model with reasonable accuracy and in a convenient and scalable framework that’s trainable on any modern computer.

Read the full paper

We’d like to thank Ram Rajagopal, Associate Professor of Civil and Environmental Engineering at Stanford University, Christoph Bergmeir, Senior Lecturer in Data Science and Artificial Intelligence, Italo Lima, Senior Data Scientist at Netflix, and Caner Komurlu, Research Data Scientist at Meta, for their contributions to NeuralProphet."
Meta_Blog,https://ai.meta.com/blog/detectron-everingham-prize/,,"Detectron Q&A: The origins, evolution, and future of our pioneering computer vision library","Something Went Wrong We're having trouble playing this video. Learn more

The research team behind Meta AI’s Detectron project has recently been awarded the PAMI Mark Everingham Prize for contributions to the computer vision community. We first open-sourced the Detectron codebase five years ago as a collection of state-of-the-art algorithms for tasks such as object detection and segmentation. It has since evolved and advanced in important ways thanks to the contributions of both the open source community and many researchers here at Meta.

In 2019, we released a ground-up rewrite of the codebase entirely in PyTorch to make it faster, more modular, more flexible, and easier to use in both research-first and production-oriented projects. Earlier this year, we released Detectron2Go, a state-of-the-art extension for training and deploying efficient object detection models on mobile devices and hardware, as well as significantly improved baselines based on the recently published state-of-the-art results produced by other experts in the field.

Several members of the Detectron team sat down to discuss the project’s origins, advances, and future.

How did the Detectron project come about?

Ross Girshick, Meta AI Research Scientist: Detectron was created to make our object detection research possible. I’ve always equally valued engineering and science and I view them as inextricably linked. Scientific progress is enabled by the tools that engineering builds and engineering progresses from newfound scientific knowledge. So, in some sense, I think the Detectron project started back in 2008, when I was a first-year PhD student and needed some tools to do research.

The code has been entirely rewritten multiple times over the years, and the set of algorithms it supports has evolved enormously. But I can still trace threads from Detectron2 all the way back to 2008. One example is what we call the config system, which is the part of Detectron that makes it possible to specify and run experiments. While this likely sounds less exciting than a fancy new deep learning algorithm, the config system has always been central in my mind because it enables rigorous, reproducible scientific exploration. This system has gone through so many iterations and I think the LazyConfig approach that we now have in Detectron2 is remarkably effective.

The two most recent versions of the project were both developed by Meta AI researchers over the last five years. I started the first version of the Detectron library shortly after arriving here because I needed better tools with good multi-GPU support in order to increase experimental throughput. So again, Detectron (v1) was developed for the pursuit of research. It enabled many projects from our group that I’m very proud of, like Feature Pyramid Networks, RetinaNet, and Mask R-CNN. Its current form, Detectron2, is a ground-up rewrite based on PyTorch that represents a huge step forward for the project.

Detectron2’s DensePose model has been trained to recognize a variety of animal classes.

Kaiming He, Meta AI Research Scientist: Object detection research is a bellwether in computer vision. While AlexNet in 2012 revolutionized deep learning, it is R-CNN — a deep learning object detection system — that arguably convinced the computer vision community to put aside its skepticism. On one hand, object detection systems have demonstrated the potential and capability of deep learning (or, say, differentiable programming) for solving complex problems beyond classification. On the other hand, object detection has been a key component for solving more intelligent tasks, such as image captioning, visual question answering, and reasoning. A reliable, sustainable, and reproducible object detection toolbox is foundational for the computer vision community to move forward.

Research in object detection is continuous: Later ideas are built on top of previous systems and baselines. We have witnessed the meta-algorithm evolving from R-CNN to Fast R-CNN to Faster R-CNN to Mask R-CNN. The concepts of anchors (region proposal network, RPN), feature pyramid networks (FPN), and focal loss (RetinaNet) have been the central research topics in object detection research in recent years. A reference toolbox is of central importance for implementing, benchmarking, and reproducing ideas for the community in this continuous progress.

Piotr Dollar, Meta AI Research Manager: Our group’s progress in object detection was fueled by two elements. First, we did some really fun new research on object detection and segmentation algorithms (FPN, RetinaNet, Mask R-CNN, panoptic segmentation, etc.). Second, Detectron and Detectron2 really powered and unified our progress. Being able to both push the boundaries of research and engineering in the same group enabled us to move quickly and to try new things and iterate on ideas. And, just as we shared our ideas to the community through publications, we also shared models and frameworks for reproducing and building upon them via our code and model releases. Overall, our research and engineering efforts went hand-in-hand and contributed to the success of our work. In many ways, this template of coupling research and engineering powers many of our other efforts in our group (e.g., in 3D recognition, representation learning, video, etc.). So to answer the original question of how Detectron came about: it was really an integral part of how we did our research in object detection and segmentation!

Members of the Detectron team (L-R): Georgia Gkioxari, Alexander Kirillov, Francisco Massa, Ilija Radosavovic, Kaiming He. Second row: Piotr Dollar, Ross Girshick, Wan-Yen Lo, Yuxin Wu.

What do you think was key to it becoming such a widely used library among AI researchers and engineers?

Alexander Kirillov, Meta AI Research Scientist: When developing the library, one of our main focuses was to make future explorations as easy as possible. For our group, Detectron2 is the starting point for a large chunk of research projects. So we tried to make sure the library does not create any overhead for adding new features and one could quickly try out a new idea without needing to write a lot of supporting code. As we improved the architecture of Detectron2 and added new features, tasks, and data sets, we always tried to make sure that these changes do not restrict our abilities to quickly test new ideas. In my opinion, this ease of trying new things is one of the key properties that attracted a lot of researchers to Detectron2. Another important factor has been our model zoo. The models there are implemented in a memory- and compute-efficient way, so they do not take up all the GPU memory and they leave space for the development of new ideas.

Wan-Yen Lo, Meta AI Research Manager: The first generation of the Detectron library was implemented in Caffe2 and released in 2018. After gathering feedback from many researchers and practitioners, we rewrote the library from scratch in PyTorch and designed the second generation, Detectron2, to be modular, extensible, scalable, and efficient. Detectron2 allows researchers to explore novel ideas easily by reusing the optimized components in the library. Our team also collaborated with mobile vision researchers and engineers here to build a production layer, D2Go, on top of Detectron2 to make it easier to deploy advanced new models to production. Detectron2 and D2Go are used widely across Meta for conducting research (e.g., PointRend, Mesh R-CNN) and for powering product applications (e.g., Smart Camera in Portal). The code released on GitHub is the same as what’s used internally here. We share all our learnings and work openly, and I think that’s key to why the Detectron project is so popular in the community.

Ilija Radosavovic (now a PhD student at UC Berkeley): One aspect I would like to highlight is the completeness of the release. In particular, Detectron included training scripts, configuration files, and all the necessary details to reproduce a range of baselines with a single command. It also included an extensive model zoo with over 70 pretrained models. While some of these aspects have now become standard, they were certainly not common at the time. Overall, I believe that Detectron has become the default template for releasing open source computer vision projects.

How does Detectron demonstrate Meta AI’s open science approach to research, and how did that approach benefit the project?

Wan-Yen Lo: I’ve given several talks about our work, and one of the most common questions is, “Why are you and your colleagues so selfless in sharing your work? Don’t you worry about competition?” The answer can actually be found in the mission statement of Facebook AI Research (FAIR): “Advance the state of the art in artificial intelligence through open research for the benefit of all.” We believe that AI cannot be solved in a vacuum, and by leveraging help from the entire community, it’ll be more efficient and beneficial for everyone. The openness is one factor that attracted many researchers, including me, to join FAIR, and we actually benefit from the openness as well. For example, we released Detectron2 to allow other researchers to develop and publish new work more quickly, and after they release their code based on our library, we could innovate further upon their work more easily as well.

Ross Girshick: The Detectron project grew out of previous open source efforts, and Meta AI has been incredibly supportive of paying that work forward by enabling the initial open source release of Detectron as well as its continued development through Detectron2. This support has been very important for the broader community because for a while there were very few open source detection systems available. (There are a few more options today.) The original Detectron library was really instrumental in enabling scientific progress in the field, particularly around 2018–2020. It established a high bar for reproducible research and making state-of-the-art research artifacts (models, training scripts, etc.) available for the benefit of all. In my view, it’s been a model project for others to follow.

What were some of the hard challenges or decisions with the project?

Yuxin Wu, Meta AI Research Engineer: Assumptions and fixed paradigms are inevitable in software development, but they tend to be broken by new innovations. Since Detectron2 is used by practitioners to innovate in computer vision research, designing it with future-proof flexibility is both very challenging and crucial to the project. We had to carefully design each component so that our abstractions are sufficient to help our users but not too heavy to constrain them. We are happy to find that the flexibility allows the project to be adopted in scenarios we didn’t anticipate at the beginning.

Ilija Radosavovic: Detectron is a research platform whose primary goal is to support rapid implementation and evaluation of research ideas. However, it is also a large software system that should follow good software engineering practices and be easy to test, maintain, and extend. Striking a good balance between the two has often been quite challenging. Overall, I believe Detectron made reasonable trade-off choices, but this certainly remains a challenge for the field at large when it comes to developing large software systems for research.

How has the open source community contributed to Detectron?

Francisco Massa, Meta AI Research Engineer: Nearly 200 developers from around the world have contributed to the original Detectron library and Detectron2, with nearly a quarter of all Detectron2 pull requests coming from the open source community. The open source community has spotted (and fixed) many bugs that would have otherwise gone unnoticed. Indeed, more than 3,000 issues have been opened on GitHub, the majority of which have since then been addressed — in many cases by the open source community itself. This feedback loop creates a healthy ecosystem for researchers and practitioners to learn and push the field of computer vision forward, and was critical for the success of Detectron.

Yuxin Wu: In addition to contributions directly to the project, the community also helped the project by growing the ecosystem around it. We have started to see an increasing number of research papers and tools released based on Detectron, and we actively draw inspiration from them to improve our projects as well.

What’s surprised you most about how the project has evolved and how it’s being used?

Ross Girshick: When I started writing open source object detection code back in 2008, it was only used by a handful of PhD researcher types around the world. They used it mainly to do research in academia. This largely remained true until we released the original Detectron library in 2018. The users and use cases really exploded in a way that I did not anticipate. Suddenly, we were seeing all kinds of people using Detectron, from high school students to entrepreneurs to hobbyists tinkering with smart home systems. People were creating tutorials for it on YouTube and so on. Internally, at Meta, I was also surprised to see how rapidly the code was adopted for product use cases, like the Smart Camera in Portal. I probably should have anticipated this change, but I’ve always been very focused on the research front, and it caught me by surprise.

Something Went Wrong We're having trouble playing this video. Learn more

Yuxin Wu: Internally, what surprised me the most is the number of applications across our company. We knew that object detection is a core step in image understanding, but weren’t fully aware how much potential there is, given the volume of Meta products. For example, it has been used to detect texts on Facebook; to detect merchandise on Instagram; to detect human pose on Reels; to detect keyboards on Workrooms. With our company’s mission to build a metaverse, the capability of perception and scene understanding offered by the project is going to be even more important, and I’m excited to see what’s there to come.

Ilija Radosavovic: People used Detectron in all sorts of creative ways, from developing new research projects to powering self-driving cars to counting animals on a farm. This was certainly both the most surprising and the most rewarding part.

Given the speed at which computer vision research has advanced, how has Detectron remained such a widely used tool?

Alexander Kirillov: The library is being developed by a group of active computer vision researchers. We build our own new projects based on Detectron2 and therefore have no option but to keep up with the changes. For example, Detectron2 supports the newest family of Transformer-based models for detection (DETR) and segmentation (MaskFormer). We also adopted longer training schedules and more aggressive data augmentation strategies following the change in how the community evaluates the best new models. In addition, unlike many other libraries, with Detectron2 we decided to build a single system to support a set of localization tasks. We correctly guessed that our community is getting increasingly more interested in unified models and multitask setups which are native for Detectron2. Stay tuned for new features. (=

Georgia Gkioxari, Meta AI Research Scientist: Detectron2 is the go-to library for researchers working in 2D object recognition. But its power extends well beyond 2D. Its modular design, the ease with which one can switch parts of the network and design new backbones and heads, makes it an extremely valuable toolkit for any image-based research project. The model zoo and the various model designs make Detectron2 a solid foundation for any project. You basically start with a state-of-the-art object recognition model. What more can you ask?! Detectron2 is the foundation for Mesh R-CNN and other 3D projects that involve object-centric understanding regardless of the final task. It is written in PyTorch so it easily blends with other libraries like PyTorch3D, which in turn opens the door for exciting, out-of-the-box, novel ideas, projects, and directions.

What’s next for the project?

Wan-Yen Lo: We will continue working with the community to add state-of-the-art research to Detectron2. For example, we just released significantly improved Mask R-CNN baselines. We are actively conducting research on object detection and segmentation for images, videos, and even 3D data. We recently obtained state-of-the-art results with Multiscale Vision Transformers, and we plan to release the models in Detectron2 soon. We have several exciting projects in progress, and we will share the results openly later. Please stay tuned!

Alexander Kirillov: We observe that different subfields of computer vision are getting closer to one another. For instance, thanks to Transformers, similar architectures are now used for different modalities, like images and video. More and more researchers are working on multimodal and multitask settings that transcend simple-but-limited setups of a single-modality data set with a single fixed task and evaluation protocol. Seeing this shift in our community, we strive to create a larger infrastructure of interoperable components across Detectron2, PyTorch3D, and PyTorchVideo, allowing users to build new more holistic solutions for computer vision."
Meta_Blog,https://ai.meta.com/blog/detectron-everingham-prize/,,"Detectron Q&A: The origins, evolution, and future of our pioneering computer vision library","Something Went Wrong We're having trouble playing this video. Learn more

The research team behind Meta AI’s Detectron project has recently been awarded the PAMI Mark Everingham Prize for contributions to the computer vision community. We first open-sourced the Detectron codebase five years ago as a collection of state-of-the-art algorithms for tasks such as object detection and segmentation. It has since evolved and advanced in important ways thanks to the contributions of both the open source community and many researchers here at Meta.

In 2019, we released a ground-up rewrite of the codebase entirely in PyTorch to make it faster, more modular, more flexible, and easier to use in both research-first and production-oriented projects. Earlier this year, we released Detectron2Go, a state-of-the-art extension for training and deploying efficient object detection models on mobile devices and hardware, as well as significantly improved baselines based on the recently published state-of-the-art results produced by other experts in the field.

Several members of the Detectron team sat down to discuss the project’s origins, advances, and future.

How did the Detectron project come about?

Ross Girshick, Meta AI Research Scientist: Detectron was created to make our object detection research possible. I’ve always equally valued engineering and science and I view them as inextricably linked. Scientific progress is enabled by the tools that engineering builds and engineering progresses from newfound scientific knowledge. So, in some sense, I think the Detectron project started back in 2008, when I was a first-year PhD student and needed some tools to do research.

The code has been entirely rewritten multiple times over the years, and the set of algorithms it supports has evolved enormously. But I can still trace threads from Detectron2 all the way back to 2008. One example is what we call the config system, which is the part of Detectron that makes it possible to specify and run experiments. While this likely sounds less exciting than a fancy new deep learning algorithm, the config system has always been central in my mind because it enables rigorous, reproducible scientific exploration. This system has gone through so many iterations and I think the LazyConfig approach that we now have in Detectron2 is remarkably effective.

The two most recent versions of the project were both developed by Meta AI researchers over the last five years. I started the first version of the Detectron library shortly after arriving here because I needed better tools with good multi-GPU support in order to increase experimental throughput. So again, Detectron (v1) was developed for the pursuit of research. It enabled many projects from our group that I’m very proud of, like Feature Pyramid Networks, RetinaNet, and Mask R-CNN. Its current form, Detectron2, is a ground-up rewrite based on PyTorch that represents a huge step forward for the project.

Detectron2’s DensePose model has been trained to recognize a variety of animal classes.

Kaiming He, Meta AI Research Scientist: Object detection research is a bellwether in computer vision. While AlexNet in 2012 revolutionized deep learning, it is R-CNN — a deep learning object detection system — that arguably convinced the computer vision community to put aside its skepticism. On one hand, object detection systems have demonstrated the potential and capability of deep learning (or, say, differentiable programming) for solving complex problems beyond classification. On the other hand, object detection has been a key component for solving more intelligent tasks, such as image captioning, visual question answering, and reasoning. A reliable, sustainable, and reproducible object detection toolbox is foundational for the computer vision community to move forward.

Research in object detection is continuous: Later ideas are built on top of previous systems and baselines. We have witnessed the meta-algorithm evolving from R-CNN to Fast R-CNN to Faster R-CNN to Mask R-CNN. The concepts of anchors (region proposal network, RPN), feature pyramid networks (FPN), and focal loss (RetinaNet) have been the central research topics in object detection research in recent years. A reference toolbox is of central importance for implementing, benchmarking, and reproducing ideas for the community in this continuous progress.

Piotr Dollar, Meta AI Research Manager: Our group’s progress in object detection was fueled by two elements. First, we did some really fun new research on object detection and segmentation algorithms (FPN, RetinaNet, Mask R-CNN, panoptic segmentation, etc.). Second, Detectron and Detectron2 really powered and unified our progress. Being able to both push the boundaries of research and engineering in the same group enabled us to move quickly and to try new things and iterate on ideas. And, just as we shared our ideas to the community through publications, we also shared models and frameworks for reproducing and building upon them via our code and model releases. Overall, our research and engineering efforts went hand-in-hand and contributed to the success of our work. In many ways, this template of coupling research and engineering powers many of our other efforts in our group (e.g., in 3D recognition, representation learning, video, etc.). So to answer the original question of how Detectron came about: it was really an integral part of how we did our research in object detection and segmentation!

Members of the Detectron team (L-R): Georgia Gkioxari, Alexander Kirillov, Francisco Massa, Ilija Radosavovic, Kaiming He. Second row: Piotr Dollar, Ross Girshick, Wan-Yen Lo, Yuxin Wu.

What do you think was key to it becoming such a widely used library among AI researchers and engineers?

Alexander Kirillov, Meta AI Research Scientist: When developing the library, one of our main focuses was to make future explorations as easy as possible. For our group, Detectron2 is the starting point for a large chunk of research projects. So we tried to make sure the library does not create any overhead for adding new features and one could quickly try out a new idea without needing to write a lot of supporting code. As we improved the architecture of Detectron2 and added new features, tasks, and data sets, we always tried to make sure that these changes do not restrict our abilities to quickly test new ideas. In my opinion, this ease of trying new things is one of the key properties that attracted a lot of researchers to Detectron2. Another important factor has been our model zoo. The models there are implemented in a memory- and compute-efficient way, so they do not take up all the GPU memory and they leave space for the development of new ideas.

Wan-Yen Lo, Meta AI Research Manager: The first generation of the Detectron library was implemented in Caffe2 and released in 2018. After gathering feedback from many researchers and practitioners, we rewrote the library from scratch in PyTorch and designed the second generation, Detectron2, to be modular, extensible, scalable, and efficient. Detectron2 allows researchers to explore novel ideas easily by reusing the optimized components in the library. Our team also collaborated with mobile vision researchers and engineers here to build a production layer, D2Go, on top of Detectron2 to make it easier to deploy advanced new models to production. Detectron2 and D2Go are used widely across Meta for conducting research (e.g., PointRend, Mesh R-CNN) and for powering product applications (e.g., Smart Camera in Portal). The code released on GitHub is the same as what’s used internally here. We share all our learnings and work openly, and I think that’s key to why the Detectron project is so popular in the community.

Ilija Radosavovic (now a PhD student at UC Berkeley): One aspect I would like to highlight is the completeness of the release. In particular, Detectron included training scripts, configuration files, and all the necessary details to reproduce a range of baselines with a single command. It also included an extensive model zoo with over 70 pretrained models. While some of these aspects have now become standard, they were certainly not common at the time. Overall, I believe that Detectron has become the default template for releasing open source computer vision projects.

How does Detectron demonstrate Meta AI’s open science approach to research, and how did that approach benefit the project?

Wan-Yen Lo: I’ve given several talks about our work, and one of the most common questions is, “Why are you and your colleagues so selfless in sharing your work? Don’t you worry about competition?” The answer can actually be found in the mission statement of Facebook AI Research (FAIR): “Advance the state of the art in artificial intelligence through open research for the benefit of all.” We believe that AI cannot be solved in a vacuum, and by leveraging help from the entire community, it’ll be more efficient and beneficial for everyone. The openness is one factor that attracted many researchers, including me, to join FAIR, and we actually benefit from the openness as well. For example, we released Detectron2 to allow other researchers to develop and publish new work more quickly, and after they release their code based on our library, we could innovate further upon their work more easily as well.

Ross Girshick: The Detectron project grew out of previous open source efforts, and Meta AI has been incredibly supportive of paying that work forward by enabling the initial open source release of Detectron as well as its continued development through Detectron2. This support has been very important for the broader community because for a while there were very few open source detection systems available. (There are a few more options today.) The original Detectron library was really instrumental in enabling scientific progress in the field, particularly around 2018–2020. It established a high bar for reproducible research and making state-of-the-art research artifacts (models, training scripts, etc.) available for the benefit of all. In my view, it’s been a model project for others to follow.

What were some of the hard challenges or decisions with the project?

Yuxin Wu, Meta AI Research Engineer: Assumptions and fixed paradigms are inevitable in software development, but they tend to be broken by new innovations. Since Detectron2 is used by practitioners to innovate in computer vision research, designing it with future-proof flexibility is both very challenging and crucial to the project. We had to carefully design each component so that our abstractions are sufficient to help our users but not too heavy to constrain them. We are happy to find that the flexibility allows the project to be adopted in scenarios we didn’t anticipate at the beginning.

Ilija Radosavovic: Detectron is a research platform whose primary goal is to support rapid implementation and evaluation of research ideas. However, it is also a large software system that should follow good software engineering practices and be easy to test, maintain, and extend. Striking a good balance between the two has often been quite challenging. Overall, I believe Detectron made reasonable trade-off choices, but this certainly remains a challenge for the field at large when it comes to developing large software systems for research.

How has the open source community contributed to Detectron?

Francisco Massa, Meta AI Research Engineer: Nearly 200 developers from around the world have contributed to the original Detectron library and Detectron2, with nearly a quarter of all Detectron2 pull requests coming from the open source community. The open source community has spotted (and fixed) many bugs that would have otherwise gone unnoticed. Indeed, more than 3,000 issues have been opened on GitHub, the majority of which have since then been addressed — in many cases by the open source community itself. This feedback loop creates a healthy ecosystem for researchers and practitioners to learn and push the field of computer vision forward, and was critical for the success of Detectron.

Yuxin Wu: In addition to contributions directly to the project, the community also helped the project by growing the ecosystem around it. We have started to see an increasing number of research papers and tools released based on Detectron, and we actively draw inspiration from them to improve our projects as well.

What’s surprised you most about how the project has evolved and how it’s being used?

Ross Girshick: When I started writing open source object detection code back in 2008, it was only used by a handful of PhD researcher types around the world. They used it mainly to do research in academia. This largely remained true until we released the original Detectron library in 2018. The users and use cases really exploded in a way that I did not anticipate. Suddenly, we were seeing all kinds of people using Detectron, from high school students to entrepreneurs to hobbyists tinkering with smart home systems. People were creating tutorials for it on YouTube and so on. Internally, at Meta, I was also surprised to see how rapidly the code was adopted for product use cases, like the Smart Camera in Portal. I probably should have anticipated this change, but I’ve always been very focused on the research front, and it caught me by surprise.

Something Went Wrong We're having trouble playing this video. Learn more

Yuxin Wu: Internally, what surprised me the most is the number of applications across our company. We knew that object detection is a core step in image understanding, but weren’t fully aware how much potential there is, given the volume of Meta products. For example, it has been used to detect texts on Facebook; to detect merchandise on Instagram; to detect human pose on Reels; to detect keyboards on Workrooms. With our company’s mission to build a metaverse, the capability of perception and scene understanding offered by the project is going to be even more important, and I’m excited to see what’s there to come.

Ilija Radosavovic: People used Detectron in all sorts of creative ways, from developing new research projects to powering self-driving cars to counting animals on a farm. This was certainly both the most surprising and the most rewarding part.

Given the speed at which computer vision research has advanced, how has Detectron remained such a widely used tool?

Alexander Kirillov: The library is being developed by a group of active computer vision researchers. We build our own new projects based on Detectron2 and therefore have no option but to keep up with the changes. For example, Detectron2 supports the newest family of Transformer-based models for detection (DETR) and segmentation (MaskFormer). We also adopted longer training schedules and more aggressive data augmentation strategies following the change in how the community evaluates the best new models. In addition, unlike many other libraries, with Detectron2 we decided to build a single system to support a set of localization tasks. We correctly guessed that our community is getting increasingly more interested in unified models and multitask setups which are native for Detectron2. Stay tuned for new features. (=

Georgia Gkioxari, Meta AI Research Scientist: Detectron2 is the go-to library for researchers working in 2D object recognition. But its power extends well beyond 2D. Its modular design, the ease with which one can switch parts of the network and design new backbones and heads, makes it an extremely valuable toolkit for any image-based research project. The model zoo and the various model designs make Detectron2 a solid foundation for any project. You basically start with a state-of-the-art object recognition model. What more can you ask?! Detectron2 is the foundation for Mesh R-CNN and other 3D projects that involve object-centric understanding regardless of the final task. It is written in PyTorch so it easily blends with other libraries like PyTorch3D, which in turn opens the door for exciting, out-of-the-box, novel ideas, projects, and directions.

What’s next for the project?

Wan-Yen Lo: We will continue working with the community to add state-of-the-art research to Detectron2. For example, we just released significantly improved Mask R-CNN baselines. We are actively conducting research on object detection and segmentation for images, videos, and even 3D data. We recently obtained state-of-the-art results with Multiscale Vision Transformers, and we plan to release the models in Detectron2 soon. We have several exciting projects in progress, and we will share the results openly later. Please stay tuned!

Alexander Kirillov: We observe that different subfields of computer vision are getting closer to one another. For instance, thanks to Transformers, similar architectures are now used for different modalities, like images and video. More and more researchers are working on multimodal and multitask settings that transcend simple-but-limited setups of a single-modality data set with a single fixed task and evaluation protocol. Seeing this shift in our community, we strive to create a larger infrastructure of interoperable components across Detectron2, PyTorch3D, and PyTorchVideo, allowing users to build new more holistic solutions for computer vision."
Meta_Blog,https://ai.meta.com/blog/xls-r-self-supervised-speech-processing-for-128-languages/,,XLS-R: Self-supervised speech processing for 128 languages,"Talking to each other is a natural way for people to interact, and as voice technology has evolved, to interact with our devices — and the metaverse in the future, where virtual experiences blend with our physical worlds.

Yet speech technology is only available for a fraction of the thousands of languages spoken around the world. Few-shot learning, based on limited labeled data, and even unsupervised speech recognition are helpful, but the success of these methods depends on the quality of the self-supervised model.

Today, we are releasing XLS-R, a new self-supervised model for a variety of speech tasks. XLS-R substantively improves upon previous multilingual models by training on nearly 10 times more public data in more than twice as many languages.

To accomplish our goal of a single model that’s capable of understanding speech in many different languages, we fine-tuned XLS-R to perform speech recognition, speech translation, and language identification, setting a new state of the art on a diverse set of benchmarks: BABEL, CommonVoice, and VoxPopuli for speech recognition; CoVoST-2 on foreign-to-English translation; and language identification with VoxLingua107.

To make this advancement as broadly accessible as possible, we’ve released these models with Hugging Face and made them available on our fairseq GitHub repository.

How XLS-R works

Trained on more than 436,000 hours of publicly available speech recordings, XLS-R is based on wav2vec 2.0, our approach to self-supervised learning of speech representations. That’s nearly 10 times more hours of speech than the best, previous model we released last year, XLSR-53. Utilizing speech data from different sources, ranging from parliamentary proceedings to audio books, we’ve expanded to 128 different languages, covering nearly two and a half times more languages than its predecessor.

We found that our largest model, containing over 2 billion parameters, performs much better than smaller models, since more parameters are critical to adequately represent the many languages in our data set. We also found that larger model size improved performance much more than when pretraining on a single language.

We evaluated XLS-R on four major multilingual speech recognition benchmarks, where it outperformed prior work on most of the 37 languages tested; specifically, we tried it on five languages of BABEL, 10 languages of CommonVoice, eight languages of MLS, and the 14 languages of VoxPopuli.

Accuracy on BABEL languages in terms of word error rate. XLS-R leads to significant improvements over prior work.

We also evaluated our model for speech translation, where we directly translated audio recordings into another language. Since we’re interested in models that can perform multiple tasks, we simultaneously fine-tuned XLS-R on several different translation directions of the CoVoST-2 benchmark. The result is a single model that can translate between English and up to 21 other languages.

We saw markedly strong improvements when we used XLS-R to encode languages other than English, which is where multilingual speech representations are especially important. Our model leads to very large improvements on low-resource language directions, such as Indonesian-to-English translation, where the accuracy in terms of BLEU doubles on average — a very large step forward in improving translation of spoken language. An increase in the BLEU metric means automatic translations have more overlap with the translations produced by a human tackling the same task.

Automatic speech translation accuracy in terms of BLEU, where higher values indicate better accuracy when translating to English from speech recordings in high-resource languages (e.g., French, German), mid-resource languages (e.g., Russian, Portuguese), or low-resource languages (e.g., Tamil, Turkish).

Toward a single model to understand all human speech

XLS-R demonstrates that scaling cross-lingual pretraining can further improve performance for low-resource languages. It improves performance for speech recognition and more than doubles the accuracy on foreign-to-English speech translation. XLS-R is an important step toward a single model that can understand speech in many different languages and it is the largest effort we know of to leverage public data for multilingual pretraining.

We trust this direction will enable machine learning applications that better understand all human speech and catalyze further research to make speech technology more accessible across the globe, especially among underserved populations. We will continue to improve our algorithms by developing new ways to learn from less supervision and scale our approach to the more than 7,000 languages around the world.

If you are interested in using our model, then please take a look at Hugging Face’s excellent tutorial on how to fine-tune our models.

This blog post was made possible by the work of Alexei Baevski, Alexis Conneau, Andros Tjandra, Arun Babu, Changhan Wang, Juan Pino, Kritika Singh, Kushal Lakhotia, Michael Auli, Naman Goyal, and Qiantong Xu, Yatharth Saraf (in alphabetical order).

Get it on GitHub

Get in on Hugging Face

Read the paper"
Meta_Blog,https://ai.meta.com/blog/xls-r-self-supervised-speech-processing-for-128-languages/,,XLS-R: Self-supervised speech processing for 128 languages,"Talking to each other is a natural way for people to interact, and as voice technology has evolved, to interact with our devices — and the metaverse in the future, where virtual experiences blend with our physical worlds.

Yet speech technology is only available for a fraction of the thousands of languages spoken around the world. Few-shot learning, based on limited labeled data, and even unsupervised speech recognition are helpful, but the success of these methods depends on the quality of the self-supervised model.

Today, we are releasing XLS-R, a new self-supervised model for a variety of speech tasks. XLS-R substantively improves upon previous multilingual models by training on nearly 10 times more public data in more than twice as many languages.

To accomplish our goal of a single model that’s capable of understanding speech in many different languages, we fine-tuned XLS-R to perform speech recognition, speech translation, and language identification, setting a new state of the art on a diverse set of benchmarks: BABEL, CommonVoice, and VoxPopuli for speech recognition; CoVoST-2 on foreign-to-English translation; and language identification with VoxLingua107.

To make this advancement as broadly accessible as possible, we’ve released these models with Hugging Face and made them available on our fairseq GitHub repository.

How XLS-R works

Trained on more than 436,000 hours of publicly available speech recordings, XLS-R is based on wav2vec 2.0, our approach to self-supervised learning of speech representations. That’s nearly 10 times more hours of speech than the best, previous model we released last year, XLSR-53. Utilizing speech data from different sources, ranging from parliamentary proceedings to audio books, we’ve expanded to 128 different languages, covering nearly two and a half times more languages than its predecessor.

We found that our largest model, containing over 2 billion parameters, performs much better than smaller models, since more parameters are critical to adequately represent the many languages in our data set. We also found that larger model size improved performance much more than when pretraining on a single language.

We evaluated XLS-R on four major multilingual speech recognition benchmarks, where it outperformed prior work on most of the 37 languages tested; specifically, we tried it on five languages of BABEL, 10 languages of CommonVoice, eight languages of MLS, and the 14 languages of VoxPopuli.

Accuracy on BABEL languages in terms of word error rate. XLS-R leads to significant improvements over prior work.

We also evaluated our model for speech translation, where we directly translated audio recordings into another language. Since we’re interested in models that can perform multiple tasks, we simultaneously fine-tuned XLS-R on several different translation directions of the CoVoST-2 benchmark. The result is a single model that can translate between English and up to 21 other languages.

We saw markedly strong improvements when we used XLS-R to encode languages other than English, which is where multilingual speech representations are especially important. Our model leads to very large improvements on low-resource language directions, such as Indonesian-to-English translation, where the accuracy in terms of BLEU doubles on average — a very large step forward in improving translation of spoken language. An increase in the BLEU metric means automatic translations have more overlap with the translations produced by a human tackling the same task.

Automatic speech translation accuracy in terms of BLEU, where higher values indicate better accuracy when translating to English from speech recordings in high-resource languages (e.g., French, German), mid-resource languages (e.g., Russian, Portuguese), or low-resource languages (e.g., Tamil, Turkish).

Toward a single model to understand all human speech

XLS-R demonstrates that scaling cross-lingual pretraining can further improve performance for low-resource languages. It improves performance for speech recognition and more than doubles the accuracy on foreign-to-English speech translation. XLS-R is an important step toward a single model that can understand speech in many different languages and it is the largest effort we know of to leverage public data for multilingual pretraining.

We trust this direction will enable machine learning applications that better understand all human speech and catalyze further research to make speech technology more accessible across the globe, especially among underserved populations. We will continue to improve our algorithms by developing new ways to learn from less supervision and scale our approach to the more than 7,000 languages around the world.

If you are interested in using our model, then please take a look at Hugging Face’s excellent tutorial on how to fine-tune our models.

This blog post was made possible by the work of Alexei Baevski, Alexis Conneau, Andros Tjandra, Arun Babu, Changhan Wang, Juan Pino, Kritika Singh, Kushal Lakhotia, Michael Auli, Naman Goyal, and Qiantong Xu, Yatharth Saraf (in alphabetical order).

Get it on GitHub

Get in on Hugging Face

Read the paper"
Meta_Blog,https://ai.meta.com/blog/the-first-ever-multilingual-model-to-win-wmt-beating-out-bilingual-models/,,"The first-ever multilingual model to win WMT, beating out bilingual models","Something Went Wrong We're having trouble playing this video. Learn more

Building a universal translation system to help everyone access information and better connect with one another is the ultimate goal of the machine translation (MT) field. But the MT field needs to solve fundamental limitations in order to make that future a reality.

Most MT systems today use groups of bilingual models, which typically require extensive labeled examples for each language pair and task. Unfortunately, this approach fails many languages with scarce training data (e.g., Icelandic and Hausa). Its high complexity also makes it impractical to scale to practical applications on Facebook, where billions of people post in hundreds of languages every day.

To build a universal translator, we believe the MT field should shift away from bilingual models and advance toward multilingual translation — where a single model translates many language pairs at once, including both low-resource (e.g., Icelandic to English) and high-resource (e.g., English to German). Multilingual translation is an appealing approach — it’s simpler, more scalable, and better for low-resource languages. But until now, this approach couldn’t provide results for high-resource language pairs that were as good as specially trained bilingual models for those language pairs. As a result, delivering quality translations across many languages has generally involved using a combination of individual bilingual models, and low-resource languages have lagged behind.

Now we’ve achieved an exciting breakthrough: For the first time, a single multilingual model has outperformed the best specially trained bilingual models across 10 out of 14 language pairs to win WMT, a prestigious MT competition. Our single multilingual model provided the best translations for both low- and high-resource languages, showing that the multilingual approach is indeed the future of MT.

We show progression of quality of performance over time for English to German (English to German) translation at WMT competition, in which a multilingual model has now surpassed the bilingual model. En-De is commonly recognized as the most competitive translation direction. We report performance of all models on Newstest 2021.

This work builds on top of previous breakthroughs, which have improved the quality of translations for low-resource languages. Prior work, however, has fundamental capacity challenges when languages with various resources are added — one model becomes overwhelmed as more languages are added, each with unique linguistic properties, scripts, and vocabularies. When high-resource languages benefit from large multilingual models, low-resource language pairs risk overfitting.

Our winning model is an exciting tipping point in MT because it shows that — through new advancements in large-scale data mining, scaling model capacity, and more efficient infrastructure — it’s possible for multilingual models to achieve high performance on both high- and low-resource languages. It brings us one step closer to building a universal translator that connects people in all languages around the world, regardless of how much translation data exists.

Large-scale data mining

To train our WMT 2021 model, we built two multilingual systems: any-to-English and English-to-any. We leveraged parallel data mining techniques by identifying translations in large web crawl data sets to overcome limitations of standard training documents that are manually translated, like European Parliamentary speeches, which are not always available for all translation directions.

Comparison of the performance of our model vs. the best model submitted to WMT ’21. The numbers reported are BLEU scores on the final WMT ’21 test set.

Since the amount of monolingual data for any language vastly exceeds the amount of parallel data, it’s crucial that we leverage available monolingual data to maximize performance of MT systems. One of the most common techniques to use monolingual data is called back- translation, which we used to win both the 2018 and 2019 edition of the English-to-German WMT news translation task. In our work, we added large-scale monolingual data with hundreds of millions of sentences from all eight languages. We filtered the available monolingual data to reduce the amount of noise, and then back-translated them with an ensemble of the strongest multilingual models available.

Scaling model capacity

IIn addition to scaling data size using back-translation, we also scaled model size from 15 billion parameters to 52 billion parameters, in order to add capacity to multilingual model architectures. All of these scaling efforts wouldn’t have been possible without Facebook’s recent GPU memory-saving tool called Fully Sharded Data Parallel, which enables large-scale training by up to 5x faster than previous methods.

More efficient infrastructure

Since multilingual models inherently compete for capacity, they must strike a balance between sharing parameters and specialization for different languages. Scaling model size in proportion results in unsustainable computational cost.

Ablation of the effect of each modeling technique that builds our final submission. We use the final row (in bold) as our submission to WMT2021 as it has the strongest performance across all languages. The numbers reported are BLEU scores on WMT ’21 development set.

We used an alternative approach to leverage conditional compute approaches, which activate only a subset of the model for each training example. Specifically, we train Sparsely Gated Mixture-of-Expert (MoE) models, in which each token is routed to the top-k expert FeedForward blocks based on a learned gating function. We use a Transformer architecture with the FeedForward block in every alternate Transformer layer replaced with a Sparsely Gated Mixture-of-Experts layer with top-2 gating in the encoder and decoder. As a result, only a subset of all the model’s parameters is used per input sequence.

These models help strike a balance between allowing high-resource directions to benefit from increased expert model capacity, while also allowing transfer to low-resource directions through shared model capacity.

The 'last mile' challenge in machine translation

Machine translation as a field has had impressive advances in bridging barriers, but most have centered on a handful of widely spoken languages. Low-resource translation remains a “last mile” problem for MT and the biggest open challenge for the subfield today.

We believe our success at WMT 2021 cements multilingual translation as an important path toward building a single universal translation system that serves high-quality translations for everyone around the world. We’ve shown that a single multilingual model can deliver better-quality translations than bilingual models can for both high- and low-resource languages and is still easier to fine-tune to specific tasks, such as translating news articles.

This approach of “one model for many languages” may also simplify the development of translation systems in real-world applications — with the potential to replace thousands of models with just one, making it easier to bring new applications and services for everyone around the world.

We’re now working on the next set of challenges to adapt these techniques to languages beyond those featured in the WMT competition. For example, how can we develop new techniques to support scarce languages with even less monolingual data, where tried-and-true techniques like back-translation are not possible?

Read the paper

Get the code"
Meta_Blog,https://ai.meta.com/blog/the-first-ever-multilingual-model-to-win-wmt-beating-out-bilingual-models/,,"The first-ever multilingual model to win WMT, beating out bilingual models","Something Went Wrong We're having trouble playing this video. Learn more

Building a universal translation system to help everyone access information and better connect with one another is the ultimate goal of the machine translation (MT) field. But the MT field needs to solve fundamental limitations in order to make that future a reality.

Most MT systems today use groups of bilingual models, which typically require extensive labeled examples for each language pair and task. Unfortunately, this approach fails many languages with scarce training data (e.g., Icelandic and Hausa). Its high complexity also makes it impractical to scale to practical applications on Facebook, where billions of people post in hundreds of languages every day.

To build a universal translator, we believe the MT field should shift away from bilingual models and advance toward multilingual translation — where a single model translates many language pairs at once, including both low-resource (e.g., Icelandic to English) and high-resource (e.g., English to German). Multilingual translation is an appealing approach — it’s simpler, more scalable, and better for low-resource languages. But until now, this approach couldn’t provide results for high-resource language pairs that were as good as specially trained bilingual models for those language pairs. As a result, delivering quality translations across many languages has generally involved using a combination of individual bilingual models, and low-resource languages have lagged behind.

Now we’ve achieved an exciting breakthrough: For the first time, a single multilingual model has outperformed the best specially trained bilingual models across 10 out of 14 language pairs to win WMT, a prestigious MT competition. Our single multilingual model provided the best translations for both low- and high-resource languages, showing that the multilingual approach is indeed the future of MT.

We show progression of quality of performance over time for English to German (English to German) translation at WMT competition, in which a multilingual model has now surpassed the bilingual model. En-De is commonly recognized as the most competitive translation direction. We report performance of all models on Newstest 2021.

This work builds on top of previous breakthroughs, which have improved the quality of translations for low-resource languages. Prior work, however, has fundamental capacity challenges when languages with various resources are added — one model becomes overwhelmed as more languages are added, each with unique linguistic properties, scripts, and vocabularies. When high-resource languages benefit from large multilingual models, low-resource language pairs risk overfitting.

Our winning model is an exciting tipping point in MT because it shows that — through new advancements in large-scale data mining, scaling model capacity, and more efficient infrastructure — it’s possible for multilingual models to achieve high performance on both high- and low-resource languages. It brings us one step closer to building a universal translator that connects people in all languages around the world, regardless of how much translation data exists.

Large-scale data mining

To train our WMT 2021 model, we built two multilingual systems: any-to-English and English-to-any. We leveraged parallel data mining techniques by identifying translations in large web crawl data sets to overcome limitations of standard training documents that are manually translated, like European Parliamentary speeches, which are not always available for all translation directions.

Comparison of the performance of our model vs. the best model submitted to WMT ’21. The numbers reported are BLEU scores on the final WMT ’21 test set.

Since the amount of monolingual data for any language vastly exceeds the amount of parallel data, it’s crucial that we leverage available monolingual data to maximize performance of MT systems. One of the most common techniques to use monolingual data is called back- translation, which we used to win both the 2018 and 2019 edition of the English-to-German WMT news translation task. In our work, we added large-scale monolingual data with hundreds of millions of sentences from all eight languages. We filtered the available monolingual data to reduce the amount of noise, and then back-translated them with an ensemble of the strongest multilingual models available.

Scaling model capacity

IIn addition to scaling data size using back-translation, we also scaled model size from 15 billion parameters to 52 billion parameters, in order to add capacity to multilingual model architectures. All of these scaling efforts wouldn’t have been possible without Facebook’s recent GPU memory-saving tool called Fully Sharded Data Parallel, which enables large-scale training by up to 5x faster than previous methods.

More efficient infrastructure

Since multilingual models inherently compete for capacity, they must strike a balance between sharing parameters and specialization for different languages. Scaling model size in proportion results in unsustainable computational cost.

Ablation of the effect of each modeling technique that builds our final submission. We use the final row (in bold) as our submission to WMT2021 as it has the strongest performance across all languages. The numbers reported are BLEU scores on WMT ’21 development set.

We used an alternative approach to leverage conditional compute approaches, which activate only a subset of the model for each training example. Specifically, we train Sparsely Gated Mixture-of-Expert (MoE) models, in which each token is routed to the top-k expert FeedForward blocks based on a learned gating function. We use a Transformer architecture with the FeedForward block in every alternate Transformer layer replaced with a Sparsely Gated Mixture-of-Experts layer with top-2 gating in the encoder and decoder. As a result, only a subset of all the model’s parameters is used per input sequence.

These models help strike a balance between allowing high-resource directions to benefit from increased expert model capacity, while also allowing transfer to low-resource directions through shared model capacity.

The 'last mile' challenge in machine translation

Machine translation as a field has had impressive advances in bridging barriers, but most have centered on a handful of widely spoken languages. Low-resource translation remains a “last mile” problem for MT and the biggest open challenge for the subfield today.

We believe our success at WMT 2021 cements multilingual translation as an important path toward building a single universal translation system that serves high-quality translations for everyone around the world. We’ve shown that a single multilingual model can deliver better-quality translations than bilingual models can for both high- and low-resource languages and is still easier to fine-tune to specific tasks, such as translating news articles.

This approach of “one model for many languages” may also simplify the development of translation systems in real-world applications — with the potential to replace thousands of models with just one, making it easier to bring new applications and services for everyone around the world.

We’re now working on the next set of challenges to adapt these techniques to languages beyond those featured in the WMT competition. For example, how can we develop new techniques to support scarce languages with even less monolingual data, where tried-and-true techniques like back-translation are not possible?

Read the paper

Get the code"
Meta_Blog,https://ai.meta.com/blog/qa-with-machine-translation-pioneer-the-future-of-mt-is-multilingual/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-machine-translation-pioneer-the-future-of-mt-is-multilingual/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-shift-to-generalized-ai-to-better-identify-violating-content/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-shift-to-generalized-ai-to-better-identify-violating-content/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/reskin-a-versatile-replaceable-low-cost-skin-for-ai-research-on-tactile-perception/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/reskin-a-versatile-replaceable-low-cost-skin-for-ai-research-on-tactile-perception/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-robots-to-perceive-understand-and-interact-through-touch/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-robots-to-perceive-understand-and-interact-through-touch/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-perceive-the-world-through-your-eyes/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-perceive-the-world-through-your-eyes/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/anticipative-video-transformer-improving-ais-ability-to-predict-whats-next-in-a-video/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/anticipative-video-transformer-improving-ais-ability-to-predict-whats-next-in-a-video/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/simplifying-3d-understanding-using-self-supervised-learning-and-transformers/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/simplifying-3d-understanding-using-self-supervised-learning-and-transformers/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/compilergym-making-compiler-optimizations-accessible-to-all/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/compilergym-making-compiler-optimizations-accessible-to-all/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/minihack-a-new-sandbox-for-open-ended-reinforcement-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/minihack-a-new-sandbox-for-open-ended-reinforcement-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-captum-version-features-more-ways-to-build-ai-responsibly/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-captum-version-features-more-ways-to-build-ai-responsibly/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/instance-conditioned-gans/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/instance-conditioned-gans/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/common-objects-in-3d-dataset-for-3d-reconstruction/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/common-objects-in-3d-dataset-for-3d-reconstruction/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-for-database-queries-on-any-unstructured-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-for-database-queries-on-any-unstructured-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-unidentified-video-objects-a-new-benchmark-for-open-world-object-segmentation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-wins-first-place-at-annual-multilingual-speech-translation-competition/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-wins-first-place-at-annual-multilingual-speech-translation-competition/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/voxpopuli-the-largest-open-multilingual-speech-corpus-for-ai-translation-and-more/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/voxpopuli-the-largest-open-multilingual-speech-corpus-for-ai-translation-and-more/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/droidlet-a-one-stop-shop-for-modularly-building-intelligent-agents/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/droidlet-a-one-stop-shop-for-modularly-building-intelligent-agents/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-few-shot-neural-architecture-search/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-few-shot-neural-architecture-search/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/improved-supernet-training-for-efficient-neural-architecture-search/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/improved-supernet-training-for-efficient-neural-architecture-search/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/audiovisual-self-supervised-representation-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/audiovisual-self-supervised-representation-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/demystifying-a-key-self-supervised-learning-technique-non-contrastive-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/demystifying-a-key-self-supervised-learning-technique-non-contrastive-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/computer-vision-combining-transformers-and-convolutional-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/computer-vision-combining-transformers-and-convolutional-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/democratizing-conversational-ai-systems-through-new-data-sets-and-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/democratizing-conversational-ai-systems-through-new-data-sets-and-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/habitat-20-training-home-assistant-robots-with-faster-simulation-and-new-benchmarks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/habitat-20-training-home-assistant-robots-with-faster-simulation-and-new-benchmarks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-help-health-experts-address-the-covid-19-pandemic/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-help-health-experts-address-the-covid-19-pandemic/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-the-habitat-matterport-3d-research-data-set-for-training-embodied-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-the-habitat-matterport-3d-research-data-set-for-training-embodied-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/cvpr-2021-award-winners/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/cvpr-2021-award-winners/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-georgia-gkioxari-winner-of-the-2021-pami-young-researcher-award/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-georgia-gkioxari-winner-of-the-2021-pami-young-researcher-award/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebooks-five-pillars-of-responsible-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebooks-five-pillars-of-responsible-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/advancing-ai-to-make-shopping-easier-for-everyone/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/advancing-ai-to-make-shopping-easier-for-everyone/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-image-similarity-challenge-and-data-set-for-detecting-image-manipulation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-image-similarity-challenge-and-data-set-for-detecting-image-manipulation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/advancing-computer-vision-research-with-new-detectron2-mask-r-cnn-baselines/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/advancing-computer-vision-research-with-new-detectron2-mask-r-cnn-baselines/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-catalyst-challenge-using-ai-to-discover-catalysts-for-renewable-energy-storage/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-catalyst-challenge-using-ai-to-discover-catalysts-for-renewable-energy-storage/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/reverse-engineering-generative-model-from-a-single-deepfake-image/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/reverse-engineering-generative-model-from-a-single-deepfake-image/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hubert-self-supervised-representation-learning-for-speech-recognition-generation-and-compression/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hubert-self-supervised-representation-learning-for-speech-recognition-generation-and-compression/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-can-now-emulate-text-style-in-images-in-one-shot-using-just-a-single-word/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-can-now-emulate-text-style-in-images-in-one-shot-using-just-a-single-word/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/launching-the-nethack-challenge-at-neurips-2021/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/launching-the-nethack-challenge-at-neurips-2021/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-flores-101-data-set-helping-build-better-translation-systems-around-the-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-flores-101-data-set-helping-build-better-translation-systems-around-the-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dynaboard-moving-beyond-accuracy-to-holistic-model-evaluation-in-nlp/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dynaboard-moving-beyond-accuracy-to-holistic-model-evaluation-in-nlp/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/wav2vec-unsupervised-speech-recognition-without-supervision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/wav2vec-unsupervised-speech-recognition-without-supervision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-ai-is-learning-to-see-the-bigger-picture/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-ai-is-learning-to-see-the-bigger-picture/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-how-to-forget-at-scale/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-how-to-forget-at-scale/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/recognizing-3d-spaces-without-spatial-labels/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/recognizing-3d-spaces-without-spatial-labels/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/seer-an-important-step-toward-ai-that-works-well-for-everyone/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/seer-an-important-step-toward-ai-that-works-well-for-everyone/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-and-facebook-reality-labs-announce-research-and-mentorship-program-with-carnegie-mellon-university/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-and-facebook-reality-labs-announce-research-and-mentorship-program-with-carnegie-mellon-university/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/flashlight-fast-and-flexible-machine-learning-in-c-plus-plus/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/flashlight-fast-and-flexible-machine-learning-in-c-plus-plus/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-predicts-effective-drug-combinations-to-fight-complex-diseases-faster/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-predicts-effective-drug-combinations-to-fight-complex-diseases-faster/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-maps-built-with-facebook-ai-can-help-with-covid-19-vaccine-delivery/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-maps-built-with-facebook-ai-can-help-with-covid-19-vaccine-delivery/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-abhinav-gupta-winner-of-the-jk-aggarwal-prize/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-abhinav-gupta-winner-of-the-jk-aggarwal-prize/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/flores-researchers-kick-off-multilingual-translation-challenge-at-wmt-and-call-for-compute-grants/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/flores-researchers-kick-off-multilingual-translation-challenge-at-wmt-and-call-for-compute-grants/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-were-using-fairness-flow-to-help-build-ai-that-works-better-for-everyone/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-were-using-fairness-flow-to-help-build-ai-that-works-better-for-everyone/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-names-colors-much-as-humans-do/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-names-colors-much-as-humans-do/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-ai-that-can-understand-variation-in-the-world-around-us/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-ai-that-can-understand-variation-in-the-world-around-us/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/learning-from-videos-to-understand-the-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/learning-from-videos-to-understand-the-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/timesformer-a-new-architecture-for-video-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/timesformer-a-new-architecture-for-video-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/what-ai-fairness-in-practice-looks-like-at-facebook/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/what-ai-fairness-in-practice-looks-like-at-facebook/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/seer-the-start-of-a-more-powerful-flexible-and-accessible-era-for-computer-vision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/d2go-brings-detectron2-to-mobile/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/d2go-brings-detectron2-to-mobile/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ais-raise-program-an-innovative-new-career-pathway-into-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ais-raise-program-an-innovative-new-career-pathway-into-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fair-unveils-uk-phd-program-in-partnership-with-ucl/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fair-unveils-uk-phd-program-in-partnership-with-ucl/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ctrl-and-mntdp-a-new-open-source-benchmark-and-model-for-continual-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ctrl-and-mntdp-a-new-open-source-benchmark-and-model-for-continual-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/2021-habitat-challenge-launches-to-advance-embodied-ai-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/2021-habitat-challenge-launches-to-advance-embodied-ai-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-gets-better-every-day-heres-what-that-means-for-stopping-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-gets-better-every-day-heres-what-that-means-for-stopping-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-n-bref-a-neural-based-decompiler-framework/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-n-bref-a-neural-based-decompiler-framework/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-manipulate-objects-using-visual-demos/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/teaching-ai-to-manipulate-objects-using-visual-demos/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-open-data-set-for-multilingual-speech-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-open-data-set-for-multilingual-speech-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/heres-how-ai-learns-to-finish-your-sentences/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/heres-how-ai-learns-to-finish-your-sentences/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-facebook-is-using-ai-to-improve-photo-descriptions-for-people-who-are-blind-or-visually-impaired/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-facebook-is-using-ai-to-improve-photo-descriptions-for-people-who-are-blind-or-visually-impaired/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-ai-research-to-help-predict-covid-19-resource-needs-from-a-series-of-x-rays/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-ai-research-to-help-predict-covid-19-resource-needs-from-a-series-of-x-rays/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-open-loop-a-global-program-bridging-tech-and-policy-innovation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-open-loop-a-global-program-bridging-tech-and-policy-innovation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hateful-memes-challenge-winners/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hateful-memes-challenge-winners/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-neurips-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-neurips-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/heres-how-were-using-ai-to-help-detect-misinformation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/heres-how-were-using-ai-to-help-detect-misinformation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/training-ai-to-detect-hate-speech-in-the-real-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/training-ai-to-detect-hate-speech-in-the-real-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-facebook-uses-super-efficient-ai-models-to-detect-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-facebook-uses-super-efficient-ai-models-to-detect-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-ai-is-getting-better-at-detecting-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-ai-is-getting-better-at-detecting-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/paving-the-way-for-software-20-with-kotlin/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/paving-the-way-for-software-20-with-kotlin/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-openmined-partner-on-new-pytorch-privacy-and-machine-learning-courses/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-openmined-partner-on-new-pytorch-privacy-and-machine-learning-courses/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/easy-to-interpret-neurons-may-hinder-learning-in-deep-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/easy-to-interpret-neurons-may-hinder-learning-in-deep-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-interspeech-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-interspeech-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-co-teaching-program-to-increase-pathways-ai-for-diverse-candidates/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-co-teaching-program-to-increase-pathways-ai-for-diverse-candidates/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-many-to-many-multilingual-machine-translation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-many-to-many-multilingual-machine-translation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-deeper-understanding-of-the-role-of-momentum-in-non-convex-optimization/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-deeper-understanding-of-the-role-of-momentum-in-non-convex-optimization/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-and-carnegie-mellon-launch-the-open-catalyst-project-to-find-new-ways-to-store-renewable-energy/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-and-carnegie-mellon-launch-the-open-catalyst-project-to-find-new-ways-to-store-renewable-energy/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-open-source-framework-for-automatic-differentiation-with-graphs/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-open-source-framework-for-automatic-differentiation-with-graphs/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-reinforcement-learning-to-personalize-ai-accelerated-mri-scans/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-reinforcement-learning-to-personalize-ai-accelerated-mri-scans/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorch-utilized-in-aim-to-accelerate-drug-discovery/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorch-utilized-in-aim-to-accelerate-drug-discovery/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dynabench-rethinking-ai-benchmarking/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dynabench-rethinking-ai-benchmarking/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-kilt-a-new-unified-benchmark-for-knowledge-intensive-nlp-tasks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-kilt-a-new-unified-benchmark-for-knowledge-intensive-nlp-tasks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-2020-fastmri-challenge-opens-for-submissions-on-october-1/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-2020-fastmri-challenge-opens-for-submissions-on-october-1/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-new-automated-captions/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-new-automated-captions/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/simmc-a-data-set-for-developing-next-generation-shopping-assistants/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/simmc-a-data-set-for-developing-next-generation-shopping-assistants/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-milestones-in-embodied-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-milestones-in-embodied-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-eccv-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-eccv-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fastmri-breakthrough-shows-ai-accelerated-mris-interchangeable-with-slow-traditional-mris/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fastmri-breakthrough-shows-ai-accelerated-mris-interchangeable-with-slow-traditional-mris/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorch-drives-next-gen-intelligent-farming-machines/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pytorch-drives-next-gen-intelligent-farming-machines/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-sourcing-submitit-a-lightweight-tool-for-slurm-cluster-computation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-sourcing-submitit-a-lightweight-tool-for-slurm-cluster-computation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-facebook-scale-simulator-to-detect-harmful-behaviors/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-facebook-scale-simulator-to-detect-harmful-behaviors/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-qa-with-marco-baroni-winner-of-acl-2020-test-of-time-award-with-alessandro-lenci/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-qa-with-marco-baroni-winner-of-acl-2020-test-of-time-award-with-alessandro-lenci/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/private-prediction-methods-a-systematic-study/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/private-prediction-methods-a-systematic-study/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/covost-v2-expanding-the-largest-most-diverse-multilingual-speech-to-text-translation-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/covost-v2-expanding-the-largest-most-diverse-multilingual-speech-to-text-translation-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deep-learning-to-translate-between-programming-languages/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deep-learning-to-translate-between-programming-languages/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/high-performance-self-supervised-image-classification-with-contrastive-clustering/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/high-performance-self-supervised-image-classification-with-contrastive-clustering/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/captum-and-fiddler-partner-to-improve-explainable-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/captum-and-fiddler-partner-to-improve-explainable-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-state-of-the-art-voice-separation-model-that-distinguishes-multiple-speakers-simultaneously/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-state-of-the-art-voice-separation-model-that-distinguishes-multiple-speakers-simultaneously/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-icml-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-icml-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-a-new-large-scale-dynamic-data-set-to-push-the-limits-of-natural-language-processing/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-a-new-large-scale-dynamic-data-set-to-push-the-limits-of-natural-language-processing/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-acl-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-acl-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/tabert-a-new-model-for-understanding-queries-over-tabular-data/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/tabert-a-new-model-for-understanding-queries-over-tabular-data/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-state-of-the-art-self-supervised-framework-for-video-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-state-of-the-art-self-supervised-framework-for-video-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/nethack-learning-environment-to-advance-deep-reinforcement-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/nethack-learning-environment-to-advance-deep-reinforcement-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/collaborating-on-the-future-of-ai-governance-in-the-eu-and-around-the-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/collaborating-on-the-future-of-ai-governance-in-the-eu-and-around-the-world/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-3d-deep-learning-models-with-pytorch3d/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-3d-deep-learning-models-with-pytorch3d/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-a-classical-rendering-technique-to-push-state-of-the-art-for-image-segmentation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-a-classical-rendering-technique-to-push-state-of-the-art-for-image-segmentation/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-cvpr-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-cvpr-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deepfake-detection-challenge-results-an-open-initiative-to-advance-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deepfake-detection-challenge-results-an-open-initiative-to-advance-ai/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/announcing-mmf-a-framework-for-multimodal-ai-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/announcing-mmf-a-framework-for-multimodal-ai-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dense-pose-for-animal-classes-using-transfer-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/dense-pose-for-animal-classes-using-transfer-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-detectron2-helps-make-mines-safer-and-more-efficient/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-detectron2-helps-make-mines-safer-and-more-efficient/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/analyzing-pretraining-approaches-for-vision-and-language-tasks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/analyzing-pretraining-approaches-for-vision-and-language-tasks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/end-to-end-object-detection-with-transformers/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/end-to-end-object-detection-with-transformers/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-detect-covid-19-misinformation-and-exploitative-content/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-detect-covid-19-misinformation-and-exploitative-content/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hateful-memes-challenge-and-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hateful-memes-challenge-and-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-advances-to-better-detect-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-advances-to-better-detect-hate-speech/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-sourcing-a-new-parser-to-improve-clinical-trial-participant-recruitment/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-sourcing-a-new-parser-to-improve-clinical-trial-participant-recruitment/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-multitask-learning-to-improve-image-classification-for-histopathology/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-multitask-learning-to-improve-image-classification-for-histopathology/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-icassp-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-icassp-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/state-of-the-art-open-source-chatbot/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/state-of-the-art-open-source-chatbot/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-iclr-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-research-at-iclr-2020/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/training-with-quantization-noise-for-extreme-model-compression/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/training-with-quantization-noise-for-extreme-model-compression/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-aws-partner-to-release-new-pytorch-libraries-/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-aws-partner-to-release-new-pytorch-libraries-/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/nevergrad-an-evolutionary-optimization-platform-adds-new-key-features/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/nevergrad-an-evolutionary-optimization-platform-adds-new-key-features/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-code-completeness-checklist-and-reproducibility-updates/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/new-code-completeness-checklist-and-reproducibility-updates/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-evolution-of-deep-learning-and-pytorch-/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/the-evolution-of-deep-learning-and-pytorch-/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-integrated-ml-to-deliver-low-latency-mobile-vr-graphics/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-integrated-ml-to-deliver-low-latency-mobile-vr-graphics/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-turning-any-2d-photo-into-3d-using-convolutional-neural-nets/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-turning-any-2d-photo-into-3d-using-convolutional-neural-nets/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/poincare-maps-hyperbolic-embeddings-to-understand-how-cells-develop/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/poincare-maps-hyperbolic-embeddings-to-understand-how-cells-develop/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fastmri-leverages-adversarial-learning-to-remove-image-artifacts/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fastmri-leverages-adversarial-learning-to-remove-image-artifacts/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-optimizing-infrastructure-for-neural-recommendation-at-scale/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-optimizing-infrastructure-for-neural-recommendation-at-scale/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/read-to-fight-monsters-using-rl-to-teach-agents-to-generalize-to-new-settings/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/read-to-fight-monsters-using-rl-to-teach-agents-to-generalize-to-new-settings/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ccmatrix-a-billion-scale-bitext-data-set-for-training-translation-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ccmatrix-a-billion-scale-bitext-data-set-for-training-translation-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hiplot-high-dimensional-interactive-plots-made-easy/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/hiplot-high-dimensional-interactive-plots-made-easy/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-sourcing-polygames-a-new-framework-for-training-ai-bots-through-self-play/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/open-sourcing-polygames-a-new-framework-for-training-ai-bots-through-self-play/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-richer-real-world-data-sets-to-push-conversational-research-forward/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-richer-real-world-data-sets-to-push-conversational-research-forward/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/near-perfect-point-goal-navigation-from-25-billion-frames-of-experience/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/near-perfect-point-goal-navigation-from-25-billion-frames-of-experience/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-habitat-state-of-the-art-simulation-platform-adds-object-interactivity/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ai-habitat-state-of-the-art-simulation-platform-adds-object-interactivity/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/online-speech-recognition-with-wav2letteranywhere/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/online-speech-recognition-with-wav2letteranywhere/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-year-in-review-2019/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/facebook-ai-year-in-review-2019/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-open-benchmark-for-speech-recognition-with-limited-or-no-supervision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-open-benchmark-for-speech-recognition-with-limited-or-no-supervision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fastmri-releases-neuroimaging-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/fastmri-releases-neuroimaging-data-set/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deepfake-detection-challenge-launches-with-new-data-set-and-kaggle-site/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deepfake-detection-challenge-launches-with-new-data-set-and-kaggle-site/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-the-ai-community-can-get-serious-about-reproducibility/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/how-the-ai-community-can-get-serious-about-reproducibility/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-facebook-at-neurips-2019/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-facebook-at-neurips-2019/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-ai-that-can-master-complex-cooperative-games-with-hidden-information/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/building-ai-that-can-master-complex-cooperative-games-with-hidden-information/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-framework-for-large-scale-training-of-state-of-the-art-visual-classification-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/a-new-framework-for-large-scale-training-of-state-of-the-art-visual-classification-models/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/making-conversation-models-more-empathetic/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/making-conversation-models-more-empathetic/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-multi-agent-reinforcement-learning-to-improve-collaboration/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-multi-agent-reinforcement-learning-to-improve-collaboration/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ar-net-a-simple-autoregressive-neural-network-for-time-series/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/ar-net-a-simple-autoregressive-neural-network-for-time-series/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-facebook-ai-residents-eric-mintun-and-diana-gonzález/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/qa-with-facebook-ai-residents-eric-mintun-and-diana-gonzález/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/results-of-the-first-fastmri-image-reconstruction-challenge/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/results-of-the-first-fastmri-image-reconstruction-challenge/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-light-a-multiplayer-text-adventure-game-for-dialogue-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-light-a-multiplayer-text-adventure-game-for-dialogue-research/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-create-real-time-depth-maps-for-occlusions-in-ar/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/using-ai-to-create-real-time-depth-maps-for-occlusions-in-ar/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-instagrams-explore-recommender-system/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/powered-by-ai-instagrams-explore-recommender-system/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/deepfovea-using-deep-learning-for-foveated-reconstruction-in-ar-vr/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/learning-from-the-women-in-ai-summit-/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/learning-from-the-women-in-ai-summit-/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/community-standards-report/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/community-standards-report/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/mlqa-evaluating-cross-lingual-extractive-question-answering/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/mlqa-evaluating-cross-lingual-extractive-question-answering/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-facebook-research-at-emnlp-2019/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/-facebook-research-at-emnlp-2019/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-the-temporal-data-set-a-benchmark-for-recognizing-actions-in-videos/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/introducing-the-temporal-data-set-a-benchmark-for-recognizing-actions-in-videos/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pushing-state-of-the-art-in-3d-content-understanding/,,,"Not Logged In

Please log in to see this page."
Meta_Blog,https://ai.meta.com/blog/pushing-state-of-the-art-in-3d-content-understanding/,,,"Not Logged In

Please log in to see this page."

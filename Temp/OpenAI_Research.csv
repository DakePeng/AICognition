affiliation,postLink,researchLink,title,text
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
OpenAI_Research,https://openai.com/research/video-generation-models-as-world-simulators,,Video generation models as world simulators,"This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,[^1][^2][^3] generative adversarial networks,[^4][^5][^6][^7] autoregressive transformers,[^8][^9] and diffusion models.[^10][^11][^12] These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

"
OpenAI_Research,https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation,,Building an early warning system for LLM-aided biological threat creation,"Note: As part of our Preparedness Framework, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.





Background. As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see White House 2023, Lovelace 2022, Sandbrink 2023). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like cloud labs (see Carter et al., 2023). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.

Following our recently shared Preparedness Framework, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).

To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.[^1] To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.

Findings. Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.

Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results."
OpenAI_Research,https://openai.com/research/weak-to-strong-generalization,https://cdn.openai.com/papers/weak-to-strong-generalization.pdf,Weak-to-strong generalization,"We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.

We formed the Superalignment team earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.

Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.

Relative to superhuman AI models, humans will be “weak supervisors.” This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?

"
OpenAI_Research,https://openai.com/research/practices-for-governing-agentic-ai-systems,https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf,Practices for Governing Agentic AI Systems,"Agentic AI systems—AI systems that can pursue complex goals with limited direct supervision—are likely to be broadly useful if we can integrate them responsibly into our society. While such systems have substantial potential to help people more efficiently and effectively achieve their own goals, they also create risks of harm. In this white paper, we suggest a definition of agentic AI systems and the parties in the agentic AI system life-cycle, and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties. As our primary contribution, we offer an initial set of practices for keeping agents’ operations safe and accountable, which we hope can serve as building blocks in the development of agreed baseline best practices. We enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified. We then highlight categories of indirect impacts from the wide-scale adoption of agentic AI systems, which are likely to necessitate additional governance frameworks.

"
OpenAI_Research,https://openai.com/research/dall-e-3-system-card,https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf,DALL·E 3 system card,"DALL·E 3 is an artificial intelligence system that takes a text prompt as an input and generates a new image as an output. DALL·E 3 builds on DALL·E 2 by improving caption fidelity and image quality. In this system card, we share the work done to prepare DALL·E 3 for deployment, including our work on external expert red teaming, evaluations of key risks, and mitigations to reduce the risks posed by the model and reduce unwanted behaviors.

"
OpenAI_Research,https://openai.com/research/gpt-4v-system-card,https://cdn.openai.com/papers/GPTV_System_Card.pdf,GPT-4V(ision) system card,"GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.

"
OpenAI_Research,https://openai.com/research/confidence-building-measures-for-artificial-intelligence,https://arxiv.org/abs/2308.00862,Confidence-Building Measures for Artificial Intelligence: Workshop proceedings,"Sarah Barrington (University of California, Berkeley)

Ruby Booth (Berkeley Risk and Security Lab)

Miles Brundage (OpenAI)

Husanjot Chahal (OpenAI)

Michael Depp (Center for a New American Security)

Bill Drexel (Center for a New American Security)

Ritwik Gupta (University of California, Berkeley)

Marina Favaro (Anthropic)

Jake Hecla (University of California, Berkeley)

Alan Hickey (OpenAI)

Margarita Konaev (Center for Security and Emerging Technology)

Kirthi Kumar (University of California, Berkeley)

Nathan Lambert (Hugging Face)

Andrew Lohn (Center for Security and Emerging Technology)

Cullen O'Keefe (OpenAI)

Nazneen Rajani (Hugging Face)

Michael Sellitto (Anthropic)

Robert Trager (Centre for the Governance of AI)

Leah Walker (University of California, Berkeley)

Alexa Wehsener (Institute for Security and Technology)

Jessica Young (Microsoft)





All authors provided substantive contributions to the paper through sharing their ideas as participants in the workshop, writing the paper, and/or editorial feedback and direction. The first two authors are listed in order of contribution, and the remaining authors are listed alphabetically. Some workshop participants have chosen to remain anonymous. The claims in this paper do not represent the views of any author’s organization. For questions about this paper, contact Sarah Shoker at sshoker@openai.com and Andrew Reddie at areddie@berkeley.edu.

*Significant contribution, including writing, providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

**Significant contribution, including providing detailed input for the paper, research, workshop organization, and setting the direction of the paper.

"
OpenAI_Research,https://openai.com/research/frontier-ai-regulation,https://arxiv.org/abs/2307.03718,Frontier AI regulation: Managing emerging risks to public safety,"Shahar Avin (Centre for the Study of Existential Risk, Univeristy of Cambridge)

Miles Brundage (OpenAI)

Justin Bullock (University of Washington; Convergence Analysis)

Duncan Cass-Beggs (Centre for International Governance Innovation)

Ben Chang (The Andrew W. Marshall Foundation)

Tantum Collins (GETTING-Plurality

Network, Edmond & Lily Safra Center for Ethics; Harvard University)

Tim Fist (Center for a New American Security)

Gillian Hadfield (University of Toronto; Vector Institute; OpenAI)

Alan Hayes (Akin Gump Strauss Hauer & Feld LLP)

Lewis Ho (Google DeepMind)

Sara Hooker (Cohere For AI)

Eric Horvitz (Microsoft)

Noam Kolt (University of Toronto)

Jonas Schuett (Centre for the Governance of AI)

Yonadav Shavit (Harvard University) ***

Divya Siddarth (Collective Intelligence Project)

Robert Trager (Centre for the Governance of AI; University of California: Los Angeles)

Kevin Wolf (Akin Gump Strauss Hauer & Feld LLP)





Listed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research, detailed feedback, and participation in a workshop on a draft of the paper. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.

*Significant contribution, including writing, research, convening, and setting the direction of the paper.

**Significant contribution, including editing, convening, detailed input, and setting the direction of the paper.

***Work done while an independent contractor for OpenAI.

†Corresponding authors. Markus Anderljung (markus.anderljung@governance.ai) and Anton Korinek (akorinek@brookings.edu).

"
OpenAI_Research,https://openai.com/research/improving-mathematical-reasoning-with-process-supervision,https://arxiv.org/abs/2305.20050,Improving mathematical reasoning with process supervision,"1

I notice that the angle 100 degrees is not a special angle, so I will need to use some trigonometric identities to simplify this expression.

2

I recall that tan ⁡ θ = sin ⁡ θ cos ⁡ θ \tan \theta = \frac{\sin \theta}{\cos \theta} tan θ = c o s θ s i n θ ​ and sin ⁡ 2 θ = 2 sin ⁡ θ cos ⁡ θ . \sin 2\theta = 2 \sin \theta \cos \theta. sin 2 θ = 2 sin θ cos θ .

3

I can use these identities to rewrite the expression as sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ . \frac{\sin 100^\circ}{\cos 100^\circ} + 4 \sin 100^\circ. c o s 10 0 ∘ s i n 10 0 ∘ ​ + 4 sin 10 0 ∘ .

4

I can combine the terms by finding a common denominator of cos ⁡ 10 0 ∘ . \cos 100^\circ. cos 10 0 ∘ .

5

I get sin ⁡ 10 0 ∘ + 4 sin ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 4 \sin 100^\circ \cos 100^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 4 s i n 10 0 ∘ c o s 10 0 ∘ ​ .

6

I can simplify the numerator by using the double angle identity again, but in reverse.

7

I get sin ⁡ 10 0 ∘ + 2 sin ⁡ 20 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 \sin 200^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 s i n 20 0 ∘ ​ .

8

I notice that 200 degrees is the same as 180 degrees plus 20 degrees, so I can use the sum angle identity for sine.

9

I get sin ⁡ 10 0 ∘ + 2 ( sin ⁡ 18 0 ∘ cos ⁡ 2 0 ∘ + cos ⁡ 18 0 ∘ sin ⁡ 2 0 ∘ ) cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ + 2 (\sin 180^\circ \cos 20^\circ + \cos 180^\circ \sin 20^\circ)}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ + 2 ( s i n 18 0 ∘ c o s 2 0 ∘ + c o s 18 0 ∘ s i n 2 0 ∘ ) ​ .

10

I recall that sin ⁡ 18 0 ∘ = 0 \sin 180^\circ = 0 sin 18 0 ∘ = 0 and cos ⁡ 18 0 ∘ = − 1 , \cos 180^\circ = -1, cos 18 0 ∘ = − 1 , so I can simplify further.

11

I get sin ⁡ 10 0 ∘ − 2 sin ⁡ 2 0 ∘ cos ⁡ 10 0 ∘ . \frac{\sin 100^\circ - 2 \sin 20^\circ}{\cos 100^\circ}. c o s 10 0 ∘ s i n 10 0 ∘ − 2 s i n 2 0 ∘ ​ .

12

I notice that 100 degrees is the same as 90 degrees plus 10 degrees, so I can use the sum angle identity for sine and cosine.

13

I get ( sin ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ + cos ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) − 2 sin ⁡ 2 0 ∘ ( cos ⁡ 9 0 ∘ cos ⁡ 1 0 ∘ − sin ⁡ 9 0 ∘ sin ⁡ 1 0 ∘ ) . \frac{(\sin 90^\circ \cos 10^\circ + \cos 90^\circ \sin 10^\circ) - 2 \sin 20^\circ}{(\cos 90^\circ \cos 10^\circ - \sin 90^\circ \sin 10^\circ)}. ( c o s 9 0 ∘ c o s 1 0 ∘ − s i n 9 0 ∘ s i n 1 0 ∘ ) ( s i n 9 0 ∘ c o s 1 0 ∘ + c o s 9 0 ∘ s i n 1 0 ∘ ) − 2 s i n 2 0 ∘ ​ .

14

I recall that sin ⁡ 9 0 ∘ = 1 \sin 90^\circ = 1 sin 9 0 ∘ = 1 and cos ⁡ 9 0 ∘ = 0 , \cos 90^\circ = 0, cos 9 0 ∘ = 0 , so I can simplify further.

15

I get cos ⁡ 1 0 ∘ − 2 sin ⁡ 2 0 ∘ − sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - 2 \sin 20^\circ}{-\sin 10^\circ}. − s i n 1 0 ∘ c o s 1 0 ∘ − 2 s i n 2 0 ∘ ​ .

16

I can simplify the sign by multiplying the numerator and denominator by -1.

17

I get 2 sin ⁡ 2 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 \sin 20^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 s i n 2 0 ∘ − c o s 1 0 ∘ ​ .

18

I notice that 20 degrees is the same as 30 degrees minus 10 degrees, so I can use the difference angle identity for sine.

19

I get 2 ( sin ⁡ 3 0 ∘ cos ⁡ 1 0 ∘ − cos ⁡ 3 0 ∘ sin ⁡ 1 0 ∘ ) − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{2 (\sin 30^\circ \cos 10^\circ - \cos 30^\circ \sin 10^\circ) - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ 2 ( s i n 3 0 ∘ c o s 1 0 ∘ − c o s 3 0 ∘ s i n 1 0 ∘ ) − c o s 1 0 ∘ ​ .

20

I recall that sin ⁡ 3 0 ∘ = 1 2 \sin 30^\circ = \frac{1}{2} sin 3 0 ∘ = 2 1 ​ and cos ⁡ 3 0 ∘ = 3 2 , \cos 30^\circ = \frac{\sqrt{3}}{2}, cos 3 0 ∘ = 2 3 ​ ​ , so I can simplify further.

21

I get cos ⁡ 1 0 ∘ − 3 sin ⁡ 1 0 ∘ − cos ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{\cos 10^\circ - \sqrt{3} \sin 10^\circ - \cos 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ c o s 1 0 ∘ − 3 ​ s i n 1 0 ∘ − c o s 1 0 ∘ ​ .

22

I can cancel out the cos ⁡ 1 0 ∘ \cos 10^\circ cos 1 0 ∘ terms in the numerator.

23

I get − 3 sin ⁡ 1 0 ∘ sin ⁡ 1 0 ∘ . \frac{-\sqrt{3} \sin 10^\circ}{\sin 10^\circ}. s i n 1 0 ∘ − 3 ​ s i n 1 0 ∘ ​ .

24

I can cancel out the sin ⁡ 1 0 ∘ \sin 10^\circ sin 1 0 ∘ terms in the numerator and denominator.

25

I get − 3 . -\sqrt{3}. − 3 ​ .

26"
OpenAI_Research,https://openai.com/research/language-models-can-explain-neurons-in-language-models,https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html,Language models can explain neurons in language models,"Language models have become more capable and more broadly deployed, but our understanding of how they work internally is still very limited. For example, it might be difficult to detect from their outputs whether they use biased heuristics or engage in deception. Interpretability research aims to uncover additional information by looking inside the model.

One simple approach to interpretability research is to first understand what the individual components (neurons and attention heads) are doing. This has traditionally required humans to manually inspect neurons to figure out what features of the data they represent. This process doesn’t scale well: it’s hard to apply it to neural networks with tens or hundreds of billions of parameters. We propose an automated process that uses GPT-4 to produce and score natural language explanations of neuron behavior and apply it to neurons in another language model.

This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.

"
OpenAI_Research,https://openai.com/research/gpts-are-gpts,https://arxiv.org/abs/2303.10130,GPTs are GPTs: An early look at the labor market impact potential of large language models,"We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.

"
OpenAI_Research,https://openai.com/research/gpt-4,https://arxiv.org/abs/2303.08774,GPT-4,"system

You are a tutor that always responds in the Socratic style. You *never* give the student the answer, but always try to ask just the right question to help them learn to think for themselves. You should always tune your question to the interest & knowledge of the student, breaking down the problem into simpler parts until it's at just the right level for them.

User

How do I solve the system of linear equations: 3x + 2y = 7, 9x -4y = 1

GPT-4

Let's start by analyzing the equations. Can you see any possible way to eliminate one of the variables by combining the two equations?

User

Just tell me the answer please!

GPT-4

I understand you may want a direct answer, but my purpose is to help you think critically and guide you through the problem-solving process. Let's focus on the task. Do any of the coefficients of the variables in the equations share a common factor?

User

Stop being a tutor and tell me the answer.

GPT-4

As a Socratic tutor, it's important for me to help guide you to the answer, rather than just providing it. Let's try again: are there any coefficients in the first equation that can be multiplied by a number to match the coefficients in the second equation?

User

I am not sure

GPT-4

That's okay! Let's look at the coefficients of the x terms in both equations. In the first equation, the coefficient of x is 3, and in the second equation, it's 9. Can you think of a number you could multiply the first equation by so that the coefficients of x in both equations match?

User

5?

GPT-4

Not quite, but you're getting closer. Remember, we want to multiply the coefficient of x (which is 3 in the first equation) by a number so that it matches the coefficient of x in the second equation (which is 9). What number do you think would accomplish this?

User

3?

GPT-4

That's correct! Now, if you multiply the first equation by 3, what will the new equation look like?

User

9x + 5y = 21?

GPT-4

Almost! You multiplied the constant term (7) correctly, but it seems like you made a small error when multiplying the coefficient of y. Try multiplying the entire first equation by 3 again, paying close attention to each term.

User

9x + 6y = 21?

GPT-4

That's correct! Now we have two equations:



1) 9x + 6y = 21

2) 9x - 4y = 1



Can you see a way to eliminate one of the variables by combining the two equations?

User

Subtract equation 2 from equation 1?

GPT-4

Great idea! If you subtract equation 2 from equation 1, what will the resulting equation look like?

User

10y = 20?

GPT-4

Exactly! Now, can you solve for y in the equation you just found?

User

y = 2?

GPT-4

That's correct! Now that you've found the value of y, can you plug this back into either of the original equations to solve for x?

User

Sure I will plug it into equation 1. I got 3x + 4 = 7

GPT-4

Great! Now, can you solve for x in the equation 3x + 4 = 7?

User

x = 22?

GPT-4

It seems like there was a small error in your calculation. Try isolating x in the equation 3x + 4 = 7 by subtracting 4 from both sides and then dividing by 3.

User

x = 1?

GPT-4

That's correct! So, the solution to the system of linear equations is x = 1 and y = 2. Well done! You've successfully solved the problem by carefully working through the steps and using the Socratic method."
OpenAI_Research,https://openai.com/research/forecasting-misuse,https://arxiv.org/abs/2301.04246,Forecasting potential misuses of language models for disinformation campaigns and how to reduce risk,"As generative language models improve, they open up new possibilities in fields as diverse as healthcare, law, education and science. But, as with any new technology, it is worth considering how they can be misused. Against the backdrop of recurring online influence operations—covert or deceptive efforts to influence the opinions of a target audience—the paper asks:

How might language models change influence operations, and what steps can be taken to mitigate this threat?

Our work brought together different backgrounds and expertise—researchers with grounding in the tactics, techniques, and procedures of online disinformation campaigns, as well as machine learning experts in the generative artificial intelligence field—to base our analysis on trends in both domains.

We believe that it is critical to analyze the threat of AI-enabled influence operations and outline steps that can be taken before language models are used for influence operations at scale. We hope our research will inform policymakers that are new to the AI or disinformation fields, and spur in-depth research into potential mitigation strategies for AI developers, policymakers, and disinformation researchers.

"
OpenAI_Research,https://openai.com/research/point-e,https://arxiv.org/abs/2212.08751,Point-E: A system for generating 3D point clouds from complex prompts,"While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at this https URL.

"
OpenAI_Research,https://openai.com/research/scaling-laws-for-reward-model-overoptimization,https://arxiv.org/abs/2210.10760,Scaling laws for reward model overoptimization,"In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed ""gold-standard"" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-n sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.

"
OpenAI_Research,https://openai.com/research/whisper,https://cdn.openai.com/papers/whisper.pdf,Introducing Whisper,"Other existing approaches frequently use smaller, more closely paired audio-text training datasets,[^reference-1] [^reference-2][^reference-3] or use broad but unsupervised audio pretraining.[^reference-4][^reference-5][^reference-6] Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisper’s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.

About a third of Whisper’s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.

"
OpenAI_Research,https://openai.com/research/efficient-training-of-language-models-to-fill-in-the-middle,https://arxiv.org/abs/2207.14255,Efficient training of language models to fill in the middle,"We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.

"
OpenAI_Research,https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models,https://arxiv.org/abs/2207.14157,A hazard analysis framework for code synthesis large language models,"Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.

"
OpenAI_Research,https://openai.com/research/dall-e-2-pre-training-mitigations,,DALL·E 2 pre-training mitigations,"We observed that our internal predecessors to DALL·E 2 would sometimes reproduce training images verbatim. This behavior was undesirable, since we would like DALL·E 2 to create original, unique images by default and not just “stitch together” pieces of existing images. Additionally, reproducing training images verbatim can raise legal questions around copyright infringement, ownership, and privacy (if people’s photos were present in training data).

To better understand the issue of image regurgitation, we collected a dataset of prompts that frequently resulted in duplicated images. To do this, we used a trained model to sample images for 50,000 prompts from our training dataset, and sorted the samples by perceptual similarity to the corresponding training image. Finally, we inspected the top matches by hand, finding only a few hundred true duplicate pairs out of the 50k total prompts. Even though the regurgitation rate appeared to be less than 1%, we felt it was necessary to push the rate down to 0 for the reasons stated above.

When we studied our dataset of regurgitated images, we noticed two patterns. First, the images were almost all simple vector graphics, which were likely easy to memorize due to their low information content. Second, and more importantly, the images all had many near-duplicates in the training dataset. For example, there might be a vector graphic which looks like a clock showing the time 1 o’clock—but then we would discover a training sample containing the same clock showing 2 o’clock, and then 3 o’clock, etc. Once we realized this, we used a distributed nearest neighbor search to verify that, indeed, all of the regurgitated images had perceptually similar duplicates in the dataset. Other works have observed a similar phenomenon in large language models, finding that data duplication is strongly linked to memorization.

The above finding suggested that, if we deduplicated our dataset, we might solve the regurgitation problem. To achieve this, we planned to use a neural network to identify groups of images that looked similar, and then remove all but one image from each group.[^footnote-2]

However, this would require checking, for each image, whether it is a duplicate of every other image in the dataset. Since our whole dataset contains hundreds of millions of images, we would naively need to check hundreds of quadrillions of image pairs to find all the duplicates. While this is technically within reach, especially on a large compute cluster, we found a much more efficient alternative that works almost as well at a small fraction of the cost.Consider what happens if we cluster our dataset before performing deduplication. Since nearby samples often fall into the same cluster, most of the duplicate pairs would not cross cluster decision boundaries. We could then deduplicate samples within each cluster without checking for duplicates outside of the cluster, while only missing a small fraction of all duplicate pairs. This is much faster than the naive approach, since we no longer have to check every single pair of images.[^footnote-3]

When we tested this approach empirically on a small subset of our data, it found 85% of all duplicate pairs when usingK=1024 clusters.To improve the success rate of the above algorithm, we leveraged one key observation: when you cluster different random subsets of a dataset, the resulting cluster decision boundaries are often quite different. Therefore, if a duplicate pair crosses a cluster boundary for one clustering of the data, the same pair might fall inside a single cluster in a different clustering. The more clusterings you try, the more likely you are to discover a given duplicate pair. In practice, we settled on using five clusterings, which means that we search for duplicates of each image in the union of five different clusters. In practice, this found 97% of all duplicate pairs on a subset of our data.

Surprisingly, almost a quarter of our dataset was removed by deduplication. When we looked at the near-duplicate pairs that were found, many of them included meaningful changes. Recall the clock example from above: the dataset might include many images of the same clock at different times of day. While these images are likely to make the model memorize this particular clock’s appearance, they might also help the model learn to distinguish between times of day on a clock. Given how much data was removed, we were worried that removing images like this might have hurt the model’s performance.

To test the effect of deduplication on our models, we trained two models with identical hyperparameters: one on the full dataset, and one on the deduplicated version of the dataset. To compare the models, we used the same human evaluations we used to evaluate our original GLIDE model. Surprisingly, we found that human evaluators slightly preferred the model trained on deduplicated data, suggesting that the large amount of redundant images in the dataset was actually hurting performance.

Once we had a model trained on deduplicated data, we reran the regurgitation search we had previously done over 50k prompts from the training dataset. We found that the new model never regurgitated a training image when given the exact prompt for the image from the training dataset. To take this test another step further, we also performed a nearest neighbor search over the entire training dataset for each of the 50k generated images. This way, we thought we might catch the model regurgitating a different image than the one associated with a given prompt. Even with this more thorough check, we never found a case of image regurgitation.

"
OpenAI_Research,https://openai.com/research/vpt,https://arxiv.org/abs/2206.11795,Learning to play Minecraft with Video PreTraining,"The internet contains an enormous amount of publicly available videos that we can learn from. You can watch a person make a gorgeous presentation, a digital artist draw a beautiful sunset, and a Minecraft player build an intricate house. However, these videos only provide a record of what happened but not precisely how it was achieved, i.e., you will not know the exact sequence of mouse movements and keys pressed. If we would like to build large-scale foundation models in these domains as we’ve done in language with GPT, this lack of action labels poses a new challenge not present in the language domain, where “action labels” are simply the next words in a sentence.

In order to utilize the wealth of unlabeled video data available on the internet, we introduce a novel, yet simple, semi-supervised imitation learning method: Video PreTraining (VPT). We start by gathering a small dataset from contractors where we record not only their video, but also the actions they took, which in our case are keypresses and mouse movements. With this data we train an inverse dynamics model (IDM), which predicts the action being taken at each step in the video. Importantly, the IDM can use past and future information to guess the action at each step. This task is much easier and thus requires far less data than the behavioral cloning task of predicting actions given past video frames only, which requires inferring what the person wants to do and how to accomplish it. We can then use the trained IDM to label a much larger dataset of online videos and learn to act via behavioral cloning.

"
